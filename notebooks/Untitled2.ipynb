{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "381cf4e3-b0c5-4b53-b055-650c23a3bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from stackapi import StackAPI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "92090346-3b42-4b76-8ac8-c8f7f004bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"alpaca\", \"bloom\", \"cerebras-gpt\", \"chatglm\", \"chinchilla\", \"codex\", \"codegen\", \"codegx\", \"dolly-v2\", \"eleuther-pythia\", \"falcon\", \"fastchat-t5\", \"gal\", \"gpt-3\", \"gpt-3.5\", \"gpt-4\", \"gpt4all\", \"gpt-neox\", \"gpt-j\", \"koala\", \"llama\", \"mpt\", \"oasst-pythia\", \"opt\", \"palm\", \"palm-coder\", \"replit-code-v1\", \"stablelm-base-alpha\", \"stablelm-tuned-alpha\", \"starcoder-base\", \"starcoder\", \"vicuna\", \"llama-2\"]\n",
    "topics = [\"large-language-models\", \"llm\", \"word-embedding\", \"gpt\", \"intelligent-agent\"]\n",
    "tags = models + topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "74fc75c7-1942-45e1-bd07-cf3dd5a2a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:13<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "SITE = StackAPI('ai', key=\"GXK03oiqVULDxPyWtLmyNA((\")\n",
    "q_blocks = {}\n",
    "for tag in tqdm(tags):\n",
    "    questions = SITE.fetch('questions', tagged=tag, sort = 'votes', filter='withbody')\n",
    "    \n",
    "    for question in questions['items']:\n",
    "        content = BeautifulSoup(question['body']).get_text()\n",
    "        q_blocks[question['question_id']] = {\"link\": question['link'], \"metadata\": question, \"title\": question[\"title\"], \"content\": content, \"question_id\": question['question_id']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1ff3ec40-3406-40f8-af7e-e41a1d3c7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "SITE = StackAPI('genai', key=\"GXK03oiqVULDxPyWtLmyNA((\")\n",
    "questions = SITE.fetch('questions', sort = 'votes', filter='withbody')\n",
    "for question in questions['items']:\n",
    "    content = BeautifulSoup(question['body']).get_text()\n",
    "    q_blocks[question['question_id']] = {\"link\": question['link'], \"metadata\": question, \"title\": question[\"title\"], \"content\": content, \"question_id\": question['question_id']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d3f540b3-d7e3-4ead-97e4-932b02ebf09b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mq_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "q_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "752c87da-267c-4b45-8392-0da6a25afe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 443/443 [02:24<00:00,  3.06it/s]\n"
     ]
    }
   ],
   "source": [
    "for question_id, q_block in tqdm(q_blocks.items()):\n",
    "    answers = SITE.fetch('questions/{ids}/answers', ids = [question_id], sort = 'votes', filter='withbody')\n",
    "    \n",
    "    # Find top voted answers\n",
    "    if q_block.get(\"answers\") is None:\n",
    "        q_block['answers'] = []\n",
    "    for item in answers['items']:\n",
    "        body = BeautifulSoup(item['body']).get_text()\n",
    "        q_block['answers'].append({\"answer_id\": item['answer_id'], \"body\": body, \"score\": item['score'], \"is_accepted\": item[\"is_accepted\"]}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b61790f6-578c-4802-a92b-f1ce4ba7b750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: What is the &quot;temperature&quot; in the GPT models?\n",
      "\n",
      "content: What does the temperature parameter mean when talking about the GPT models?\n",
      "I know that a higher temperature value means more randomness, but I want to know how randomness is introduced.\n",
      "Does temperature mean we add noise to the weights/activations o...\n",
      "\n",
      "title: What language is the GPT-3 engine written in?\n",
      "\n",
      "content: I know that the API is python based, but what's the gpt-3 engine written in mostly? C? C++? I'm having some trouble finding this info.\n",
      "...\n",
      "\n",
      "title: Is GPT-4 based on GPT-3 or was it trained from the scratch?\n",
      "\n",
      "content: To me it looks like GPT-4 is based on GPT-3.\n",
      "On the other hand, there were rumors that training of GPT-3 was done with errors, but re-train was impossible due to the costs.\n",
      "...\n",
      "\n",
      "title: How to get GPT-3 to translate a specific word in a sentence?\n",
      "\n",
      "content: I just gave GPT-3 the following prompt (in the playground, using text-davinci-001 with default settings):\n",
      "What's the German word for \"can\" in the sentence \"The man removes the can.\"?\n",
      "\n",
      "The word \"can\" in this sentence is obviously a noun and not a verb...\n",
      "\n",
      "title: How can GPT-3 be used for designing electronic circuits from text descriptions?\n",
      "\n",
      "content: I was wondering if it is possible to use GPT-3 to translate text description of a circuit to any circuit design language program, which in turn can be used to make the circuit.\n",
      "If it is possible, what approach will you suggest?\n",
      "...\n",
      "\n",
      "title: What&#39;s the difference between GPT3.5 and InstructGPT?\n",
      "\n",
      "content: I read about the different model series in GPT3.5 here - https://platform.openai.com/docs/models/gpt-3-5\n",
      "At the beginning of the page, it mentions to look at https://platform.openai.com/docs/model-index-for-researchers to understand the difference be...\n",
      "\n",
      "title: If GPT-3 is trained on predicting the next token, how is it able to take commands?\n",
      "\n",
      "content: From my understanding, GPT-3 is trained on predicting the next token from a sequence of tokens. Given this, how is it able to take commands? For instance, in this example input, wouldn't the statistically most likely prediction be to insert a period ...\n",
      "\n",
      "title: Computation required for GPT model to choose likely word from n-options where n &lt; total vocabulary size\n",
      "\n",
      "content: Let’s imagine two different use cases for a LLM/GPT-3.\n",
      "\n",
      "Predicting the next most likely word in a sequence using all ~50k words in its dictionary (i.e. the standard method of prompting a LLM)\n",
      "Checking whether \"Word-1\" is more likely than \"Word-2\" to ...\n",
      "\n",
      "title: Fine-tune GPT-Neo with prompt and completion?\n",
      "\n",
      "content: I'm new to AI and machine learning.\n",
      "To fine-tune GPT-3, I understand that we need a set of training examples that each consist of a single input (\"prompt\") and its associated output (\"completion\").\n",
      "I have prepared a dataset with \"prompt\" and \"complet...\n",
      "\n",
      "title: Privacy implications of storing and transmitting GPT-3 embeddings?\n",
      "\n",
      "content: We are exploring implementing a feature where a user might enter \"which product has optional all wheel drive\" into a search input, which would be transformed to GPT-3 embeddings, and compared against a set of embeddings for product descriptions to re...\n",
      "\n",
      "title: How can I train GPT-3 (or similar) to search for answers my text file\n",
      "\n",
      "content: My company has large sets of standards that are quite dry to read through and find answers to questions.  How can I have an AI perform the search?\n",
      "The text files are too long to submit with every query, so it seems like I would need to train the mode...\n",
      "\n",
      "title: Can you train GPT-J to use a specific list of words and prioritise them?\n",
      "\n",
      "content: Can you train GPT-J to use a specific list of words and prioritise them? If so, please could you share how I would go about this?\n",
      "Say you're using GPT-J to write a story, you might wish to mention certain key terms more than others, or in a specific ...\n",
      "\n",
      "title: What is the meaning of &quot;Our current objective weights every token equally and lacks a notion of what is most important to predict&quot; in the GPT-3 paper?\n",
      "\n",
      "content: On page 34 of OpenAI's GPT-3, there is a sentence demonstrating the limitation of objective function:\n",
      "\n",
      "Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important.\n",
      "\n",
      "I am not sur...\n",
      "\n",
      "title: How is InstructGPT a fine-tuned version of GPT-3 and at the same time has fewer parameters than the original GPT3?\n",
      "\n",
      "content: I am reading the paper \"Training language models to follow instructions with human feedback\"\n",
      "It says:\n",
      "\n",
      "Our labelers provide demonstrations of the desired behavior on the input prompt distribution (see Section 3.2 for details on this distribution). We...\n",
      "\n",
      "title: Papers on Prompt Engineering\n",
      "\n",
      "content: I am into AI in general and NLP in particular. Besides, I have a background in philosophy, and the new LLMs like GPT-3 seem to have exciting capabilities. I want to study prompt engineering (for example, teaching the model to reason, etc.)\n",
      "Do you kno...\n",
      "\n",
      "title: Improving Contextual Consistency and Quality in OpenAI API Responses\n",
      "\n",
      "content: I'm currently wrestling with an issue using the OpenAI npm package for API calls. My setup involves prompts, user history, and questions with parameters like temperature, top_p, and frequency_penalty. Strangely, responses from the API lack the contex...\n",
      "\n",
      "title: Update OpenAI embedding based on own domain corpus\n",
      "\n",
      "content: I have a large domain corpus. Is there anyway to update the word/sentence/document embedding obtained from OpenAI embedding API based on my own domain corpus? There may be some words in my corpus that are close to each other but far from each other i...\n",
      "\n",
      "title: Extract multiple records from raw text using open AI API function calls\n",
      "\n",
      "content: Trying to extract records from pdf. The plan was to convert pdf data to raw text and use open AI api calls to extract the data in a desired format (csv). This is because the pdf data is very unstructured. While I do get records back in the output, I ...\n",
      "\n",
      "title: How is OpenAI embeddings obtained\n",
      "\n",
      "content: There is OpenAI embedding API https://platform.openai.com/docs/guides/embeddings. How is this embedding related to the GPT3.5 transformer model architecture? Is it the vectors learned from the input embeddings part, which is the summation of word emb...\n",
      "\n",
      "title: As a user, can I fine-tune end-to-end with GPT-3 if I want to add custom layers?\n",
      "\n",
      "content: I am looking at the OpenAI API, and it seems that the only way to get embeddings for a specific text is to use the embedding API. In other words, I can only use the embeddings as fixed features in my own model, as opposed to fine-tuning GPT-3 end-to-...\n",
      "\n",
      "title: Tool for detecting AI Generated code?\n",
      "\n",
      "content: Can you recommend any tools for detecting AI generated code? I am aware of AI-generated text detectors such as ZeroGPT and OpenAI-Classifier, but I am specifically interested in detecting AI-generated code. As of now, I have not been able to find any...\n",
      "\n",
      "title: Are there any transcripts of GPT-3 arguing that it is not conscious?\n",
      "\n",
      "content: There have been a lot of transcripts showing GPT-3 arguing that it is self-conscious. In response, it was pointed out that GPT-3 can argue anything and pretend to be anything, given appropriate leading questions. Has anyone made GPT-3 specifically ar...\n",
      "\n",
      "title: How is GPT 4 able to solve math?\n",
      "\n",
      "content: How can GPT 4 solve complex calculus and other math problems. I believe these problems require analytical reasoning and ability to compute numbers. Does it still use a LLM to complete this process or does it add on to this?\n",
      "Here is the link to the of...\n",
      "\n",
      "title: What researched-backed findings is there for prompting LLM’s / GPT-4 to give specific information or actionable plans?\n",
      "\n",
      "content: I have learned a bit recently about prompt strategies. For example, there was a paper about how just by saying “Let’s think step by step” can increase answer quality by like 40%. I have also come to appreciate that models like GPT4 sometimes actually...\n",
      "\n",
      "title: How do open source LLMs compare to GPT-4?\n",
      "\n",
      "content: I have heard some back and forth regarding open source LLMs like Llama.\n",
      "I have heard that on certain benchmarks they perform close, the same or better than GPT-4, but caveats that they tend to lack the diversity and range of GPT-4, and also fail to b...\n",
      "\n",
      "title: AI-driven tool for generating or finding short, context-aware jokes for online forum posts (GPT-4 not effective)\n",
      "\n",
      "content: I've tried using GPT-4 to generate jokes with various prompts for my online forum posts, but most of the generated jokes were unfunny. For example, I asked my AI for a joke, and it said \"your coding skills.\" I'm looking for an alternative tool or app...\n",
      "\n",
      "title: Contradiction in a single sentence - Is this an artifact of an external safety mechanisms?\n",
      "\n",
      "content: My system prompt contained \"Never apologize.\"\n",
      "An answer started with\n",
      "\n",
      "Entschuldigung, ich habe Ihre Anweisung, sich nicht zu entschuldigen, übersehen.\n",
      "\n",
      "which is quite well translated from German with\n",
      "\n",
      "Apologies, I missed your instruction not to apolo...\n",
      "\n",
      "title: Why LLMs and RNNs learn so fast during inference but, ironically, are so slow during training?\n",
      "\n",
      "content: Why LLMs learn so fast during inference, but, ironically, are so slow during training? That is, if you teach an AI a new concept in a prompt, it will learn and use the concept perfectly and flawless, through the whole prompt, after just one shot. Yet...\n",
      "\n",
      "title: How does the (decoder-only) transformer architecture work?\n",
      "\n",
      "content: How does the (decoder-only) transformer architecture work which is used in impressive models such as GPT-4?\n",
      "...\n",
      "\n",
      "title: LLM-like architecture capable of dynamically learning from its own output\n",
      "\n",
      "content: Language Learning Models (LLMs) have demonstrated remarkable capabilities in quick learning during inference. They can effectively grasp a concept from a single example and generate relevant outputs. However, a noticeable limitation of LLMs is their ...\n",
      "\n",
      "title: Why can&#39;t Lucene search be used to power LLM applications?\n",
      "\n",
      "content: w.r.t. LLM applications using the RAG (retriever-augmented-generation) architecture, people have started taken it for granted that it will be powered by a vector database. e.g., see this:\n",
      "\n",
      "The most important piece of the preprocessing pipeline, from ...\n",
      "\n",
      "title: Why do LLMs like GPT-3 or Bloom use Vanilla Transformer instead of long sequence variants like Transformer-XL?\n",
      "\n",
      "content: Is there any particular reason that the most recent and successful large language models like GPT-3 or Bloom utilize a vanilla Transformer architecture instead of an arguably superior long sequence architecture like, e.g. Transformer-XL, LongFormer, ...\n",
      "\n",
      "title: Why do LLMs need massive distributed training across nodes -- if the models fit in one GPU while batch decreases the variance of gradients?\n",
      "\n",
      "content: Why do large language models (LLMs) need massive distributed training across nodes -- if the models fit in one GPU and larger batch only decreases the variance of gradients?\n",
      "tldr: assuming for models that don't need sharding across nodes, why do we n...\n",
      "\n",
      "title: Is it possible to use LLMs for regression tasks?\n",
      "\n",
      "content: I want to use LLMs to predict edge weights in a graph based on attributes between two nodes. Is this even possible? If not, what would you recommend?\n",
      "I tried to look up uses of LLM in regression tasks, but haven't had much luck finding anything helpf...\n",
      "\n",
      "title: OpenAI: What is the difference between model &quot;gpt-3.5-turbo&quot; and &quot;gpt-3.5-turbo-0301&quot;?\n",
      "\n",
      "content: I have performed an API call to OpenAI's endpoint https://api.openai.com/v1/models .\n",
      "The endpoint lists the currently available engines, and provides basic information about each one such as the owner and availability.\n",
      "As a logged-in user, I get a JS...\n",
      "\n",
      "title: RAM Capacity of Mac Studio with M2 Ultra for inference of 65B LLM\n",
      "\n",
      "content: How much RAM would be needed on Mac Studio M2 Ultra for inferring from a 65B LLM model. There are three options: 64GB, 128GB and 192GB. If using Apple M2 Ultra with 24‑core CPU, 76‑core GPU, 32‑core Neural Engine with 192GB Unified Memory, how would ...\n",
      "\n",
      "title: What will happen if to train an LLM on mathematical exersises?\n",
      "\n",
      "content: What will happen if to train an LLM on taking integrals and solving equations? The process of mathematical education can be absolutely automated by a computer algebra system because the verification is easy.\n",
      "Is it possible that LLM will gan the abili...\n",
      "\n",
      "title: For an LLM model, how can I estimate its memory requirements based on storage usage?\n",
      "\n",
      "content: It is easy to see the amount of disk space consumed by an LLM model (downloaded from huggingface, for instance). Just go in the relevant directory and check the file sizes.\n",
      "How can I estimate the amount of GPU RAM required to run the model?\n",
      "For examp...\n",
      "\n",
      "title: SparseGPT code reproduction\n",
      "\n",
      "content: SparseGPT: https://arxiv.org/pdf/2301.00774.pdf\n",
      "Pruning on the super large language model based on the Transformers structure has achieved a high compression rate with a small loss of accuracy. Is there any related code reproduction or research?\n",
      "Some...\n",
      "\n",
      "title: What is considered the pre-fill, and what is considered the decoding phase in this process?\n",
      "\n",
      "content: I've seen conflicting information about this online so I'm looking for clarification. I'm dealing with the causal LLaMAF model specifically.\n",
      "I used to think that a sequence of tokens is generated in, and a sequence of probabilities for the next token...\n",
      "\n",
      "title: Surveys/Important papers in Explainability for LLM?\n",
      "\n",
      "content: I'm interested in the topic of Explainability for LLM: the attempts to find some higher understandable structures inside the LLMs or, to put it simply (though may be not completely correctly), the attempts to understand how LLMs \"think\".\n",
      "Would anybod...\n",
      "\n",
      "title: How is a parameter explosion prevented, when connecting a mutlihead attention layer with the dense layers in LLMs (speciafially, LLama)?\n",
      "\n",
      "content: I have had a look at LLamas model card, specifically the 7B parameter version:\n",
      "https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md\n",
      "which I assume is an encoder only transformer similar to this:\n",
      "\n",
      "But then I did some math.If the dimension...\n",
      "\n",
      "title: Using LLM to query specific databases - where can I find implementation examples\n",
      "\n",
      "content: I've been doing some research on how to leverage a LLM model to \"translate\" English into a sql query that's capable to returning the desired results (like a little Q/A bot). For instance, one would ask \"show me the top growth account in the last thre...\n",
      "\n",
      "title: For a transformer decoder, how exactly are K, Q, and V for each decoding step?\n",
      "\n",
      "content: For a transformer decoder, how exactly are K, Q, and V for each decoding step?\n",
      "Assume my input prompt is \"today is a\" (good day).\n",
      "At t= 0 (generation step 0):\n",
      "K, Q, and V are the projections of the sequence (\"today is a\")\n",
      "Then say the next token gene...\n",
      "\n",
      "title: Large Language Models vs Tabular Data\n",
      "\n",
      "content: Problem:\n",
      "Let's say we want to predict insurance frauds. Whenever we obtain an insurance claim, we are provided with a free-form description detailing the loss and a substantial amount of data on the claimant, presented in a tabular format.\n",
      "Questions:...\n",
      "\n",
      "title: Ignoring aspects of text embeddings, e.g. making the embedding topic-agnostic\n",
      "\n",
      "content: Imagine a large set of text embeddings (e.g. by OpenAI model), created on user inputs in a natural language interface (e.g. a semantic search app), which we want to cluster on some \"non-topic aspect\" of the text (e.g. clarity of query). As expected, ...\n",
      "\n",
      "title: In which process does filter and watermark take place?\n",
      "\n",
      "content: In LLM, in order to avoid discrimination and abuse, the author will add filter and watermark functionalities. Could you tell me in which process these functionalities take place? Is it in weights(pertaining), output layer(transformer), or fine-tuning...\n",
      "\n",
      "title: Does Embeddings and Vector Databases solve the need of having longer context windows?\n",
      "\n",
      "content: I am learning to use the OpenAI API to build LLM-based agents. I recently came across the concept of vector databases, which use embeddings to convert text into vectors and store them in a database for easy retrieval. This technique has been shown to...\n",
      "\n",
      "title: Why does a small model become huge when sharded and loaded into GPU on text-generation-inference?\n",
      "\n",
      "content: I'm currently trying out the new Llama 32k 7b model and I'm using text-generation-inference to run it.\n",
      "For added context, I have a 4xA100 40GB SXM machine that I'm running it on using docker.\n",
      "When I tried Falcon40b in the past, sharded across all 4 G...\n",
      "\n",
      "title: Create samples out of documents for Causal Language Modelling\n",
      "\n",
      "content: I want to create an input source for Causal Language model using Llama 2 model in hugging face. I have a set of documents which are scraped from a specific website and want to fine-tune on them. Each document its basically a different corner of this ...\n",
      "\n",
      "title: What causes my loss curve to consistently oscillate when training an LLM?\n",
      "\n",
      "content: \n",
      "Why is my loss curve consistently oscillating? Every 50 steps it jumps back up. I'm assumming there's a bug in my data, since I'm using this colab notebook that shows a proper train/loss at the bottom. The only thing I changed was the dataset used. ...\n",
      "\n",
      "title: What if in DPR (dense passage retrieval), the answer belongs to more than one passage?\n",
      "\n",
      "content: In the DPR paper\n",
      "the dataset is expected to be in this format D = {<qi, pi+, pi,1-, ... >}\n",
      "With only one positive passage, but it is possible that the question requires an answer that spans knowledge from more than one positive passage.\n",
      "Another issue...\n",
      "\n",
      "title: can I finetune a model on Azure for information extraction based on &quot;question&quot;, &quot;context&quot;, and &quot;answer&quot; training data?\n",
      "\n",
      "content: I am working on extracting certain fields from a large corpus.\n",
      "I was looking at finetuning an LLM on Azure for the task. I think finetuning is the right idea (as opposed to vector databases, or RAG), since the data was OCR'd from semi-structured form...\n",
      "\n",
      "title: Optimal Quantity of Training Data for Fine-Tuning an LLM: Is Bigger Always Better?\n",
      "\n",
      "content: I am currently working on fine-tuning an LLM for a specific task, and I am trying to determine the optimal size for my training dataset. Intuitively, one might think that the more data, the better. However, I am aware that in some contexts, this may ...\n",
      "\n",
      "title: Generative AI Use Case for Search Domain\n",
      "\n",
      "content: I am trying to think and research for use cases that can beneficial search experience using Generative AI. There are two things,\n",
      "\n",
      "The final search results that can be more relevant and personalised per user. This would take time and more effort - in ...\n",
      "\n",
      "title: What are the *non-cost-related* reasons RNN+Attention underperform Transformers?\n",
      "\n",
      "content: There are obvious trainability and performance challenges with RNNs, such as having to process in serial and BPTT. But let's say we magically had an \"optimal\" set of weights for the RNN + Attention, as well as for the Transformer. Assume as many thin...\n",
      "\n",
      "title: Does anyone know about a reference where someone has aggregated the cost optimization strategies for deploying LLMs?\n",
      "\n",
      "content: I am looking for a source where someone has mentioned the most commonly used strategies and optimisations to deploy LLMs on consumer hardware. I have read about layer offloading, quantisation methods using libraries like GPTQ and GGML, model pruning ...\n",
      "\n",
      "title: Fine-Tune Llama on main and auxiliary task\n",
      "\n",
      "content: I am trying to fine-tune Llama model on two task at the same time, using hugging face library:\n",
      "Main task: Causal language model like the model was initially trained for\n",
      "A classification task based on the whole input sequence (recommend an article). F...\n",
      "\n",
      "title: What role does data quality plays in the LLM scaling laws?\n",
      "\n",
      "content: DeepMind released the Training Compute-Optimal Large Language Models paper in 2022 which describe some scaling laws for LLMs. As far as I understand this is the most accredited reference to estimate the optimal relation between dataset size, compute ...\n",
      "\n",
      "title: A technique to show what tokens are relatively predicted by an LLM\n",
      "\n",
      "content: I’m picturing a technique where you can see what an LLM is likely to respond with, which updates in real time.\n",
      "It’s a bit trippy, but it’s like GitHub Copilot, in that there is predicted text while you type, but it’s predicting what an LLM would say ...\n",
      "\n",
      "title: What is better: train a model from scratch on your own data vs. fine-tune pretrained model?\n",
      "\n",
      "content: Problem: I am interested in building a Q&A engine on top of my private data. I am only interested in asking questions related to my data.\n",
      "Options:\n",
      "\n",
      "I train a model from scratch on my own data\n",
      "I pick a pretrained large language model and fine-tune it ...\n",
      "\n",
      "title: Relation between Batch Size and Micro Batch Size\n",
      "\n",
      "content: In distributed training of large models (pipeline parallelism), a mini batch of training samples is divided into n-micro batches. Each device performs forward and backward passes for a micro batch.\n",
      "What is the relation between Batch Size, Micro Batch...\n",
      "\n",
      "title: Is it possible to generate new text matching a vector embedding with an LLM\n",
      "\n",
      "content: I'm interested in generating variations of text using an LLM - is it possible to take a text embedding, move it in different directions in vector space, and generate new text from the resulting vectors?\n",
      "...\n",
      "\n",
      "title: How does vocabulary size affect quality?\n",
      "\n",
      "content: I think the vocabulary size in LLMs makes two trade-offs:\n",
      "\n",
      "The bigger tokens you have, the less frequent they will be.\n",
      "The more tokens you have, the more parameters you dedicate to input and output.\n",
      "\n",
      "I'm looking for a chart of the effect of the token...\n",
      "\n",
      "title: Why do LLaMa and its variants have non-“round” numbers of parameters?\n",
      "\n",
      "content: LLaMa was released in several sizes, with 7B, 13B, 33B, and 65B parameters. These values look a little weird, because they are very close to powers of two (8, 16, 32, 64) that would be more conventionally considered “round numbers” in software. Why w...\n",
      "\n",
      "title: Improving embedding similarity search of aggregated embeddings\n",
      "\n",
      "content: I am building an author suggestion tool that proposes authors writing about similar topics as a given input text. I want to use embeddings for this. The way I currently do it is to store embeddings of many articles of many different authors in a vect...\n",
      "\n",
      "title: fine-tune nanoGPT for instructions\n",
      "\n",
      "content: I've been playing around with nanoGPT, and recently I decided I wanted to fine-tune it using the dolly instruction set. This data set consists of roughly 15k examples and each example has the following features: question-type, context, instruction, a...\n",
      "\n",
      "title: What&#39;s the right approach to &quot;teach&quot; a chatbot about a big specific data set: training it or feeding it the input once?\n",
      "\n",
      "content: I would like to build a chatbot that I can talk to about any specific data set I provide and I have a theoretical question.\n",
      "Let's say, I want to discuss French cuisine. This could involve providing the model with information like culinary history, bi...\n",
      "\n",
      "title: Do LLM or Machine Learning models with a large number of classes employ standard cross entropy?\n",
      "\n",
      "content: In language modeling, the next token is predicted from the entire vocabulary during inferenc. However, when training models with large vocabularies or many different classes, standard cross entropy can be suboptimal due to its computational expense a...\n",
      "\n",
      "title: Could it be probable to quantify or measure the IQ of a super-intelligent machine like GPT?\n",
      "\n",
      "content: In the age of artificial intelligence, super-intelligent machines like GPT have become a reality, leading to the question of how to quantify or measure their intelligence. While IQ tests are widely used to measure human intelligence, could a similar ...\n",
      "\n",
      "title: Open access Adept-like dataset? (LLM-to-computer-input)\n",
      "\n",
      "content: Here's a demo for Adept ACT-1 for Transformers. I don't doubt that one could create a demo video using zero-shot; actually I tested just now and the basic chat.openai.com interface was able to do some web browsing stuff on the first try if I just pro...\n",
      "\n",
      "title: What is the difference between latent and embedding spaces?\n",
      "\n",
      "content: In general, the word \"latent\" means \"hidden\" and \"to embed\" means \"to incorporate\". In machine learning, the expressions \"hidden (or latent) space\" and \"embedding space\" occur in several contexts. More specifically, an embedding can refer to a vector...\n",
      "\n",
      "title: What kind of word embedding is used in the original transformer?\n",
      "\n",
      "content: I am currently trying to understand transformers.\n",
      "To start, I read Attention Is All You Need and also this tutorial.\n",
      "What makes me wonder is the word embedding used in the model. Is word2vec or GloVe being used? Are the word embeddings trained from s...\n",
      "\n",
      "title: &quot;Attention is all you need&quot; paper : How are the Q, K, V values calculated?\n",
      "\n",
      "content: The seminal Attention is all you need paper introduces Transformers and implements the attention mecanism with \"queries, keys, values\", in an analogy to a retrieval system.\n",
      "I understand the whole process of multi-head attention and such (i.e., what i...\n",
      "\n",
      "title: What is the intuition behind how word embeddings bring information to a neural network?\n",
      "\n",
      "content: How is it that a word embedding layer (say word2vec) brings more insights to the neural network compared to a simple one-hot encoded layer?\n",
      "I understand how the word embedding carries some semantic meaning, but it seems that this information would ge...\n",
      "\n",
      "title: Is an embedding a representation of a word or its meaning?\n",
      "\n",
      "content: What does the term \"embedding\" actually mean?\n",
      "An embedding is a vector, but is that vector a representation of a word or its meaning? Literature loosely uses the word for both purposes. Which one is actually correct?\n",
      "Or is there anything like: A word...\n",
      "\n",
      "title: What is the difference between a language model and a word embedding?\n",
      "\n",
      "content: I am self-studying applications of deep learning on the NLP and machine translation.\n",
      "I am confused about the concepts of \"Language Model\", \"Word Embedding\", \"BLEU Score\".\n",
      "It appears to me that a language model is a way to predict the next word given ...\n",
      "\n",
      "title: Does summing up word vectors destroy their meaning?\n",
      "\n",
      "content: For example, I have a paragraph that I want to classify in a binary manner. But because the inputs have to have a fixed length, I need to ensure that every paragraph is represented by a uniform quantity.\n",
      "One thing I've done is taken every word in the...\n",
      "\n",
      "title: Do individual dimensions in vector space have meaning?\n",
      "\n",
      "content: Word2vec assigns an N-dimensional vector to given words (which can be considered a form of dimensionality reduction). \n",
      "It turns out that, at least with a number of canonical examples, vector arithmetic seems to work intuitively. For example \"king + w...\n",
      "\n",
      "title: Will BERT embedding be always same for a given document when used as a feature extractor\n",
      "\n",
      "content: When we use BERT embeddings for a classification task, would we get different embeddings every time we pass the same text through the BERT architecture? If yes, is it the right way to use the embeddings as features? Ideally, while using any feature e...\n",
      "\n",
      "title: Why does all of NLP literature use noise contrastive estimation loss for negative sampling instead of sampled softmax loss?\n",
      "\n",
      "content: A sampled softmax function is like a regular softmax but randomly selects a given number of 'negative' samples.\n",
      "This is difference than NCE Loss, which doesn't use a softmax at all, it uses a logistic binary classifier for the context/labels. In NLP,...\n",
      "\n",
      "title: How embeddings learned from one model can be used in another?\n",
      "\n",
      "content: In the website the following explanation is provided about Embedding layer:\n",
      "\n",
      "The Embedding layer is initialized with random weights and will learn\n",
      "an embedding for all of the words in the training dataset.\n",
      "It is a flexible layer that can be used in a...\n",
      "\n",
      "title: Why do we multipy context_size with embedding_dim? (PyTorch)\n",
      "\n",
      "content: I've been using Tensorflow and just started learning PyTorch. I was following the tutorial: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py\n",
      "Where we try to create an n-gram la...\n",
      "\n",
      "title: When to convert data to word embeddings in NLP\n",
      "\n",
      "content: When training a network using word embeddings, it is standard to add an embedding layer to first convert the input vector to the embeddings.\n",
      "However, assuming the embeddings are pre-trained and frozen, there is another option. We could simply preproc...\n",
      "\n",
      "title: Would initializing transformers with pre-trained word embedding speed up the training of transformers?\n",
      "\n",
      "content: I read the answers for that question What kind of word embedding is used in the original transformer?. It says that transforms like bert start the first word embedding layer with random values.\n",
      "Initializing the first word embedding layer in transform...\n",
      "\n",
      "title: Can ELMO embeddings be used to find the n most similar sentences?\n",
      "\n",
      "content: Assume I have a list of sentences, which is just a list of strings. I need a way of comparing some input string against those sentences to find the most similar. Can ELMO embeddings be used to train a model that can give you the $n$ most similar sent...\n",
      "\n",
      "title: What is $USV^T$ in the context of word embeddings?\n",
      "\n",
      "content: Here is an excerpt from the notes of the first lecture of the course CS224n: Natural Language Processing with Deep Learning.\n",
      "\n",
      "3 SVD Based Methods\n",
      "For this class of methods to find word embeddings (otherwise known\n",
      "as word vectors), we first loop over ...\n",
      "\n",
      "title: How can I create an embedding layer to convert words to a vector space from scratch?\n",
      "\n",
      "content: For an upcoming project, I am trying to build a neural network for classifying text from scratch, without the use of libraries. This requires an embedding layer, or a way to convert words to some vector representation. I understand the gist, but I ca...\n",
      "\n",
      "title: What should the dimension of the input be for text summarization?\n",
      "\n",
      "content: I am trying to build a model for extractive text summarization using keras sequential layers. I am having a hard time trying to understand how to input my x data. Should it be an array of documents with each document containing an array of sentences?...\n",
      "\n",
      "title: How should the output layer of an LSTM be when the output are word embeddings?\n",
      "\n",
      "content: I'm having trouble grasping how to output word embeddings from an LSTM model. I'm seeing many examples using a softmax activation function on the output, but for that I would need to output one hot vectors as long as the vocabulary (which is too long...\n",
      "\n",
      "title: Is there a good book or paper on word embeddings?\n",
      "\n",
      "content: Is there a good and modern book that focuses on word embeddings and their applications? It would also be ok to provide the name of a paper that provides a good overview of word embeddings.\n",
      "...\n",
      "\n",
      "title: Why embedding layer is used in the character-level Natural Language Processing models\n",
      "\n",
      "content: Problem Background\n",
      "I am working with a problem, which requires a character-level, deep learning model. Previously I was working with word-level deep NLP (Natural Language Processing) models, and in these models almost always embedding encoding was us...\n",
      "\n",
      "title: Adding BERT embeddings in LSTM embedding layer\n",
      "\n",
      "content: I am planning to use BERT embeddings in the LSTM embedding layer instead of the usual Word2vec/Glove Embeddings. What are the possible ways to do that? \n",
      "...\n",
      "\n",
      "title: Do we have cross-language vector space for word embedding?\n",
      "\n",
      "content: Do we have cross-language vector space for word embedding?\n",
      "When measure similarity for apple/Pomme/mela/Lacus/苹果/りんご, they should be the same\n",
      "If would be great if there's available internet service of neuron network which already be trained by multip...\n",
      "\n",
      "title: Why are BERT embeddings interpreted as representations of the corresponding words?\n",
      "\n",
      "content: It's often assumed in literature that BERT embeddings are contextual representations of the corresponding word. That is, if the 5th word is \"cold\", then the 5th BERT embedding is a representation of that word, using context to disambiguate the word (...\n",
      "\n",
      "title: How does FastText support online learning?\n",
      "\n",
      "content: I'm using FastText pre-trained-embedding for tackling a classification task, but I saw it supports also online training (incremental training) for adding domain-specific corpus.\n",
      "How does it work? \n",
      "As far as I know, starting from the \"model.bin\" file ...\n",
      "\n",
      "title: What do the vectors of the center and outside word look like in word2vec?\n",
      "\n",
      "content: In word2vec, the task is to learn to predict which words are most likely to be near each other in some long corpus of text. For each word $c$ in the corpus, the model outputs the probability distribution $P(O=o|C=c)$ of how likely each other word $o$...\n",
      "\n",
      "title: Can One-Hot Vectors be used as Inputs for Recurrent Neural Networks?\n",
      "\n",
      "content: When using an RNN to encode a sentence, one normally takes each word, passes it through an embedding layer, and then uses the dense embedding as the input into the RNN.\n",
      "Lets say instead of using dense embeddings, I used a one-hot representation for e...\n",
      "\n",
      "title: Doubt on formulating cost function for GloVe\n",
      "\n",
      "content: I'm reading the notes here and have a doubt on page 2 (\"Least squares objective\" section). The probability of a word $j$ occurring in the context of word $i$ is $$Q_{ij}=\\frac{\\exp(u_j^Tv_i)}{\\sum_{w=1}^W\\exp(u_w^Tv_i)}$$\n",
      "The notes read:\n",
      "\n",
      "Training pr...\n",
      "\n",
      "title: What is the intuition behind position-encoding?\n",
      "\n",
      "content: It is clear that word positions are essential for the meaning of a sentence, and so are essential when feeding a sentence (= sequence of words) as a matrix of word embedding vectors into a transformer. I also have understood roughly how positions are...\n",
      "\n",
      "title: What information does the word embedding in Transformers will encode about the word when analysed outside of the model?\n",
      "\n",
      "content: Word2vec and similar architectures create word embedding vectors as a byproduct from a supervised learning task, where they need to predict the correct context word. Consequently, the inner representation of words inside this network will preserve so...\n",
      "\n",
      "title: Is categorical encoding a type of word embedding?\n",
      "\n",
      "content: Word embedding refers to the techniques in which a word is represented by a vector. There are also integer encoding and one-hot encoding, which I will collectively call categorical encoding.\n",
      "I can see no fundamental difference between the categorical...\n",
      "\n",
      "title: Why is embedding important in NLP, and how does autoencoder work?\n",
      "\n",
      "content: People say embedding is necessary in NLP because if using just the word indices, the efficiency is not high as similar words are supposed to be related to each other. However, I still don't truly get it why.\n",
      "The subword-based embedding (aka syllable-...\n",
      "\n",
      "title: How is the word embedding represented in the paper &quot;Recurrent neural network based language model&quot;?\n",
      "\n",
      "content: I'm reading \"Recurrent neural network based language model\" of Mikolov et al. (2010). Although the article is straight forward, I'm not sure how word embedding $w(t)$ is obtained:\n",
      "\n",
      "The reason I wonder is that in the classic \"A Neural Probabilistic La...\n",
      "\n",
      "title: What&#39;s computationally more efficient between bag-of-words representation and bag-of-ngrams representation, with special regard to words order?\n",
      "\n",
      "content: I cannot figure out what is more computationally efficient between the two representations mentioned in my question in terms of training time and the amount of data required. Especially, when it comes to considering the order of the words in a senten...\n",
      "\n",
      "title: What exactly is embedding layer used in RNN encoders?\n",
      "\n",
      "content: I am reading about RNN encoders. I came across the following line from this code. And I am facing difficulty in understanding the theoretical details regarding it.\n",
      "emb = self.drop(self.encoder(input))\n",
      "\n",
      "The input is a tensor of shape $[32, 100]$. Here...\n",
      "\n",
      "title: Should I need to use BERT embeddings while tokenizing using BERT tokenizer?\n",
      "\n",
      "content: I am new to BERT and NLP and I am a little confused with tokenization and word embedding.\n",
      "My doubt is if I use the BertTokenizer for tokenizing a sentence then do I have to compulsorily use BertEmbedding for generating its corresponding word vectors ...\n",
      "\n",
      "title: How is dropout applied to the embedding layer&#39;s output?\n",
      "\n",
      "content: model = tf.keras.Sequential([\n",
      "    tf.keras.layers.Embedding(1000, 16, input_length=20), \n",
      "    tf.keras.layers.Dropout(0.2),                           # <- How does the dropout work?\n",
      "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
      "    tf.keras.la...\n",
      "\n",
      "title: Using word embedding to extend words for searching POI names\n",
      "\n",
      "content: I am developing my own mobile app related to digital map. One of the functions is searching POIs (points of interest) in the map according to relevance between user query and POI name.\n",
      "Besides the POIs whose names contain exact words in the query, th...\n",
      "\n",
      "title: How can we create a vector space where word spelling and pronunciation can be easily compared?\n",
      "\n",
      "content: In natural language processing, we can convert words to vectors (or word embeddings). In this vector space, we can measure the similarity between these word embeddings.\n",
      "How can we create a vector space where word spelling and pronunciation can be eas...\n",
      "\n",
      "title: Skip-Gram Model Training\n",
      "\n",
      "content: Suppose we want to predict context words $w_{i-h}, \\dots, w_{i+h}$ given a target word $w_i$ for a window size $h$ around the target word $w_i$. We can represent this as: $$p(w_{i-h}, \\dots, w_{i+h}|w_i) = \\prod_{-h \\leq k \\leq h, \\ k \\neq 0} p(w_{i+...\n",
      "\n",
      "title: Is the input embedding split along the embedding dimension so that every head of the multi-head-attention module just gets a part of the input data?\n",
      "\n",
      "content: So I found two contradictory explanations of the MHA (multi-head-self-attention-module):\n",
      "In the first approach, the input embedding (= the input matrix) is split along the embedding dimension and all heads are given a subset of the dimensions/feature...\n",
      "\n",
      "title: Given embedding vector A and vector B, how to find top k embedding vectors such that they are similar to vector A and dissimilar to vector B\n",
      "\n",
      "content: Which would be better approach for getting top k embedding vectors such that they are similar to embedding vector A and dissimilar to vector B.\n",
      "Approach 1:\n",
      "\n",
      "calculate f(V) = cosine_similarity(A,V) - cosine_similarity(B,V) for each vector V\n",
      "sort vecto...\n",
      "\n",
      "title: General approaches in text encoding and labelling for NLP\n",
      "\n",
      "content: What are the approaches of encoding text data? I would be glad to hear some summarization from experienced persons.\n",
      "And are there any solutions accepting words outside the vocabulary and including them to the results (online machine learning)?\n",
      "Data i...\n",
      "\n",
      "title: Why can&#39;t recurrent neural network handle large corpus for obtaining embeddings?\n",
      "\n",
      "content: In order to learn the embeddings, we need to train a model based on some objective function. The model can be an RNN and the objective function can be the likelihood. We learn the embeddings by calculating the likelihood, and the embeddings are consi...\n",
      "\n",
      "title: How to generate text descriptions from keywords?\n",
      "\n",
      "content: I wonder how can I build a neural network which will generate text description from given tag/tags. Let's assume I have created such data structure:\n",
      "{\n",
      " 'tag1': ['some description1', 'some description2', 'some description3'],\n",
      " 'tag2': ['some descripti...\n",
      "\n",
      "title: Given the word embeddings, how do I create the sentence composed of the corresponding words?\n",
      "\n",
      "content: I have done some reading. I want to implement an LSTM with pre-trained word embeddings (I also have plans to create my word embeddings, but let's cross that bridge when we come to it).\n",
      "In any given sentence, you don't usually need to have all the wor...\n",
      "\n",
      "title: What does the outputlayer of BERT for masked language modelling look like?\n",
      "\n",
      "content: In the tutorial BERT – State of the Art Language Model for NLP the masked language modeling pre-training steps are described as follows:\n",
      "\n",
      "In technical terms, the prediction of the output words requires:\n",
      "\n",
      "Adding a classification layer on top of the en...\n",
      "\n",
      "title: Bechmark models for Text Classification / Sentiment Classification\n",
      "\n",
      "content: I am currently working on a novel application in NLP where I try to classify empathic and non-empathic texts. I would like to compare the performance of my model to some benchmark models. As I am working with models based on Word2Vec embeddings, the ...\n",
      "\n",
      "title: How homographs is an NLP task can be treated?\n",
      "\n",
      "content: \n",
      "A homograph - is a word that shares  the same written form as another word but has a different meaning.\n",
      "\n",
      "They can be even different parts of speech. For example:\n",
      "\n",
      "close(verb) - close(adverb)\n",
      "lead(verb) - lead(noun)\n",
      "wind(noun) - wind(verb)\n",
      "\n",
      "And there...\n",
      "\n",
      "title: Is it good practice to save NLP Transformer based pre-trained models into file system in production environment\n",
      "\n",
      "content: I have developed a multi label classifier using BERT. I'm leveraging Hugging Face Pytorch implementation for transformers.\n",
      "I have saved the pretrained model into the file directory in dev environment. Now, the application is ready to be moved the pro...\n",
      "\n",
      "title: Creating Text Features using word2vec\n",
      "\n",
      "content: My task is to classify some texts. I have used word2vec to represent text words and I pass them to an LSTM as input. Taking into account that texts do not contain the same number of words, is it a good idea to create text features of fixed dimension ...\n",
      "\n",
      "title: Why I have a different number of terms in word2vec and TFIDF? How I can fix it?\n",
      "\n",
      "content: I need multiply the weigths of terms in TFIDF matrix by the word-embeddings of word2vec matrix but I can't do it because each matrix have a different number of terms. I am using the same corpus for get both matrix, I don't know why each matrix have a...\n",
      "\n",
      "title: Is there a way to parallelize GloVe cooccur function?\n",
      "\n",
      "content: I would like to create a GloVe word embedding on a very large corpus (trillions of words). However, creating the co-occurence matrix with the GloVe cooccur script is projected to take weeks. Is there any way to parallelize the process of creating a c...\n",
      "\n",
      "title: Reference request: one-hot encoding outperforming random orthogonal encoding\n",
      "\n",
      "content: I experimented with a CNN operating on texts encoded as sequences of character vectors, where characters are encoded as one-hot vectors in one embedding and as random unit length pairwise orthogonal vectors (orthogonal matrix) in another. While geome...\n",
      "\n",
      "title: How do Transformers compute the words embeddings at inference time since the embeddings are dynamic?\n",
      "\n",
      "content: In Word2Vec, the embeddings don't depend on the context.\n",
      "But in Transformers, the embeddings depend on the context.\n",
      "So how are the words' embeddings set at inference time?\n",
      "...\n",
      "\n",
      "title: Book(s) for text embedding\n",
      "\n",
      "content: Text here refers to either character or word or sentence.\n",
      "Is there any recent textbook that encompasses from classical methods to the modern techniques for embedding texts?\n",
      "If a single textbook is unavailable then please recommend a list of books cov...\n",
      "\n",
      "title: Do different ngrams share embedding in Fasttext?\n",
      "\n",
      "content: As per Section 3.2 in the original paper on Fasttext, the authors state:\n",
      "\n",
      "In order to bound the memory requirements of our model, we use a\n",
      "hashing function that maps n-grams to integers in 1 to K\n",
      "\n",
      "Does this mean the model computes only K embeddings r...\n",
      "\n",
      "title: How to add a pretrained model to my layers to get embeddings?\n",
      "\n",
      "content: I want to use a pretrained model found in [BERT Embeddings] https://github.com/UKPLab/sentence-transformers and I want to add a layer to get the sentence embeddings from the model and pass on to the next layer. How do I approach this?\n",
      "The inputs woul...\n",
      "\n",
      "title: How could I compute in real-time the similarity between tickets?\n",
      "\n",
      "content: I'm dealing with a \"ticket similarity task\".\n",
      "Every time new tickets arrive at the help desk (customer service), I need to compare them and find out about similar ones.\n",
      "In this way, once the operator responds to a ticket, at the same time he can solve...\n",
      "\n",
      "title: How word2vec de-embeds the special names in language models which output text\n",
      "\n",
      "content: I am new to nlp field. I have some questions about word2vec embeddings. as I know they have a fixed size dictionary of vocabs. so definitely there some words which is not in that predefined dictionary of vocab, for i.e. we may get an special name (li...\n",
      "\n",
      "title: Are the dimensions in embedding vectors ordered (similar to PCA)?\n",
      "\n",
      "content: I am getting started with the vector embeddings. I have a general question about the embedding vectors generated by popular algorithms.\n",
      "In PCA, usually, there is an implicit order of importance in the dimensions, with the most informative (by largest...\n",
      "\n",
      "title: Why skip gram doesnt just use the probabilities as the encoded vector?\n",
      "\n",
      "content: I am very confused but this is what's on my head now:\n",
      "\n",
      "The skip-gram algorithm just multiplies hot-encoded-words with a weights-matrix,\n",
      "and since the word is hot encoded it is just multiplying a row of the matrix\n",
      "And so this matrix W will end up bein...\n",
      "\n",
      "title: Word Embeddings but for Logical reasoning in custom knowledge GPT-3.5 bot\n",
      "\n",
      "content: So I have created a chatbot using GPT-3.5 turbo. I have a vector database that holds vector embeddings of brands, ratings, commission percentages, outlets, tags, etc. Here's how the system is designed.\n",
      "\n",
      "User Asks a question.\n",
      "The question is converted...\n",
      "\n",
      "title: How are the softmax normalized weights in ELMo actually learned and computed?\n",
      "\n",
      "content: I was reading the ELMo paper, and they speak of task-specific representations of words (or tokens generally speaking) by using the following equation: $ELMo_{k}^{task} = \\gamma^{task}\\sum_{0}^{L}{s_{j}^{task}h_{k,j}^{LM}}$ where $ELMo_{k}^{task}$ is ...\n",
      "\n",
      "title: Does attention in transformers encode any information from positional embeddings?\n",
      "\n",
      "content: I know we account for positional embeddings before feeding into attention layers, but would we be able to say that the Q and K dot products intrinsically encode relative positions\n",
      "...\n",
      "\n",
      "title: I created joint embeddings by training a NN with contrastive loss. Why are my resulting embeddings so sparse?\n",
      "\n",
      "content: Using BERT and Word2Vec word embeddings as two inputs, I trained a small neural network using Contrastive loss. The NN looks like this:\n",
      "Net(\n",
      "(fcin1): Linear(in_features=768, out_features=500, bias=True)\n",
      "(fcin2): Linear(in_features=300, out_features=5...\n",
      "\n",
      "title: How to Create a Fixed-Length, Binary, Sequence of Tokens Embedding?\n",
      "\n",
      "content: Say I have ten classes represented by 1 x n_classes vector of binary.\n",
      "My goal is to embed a sequence of 1xN binary so that I can also model the class-co occurrence.\n",
      "Say, class A, B, D are present and represented as [1, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "The ...\n",
      "\n",
      "title: Do Transformers and LSTMs use the same word embeddings (except for the position encoding, which only Transformers use)?\n",
      "\n",
      "content: In NLP, the first step is always to \"convert\" the given words of a sentence into representation vectors (word embeddings).\n",
      "As I understand it, in the case of transformers, the words/vocabulary are one-hot encoded and then multiplied by a weight matri...\n",
      "\n",
      "title: Is BERT capable of producing semantically close word embeddings for synonyms?\n",
      "\n",
      "content: I am currently working on my undergraduate thesis on matching job descriptions to resumes based on the contents of both. Recently, I came across the following statement by Schmitt et al., 2016: \"[...] [Recruiters] and\n",
      "job seekers [...] do not seem to...\n",
      "\n",
      "title: How is the training comlexity of NNLM word2vec calculated?\n",
      "\n",
      "content: I was reading this paper on word2vec, and came around the following description of a feedforward NNLM:\n",
      "\n",
      "It consists of input, projection, hidden and output layers. At the input layer, N previous words are encoded using 1-of-V coding, where V is size ...\n",
      "\n",
      "title: What are the types of inputs used for RNN in literature given sentences?\n",
      "\n",
      "content: Suppose there are $m$ sentences in a text file and the number of distinct words is equal to $n$. The goal is to get word embeddings using RNN.\n",
      "We know that it is impossible to pass any word, which is in text format, as an input to RNN. We need to con...\n",
      "\n",
      "title: Are the Word2Vec encoded embeddings available online?\n",
      "\n",
      "content: I am trying to do an NLP project and was wondering if there is anywhere online where the Word2Vec embeddings are stored (the actual n-dimmensional vectors).\n",
      "I want to search up a word and see what its encoding is. I have tried looking but couldn't fi...\n",
      "\n",
      "title: NLP: Are hashtags tokenised?\n",
      "\n",
      "content: I am exploring a potential NLP project. I was wondering what generally is done with the hashtags words (e.g. #hello). Are those words ignored? is the # removed and the word tokenised? Is it tokenised with the #?\n",
      "...\n",
      "\n",
      "title: Is there a reason why no one combines word embeddings with the median?\n",
      "\n",
      "content: Could you combine word embeddings with the median per dimension to get a document embedding? In my case I have a huge amount of words to build one document, which in turn should describe a topic. I feel like using the median is the right thing to do,...\n",
      "\n",
      "title: Why would adding all the possible embeddings be &quot;worse&quot; than using 1D-convolutions?\n",
      "\n",
      "content: Suppose we are using word2vec and have embeddings of individual words $w_1, \\dots, w_{10}$. Let's say we wanted to analyze $2$ grams or $3$ grams.\n",
      "Why would adding all the possible embeddings, $\\binom{10}{2}$ or $\\binom{10}{3}$, be \"worse\" than using...\n",
      "\n",
      "title: What exactly are the &quot;parameters&quot; in GPT-3&#39;s 175 billion parameters and how are they chosen/generated?\n",
      "\n",
      "content: When I studied neural networks, parameters were learning rate, batch size etc. But even GPT3's ArXiv paper does not mention anything about what exactly the parameters are, but gives a small hint that they might just be sentences.\n",
      "\n",
      "Even tutorial sites...\n",
      "\n",
      "title: Why does GPT-2 Exclude the Transformer Encoder?\n",
      "\n",
      "content: After looking into transformers, BERT, and GPT-2, from what I understand, GPT-2 essentially uses only the decoder part of the original transformer architecture and uses masked self-attention that can only look at prior tokens.\n",
      "Why does GPT-2 not requ...\n",
      "\n",
      "title: What is the difference between the positional encoding techniques of the Transformer and GPT?\n",
      "\n",
      "content: I know the original Transformer and the GPT (1-3) use two slightly different positional encoding techniques.\n",
      "More specifically, in GPT they say positional encoding is learned. What does that mean? OpenAI's papers don't go into detail very much.\n",
      "How d...\n",
      "\n",
      "title: How do we know if GPT-2 is a better language model?\n",
      "\n",
      "content: You may have heard of GPT2, a new language model. It has recently attracted attention from the general public as the foundation that published the paper, OpenAI, ironically refused to share the whole model fearing dangerous implications. Along the pa...\n",
      "\n",
      "title: Is the Mask Needed for Masked Self-Attention During Inference with GPT-2\n",
      "\n",
      "content: My understanding is that masked self-attention is necessary during training of GPT-2, as otherwise it would be able to directly see the correct next output at each iteration. My question is whether the attention mask is necessary, or even possible, d...\n",
      "\n",
      "title: How is the next token predicted in transformers?\n",
      "\n",
      "content: In the transformer (or GPT/decoder only), at the end of the decoder blocks but before the final linear layer you have X vectors (for the X tokens at the input of the decoder). We then want to compute the probabilities for the next token of the sequen...\n",
      "\n",
      "title: Where can I find pre-trained language models in English and German?\n",
      "\n",
      "content: Where can I find (more) pre-trained language models? I am especially interested in neural network-based models for English and German.\n",
      "I am aware only of Language Model on One Billion Word Benchmark and TF-LM: TensorFlow-based Language Modeling Toolk...\n",
      "\n",
      "title: What sort of computer would be necessary to run queries on a LLM?\n",
      "\n",
      "content: I've heard that to train a model like GPT 4.0 you need a very powerful computer and ~$10M of computing power, but once you've produced the trained ~570GB model, what sort of computing power is necessary to execute specific queries with it?\n",
      "...\n",
      "\n",
      "title: Why is GPT-3 such a game changer?\n",
      "\n",
      "content: I've been hearing a lot about GPT-3 by OpenAI, and that it's a simple to use API with text in text out and has a big neural network off 175B parameters.\n",
      "But how did they achieve this huge number of parameters, and why is it being predicted as one of ...\n",
      "\n",
      "title: Can in principle GPT language models learn physics?\n",
      "\n",
      "content: Does anyone know of research involving the GPT models to learn not only regular texts, but also learn from physics books with the equations written in latex format?\n",
      "My intuition is that the model might learn the rules relating equations and deduction...\n",
      "\n",
      "title: Process 2TB worth of conversational data hoarded over 40 years. How can I pass this into GPT to ask questions about it?\n",
      "\n",
      "content: I'm still very new to this stuff. I have close to 2TB worth of data hoarded from IRC chats to everyday chats with friends and family.\n",
      "But is there a way to pass in this much data into GPT to ask questions about it? Or would I require something else?\n",
      "...\n",
      "\n",
      "title: How to Select Model Parameters for Transformer (Heads, number of layers, etc)\n",
      "\n",
      "content: Is there a general guideline on how the Transformer model parameters should be selected, or the range of these parameters that should be included in a hyperparameter sweep?\n",
      "\n",
      "Number of heads\n",
      "Number of encoder & decoder layers\n",
      "Size of transformer model...\n",
      "\n",
      "title: Is it realistic to train a transformer-based model (e.g. GPT) in a self-supervised way directly on the Mel spectrogram?\n",
      "\n",
      "content: In music information retrieval, one usually converts an audio signal into some kind \"sequence of frequency-vectors\", such as STFT or Mel-spectrogram.\n",
      "I'm wondering if it is a good idea to use the transformer architecture in a self-supervised manner -...\n",
      "\n",
      "title: Is it possible to integrate the GPT-3 by OpenAPI inside Unity3D or any game-engine?\n",
      "\n",
      "content: My company has full access to beta testing for GPT-3. We wanted to try it for some games or game mechanics within Unity3D. Is it possible to use it for dialogues or with unity scripts?\n",
      "The Documents of OpenAI does not say anything about this possibil...\n",
      "\n",
      "title: How large should the corpus be to optimally retrain the GPT-2 model?\n",
      "\n",
      "content: I just started working with the GPT-2 models and want to retrain one on a pretty narrow topic, so I have problems finding training material.\n",
      "How large should the corpus be to optimally retrain the GPT-2 model? And what is the bare minimum size? Shoul...\n",
      "\n",
      "title: What&#39;s the most efficient way of performing batched training of Causal Language Models?\n",
      "\n",
      "content: I have seen a number of ways to train (yes, train, not fine-tune) these models efficiently with batches. I will illustrate these techniques with the following example dataset and context window:\n",
      "Context window:\n",
      "   -----------------\n",
      "Data samples:\n",
      "1. #...\n",
      "\n",
      "title: Pretrained Models for Keyword-Based Text Generation\n",
      "\n",
      "content: I'm looking for an implementation that allows me to generate text based on a pre-trained model (e.g. GPT-2).\n",
      "An example would be gpt-2-keyword-generation (click here for demo). As the author notes, there is\n",
      "\n",
      "[...] no explicit mathematical/theoetical ...\n",
      "\n",
      "title: Can we use GPT-2 to smooth out / correct text?\n",
      "\n",
      "content: Are we able to use models like GPT-2 to smooth out/correct text? For instance if I have two paragraphs that need some text to make the transition easier to read, could this text be generated? And, could it find inconsistencies between the paragraphs ...\n",
      "\n",
      "title: How to interpret a large variance of the loss function?\n",
      "\n",
      "content: How do I interpret a large variance of a loss function?\n",
      "I am currently training a transformer network (using the software, but not the model from GPT-2) from scratch and my loss function looks like this:\n",
      "\n",
      "The green dots are the loss averaged over 100...\n",
      "\n",
      "title: How can I generate a document from a single word using GPT or BERT?\n",
      "\n",
      "content: I have a dataset of 100000 documents each labelled with a topic to it. I want to create a model such that, given a topic, the model can generate a document from it. \n",
      "I came across language models GPT, GPT-2 and BERT. I learned that they can be used f...\n",
      "\n",
      "title: How much computing power does it cost to run GPT-3?\n",
      "\n",
      "content: I know it cost around $4.3 million dollars to train, but how much computing power does it cost to run the finished program? IBM Watson chatbot AI only costs a few cents per chat message to use, OpeenAI Five seemed to run on a single gaming PC setup. ...\n",
      "\n",
      "title: Is it possible to use the GPT-2 model for time-series data prediction?\n",
      "\n",
      "content: Is it possible and how trivial (or not) might it be (if possible) to retrain GPT-2 on time-series data instead of text?\n",
      "...\n",
      "\n",
      "title: Left-to-Right vs Encoder-decoder Models\n",
      "\n",
      "content: Xu et al. (2022) distinguishes between popular pre-training methods for language modeling: (see Section 2.1 PRETRAINING METHODS)\n",
      "\n",
      "Left-to-Right:\n",
      "\n",
      "\n",
      "Auto-regressive, Left-to-right models, predict the probability of a\n",
      "token given the previous tokens.\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "title: Can an existing transformer model be modified to estimate the next most probable number in a sequence of numbers?\n",
      "\n",
      "content: Models based on the transformer architectures (GPT, BERT, etc.) work awesome for NLP tasks including taking an input generated from words and producing probability estimates of the next word as the output.\n",
      "Can an existing transformer model, such as G...\n",
      "\n",
      "title: What is the difference between T5 and T0 models?\n",
      "\n",
      "content: What is the difference between T5 and T0 models? I had read that T0 is T5 + LM. But as I know T5 uses encoder-decoder model like BART but BART can be used as LM so that's mean that T5 has a LM function too. So what is the difference between T5 and T0...\n",
      "\n",
      "title: Is it possible for a GPT model to run in a distributed way?\n",
      "\n",
      "content: Say that we're on GPT20 - maybe the model that's resulted from training is 10PB large (maybe unlikely but this is an example). Is it possible for a GPT model to be distributed across machines? How does this work if distributed inference is needed?\n",
      "...\n",
      "\n",
      "title: How do they make transformers bigger/deeper?\n",
      "\n",
      "content: I can find a million explanations of the diagram in the original transformer paper:\n",
      "\n",
      "But I know that modern GPT models have many millions of weights. Where are they?  Or in other words, how does this thing scale?\n",
      "...\n",
      "\n",
      "title: Is the size of a neural network directly linked with an increase in its inteligence?\n",
      "\n",
      "content: Just came across this article on GPT-3, and that lead me to the question:\n",
      "In order to make a certain kind of neural network architecture smarter all one needs to do is to make it bigger?\n",
      "Also, if that is true, how does the importance of computer powe...\n",
      "\n",
      "title: Possible to use GPT for specific set of documents?\n",
      "\n",
      "content: I have a 100+ PDF documents regarding a company's policies, procedures and guidelines etc.\n",
      "Is there an AI tool that was trained in general understanding of language, that I can feed all those PDFs and make my own model where I can ask questions about...\n",
      "\n",
      "title: can I add to a language model a prompt with output example?\n",
      "\n",
      "content: I want to finetune GPT2 to extract relevant data from a given text. So for (a trivial) example, given the text \"the car was manufactured in X, can reach Y km/h, and has Z horse powers\", my desired output would be manufacturer: X, max speed:Y, horsepo...\n",
      "\n",
      "title: GPT-2: (Hardware) requirements for fine-tuning the 774M model\n",
      "\n",
      "content: I wonder if there's anyone who has actually succeeded in fine-tuning GPT-2's 774M model without using cloud TPU's. My GeForce RTX 2070 SUPER couldn't handle it in previous attempts.\n",
      "I'm running TensorFlow 1.14.0 with CUDA V 9.1 on Ubuntu 18.04. For f...\n",
      "\n",
      "title: GPT beam search length (number of tokens)\n",
      "\n",
      "content: Background: I'm currently trying to use GPT to give me numerical scores, and looking for tips on prompt design, see my previous StackExchange post.\n",
      "To craft good prompts it seems important to have a good understanding of how the generative model work...\n",
      "\n",
      "title: Prompt engineering GPT for numerical scores?\n",
      "\n",
      "content: Background: I am currently working on an NLP project using GPT 3.5 via the OpenAI python API.\n",
      "Engineering good prompts seems to be the most critical part of my pipeline so far. Crucially I have long input texts, for which I want to get numerical sent...\n",
      "\n",
      "title: What is actually tuned during prompt engineering of autoregressive LLMs like GPT?\n",
      "\n",
      "content: There are a lot of sites going over prompt engineering, but I don't see them explaining what is actually being changed. Is it hidden activation layers being tuned?\n",
      "...\n",
      "\n",
      "title: Issues with larger context lengths in a transformer model like GPT\n",
      "\n",
      "content: Based on my understanding, one of the issues with longer context lengths is the computational complexity of attention mechanism which is quadratic. But is this really a problem on modern hardware with parallel CPU's?\n",
      "Eg: 32k context length (GPT4) has...\n",
      "\n",
      "title: How can I send vectors as a chat context?\n",
      "\n",
      "content: Since the context/memory of a chat or question for LLMs more precisely GPT is limited to a token length I struggle about how to provide own data that the model got not trained on.\n",
      "A very common approach looks like embeddings are the way to.\n",
      "OpenAI pr...\n",
      "\n",
      "title: Does GPT not need to compress its training data?\n",
      "\n",
      "content: In his recent short pamphlet on GPT, Stephan Wolfram says\n",
      "\n",
      "... the 'size of the network' that seems to work well is ... comparable to the 'size of the training data'. ...\n",
      "in this representation it seems there’s in the end rather little 'compression' ...\n",
      "\n",
      "title: &quot;Following instructions&quot; as an emergent behaviour in transformer models - isn&#39;t this fundamentally different from the models&#39; basic purpose?\n",
      "\n",
      "content: I am not technically familiar with AI or neural networks beyond a tech news reading level of knowledge, so I apologise if this is a dumb question.\n",
      "I was recently reading this article on Ars Technica.  It is a high level description of the history of ...\n",
      "\n",
      "title: How do GPT models pass information for each token prediction?\n",
      "\n",
      "content: So, I'm trying to understand what is going on in the following picture (from this paper):\n",
      "\n",
      "Each decoder blocker in the above GPT model has attention heads (red) and MLPs (green). I know that we add residual connections to prevent vanishing gradients ...\n",
      "\n",
      "title: Why is BERT/GPT capable of &quot;for-all&quot; generalization?\n",
      "\n",
      "content: As shown in the figure:\n",
      "\n",
      "Why does token prediction work when \"Socrates\" is replaced with \"Plato\"?\n",
      "From the point of view of symbolic logic, the above example effectively performs the logic rule:\n",
      "∀x. human(x) ⇒ mortal(x)\n",
      "\n",
      "How might we explain this abi...\n",
      "\n",
      "title: How to fine-tune GPT-J with small dataset\n",
      "\n",
      "content: I have followed this guide as closely as possible: https://github.com/kingoflolz/mesh-transformer-jax\n",
      "I'm trying to fine-tune GPT-J with a small dataset of ~500 lines:\n",
      "You are important to me. <|endoftext|>\n",
      "I love spending time with you. <|endoftext|...\n",
      "\n",
      "title: What is a neuron in large language models?\n",
      "\n",
      "content: I'm reading OpenAI's new paper \"Language models can explain neurons in language models\" And I can't fully understand the concept of neurons here.\n",
      "Can you please explain it? Is it related to the attention mechanism?\n",
      "...\n",
      "\n",
      "title: Is the Turing Test, or any of its variants, a reliable test of artificial intelligence?\n",
      "\n",
      "content: The Turing Test was the first test of artificial intelligence and is now a bit outdated. The Total Turing Test aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelli...\n",
      "\n",
      "title: What are the necessary components to make an AI agent capable of self-programming?\n",
      "\n",
      "content: An AI agent is often thought of having \"sensors\", \"a memory\", \"machine learning processors\" and \"reaction\" components. However, a machine with these does not necessarily become a self-programming AI agent. Beyond the parts mentioned above, is there a...\n",
      "\n",
      "title: How can an AI freely make decisions?\n",
      "\n",
      "content: Suppose a deep neural network is created using Keras or Tensorflow. Usually, when you want to make a prediction, the user would invoke model.predict. However, how would the actual AI system proactively invoke their own actions (i.e. without the need ...\n",
      "\n",
      "title: What are some examples of intelligent agents for each intelligent agent class?\n",
      "\n",
      "content: There are several classes of intelligent agents, such as:\n",
      "\n",
      "simple reflex agents\n",
      "model-based reflex agents\n",
      "goal-based agents\n",
      "utility-based agents\n",
      "learning agents\n",
      "\n",
      "Each of these agents behaves slightly different from the other agents. There are certain...\n",
      "\n",
      "title: Are humans intelligent according to the definition of an intelligent agent?\n",
      "\n",
      "content: Given the following definition of an intelligent agent (taken from a Wikipedia article)\n",
      "\n",
      "If an agent acts so as to maximize the expected value of a performance measure based on past experience and knowledge then it is intelligent\n",
      "\n",
      "and given that we, ...\n",
      "\n",
      "title: What is the definition of rationality?\n",
      "\n",
      "content: I'm having a little trouble with the definition of rationality, which goes something like:\n",
      "\n",
      "An agent is rational if it maximizes its performance measure given its current knowledge.\n",
      "\n",
      "I've read that a simple reflex agent will not act rationally in a l...\n",
      "\n",
      "title: What are the differences between an agent that thinks rationally and an agent that acts rationally?\n",
      "\n",
      "content: Stuart Russell and Peter Norvig pointed out 4 four possible goals to pursue in artificial intelligence: systems that think/act humanly/rationally.\n",
      "What are the differences between an agent that thinks rationally and an agent that acts rationally?\n",
      "...\n",
      "\n",
      "title: What is the difference between simple reflex and model-based agents?\n",
      "\n",
      "content: What is the difference between simple reflex and model-based agents?\n",
      "What is the role of the internal state in the case of model-based agents?\n",
      "...\n",
      "\n",
      "title: What is the difference between abstract, autonomous and virtual intelligent agents?\n",
      "\n",
      "content: On Wikipedia, we can read about different type of intelligent agents:\n",
      "\n",
      "abstract intelligent agents (AIA),\n",
      "autonomous intelligent agents,\n",
      "virtual intelligent agent (IVA), which I've found on other websites, e.g. this one.\n",
      "\n",
      "What are the differences bet...\n",
      "\n",
      "title: How widely accepted is the definition of intelligence by Marcus Hutter &amp; Shane Legg?\n",
      "\n",
      "content: I came across several papers by M. Hutter & S. Legg.\n",
      "Especially this one:\n",
      "Universal Intelligence: A Definition of Machine Intelligence, Shane Legg, Marcus Hutter\n",
      "Given that it was published back in 2007, how much recognition or agreement has it recei...\n",
      "\n",
      "title: How can I design an AI that knows when its being spoken to?\n",
      "\n",
      "content: I am trying to make an intelligent agent similar to Jarvis from Iron Man, but much less complex. However, I want my AI to be able to determine if I am talking to it or not. So, I plan on having it always listen to my voice and convert that to text. H...\n",
      "\n",
      "title: How to detect when human voice / speech appears in an microphone stream?\n",
      "\n",
      "content: I want to build a personal assistant that listens to me continuously.. \n",
      "The flow looks like this: \n",
      "\n",
      "continuously record voice \n",
      "stream it to google speech api.\n",
      "get back the text in real time -> parse for intent etc..\n",
      "\n",
      "Problem is, google speech api get...\n",
      "\n",
      "title: What are some of the possible future applications of intelligent agents?\n",
      "\n",
      "content: I am trying to do some experiments with some intelligent agents, but I'm not sure how significant they will be in the future. \n",
      "What are some possible interesting applications or use-cases of intelligent agents in the future? \n",
      "For instance, it can be ...\n",
      "\n",
      "title: Is there anything novel about Zuckerberg&#39;s Jarvis?\n",
      "\n",
      "content: Recently Mark got some attention from the media by stating that he had created Jarvis. Not that I'm against him or anything, but this Jarvis seems to have been done a hundred times before. He's done something which most developers would classify as a...\n",
      "\n",
      "title: What is the difference between learning and non-learning agents?\n",
      "\n",
      "content: What is the difference between learning agents and other types of agents?\n",
      "In what ways learning agents can be applied? Do learning agents differ from deep learning?\n",
      "...\n",
      "\n",
      "title: Does it exist a human-like artificial intelligence?\n",
      "\n",
      "content: Does it exist a human-like artificial intelligence?\n",
      "I define human-like as something that can act like a human in most aspects. \n",
      "...\n",
      "\n",
      "title: What is a learning agent?\n",
      "\n",
      "content: What is a learning agent, and how does it work? What are examples of learning agents (e.g., in the field of robotics)?\n",
      "...\n",
      "\n",
      "title: Multi Agent Sokoban Search Solvers state of the art\n",
      "\n",
      "content: I also asked this question here but I'm repeating it on this SE because I feel it is more relevant. No intention to spam.\n",
      "I am researching into coding a solver for a variant of the Sokoban game with multiple agents, restrictions (eg. colors of stones...\n",
      "\n",
      "title: Is a team of ML scientists an &quot;intelligent agent&quot;?\n",
      "\n",
      "content: I am writing about the role of machine learning scientists in developing a solution. Is there a term for the humans who do learning?\n",
      "Can we call a \"team of machine learning scientists with their computers working on some ML problem\"   an intelligent ...\n",
      "\n",
      "title: Are there any agents that are based on quantum computing?\n",
      "\n",
      "content: Assuming the definition of an agent to be:\n",
      "\n",
      "An entity that perceives its environment, processes the perceived information, and acts on the environment such that some goal is fulfilled.\n",
      "\n",
      "Are there any agents that are based on quantum processing/comput...\n",
      "\n",
      "title: How do we define intention if there is no free will?\n",
      "\n",
      "content: There is an idea that intentionality may be a requirement of true intelligence, here defined as human intelligence.  \n",
      "But all I know for certain is that we have the appearance of free will.  Under the assumption that the universe is purely determinis...\n",
      "\n",
      "title: Can we create a conscious agent by having two units: one rational and the other conscious?\n",
      "\n",
      "content: Suppose we create two units (or programs) that run in parallel and we label them as a cognitive unit and the conscious cognitive unit. \n",
      "A human has two units analogously. A rational analyzer and not so rational analyzer. (Is there any third thing?)\n",
      "I...\n",
      "\n",
      "title: What is an agent in Artificial Intelligence?\n",
      "\n",
      "content: While studying artificial intelligence, I have often encountered the term \"agent\" (often autonomous, intelligent). For instance, in fields such as Reinforcement Learning, Multi-Agent Systems, Game Theory, Markov Decision Processes.\n",
      "In an intuitive se...\n",
      "\n",
      "title: Why would the lookup table (of a table-driven artificial agent) need to store data at pixel precision?\n",
      "\n",
      "content: While reading the book AI A modern approach, 4th ed, I came across the section of \"Agent program\" with following text:\n",
      "\n",
      "It is instructive to consider why the table-driven approach to agent\n",
      "construction is doomed to failure. Let $P$ be the set of poss...\n",
      "\n",
      "title: Do text compression tests qualify winRar or 7zip as intelligent?\n",
      "\n",
      "content: I read this paper Text Compression as a Test for Artificial Intelligence, Mahoney, 1999.\n",
      "So far I understood the following:\n",
      "Text Compression tests can be used as an alternative to Turing Tests for intelligence.\n",
      "The Bits per character score obtained f...\n",
      "\n",
      "title: Can agents be implemented with ML algorithms other than neural networks?\n",
      "\n",
      "content: Can agents be implemented with machine learning algorithms/models other than neural networks?\n",
      "If so, how do I train an agent with some predefined rules? Can we use python programming for representing those rules?\n",
      "...\n",
      "\n",
      "title: Agents meeting in a directed connected graph\n",
      "\n",
      "content: We have a directed connected graph, where nodes are places one can go to and edges are the \"roads\" between places. We have K agents whose goal is to meet in one node. Agents start in different nodes. Note that more than one agent can be at one node a...\n",
      "\n",
      "title: SEIF motion update algorithm doubt\n",
      "\n",
      "content: I want to implement Sparse Extended information slam. There is four step to implement it. The algorithm is available in Probabilistic Robotics Book at page 310, Table 12.3.\n",
      "\n",
      "In this algorithm line no:13 is not very clear to me. I have 15 landmarks. S...\n",
      "\n",
      "title: What are the examples of agents that is represent these characteristics?\n",
      "\n",
      "content: I'm looking for examples of AI systems or agents that best represent these five characteristics (one example for each characteristics):\n",
      "\n",
      "Reactivity \n",
      "Proactivity\n",
      "Adaptability\n",
      "Sociability \n",
      "Autonomy\n",
      "\n",
      "It would be better if its machine learning-based appl...\n",
      "\n",
      "title: How to teach a model-based reflex agent for doing some task using machine learning methods?\n",
      "\n",
      "content: I would like to know how to teach an agent for performing prediction of the severity of disease and also for alerting patients using machine learning methods.\n",
      "I found the model-based reflex agent can be used in medical diagnosis in some literature. \n",
      "...\n",
      "\n",
      "title: How can an AI agent tackle through existential crisis?\n",
      "\n",
      "content: Suppose a thinking AI agent exists in the future with far more computational power than the human brain. \n",
      "Also assume that they are completely free from any human interference. (The agents do not interact with humans.)  Since they are not inherently ...\n",
      "\n",
      "title: Are artificial intelligence learnings or trainings transferable from one agent to the other?\n",
      "\n",
      "content: One disadvantage or weakness of Artificial Intelligence today the slow nature of learning or training success. For instance, an AI agent might require a 100,000 samples or more to reach an appreciable level of performance with a specific task. But th...\n",
      "\n",
      "title: What types of AI agents are Djikstra&#39;s algorithm and Prim&#39;s Minimum Spanning Tree algorithm?\n",
      "\n",
      "content: From the perspective of the type of AI Agents, I would like to discuss Prim's Minimum Spanning Tree algorithm and Dijkstra's Algorithm.\n",
      "Both are model-based agents and both are \"greedy algorithms\".\n",
      "Both have their memory to store the history of verti...\n",
      "\n",
      "title: Has there been an instance of an AI agent breaking out of its sandbox?\n",
      "\n",
      "content: There have been instances of agents using edge cases like bugs in physics engines, repetitive behavior in games or word repetition in text prediction to cheat their reward function. However, these agents are arguably still contained, as while they ex...\n",
      "\n",
      "title: Are goal-reaching and optimizing the utility function special cases of performance measure?\n",
      "\n",
      "content: In AIMA, performance measure is defined as something evaluating the behavior of the agent in an environment.\n",
      "Rational agents are defined as agents acting so as to maximize the expected value of the performance measure, given the percept sequence they...\n",
      "\n",
      "title: What is the difference between a performance standard and performance measure?\n",
      "\n",
      "content: I am reading AI: A Modern Approach. In the 2nd chapter when introducing different agent types, i.e., reflex, utility-based, goal-based, and learning agents, I understood that all types of agents, except learning agents, receive feedback and choose ac...\n",
      "\n",
      "title: How do you know if an agent has learnt its environment in reinforcement learning?\n",
      "\n",
      "content: I'm new to reinforcement learning and trying to understand it. \n",
      "If you train an agent using a reinforcement learning algorithm (discrete or continuous) on an environment (real or simulated), then how do you know if the agent has learnt its environmen...\n",
      "\n",
      "title: What happens if an agent has two contrasting utility functions?\n",
      "\n",
      "content: What would happen if an agent were programmed with two utility functions which are one the opposite of the other? Would one prevail or will they cancel each other out? Are there studies in this direction?\n",
      "...\n",
      "\n",
      "title: Are there better loss functions than MSE for maze solver using deep learning?\n",
      "\n",
      "content: I am a newbie in reinforcement learning, and I was doing a project on solving an agent maze solver using deep Q Learning.  Currently, I am using the MSE loss function, but the agent is very slow or not even reaching the target.  Is there any better l...\n",
      "\n",
      "title: What is the need for _agency_ in AI?\n",
      "\n",
      "content: Why seek to develop artificially intelligent agents? Are there certain advantages and/or needs provided by such supposed intelligent agents that are preferred to simply using intelligent tools that are devoid of agency (e.g. language models)? If so, ...\n",
      "\n",
      "title: Why is this vacuum cleaner agent rational?\n",
      "\n",
      "content: This is the vacuum cleaner example of the book \"Artificial intelligence: A Modern Approach\" (4th edition).\n",
      "\n",
      "Consider the simple vacuum-cleaner agent that cleans a square if it is dirty and moves to the other square if not; this is the agent function ...\n",
      "\n",
      "title: How to train the NN of simple agents given a reward system?\n",
      "\n",
      "content: I'm not an expert in AI or NN, I gathered most of the information I have from the internet, and I'm looking for advice and guidance.\n",
      "I'm trying to design a NN that is going to be used by all the agents of my simulation (each agent will have its own m...\n",
      "\n",
      "title: Need some reviews in PEAS descriptions\n",
      "\n",
      "content: Here is the Question:\n",
      "Describe the PEAS descriptions for the following agents:\n",
      "a) A grocery store scanner that digitally scans a fruit or vegetable and\n",
      "identifies it.\n",
      "b) A GPS system for an automobile. Assume that the destination has been\n",
      "preprogramm...\n",
      "\n",
      "title: Chatbots triggering emotions\n",
      "\n",
      "content: I’m a researcher and I’m currently conducting a research project. I will conduct a study where I would like to trigger different emotions using chatbots on a smartphone (e.g. on Facebook Messenger).\n",
      "Are there any existing chatbots which are able to t...\n",
      "\n",
      "title: Can we code rules for an agent in python language other than predicate calculus?\n",
      "\n",
      "content: I have a medical dataset with 14000 rows dataset with 900 attributes. I have to predict disease severity using that. I would like to know whether we can write rules in python language for training an agent for medical diagnostic using machine learnin...\n",
      "\n",
      "title: What if we modify some Q-values while taking the action?\n",
      "\n",
      "content: Just a passing thought about Q-learning. In the tabular Q-learning, what if I play around and modify any Q-values as I am using them to take actions? Would it be a violation of any (1) theoretical rule? (2) a reduction of efficiency?\n",
      "Update: By modif...\n",
      "\n",
      "title: Do LLMs suffer from a kind of Dunning-Kruger effect, giving an inflated self-assessment in domains they lack expertise in?\n",
      "\n",
      "content: \n",
      "The Dunning–Kruger effect is a cognitive bias whereby people with low ability, expertise, or experience regarding a type of task or area of knowledge tend to overestimate their ability or knowledge.\n",
      "\n",
      "(Note that the Dunning-Kruger effect is not the M...\n",
      "\n",
      "title: How do I &quot;teach&quot; a large language model new knowledge?\n",
      "\n",
      "content: Suppose I have a copy of a pre-trained transformer-based large language model like Google's T5 or Meta's Llama. Due to the pre-training, it contains a lot of knowledge.\n",
      "However, I want to teach the model something new, knowledge it doesn't already co...\n",
      "\n",
      "title: Is there an offline GPT model I can use to generate narrative reports from excel data based on an existing set of files?\n",
      "\n",
      "content: I have a large excel spreadsheet containing data that I have used to write many narrative reports. Without knowing a ton about the technical side of GPT models it seems like it would be theoretically possible to train a GPT model on the existing data...\n",
      "\n",
      "title: How to train Bard to answer based on a private FAQ document?\n",
      "\n",
      "content: I have a private (unpublished) FAQ document about an app, it has 7000 words and 40000 characters. To simulate it you may try taking https://github.com/ankidroid/Anki-Android/wiki/FAQ and replacing the app's name with another name.\n",
      "I would like to tra...\n",
      "\n",
      "title: To what extent do LLMs have grammar rules explicitly programmed?\n",
      "\n",
      "content: Is there typically any explicit programming in LLMs that dictates how to form a sentence? Where does the \"understanding\" come to with regards to references like \"rewrite last reply in 2 words\"? How does the LLM \"know\" what is meant by \"rewrite\"?\n",
      "How ...\n",
      "\n",
      "title: Is there a way to gauge how environmentally (un)friendly popular genAIs are?\n",
      "\n",
      "content: The environmentally conscious user may prefer a genAI which is less harmful to the environment, such as those with less CO₂ emissions.  I was reading How to Make Generative AI Greener which suggests server location is a big factor in whether or not a...\n",
      "\n",
      "title: Does Llama2 LLMs have real time data?\n",
      "\n",
      "content: Meta released their open source LLM, Llama 2. Does Llama 2 have real time data or this was trained on data available till some date?\n",
      "...\n",
      "\n",
      "title: How long is a &quot;token&quot;?\n",
      "\n",
      "content: LLM max prompt length (e.g: GPT-4) and generation pricing (e.g: Azure) are both measured by the number of \"tokens\".\n",
      "How long is a \"token\"? Is it equivalent to a single character/letter?\n",
      "...\n",
      "\n",
      "title: To what extent can GenAI be used for content moderation?\n",
      "\n",
      "content: I've seen several people trying to use GenAI technologies for content moderation. This doesn't seem impossible, on the face of it: I use regular expressions to great effect, and I can see how more sophisticated language processing could achieve an ev...\n",
      "\n",
      "title: Prompt engineering GPT for numerical scores?\n",
      "\n",
      "content: Background: I am currently working on an NLP project using GPT 3.5 via the OpenAI python API.\n",
      "Engineering good prompts seems to be the most critical part of my pipeline so far. Crucially I have long input texts, for which I want to get numerical sent...\n",
      "\n",
      "title: How important are GPUs vs. CPUs when training an LLM?\n",
      "\n",
      "content: I want to train my very own large language model. I have some data, and access to an open-source foundational model. Can I use my laptop that only has a CPU and no GPU to train the model? Do I need to rent time on a cloud provider to get a VM with GP...\n",
      "\n",
      "title: Could genAI be used to generate reasonable scientific hypotheses?\n",
      "\n",
      "content: Suppose I have a trained genAI and I give it some different experiments, their results, and I were to ask it to generate a plausible hypotheses of what happend. Would it be able to do this well? If not, why?\n",
      "I am pretty sure the answer is not, otherw...\n",
      "\n",
      "title: Best-Practice in word-embeddings\n",
      "\n",
      "content: In my project I follow the retrieval augmented generation (RAG) approach. I want to create embeddings for my own dataset and use it in combination with llama-2. In the dataset are german annual reports, 548 reports as pdf-files with about 300 sites p...\n",
      "\n",
      "title: Embeddings: How can I find out, that I&#39;m out-of-vocabulary?\n",
      "\n",
      "content: I have a german dataset of financial/annual reports of companies.\n",
      "For example, I use the gensim package to embed my dataset with word2vec or use the huggingface package to embed.\n",
      "How can I find out, that I'm out-of-vocabulary? I want to check it, bec...\n",
      "\n",
      "title: Programmatic fine-tune training in AWS\n",
      "\n",
      "content: I'm trying to setup an automatic fine tuning training pipeline with DreamBooth.  I've read this tutorial https://medium.com/@IAmxIAo/automating-fine-tuning-stable-diffusion-on-sagemaker-c340f65b24f2\n",
      "However I don't understand what value does SageMake...\n",
      "\n",
      "title: How to use GPT-4 to help authors write a document with LaTeX formatting?\n",
      "\n",
      "content: I read in OpenAI's GPT-4 Technical Report:\n",
      "\n",
      "GPT-4 was used in the following ways: to help us iterate on LaTeX formatting [...]\n",
      "\n",
      "How can one use GPT-4 to help authors iterate on LaTeX formatting?\n",
      "\n",
      "Note that the training data of GPT 3.5 or 4 definitely...\n",
      "\n",
      "title: What works better: An LLM trained on better texts or an LLM with better prompts?\n",
      "\n",
      "content: Assume you have a very large corpus of high-quality documents related to a given topic, and assume you have a pretrained large language model (the foundation model) with training data not containing these documents.\n",
      "Scenario 1: You use the high-quali...\n",
      "\n",
      "title: Generative AI Use Case for Search Domain\n",
      "\n",
      "content: I am trying to think and research for use cases that can beneficial search experience using Generative AI. There are two things,\n",
      "\n",
      "The final search results that can be more relevant and personalised per user. This would take time and more effort - in ...\n",
      "\n",
      "title: Is the simulators viewpoint still valid?\n",
      "\n",
      "content: When considering the behavior of large language models, there is not yet a single canonical framework for interpreting their output in context. Common proposed frameworks include:\n",
      "\n",
      "Agents: The LLM implements an agentive mind with volition and desires...\n",
      "\n",
      "title: How can I format the input text on Google Translate so that the translation doesn&#39;t change person names?\n",
      "\n",
      "content: When I have some texts to translate, I notice that sometimes, Google Translate messes up person names. E.g.L\n",
      "\n",
      "The interview went well. Cherry is good.\n",
      "\n",
      "becomes:\n",
      "\n",
      "L'entretien s'est bien passé. La cerise est bonne.\n",
      "\n",
      "which is not good because the person...\n",
      "\n",
      "title: I want to upload document\n",
      "\n",
      "content: Wanna upload large document in slovenian language and train the model with that data, it’s for school. Is there any good reliable free option?\n",
      "...\n",
      "\n",
      "title: large language model for incident solving\n",
      "\n",
      "content: we want to run a llm locally in our company and we want to give it the code, the documantation and the incidents so it can help to solve the incidents.\n",
      "We think about alpaca or llama for this task. Which llm would be the best and how to implement/use...\n",
      "\n",
      "title: Llm without transformers\n",
      "\n",
      "content: I am curious to know that can we built large language models without transformers? If yes how to do it?\n",
      "...\n",
      "\n",
      "title: Suggestions regarding developing a Code snippets Suggestion tool\n",
      "\n",
      "content: I am looking to create a program that can automatically suggest code snippets based on comments I provide. These comments often indicate the need for reusable code sections from my repository, saving valuable time and effort. My goal is to input a co...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered = {}\n",
    "for idx, (qid, q_block) in enumerate(q_blocks.items()):\n",
    "    keywords = [\"chatgpt\", \"image\", \"stable diffusion\", \"draw\", \"deep-RL\", \".net\", \"RL\", \"nasty\", \"AUTOMATIC1111\"]\n",
    "    skip = False\n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() in q_block['title'].lower() or keyword.lower() in q_block['content'].lower():\n",
    "            skip = True\n",
    "    if skip:\n",
    "        continue\n",
    "    print(f\"title: {q_block['title']}\\n\")\n",
    "    print(f\"content: {q_block['content'][:250]}...\\n\")\n",
    "        # print(f\"answers: {q_block['answers']}\")\n",
    "    filtered[qid] = q_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7311c4-3edf-4bbe-ad5c-0d660ba625dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "69f63e6b-e53f-46be-98e3-c88b7cf5dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(filtered, open(\"aistackexchange.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220bafa1-f683-49ce-9d23-861b30f9205b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
