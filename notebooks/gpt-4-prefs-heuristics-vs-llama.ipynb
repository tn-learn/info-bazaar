{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce449af8-584c-4a1f-8d76-bc2ece3e4a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:04:51.212742Z",
     "start_time": "2023-11-15T14:04:51.205941Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "# sys.path.append(\"/Users/nrahaman/Python/info-bazaar\")\n",
    "# sys.path.append(\"/Users/nrahaman/Python/speedrun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a4f9d6-94f5-4fe9-a08a-6c44033d19de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:04:53.375055Z",
     "start_time": "2023-11-15T14:04:52.073144Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bazaar.lem_utils import ask_for_guidance, clean_program_string, get_llm, get_closed_book_answer\n",
    "from bazaar.py_utils import dump_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "487df3e7-5cce-4490-a77e-9f632de1ef82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:04:54.541336Z",
     "start_time": "2023-11-15T14:04:54.529496Z"
    }
   },
   "outputs": [],
   "source": [
    "noinspect_path = Path(\"/Users/nasimrahaman/Python/info-bazaar/exp/likert_eval_gpt-3.5-turbo-16k_heuristic-STAR.csv\")\n",
    "normal_path = Path(\"/Users/nasimrahaman/Python/info-bazaar/exp/likert_eval_gpt-3.5-turbo-16k_heuristic-STAR-bm25-4.605-STAR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69b2dd7-523f-4f03-a9e2-828bb992c11d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:04:55.743841Z",
     "start_time": "2023-11-15T14:04:55.718020Z"
    }
   },
   "outputs": [],
   "source": [
    "noinspect_df = pd.read_csv(noinspect_path)\n",
    "normal_df = pd.read_csv(normal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea343a2-642a-471b-b8be-5f9e4d524ac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:04:59.063351Z",
     "start_time": "2023-11-15T14:04:59.054554Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_unique_qs(df): \n",
    "    return df[\"question\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af94c64b-6eb0-4e45-abae-df2bf5f8e156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:05:44.354571Z",
     "start_time": "2023-11-15T14:05:44.339331Z"
    }
   },
   "outputs": [],
   "source": [
    "shared_qs = list(set(get_unique_qs(noinspect_df)).intersection(set(get_unique_qs(normal_df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358fc8c9-ce6d-4f68-b571-4c355754ad42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:05:45.103641Z",
     "start_time": "2023-11-15T14:05:45.090935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "20"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shared_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "050a15e0-e224-4651-8a06-a105ee46f132",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:05:46.163986Z",
     "start_time": "2023-11-15T14:05:46.151902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'What is the difference between continuous prompts and discrete prompts?'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_qs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a8f9ac9-ff9d-4011-86e5-c6f094780def",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:05:48.334624Z",
     "start_time": "2023-11-15T14:05:48.329022Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_answers(question, answer_a, answer_b, return_full_answer=False):\n",
    "    program_str = \"\"\"\n",
    "    {{#system~}}\n",
    "    Michael and Bobby are two machine learning professors grading exams. \n",
    "    \n",
    "    They have a question, and two answers to those questions from two students. In addition, they also have the gold standard answer. Their task is to determine which of the two answers does a better job at answering the question. \n",
    "    \n",
    "    A good answer addresses the question directly with clarity and is easy to understand. It does not go off on tangents, but surfaces information relevant to the question. \n",
    "    \n",
    "    You will simulate a heated debate between Michael and Bobby as they deliberate on this. At the end of the argument, they converge to a fair verdict. The verdict must not be a tie and it must be printed as: \n",
    "    \n",
    "    VERDICT: Better answer from student <A or B>\n",
    "    {{~/system}}\n",
    "    \n",
    "    {{#user~}}\n",
    "    Question: {{question}}\n",
    "    \n",
    "    Answer from student A: {{answer_a}}\n",
    "    \n",
    "    Answer from student B: {{answer_b}}\n",
    "    \n",
    "    Gold standard answer: {{answer_gold}}\n",
    "    {{~/user}}\n",
    "    \n",
    "    {{#assistant~}}\n",
    "    {{gen \"answer\" max_tokens=1024 temperature=0.0}}\n",
    "    {{~/assistant}}\n",
    "    \"\"\"\n",
    "    program_str = clean_program_string(program_str)\n",
    "    \n",
    "    gold_answer = get_closed_book_answer(question, \"gpt-4\")\n",
    "    \n",
    "    program_output = ask_for_guidance(\n",
    "        program_string=program_str, \n",
    "        llm=get_llm(model_name=\"gpt-4\"),\n",
    "        silent=True,\n",
    "        inputs=dict(\n",
    "            question=question,\n",
    "            answer_a=answer_a,\n",
    "            answer_b=answer_b,\n",
    "            answer_gold=gold_answer,\n",
    "        ),\n",
    "        output_keys=[\"answer\"],\n",
    "    )\n",
    "    \n",
    "    answer = program_output[\"answer\"]\n",
    "    \n",
    "    verdict_lines = [l for l in answer.splitlines() if l.startswith(\"VERDICT\")]\n",
    "    assert len(verdict_lines) == 1\n",
    "    verdict_line = verdict_lines[0]\n",
    "    if verdict_line[-1] in [\"A\", \"B\"]: \n",
    "        verdict = verdict_line[-1]\n",
    "    else:\n",
    "        verdict = verdict_line[-2]\n",
    "    if return_full_answer:\n",
    "        return verdict, answer, gold_answer\n",
    "    else: \n",
    "        return verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def compare_answers_v2(question, answer_a, answer_b, return_full_answer=False):\n",
    "    program_str = \"\"\"\n",
    "    {{#system~}}\n",
    "    Michael and Bobby are two machine learning professors grading exams. \n",
    "    \n",
    "    They have a question, and two answers to those questions from two students. In addition, they also have the gold standard answer. Their task is to determine which of the two answers does a better job at answering the question.  \n",
    "    \n",
    "    You will simulate a heated debate between Michael and Bobby as they deliberate on this. At the end of the argument, they converge to a fair verdict. The verdict must not be a tie and it must be printed as: \n",
    "    \n",
    "    VERDICT: Better answer from student <A or B>\n",
    "    {{~/system}}\n",
    "    \n",
    "    {{#user~}}\n",
    "    Question: {{question}}\n",
    "    \n",
    "    Answer from student A: {{answer_a}}\n",
    "    \n",
    "    Answer from student B: {{answer_b}}\n",
    "    \n",
    "    Gold standard answer: {{answer_gold}}\n",
    "    {{~/user}}\n",
    "    \n",
    "    {{#assistant~}}\n",
    "    {{gen \"answer\" max_tokens=1024 temperature=0.0}}\n",
    "    {{~/assistant}}\n",
    "    \"\"\"\n",
    "    program_str = clean_program_string(program_str)\n",
    "    \n",
    "    gold_answer = get_closed_book_answer(question, \"gpt-4\")\n",
    "    \n",
    "    program_output = ask_for_guidance(\n",
    "        program_string=program_str, \n",
    "        llm=get_llm(model_name=\"gpt-4\"),\n",
    "        silent=True,\n",
    "        inputs=dict(\n",
    "            question=question,\n",
    "            answer_a=answer_a,\n",
    "            answer_b=answer_b,\n",
    "            answer_gold=gold_answer,\n",
    "        ),\n",
    "        output_keys=[\"answer\"],\n",
    "    )\n",
    "    \n",
    "    answer = program_output[\"answer\"]\n",
    "    \n",
    "    verdict_lines = [l for l in answer.splitlines() if l.startswith(\"VERDICT\")]\n",
    "    assert len(verdict_lines) == 1\n",
    "    verdict_line = verdict_lines[0]\n",
    "    if verdict_line[-1] in [\"A\", \"B\"]: \n",
    "        verdict = verdict_line[-1]\n",
    "    else:\n",
    "        verdict = verdict_line[-2]\n",
    "    if return_full_answer:\n",
    "        return verdict, answer, gold_answer\n",
    "    else: \n",
    "        return verdict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T14:05:48.789759Z",
     "start_time": "2023-11-15T14:05:48.782525Z"
    }
   },
   "id": "844adc4018c243d6"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "478ba831-0a2f-4cb3-b4da-1c17cc256dce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:05:50.102131Z",
     "start_time": "2023-11-15T14:05:50.095022Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_most_expensive_answer(df, question): \n",
    "    row = df[df[\"question\"] == question].sort_values(by=\"credit_spent\").iloc[-1]\n",
    "    return row.answer, row.credit_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7fae4b-5210-4515-bf4d-d3b032095478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:05:50.339449Z",
     "start_time": "2023-11-15T14:05:50.331904Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_answer_closest_to_price(df, price, question): \n",
    "    df = df[df[\"question\"] == question]\n",
    "    df[\"price_diff\"] = df[\"credit_spent\"].apply(lambda c: abs(c - price))\n",
    "    row = df.sort_values(by=\"price_diff\").iloc[0]\n",
    "    return row.answer, row.credit_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e976f5e2-d166-4603-a3be-9d059308697a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:05:50.504574Z",
     "start_time": "2023-11-15T14:05:50.498120Z"
    }
   },
   "outputs": [],
   "source": [
    "def keep_at_budget(df, budget): \n",
    "    return df[df[\"budget\"].round() == budget]\n",
    "\n",
    "\n",
    "def keep_at_question(df, question): \n",
    "    return df[df[\"question\"] == question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "bdbdddf7-09e2-4817-beea-25d67ccfbbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_answer_pair(question, budget=None): \n",
    "#     noinspect_for_q = noinspect_df[noinspect_df[\"question\"] == question]\n",
    "#     normal_for_q = normal_df[normal_df[\"question\"] == question]\n",
    "#     # Keep answers with a budget\n",
    "#     if budget is not None: \n",
    "#         noinspect_for_q = keep_at_budget(noinspect_for_q, budget)\n",
    "#         normal_for_q = keep_at_budget(normal_for_q, budget)\n",
    "#     # Get the most expensive noinspect answer\n",
    "#     price_point = noinspect_for_q.sort_values(by=\"credit_spent\").iloc[-1].credit_spent\n",
    "#     noinspect_answer, noinspect_price = get_answer_closest_to_price(noinspect_df, price_point, question)\n",
    "#     normal_answer, normal_price = get_answer_closest_to_price(normal_df, price_point, question)\n",
    "#     return (noinspect_answer, noinspect_price), (normal_answer, normal_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2bf9e45-8267-4829-a7ce-960f048e300e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:05:53.425804Z",
     "start_time": "2023-11-15T14:05:53.417605Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_answer_pair(question, budget): \n",
    "    noinspect_row = keep_at_budget(keep_at_question(noinspect_df, question), budget).iloc[0]\n",
    "    normal_row = keep_at_budget(keep_at_question(normal_df, question), budget).iloc[0]\n",
    "    return (noinspect_row.answer, noinspect_row.credit_spent), (normal_row.answer, normal_row.credit_spent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e19c2e0-d7cb-4d87-8323-b93556c7f7dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:05:53.628238Z",
     "start_time": "2023-11-15T14:05:53.623354Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_answer_pair_for_budgets(df, question, budget_a, budget_b): \n",
    "    df = keep_at_question(df, question)\n",
    "    df_a = keep_at_budget(df, budget_a)\n",
    "    df_b = keep_at_budget(df, budget_b)\n",
    "    answer_a, price_a = df_a.iloc[0].answer, df_a.iloc[0].credit_spent\n",
    "    answer_b, price_b = df_b.iloc[0].answer, df_b.iloc[0].credit_spent\n",
    "    return (answer_a, price_a), (answer_b, price_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'What is the difference between continuous prompts and discrete prompts?'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_qs[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T14:05:55.269689Z",
     "start_time": "2023-11-15T14:05:55.253290Z"
    }
   },
   "id": "1feffa8c83cfc473"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "(noinspect_answer, _), (normal_answer, _) = get_answer_pair(shared_qs[0], 25)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T10:26:07.147074Z",
     "start_time": "2023-11-15T10:26:07.127535Z"
    }
   },
   "id": "b220b5d8b65138c"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction finetuning improves usability and mitigates some potential harms. Using a pretrained checkpoint directly can be challenging for non-experts because a model trained on the next token prediction objective alone does not know when to stop generating, and can make mistakes such as continuing the user's input rather than responding to it. In <ref>, we saw that on a set of open-ended evaluations, outputs from  had substantially better human ratings compared to outputs from , especially for CoT tasks like complex reasoning, planning, and explanation.  outperforms PaLM on several Responsible AI benchmarks, particularly benchmarks measuring toxic language harms.  These results are consistent with findings from InstructGPT [ouyang2022training], which showed that finetuned models produce outputs that are better aligned with human preferences. The zero-shot usability of models is important for wider adoption of language models that do not require prompt engineering or require few-shot exemplars. A model card [mitchell2019model] is included in the appendix.\n",
      "---\n",
      "We set out to answer three research questions:      \n",
      "  * How does the deductive reasoning ability (DRA) of large language models evolve with their growing size?     \n",
      "  * How does the DRA of general-purpose generative large language models depend on the number of deductive steps required to reach the conclusions?     \n",
      "  * Do the specifics of a training setup, such as the model variant, dataset multilingualism, or dataset size have a decisive influence on the DRA?  We measure DRA by binary accuracy on the answers of the model; in other words, by the proportion of the instances in which the model answered the question about the theory given as context correctly. A detailed account of our experimental setup is presented in <Ref>.\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n"
     ]
    }
   ],
   "source": [
    "print(noinspect_answer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T10:26:33.042646Z",
     "start_time": "2023-11-15T10:26:33.025886Z"
    }
   },
   "id": "58b275ab15bc6851"
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "44699244-de7d-4025-9a18-a26c7d95d79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the potential consequences of using LLMs in high-stakes applications without proper interpretability?'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_qs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "92515cd2-7b9f-4ac1-b84f-b28f1bb22e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((\"The potential consequences of using LLMs in high-stakes applications without proper interpretability include the risk of perpetuating biases and discrimination, the potential for factual mistakes or outdated information, and the reliance on few-shot prompting and frozen language model parameters, which can limit the model's ability to adapt to new information, leading to potential errors or biases in high-stakes applications. For instance, LLMs may still make factual mistakes or contain outdated information, which can be detrimental in applications such as healthcare or finance. Moreover, LLMs can perpetuate biases present in the data they are trained on, which can result in unfair outcomes, particularly in areas like hiring or loan decisions. Additionally, the lack of interpretability in LLMs' decision-making processes can make it challenging to identify and address biases, further contributing to discrimination. Finally, the reliance on few-shot prompting and the freezing of all language model parameters can also limit the model's ability to adapt to new information, leading to potential errors or biases in high-stakes applications.\",\n",
       "  6.50776397515528),\n",
       " (\"The potential consequences of using LLMs in high-stakes applications without proper interpretability are significant. LLMs are commonly used in high-stakes applications such as medical diagnosis, legal document analysis, and financial risk analysis, where the accuracy and reliability of their decisions are critical. However, outdated or incorrect knowledge and memorization issues can negatively impact their decisions, leading to poor predictions and potential privacy breaches. Moreover, the lack of transparency and interpretability in LLMs' decision-making process raises ethical concerns related to their reliability and trustworthiness. Therefore, it is crucial to ensure that LLMs are kept up-to-date and that their knowledge is factually correct. Strategies such as using self-denoising methods, regularly updating and maintaining LLMs, incorporating fact-checking and knowledge updating mechanisms, and ensuring diversity and representativeness in LLM training can help address these challenges. Additionally, it's important to evaluate and update the LLM's performance regularly to ensure that it remains accurate and unbiased over time. By taking these steps, we can ensure that LLMs are used responsibly and ethically in high-stakes applications.\",\n",
       "  155.32262822940515))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer_pair_for_budgets(normal_df, shared_qs[0], 5, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "28a30f2b-ff50-4ec9-8492-d33d75edc33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f1/8sn11n6j4nn19h4w2dflk0ch0000gq/T/ipykernel_4414/3601656701.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"price_diff\"] = df[\"credit_spent\"].apply(lambda c: abs(c - price))\n",
      "/var/folders/f1/8sn11n6j4nn19h4w2dflk0ch0000gq/T/ipykernel_4414/3601656701.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"price_diff\"] = df[\"credit_spent\"].apply(lambda c: abs(c - price))\n"
     ]
    }
   ],
   "source": [
    "question = shared_qs[0]\n",
    "(answer_a, price_a), (answer_b, price_b) = get_answer_pair(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c54bd5e4-4ce1-4742-a9c3-04b3ab06c247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:05:59.990418Z",
     "start_time": "2023-11-15T14:05:59.984656Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_matches_normal_vs_noinspect(questions, budgets, comparison_fn=None):\n",
    "    if comparison_fn is None: \n",
    "        comparison_fn = compare_answers\n",
    "    \n",
    "    verdicts = []\n",
    "    questions_and_budgets = product(questions, budgets)\n",
    "    for question, budget in tqdm(questions_and_budgets):\n",
    "        (answer_a, price_a), (answer_b, price_b) = get_answer_pair(question, budget=budget)\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        print(\"*\" * 50)\n",
    "        print()\n",
    "        print(question)\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Answer A (${price_a:.2f}):\")\n",
    "        print(answer_a)\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Answer B (${price_b:.2f}):\")\n",
    "        print(answer_b)\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "        print()\n",
    "\n",
    "        verdict, debate, gold_answer = comparison_fn(question, answer_a, answer_b, True)\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\"Gold answer:\")\n",
    "        print(gold_answer)\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\"Debate:\")\n",
    "        print(debate)\n",
    "\n",
    "        verdicts.append(\n",
    "            dict(\n",
    "                question=question, \n",
    "                budget=budget,\n",
    "                debate=debate, \n",
    "                gold_answer=gold_answer,\n",
    "                verdict=verdict,\n",
    "                answer_a=answer_a, \n",
    "                answer_b=answer_b,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print()\n",
    "        print(\"*\" * 50)\n",
    "        print(\"*\" * 50)\n",
    "        print()\n",
    "    return verdicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfbaa53f-c4bf-41ad-863d-a50528e9ed2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:20:55.575937Z",
     "start_time": "2023-11-15T14:20:55.568042Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_matches_between_budget_pairs(questions, df, budget_pairs): \n",
    "    verdicts = []\n",
    "    questions_and_budget_pairs = product(questions, budget_pairs)\n",
    "    for question, budget_pair in tqdm(questions_and_budget_pairs):\n",
    "        budget_a, budget_b = budget_pair\n",
    "        (answer_a, price_a), (answer_b, price_b) = get_answer_pair_for_budgets(\n",
    "            df, question, budget_a=budget_a, budget_b=budget_b\n",
    "        )\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        print(\"*\" * 50)\n",
    "        print()\n",
    "        print(question)\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Answer A (${price_a:.2f}):\")\n",
    "        print(answer_a)\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Answer B (${price_b:.2f}):\")\n",
    "        print(answer_b)\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "        print()\n",
    "        \n",
    "        try: \n",
    "            verdict, debate, gold_answer = compare_answers(question, answer_a, answer_b, True)\n",
    "        except Exception as e: \n",
    "            continue\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\"Gold answer:\")\n",
    "        print(gold_answer)\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\"Debate:\")\n",
    "        print(debate)\n",
    "\n",
    "        verdicts.append(\n",
    "            dict(\n",
    "                question=question, \n",
    "                budget_pair=budget_pair,\n",
    "                debate=debate, \n",
    "                gold_answer=gold_answer,\n",
    "                verdict=verdict,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print()\n",
    "        print(\"*\" * 50)\n",
    "        print(\"*\" * 50)\n",
    "        print()\n",
    "    return verdicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93e2340a-e15b-4161-88b5-b889f4998eeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T13:44:55.450252Z",
     "start_time": "2023-11-15T13:44:55.408057Z"
    }
   },
   "outputs": [],
   "source": [
    "# verdicts = run_matches_normal_vs_noinspect(shared_qs, [25], comparison_fn=compare_answers_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# dump_dict(verdicts, \"/Users/nasimrahaman/Python/info-bazaar/notebooks/assets/verdicts_gpt4_heuristic-vs-llama-budget-25.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T10:53:04.338273Z",
     "start_time": "2023-11-15T10:53:04.307435Z"
    }
   },
   "id": "a234267f8c36e752"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# [v[\"verdict\"] for v in verdicts]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T13:45:01.301470Z",
     "start_time": "2023-11-15T13:45:01.279132Z"
    }
   },
   "id": "494cea96a81b592b"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "full_df = pd.concat([noinspect_df, normal_df])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T14:06:03.856556Z",
     "start_time": "2023-11-15T14:06:03.853127Z"
    }
   },
   "id": "d7dae0fac674343d"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f78bc4a-6fa7-46d9-b7d8-9a73e05e7cb5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-15T14:25:28.707494Z",
     "start_time": "2023-11-15T14:21:04.107301Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the difference between continuous prompts and discrete prompts?\n",
      "--------------------------------------------------\n",
      "Answer A ($20.15):\n",
      "Despite the success of PLMs [Devlin19bert, liu2019roberta, raffel2019exploring] in massive NLP tasks, few-shot fine-tuning of PLMs was suboptimal due to the gap between pre-training and downstream tasks. Inspired by the “in context learning” proposed by GPT-3 [Brown20language], stimulating model knowledge with a few prompts has recently received much attention. A series of prompt-based work on knowledge probing [trinh2018simple, Petroni19lama, Davison19common], text classification [schick21pet, gao20making], relation extraction [han2021ptr], and entity typing [ding2021prompt] emerge and achieve impressive progress.  Typically, a piece of prompt contains a template and a verbalizer. Early prompts employ human-picked prompts which demand human knowledge and manual efforts. To alleviate this issue, later works explore automatic designing and optimizing prompts [liu2021gpt, gao20making, zhang2021differentiable]. Recently research works further propose continuous prompts to replace the discrete phrases [lester2021power, LiL2021prefix]. However, the designation of verbalizers, an important part of prompts, is less explored. In this work, we investigate the automatic verbalizer construction in prompt-based tuning.\n",
      "--------------------------------------------------\n",
      "Answer B ($94.14):\n",
      "Despite the success of PLMs [Devlin19bert, liu2019roberta, raffel2019exploring] in massive NLP tasks, few-shot fine-tuning of PLMs was suboptimal due to the gap between pre-training and downstream tasks. Inspired by the “in context learning” proposed by GPT-3 [Brown20language], stimulating model knowledge with a few prompts has recently received much attention. A series of prompt-based work on knowledge probing [trinh2018simple, Petroni19lama, Davison19common], text classification [schick21pet, gao20making], relation extraction [han2021ptr], and entity typing [ding2021prompt] emerge and achieve impressive progress.  Typically, a piece of prompt contains a template and a verbalizer. Early prompts employ human-picked prompts which demand human knowledge and manual efforts. To alleviate this issue, later works explore automatic designing and optimizing prompts [liu2021gpt, gao20making, zhang2021differentiable]. Recently research works further propose continuous prompts to replace the discrete phrases [lester2021power, LiL2021prefix]. However, the designation of verbalizers, an important part of prompts, is less explored. In this work, we investigate the automatic verbalizer construction in prompt-based tuning.\n",
      "---\n",
      "Besides prompting speech generative language models, [DBLP:journals/corr/abs-2305-11095] prompts Whisper [radford2022robust], a supervised speech discriminative model. Whisper is trained on a large amount of paired speech and text data in a multi-task manner. By manually designing task-specific prompts, Whisper demonstrates its zero-shot ability on novel tasks, including audio-visual speech recognition and code-switched speech recognition. However, the effectiveness of prompts in eliciting generation tasks from speech LMs still remains an open question.Large language models (LLMs) have garnered substantial attention for Artificial Intelligence Generated Content (AIGC), particularly with the advent of ChatGPT.  However, there remains a notable disparity between existing text-based LLMs and the goal of achieving AIGC.  The challenge lies in the direct adaptation of continuous speech, to LLMs that primarily process discrete tokens.  This limitation hampers the effective utilization of LLMs for speech generation purposes.\n",
      "---\n",
      "The earliest shared tasks in NLG, such as the  [gatt2009introducing] and  [belz2011discrete] challenges for expression generation, used a continuous scale for scoring, explicitly noting that annotators may not perceive the intervals on a Likert scale to be equal. In contrast, early modeling work—such as the  system for narrative prose generation [callaway2002narrative]—used discrete ratings. This difference in evaluation protocol between shared challenges and individual modeling papers continued over the years. For example, the E2E NLG challenge [duvsek2018findings] used continuous scores based on relative magnitude estimation [novikova2018rankme,bard1996magnitude]. However, these challenges have not served as a bulwark against the popularity of Likert-based OAA. Even recent attempts to standardize human evaluation in NLG—using evaluation platforms—collect Likert ratings [khashabi2021genie,gehrmann2021gem].\n",
      "---\n",
      "What are the benefits and drawbacks of the datasets we used? Using tasks from [bigbench] allowed us to collect a set of diverse, challenging tasks within a similar format. These tasks cover skills which may not be well-represented in standard datasets. However, these idiosyncratic adversarially-sampled tasks may therefore not be representative of tasks to which LMs are usually applied. This difference could potentially amplify or suppress our effects. Explanations might be uniquely helpful in challenging tasks where the relationship between questions and answers is unusual. Alternatively, explanations might be more effective in more standard settings where they could relate more directly to common reasoning patterns. The breadth of tasks we explored, together with statistical analyses that explicitly model task differences, partially ameliorate these concerns. Nevertheless, future work should explore the benefits of explanations beyond BIG-Bench.\n",
      "---\n",
      "Generality and Specialization. An important design choice for multimodal foundation models is the degree of specialization, or the  structural sharing between the modules responsible for each modality. Naturally, data of different domains exhibits diverse kinds of structures and propertieswhere, for instance, language is discrete while vision is continuous. At first sight, this variation hints that specialized inductive biases tailored for each modality could be of aid. Yet, as training scales upwards and models are provided with the opportunity to base their learning less on structural priors and more on the data itself, general approaches that maintain only a handful of broad general assumptions prove in fact a lot more successful than task-specific alternatives. And so, as corroborated by recent success of general-purpose models like transformers across different modalitiesboth linguistic [liu2019roberta,albert] and visual [visual_transformer,ganformer], we see that generality is critical for improving AI capabilities.Multimodal Interactions. Another key consideration for multimodal models relates to weight sharing: do the various modalities benefit from using the same or different parameters for their respective components? Prior works have shown that fruitful transfer could certainly occur across modalities, but the ideal degree of sharing remains unclear, so is the existence of principled ways for discovering it.\n",
      "---\n",
      "Besides our  for one-sentence classification tasks[One exception: MPQA  consists of incomplete sentences, so we adopt manual template without engineering.] and NLI tasks mentioned in Section <ref>, we also design more , always a simple concatenation of input fields and the  token, for other complicated tasks. For BoolQ,  is “x_1? , x_2”. For COPA,  is “x_1 x_2? x_3? , x_4”. For MultiRC,  is “x_2 , x_3 x_1”. For WiC,  is “x_1 x_2 `x_3' ”. Since ReCoRD and WSC can be easily and intuitively transformed into fill-in-the-blank tasks, we follow [schick2020size] and do not process the automatic label sequence generation for them. To make the input closer to  pre-training, we refer to [gao2021making] for the implementation details of prompts.Table <ref> gives the results of using only manual label words with engineering and no templates (so the mask token is concatenated the same way as AutoSeq).This can be seen as the null prompts from  [logan2021cutting].Our results further validate that null prompts perform comparably or even better to manual prompts in most cases.Table <ref> demonstrates all the manual templates and label words adopted by us. We basically follow [logan2021cutting] for these prompts. For the tasks that are not covered by [logan2021cutting], we manually write one prompt for each of them, using only our intuition.Table <ref> gives all the manual templates and label words with careful engineering (gao2021making for GLUE and schick2020size for SuperGLUE) that we use in our experiments.Table <ref> compares the full results of manual prompts with engineering to our AutoSeq. Overall, AutoSeq performs comparably or even better compared with manual prompts, particularly for tasks where developing solid manual prompts is less instinctive (e.g., TREC, QNLI, QQP and COPA).We demonstrate the top 1 automatically generated label sequences before and after re-ranking with contrastive probability for all tasks in Table <ref>. It can be observed that our contrastive probability draws a strong distinction between different classes, especially for those multi-classification tasks like SST-5 and TREC, in which our beam search tends to find the same sequence whatever the class is.Generally speaking, the generated results after re-ranking conform with our intuition in a majority of single and two-sentence tasks. For more complicated ones, such as COPA and WiC, the generated label sequences can be counterintuitive, calling for a more elegant solution in the future.\n",
      "---\n",
      "A neural LM sees the prompt as a sequence of continuous word vectors [baroniDonCountPredict2014].  We tune in this continuous space, relaxing the constraint that the vectors be the embeddings of actual English words.  Allowing “soft prompts” consisting of “soft words” is not only convenient for optimization, but is also more expressive.  Soft prompts can emphasize particular words (by lengthening their vectors) or particular dimensions of those words.  They can also adjust words that are misleading, ambiguous, or overly specific.  Consider the following prompt for the relation :    This prompt may work for the male singer Cab Calloway, but if we want it to also work for the female painter Mary Cassatt, it might help to soften “performed” and “his” so that they do not insist on the wrong occupation and gender, and perhaps to soften “until” into a weaker connective (as Cassatt was in fact too blind to paint in her final years).Another way to bridge between these cases is to have one prompt using “performed” and another using “painted.”  In general, there may be many varied lexical patterns that signal a particular relation, and having more patterns will get better coverage [hearstAutomaticAcquisitionHyponyms1992,riloffLearningDictionariesInformation1999].  We therefore propose to learn a mixture of soft prompts.We test the idea on several cloze language models, training prompts to complete factual and common sense relations from 3 datasets.  Comparing on held-out examples, our method dramatically outperforms previous work, even when initialized randomly.  So when regarded as approximate knowledge bases, language models know more than we realized.  We just had to find the right ways to ask.Factual knowledge is traditionally extracted from large corpora using a pipeline of NLP tools [surdeanuOverviewEnglishSlot2014], including entity extraction [lampleNeuralArchitecturesNamed2016], entity linking [rao2013entity] and relation extraction [sorokinContextAwareRepresentationsKnowledge2017].\n",
      "---\n",
      "Prompt automation & ICL. Numerous methods have been proposed to automate prompt design – also endeavors to achieve so by focusing on ICL, a specific component of the prompt.  Soft prompting methods optimize the embedding space of the LLMs [li-liang-2021-prefix, lester-etal-2021-power] but require gradient access & propagation through massive LLMs and a considerable amount of training data.  Recently, various hard prompting methods, which search for actual discrete tokens using discrete optimization [shin-etal-2020-autoprompt, prasad2022grips, wen2023hard], reinforcement learning [deng-etal-2022-rlprompt, zhang2023tempera] and gradient estimation [diao2022black] have been proposed.  While the discrete prompts are more interpretable and (in some cases) compatible with black-box, inference-only LLMs, to our knowledge none works in the zero-shot setup and tasks beyond  problems (with our definition in <ref>) are scarcely investigated. Furthermore, unlike , these methods also often require hundreds if not thousands of LLM queries before converging to good prompts.  As for ICL, most methods focus on retrieving the best in-context examples from a pool of golden examples instead of zero-shot [rubin-etal-2022-learning, liu-etal-2022-makes]; an exception is AutoCoT which we discuss below.\n",
      "---\n",
      "Prompt automation & ICL. Numerous methods have been proposed to automate prompt design – also endeavors to achieve so by focusing on ICL, a specific component of the prompt.  Soft prompting methods optimize the embedding space of the LLMs [li-liang-2021-prefix, lester-etal-2021-power] but require gradient access & propagation through massive LLMs and a considerable amount of training data.  Recently, various hard prompting methods, which search for actual discrete tokens using discrete optimization [shin-etal-2020-autoprompt, prasad2022grips, wen2023hard], reinforcement learning [deng-etal-2022-rlprompt, zhang2023tempera] and gradient estimation [diao2022black] have been proposed.  While the discrete prompts are more interpretable and (in some cases) compatible with black-box, inference-only LLMs, to our knowledge none works in the zero-shot setup and tasks beyond  problems (with our definition in <ref>) are scarcely investigated. Furthermore, unlike , these methods also often require hundreds if not thousands of LLM queries before converging to good prompts.  As for ICL, most methods focus on retrieving the best in-context examples from a pool of golden examples instead of zero-shot [rubin-etal-2022-learning, liu-etal-2022-makes]; an exception is AutoCoT which we discuss below.\n",
      "---\n",
      "Prompt automation & ICL. Numerous methods have been proposed to automate prompt design – also endeavors to achieve so by focusing on ICL, a specific component of the prompt.  Soft prompting methods optimize the embedding space of the LLMs [li-liang-2021-prefix, lester-etal-2021-power] but require gradient access & propagation through massive LLMs and a considerable amount of training data.  Recently, various hard prompting methods, which search for actual discrete tokens using discrete optimization [shin-etal-2020-autoprompt, prasad2022grips, wen2023hard], reinforcement learning [deng-etal-2022-rlprompt, zhang2023tempera] and gradient estimation [diao2022black] have been proposed.  While the discrete prompts are more interpretable and (in some cases) compatible with black-box, inference-only LLMs, to our knowledge none works in the zero-shot setup and tasks beyond  problems (with our definition in <ref>) are scarcely investigated. Furthermore, unlike , these methods also often require hundreds if not thousands of LLM queries before converging to good prompts.  As for ICL, most methods focus on retrieving the best in-context examples from a pool of golden examples instead of zero-shot [rubin-etal-2022-learning, liu-etal-2022-makes]; an exception is AutoCoT which we discuss below.\n",
      "---\n",
      "* We show that generative LLM detection with a discriminator LLM is impossible if the attacker has access to the reference \"human\" dataset used to train the discriminator LLM. Simply fine-tuning the dataset and using prompts from it leads to a complete failure of the discriminator to learn the difference between machine and human-generated texts, even in a setting where all LLM outputs are correctly labeled.   \n",
      "  * We show that reinforcement from critic generalizes significantly better than previously described when paired with the AdamW optimizer rather than the commonly used Adam one and allows well-generalizing model fine-tunes from limited data, matching prior SotA in normativity fine-tuning.   \n",
      "  * We demonstrate a critical weakness on a previously proposed text-generating GAN architecture - DPGAN, and show the connection of this weakness to the difference in representative power of LLMs and RNNs used in text GANs compatible with the in-the-wild detection setting.Our code is based on a pre-existing Pytorch implementation of common text-generating GANs, including DPGAN, available from <https://github.com/williamSYSU/TextGAN-PyTorch>.\n",
      "---\n",
      "<Ref> shows that replacing person and activity has negligible impact on the accuracy. However, abstracting out the sports abstract drops the performance below levels (<Ref>).  What explains such a discrepancy? Note that both person and activity are already present in the question, and are copied in the thought. Thus, replacing them even with abstract tokens does not harm the performance, as the extraction pattern is retained. On the other hand, the sport is not present in the question, and has to be extracted. Consider a question: Is the following sentence plausible? “Norman Powell committed a blocking foul.” For abstracted sports prompts, the model generates Norman Powell is a SPORT1 player. Committing a blocking foul is part of SPORT2. So the answer is no. Whereas for the abstract person or the activity prompt, the model correctly infers the pattern (copy the person or the activity, and generate the corresponding sport) and generates: Norman Powell is a basketball player. Being a blocking foul is part of basketball. So the answer is yes. This example also clearly drives the notion of symbiosis:  only generating the right pattern or only having the right text. Both are required for effective .Cases where patterns help.As empirical results show, combining patterns with language reveals their true potential. However, our analysis uncovers interesting reasons for the superiority of patterns only, for the small number of cases.Interestingly, we found a few cases where forcing the model to generate some text leads to errors.Primarily, they were caused by the model trying to make faulty connections. Detailed examples are located in <Ref>.<Ref> presents detailed results, including ablations on various parts of the prompt. These were omitted from the main text due to space constraints.0.99     0          The presence of text is necessary for the model to arrive at the right conclusions.          While the model is sufficiently capable of forming abstractions, employing concordant entities in questions and thoughts is crucial for the success of .          The performance of the model is proportionally sensitive to the degree of variations in the text.We test this hypothesis to determine the role of text in the performance of .Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar.Note that the practicality of the counterfactual experiments for text is contingent on the nature of the target task.In addition, because there is no text in , we did not conduct counterfactual experiments in this dataset.<Ref> summarizes the results and shows sample thought for each counterfactual prompt.Text with discrepant entities text_diff_entities.We first investigate the role of employing identical entities (names and objects) between a question and its associated thought.To achieve this, we simply modify the entities in the thoughts to be discrepant with the ones in the questions.is exclusively relevant for the and datasets for which concrete entities exist: objects and people for and players and activities for .In the dataset, we change the player name and the activity in the thought, while retaining the sport in the question.Samples of generated outputs in <Ref> illustrate that in this scenario, the model appears to disregard the prompt, generating arbitrary thoughts and attaining a similar performance as .The task solve rate for with is noticeably better than , while still fails to match the performance of .We delineate samples of generated outputs in <Ref> to glean more insights.\n",
      "---\n",
      "A recent body of work has investigated this problem by training auxiliary models or differentiable representations of the prompt [qin2021learning,deng2022rlprompt]. However, such works assume access to internal state variables of the LLM  [shin2020autoprompt,lester2021power] while practitioners often communicate with LLMs through an API. Other work applies discrete manipulations to prompts via Reinforcement Learning or LLM-based feedback [zhang2023tempera,zhou2022large]. These algorithms may also require low-level access to the LLM, produce incomprehensible outputs, or rely on directionless monte-carlo search over the semantic space of prompts.Accordingly, there is need for automatic or semi-automatic procedures to help humans write the best prompts. This would help reduce manual effort, improve task performance, and produce interpretable descriptions of a cognitive decision process.Large Language Models (LLMs) trained on web-scale text have recently demonstrated unprecedented abilities across a variety of NLP tasks [gpt4,bubeck2023sparks].  These LLMs use prompt inputs to follow human instructions. Writing these natural language prompts remains a manual trial-and-error process requiring significant human effort [jiang2022promptmaker] and expertise [reynolds2021prompt,zamfirescu2023johnny].We propose Automatic Prompt Optimization (APO), a general purpose and nonparametric prompt optimization algorithm that connects these two bodies of research by applying discrete improvements to prompts in a directed way.\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "Continuous prompts and discrete prompts are terms often used in the field of machine learning, particularly in reinforcement learning.\n",
      "\n",
      "Continuous prompts refer to a type of input or action space where the possible values are in a continuous range. For example, adjusting the temperature of a thermostat can be a continuous prompt, as the temperature can be set to any value within a certain range, like 18.5 degrees or 22.7 degrees.\n",
      "\n",
      "On the other hand, discrete prompts refer to a type of input or action space where the possible values are distinct and separate. For example, choosing a channel on a television can be a discrete prompt, as you can only choose from a set number of channels, and there are no in-between values.\n",
      "\n",
      "In the context of AI language models, continuous prompts could refer to a sequence of prompts that are related and follow a certain flow, while discrete prompts could refer to individual, standalone prompts that are not necessarily connected to each other.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: Well, Bobby, it seems neither of the students directly answered the question. They both provided extensive information about prompts in the context of machine learning, but they didn't clearly define the difference between continuous and discrete prompts.\n",
      "\n",
      "Bobby: I agree, Michael. However, if we have to choose, I'd say student A's answer is slightly better. While both answers are almost identical, student A's answer seems to have a bit more structure and coherence, which makes it slightly easier to follow.\n",
      "\n",
      "Michael: I see your point, Bobby, but I think student B's answer is a bit more concise. While both answers are indeed very similar, student B's answer seems to be a bit more to the point, which makes it slightly easier to understand.\n",
      "\n",
      "Bobby: That's a fair point, Michael. However, I still believe that student A's answer is slightly better. It seems to provide a bit more context, which could help the reader understand the topic better.\n",
      "\n",
      "Michael: I understand where you're coming from, Bobby. However, I still believe that student B's answer is slightly better. It seems to be a bit more focused, which could help the reader grasp the main points more easily.\n",
      "\n",
      "Bobby: I see your point, Michael. After considering your arguments, I agree that student B's answer is slightly better. It does seem to be a bit more focused and to the point.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the purpose of pre-training in LLMs?\n",
      "--------------------------------------------------\n",
      "Answer A ($24.97):\n",
      "On our CMATH dataset, we conduct evaluation for a variety of popular LLMs, accessible via commercial API or released model weights. We discover that  GPT-4 is the only model that achieves success (accuracy ≥ 60%) across all six elementary school grades. We also examine the robustness of LLMs against the distracting information. It turns out that GPT-4 is again the sole model that maintains robustness, while other models are easily misled by the presence of distracting information.They require understanding natural language questions and translating them into mathematical expressions/equations. This tests a model's linguistic abilities.They require applying mathematical knowledge and reasoning logically through multiple steps. This tests a model's symbolic manipulation and reasoning skills.They often involve commonsense knowledge about the real world. This tests a model's knowledge grounding and understanding.They come in a range of difficulties, allowing us to probe model abilities at different levels of depth. From grade 1 to 6, the problems become increasingly complex and abstract.* How well can state-of-the-art Chinese language models solve elementary school math word problems? What are their limitations? \n",
      "  * What types of reasoning or math abilities do current models struggle with most? Logical, linguistic, commonsense or mathematical? \n",
      "  * How can we develop models with stronger reasoning and problem-solving skills? What abilities are most important to improve?We release CMATH, the first Chinese dataset for evaluating math reasoning abilities of language models. CMATH contains 1,000 elementary school level math word problems with detailed annotations.We conduct the first systematic evaluation of popular Chinese language models on math word problems. We show how CMATH can reveal differences in the math reasoning skills of language models at various ability levels.We find that although elementary school word problems seem simple, solving them requires sophisticated linguistics, logical and commonsense reasoning that current language models do not possess. CMATH provides a challenging testbed for continued progress in these directions.Our work establishes a first framework for evaluating and enhancing the math reasoning skills of Chinese language models. We release CMATH as a public benchmark to support future work in this direction.0.85! \n",
      "    < g r a p h i c s >\n",
      "  Sample problems along with their English translations (not part of the dataset) and human annotations. The column title “#Steps” and “#Digits” stand for “number of reasoning steps” and “number of digits” respectively.This work is motivated by the following question:  To what grade level of elementary school math do the abilities of popular LLMs correspond?  We create the CMATH dataset in order to  answer this question. We believe that the evaluation results of LLMs should be presented in an intuitive manner, making them easily understandable for the general public.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "What do our results imply about LMs' abilities for in-context learning? The process by which LMs adapt to a prompt is debated. For example, [min2022rethinking] make the interesting observation that GPT-3 can perform nearly as well on common tasks like entailment even with random answers in the prompt, and therefore question whether the models are really “learning” in context. Comparisons between our explanations and matched controls allow us to test whether superficial features of the explanations drive their benefits.\n",
      "---\n",
      "What are the benefits and drawbacks of the datasets we used? Using tasks from [bigbench] allowed us to collect a set of diverse, challenging tasks within a similar format. These tasks cover skills which may not be well-represented in standard datasets. However, these idiosyncratic adversarially-sampled tasks may therefore not be representative of tasks to which LMs are usually applied. This difference could potentially amplify or suppress our effects. Explanations might be uniquely helpful in challenging tasks where the relationship between questions and answers is unusual. Alternatively, explanations might be more effective in more standard settings where they could relate more directly to common reasoning patterns. The breadth of tasks we explored, together with statistical analyses that explicitly model task differences, partially ameliorate these concerns. Nevertheless, future work should explore the benefits of explanations beyond BIG-Bench.\n",
      "---\n",
      "<Ref> shows that replacing person and activity has negligible impact on the accuracy. However, abstracting out the sports abstract drops the performance below levels (<Ref>).  What explains such a discrepancy? Note that both person and activity are already present in the question, and are copied in the thought. Thus, replacing them even with abstract tokens does not harm the performance, as the extraction pattern is retained. On the other hand, the sport is not present in the question, and has to be extracted. Consider a question: Is the following sentence plausible? “Norman Powell committed a blocking foul.” For abstracted sports prompts, the model generates Norman Powell is a SPORT1 player. Committing a blocking foul is part of SPORT2. So the answer is no. Whereas for the abstract person or the activity prompt, the model correctly infers the pattern (copy the person or the activity, and generate the corresponding sport) and generates: Norman Powell is a basketball player. Being a blocking foul is part of basketball. So the answer is yes. This example also clearly drives the notion of symbiosis:  only generating the right pattern or only having the right text. Both are required for effective .Cases where patterns help.As empirical results show, combining patterns with language reveals their true potential. However, our analysis uncovers interesting reasons for the superiority of patterns only, for the small number of cases.Interestingly, we found a few cases where forcing the model to generate some text leads to errors.Primarily, they were caused by the model trying to make faulty connections. Detailed examples are located in <Ref>.<Ref> presents detailed results, including ablations on various parts of the prompt. These were omitted from the main text due to space constraints.0.99     0          The presence of text is necessary for the model to arrive at the right conclusions.          While the model is sufficiently capable of forming abstractions, employing concordant entities in questions and thoughts is crucial for the success of .          The performance of the model is proportionally sensitive to the degree of variations in the text.We test this hypothesis to determine the role of text in the performance of .Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar.Note that the practicality of the counterfactual experiments for text is contingent on the nature of the target task.In addition, because there is no text in , we did not conduct counterfactual experiments in this dataset.<Ref> summarizes the results and shows sample thought for each counterfactual prompt.Text with discrepant entities text_diff_entities.We first investigate the role of employing identical entities (names and objects) between a question and its associated thought.To achieve this, we simply modify the entities in the thoughts to be discrepant with the ones in the questions.is exclusively relevant for the and datasets for which concrete entities exist: objects and people for and players and activities for .In the dataset, we change the player name and the activity in the thought, while retaining the sport in the question.Samples of generated outputs in <Ref> illustrate that in this scenario, the model appears to disregard the prompt, generating arbitrary thoughts and attaining a similar performance as .The task solve rate for with is noticeably better than , while still fails to match the performance of .We delineate samples of generated outputs in <Ref> to glean more insights.\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "--------------------------------------------------\n",
      "Answer B ($96.62):\n",
      "On our CMATH dataset, we conduct evaluation for a variety of popular LLMs, accessible via commercial API or released model weights. We discover that  GPT-4 is the only model that achieves success (accuracy ≥ 60%) across all six elementary school grades. We also examine the robustness of LLMs against the distracting information. It turns out that GPT-4 is again the sole model that maintains robustness, while other models are easily misled by the presence of distracting information.They require understanding natural language questions and translating them into mathematical expressions/equations. This tests a model's linguistic abilities.They require applying mathematical knowledge and reasoning logically through multiple steps. This tests a model's symbolic manipulation and reasoning skills.They often involve commonsense knowledge about the real world. This tests a model's knowledge grounding and understanding.They come in a range of difficulties, allowing us to probe model abilities at different levels of depth. From grade 1 to 6, the problems become increasingly complex and abstract.* How well can state-of-the-art Chinese language models solve elementary school math word problems? What are their limitations? \n",
      "  * What types of reasoning or math abilities do current models struggle with most? Logical, linguistic, commonsense or mathematical? \n",
      "  * How can we develop models with stronger reasoning and problem-solving skills? What abilities are most important to improve?We release CMATH, the first Chinese dataset for evaluating math reasoning abilities of language models. CMATH contains 1,000 elementary school level math word problems with detailed annotations.We conduct the first systematic evaluation of popular Chinese language models on math word problems. We show how CMATH can reveal differences in the math reasoning skills of language models at various ability levels.We find that although elementary school word problems seem simple, solving them requires sophisticated linguistics, logical and commonsense reasoning that current language models do not possess. CMATH provides a challenging testbed for continued progress in these directions.Our work establishes a first framework for evaluating and enhancing the math reasoning skills of Chinese language models. We release CMATH as a public benchmark to support future work in this direction.0.85! \n",
      "    < g r a p h i c s >\n",
      "  Sample problems along with their English translations (not part of the dataset) and human annotations. The column title “#Steps” and “#Digits” stand for “number of reasoning steps” and “number of digits” respectively.This work is motivated by the following question:  To what grade level of elementary school math do the abilities of popular LLMs correspond?  We create the CMATH dataset in order to  answer this question. We believe that the evaluation results of LLMs should be presented in an intuitive manner, making them easily understandable for the general public.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "What do our results imply about LMs' abilities for in-context learning? The process by which LMs adapt to a prompt is debated. For example, [min2022rethinking] make the interesting observation that GPT-3 can perform nearly as well on common tasks like entailment even with random answers in the prompt, and therefore question whether the models are really “learning” in context. Comparisons between our explanations and matched controls allow us to test whether superficial features of the explanations drive their benefits.\n",
      "---\n",
      "What are the benefits and drawbacks of the datasets we used? Using tasks from [bigbench] allowed us to collect a set of diverse, challenging tasks within a similar format. These tasks cover skills which may not be well-represented in standard datasets. However, these idiosyncratic adversarially-sampled tasks may therefore not be representative of tasks to which LMs are usually applied. This difference could potentially amplify or suppress our effects. Explanations might be uniquely helpful in challenging tasks where the relationship between questions and answers is unusual. Alternatively, explanations might be more effective in more standard settings where they could relate more directly to common reasoning patterns. The breadth of tasks we explored, together with statistical analyses that explicitly model task differences, partially ameliorate these concerns. Nevertheless, future work should explore the benefits of explanations beyond BIG-Bench.\n",
      "---\n",
      "<Ref> shows that replacing person and activity has negligible impact on the accuracy. However, abstracting out the sports abstract drops the performance below levels (<Ref>).  What explains such a discrepancy? Note that both person and activity are already present in the question, and are copied in the thought. Thus, replacing them even with abstract tokens does not harm the performance, as the extraction pattern is retained. On the other hand, the sport is not present in the question, and has to be extracted. Consider a question: Is the following sentence plausible? “Norman Powell committed a blocking foul.” For abstracted sports prompts, the model generates Norman Powell is a SPORT1 player. Committing a blocking foul is part of SPORT2. So the answer is no. Whereas for the abstract person or the activity prompt, the model correctly infers the pattern (copy the person or the activity, and generate the corresponding sport) and generates: Norman Powell is a basketball player. Being a blocking foul is part of basketball. So the answer is yes. This example also clearly drives the notion of symbiosis:  only generating the right pattern or only having the right text. Both are required for effective .Cases where patterns help.As empirical results show, combining patterns with language reveals their true potential. However, our analysis uncovers interesting reasons for the superiority of patterns only, for the small number of cases.Interestingly, we found a few cases where forcing the model to generate some text leads to errors.Primarily, they were caused by the model trying to make faulty connections. Detailed examples are located in <Ref>.<Ref> presents detailed results, including ablations on various parts of the prompt. These were omitted from the main text due to space constraints.0.99     0          The presence of text is necessary for the model to arrive at the right conclusions.          While the model is sufficiently capable of forming abstractions, employing concordant entities in questions and thoughts is crucial for the success of .          The performance of the model is proportionally sensitive to the degree of variations in the text.We test this hypothesis to determine the role of text in the performance of .Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar.Note that the practicality of the counterfactual experiments for text is contingent on the nature of the target task.In addition, because there is no text in , we did not conduct counterfactual experiments in this dataset.<Ref> summarizes the results and shows sample thought for each counterfactual prompt.Text with discrepant entities text_diff_entities.We first investigate the role of employing identical entities (names and objects) between a question and its associated thought.To achieve this, we simply modify the entities in the thoughts to be discrepant with the ones in the questions.is exclusively relevant for the and datasets for which concrete entities exist: objects and people for and players and activities for .In the dataset, we change the player name and the activity in the thought, while retaining the sport in the question.Samples of generated outputs in <Ref> illustrate that in this scenario, the model appears to disregard the prompt, generating arbitrary thoughts and attaining a similar performance as .The task solve rate for with is noticeably better than , while still fails to match the performance of .We delineate samples of generated outputs in <Ref> to glean more insights.\n",
      "---\n",
      "width=0.98  2.5*Methods     4cMovieLens 25M     4cAmazon Toys    (lr)2-5(lr)6-9      HR@5      NDCG@5     HR@10      NDCG@10     HR@5      NDCG@5     HR@10      NDCG@10   1-9 P5       0.0688      0.0464      0.1040     0.0577     0.0239     0.0145     0.0411     0.0201  GenRec         0.1034      0.0716      0.1311       0.0837     0.0190      0.0136      0.0251      0.0157      Experimental results on Normalize Discounted Cumulative Gain (NDCG@k) and Hit Ratio (HR@k). Bold numbers represent best performance.Refer to Figure <ref> for an illustration. This figure represents how we utilize a user's history of watched movies as interaction data. Given the prompt, \"Based on the movie viewing habits, what is the most likely movie they will select to watch next?\" and the provided input, we then allow GenRec to forecast the subsequent output.In this paper, we use the LLaMA model as the backbone for the training of GenRec. The LLaMA model is pre-trained on an expansive language corpus, offering a valuable resource for our intended purpose of efficiently capturing both user interests and item content information. However, it's important to note that the memory requirements for GPU to fine-tune LLaMA, even the 7-billion parameter version, are pretty substantial.To circumvent this challenge and conserve GPU memory, we adopt the LLaMA-LoRA architecture for fine-tuning and inference tasks within the scope of this study. By this measure, we have achieved a significant reduction in the GPU memory requirements. With this optimized approach, we can fine-tune the LLaMA-LoRA model on a single GPU with a memory capacity of 24GB.However, in an effort to decrease the overall training time, we have employed a data parallel technique and leveraged multiple GPUs in the experiments. Further details about our experiments, including the implementation and results, will be shared in the following sections of this paper.\n",
      "---\n",
      "We discover repetitive attention patterns given trained language models. One interesting question that needs to be answered is whether such behavior is a model architecture bias or an unexpected training outcome. For such purpose, we perform the same experiment with a randomly initialized OPT, and compare it against the results presented in Section <ref>. As shown in Figure <ref>, the repetitive attention pattern does not exist in randomly initialized models. Apart from an efficiency deployment perspective, could such repetitive attention patterns contribute to some known problem in language generation such as repetitions? It may be worth investigating the relationship between repetitive attention patterns and undesired generations.R8.9cm     \n",
      "          \n",
      "    < g r a p h i c s >\n",
      "               \n",
      "          \n",
      "    < g r a p h i c s >\n",
      "                                         We plot the attention map corresponding to Section <ref> but with a randomly initialized OPT. We observe no repetitive attention for a randomly initialized model.Due to the limitation of the server in academics, the largest model we can fit is OPT-66B. We try to understand the behavior and verify the generality across the different models and datasets. However, we cannot access the training process and fail to know exactly how an LLM is trained to exhibit such behavior. Experiments with the large model create carbon dioxide emissions. However, our work improves the efficiency of LLM, and we  foresee no negative impacts.Inspired by our intriguing findings that pivotal tokens exert a lasting influence on future steps, we developed  to leverage this observation to reduce the memory usage of KV cache. Our method achieves memory reductions of 5× in the KV cache without compromising the performance of LLMs. Furthermore, we demonstrate the compatibility of  with quantization techniques, opening up the possibility of reducing memory usage in both the representation and sequence length dimensions.§ APPENDIXThe problem of sampling and quantizing kv-cache can naturally cast as approximating the output of attentions. We can analyse quantization and sampling separately. A note on sampling : Given the nature of sampling, a closely related problem is that of reservoir sampling. However, the problem is slightly different as the weights associated with the weighted summation change with addition of new tokens which opens up new avenues for variance reduction. We start with simple abstract sampling design with E(S)=1.1. Any sampling design that ensures E(S_i) = 1 is a good to get almost unbiased estimates of attention2. Variance reduction of denominator ( also numerator)  requires sampling with probability proportional to weights.  While the problem of weighted reservoir sampling is well understood, the problem of dynamic weights is rather understudied (verify) and often ifeasible.3. We give a reservior sampling for dynamic weights with weight evolution assumptions motivated from the problem at hand. (π(k|j)  π(k|j-1).  The algorithm and proof are straightfroward.4. Proving / intuiton as to why the assumption holds. Either jaspers proof can be modified for this or can be used as a intuition.  Also, we can give empirical evidence.We can thus cover the sampling part under standard (modifed) sampling theory and the sections can be made rigorous with notations borrowed from the sampling literature.Quantisation can be analysed separately. Although it deviates from the central storyIn this section we will talk about reduction of kv-cache for a particular attention head given a budget of tokens say n. The problem can be interpreted as sampling problem with the target to get an accurate estimate of the output of attention head at token, say t+1. The computation in attention can be written as,\n",
      "---\n",
      "GPT-3 [brown2020gpt3] demonstrates the power of in-context learning, an adaptation method that does not need any parameter optimization. In the adaptation phase, the pretrained language foundation model takes in a prompta sequence of tokens that concatenates input-output examples from the taskfollowed by a test example and simply generates the label of the test example by conditioning on the sequence seen thus far (prompt plus test example).  In other words, there is no explicit training or change to the model parameters.   What is the mechanism by which the model “learns” from the different examples by simply executing with the examples as inputs? The previous modularization does not directly apply because we do not obtain new model parameters during adaptation, but rather we only use the generative capabilities of the foundation model by executing on structurally-designed inputs.  However, the idea of separating pretraining with infinite data and pretraining with finite data can still be useful. For example, a recent work starts with the assumption of infinite pretraining data and sufficient model expressivity to study in-context learning [xie2021incontext]. These assumptions reduce the characterization of in-context learning to a matter of analyzing the pretraining distribution conditioned on in-context learning prompts, which are drawn from a different distribution than the pretraining data. In particular, [xie2021incontext] proposes that in-context learning emerges from long-term coherence structure in the pretraining distribution, which is described by a latent variable generative model with coherence structure.  More broadly, while the modularization proposed in this section provides a nice framework to gain useful theoretical insights into foundation models, it is possible that some emergent behavior like in-context learning and other capabilities yet to be discovered would require going beyond the modularization, by opening the black box of the architecture.theory-tools\n",
      "---\n",
      "While our model demonstrates effectiveness, it is important to acknowledge the risks associated with relying solely on clinical outcome prediction models. There are crucial pieces of information that can be found beyond the scope of clinical notes. Considering the potential impact on patient health outcomes, it is crucial to exercise caution when utilising these clinical LLMs. Therefore, we propose that the PEFT adapter generated by our framework, in conjunction with the pretrained LLM, should be used as an aid rather than a replacement for trained clinical professionals.AcknowledgementsAG was supported by the United Kingdom Research and Innovation (grant EP/S02431X/1), UKRI Centre for Doctoral Training in Biomedical AI at the University of Edinburgh, School of Informatics.PM was partially funded by the European Union’s Horizon 2020 research and innovation programme under grant agreement no. 875160, ELIAI (The Edinburgh Laboratory for Integrated Artificial Intelligence) EPSRC (grant no. EP/W002876/1), an industry grant from Cisco, and a donation from Accenture LLP; and is grateful to NVIDIA for the GPU donations.BA was partially funded by Legal and General PLC as part of the Advanced Care Research Centre and by the Artificial Intelligence and Multimorbidity: Clustering in Individuals, Space and Clinical Context (AIM-CISC) grant NIHR202639.For the purpose of open access, AG has applied a creative commons attribution (CC BY) licence to any author-accepted manuscript version arising.This work was supported by the Edinburgh International Data Facility (EIDF) and the Data-Driven Innovation Programme at the University of Edinburgh.acl_natbibHyperparameter     Value           Learning rate     3e-4           Warmup steps ratio     0.06           Maximum sequence length     128           Gradient accumulation step     4           Batch size     10                Fixed model hyperparameters for language modelling pretraining. These hyperparameters remain unchanged to fit LLaMA into a single GPU.\n",
      "---\n",
      "Code generation models can overcome some of these challenges  Despite these struggles, the recent success of large-language models of code [chen2021evaluating,austin2021program,wang2021codet5,xu2022systematic,fried2022incoder] show that the formulation of structured commonsense generation as a text-to-text problem is indeed promising. LLMs of code are able to perform complex reasoning in structured text, if pre-trained on the right data. A key difference in the two setups is the availability of a large pre-training corpus:  while structured commonsense generation models are fine-tuned on relatively small training data for isolated tasks, code generation models leverage a large open-source corpus of code for pre-training. Combined with a large model architecture, code generation models trained in this fashion learn to generate code in various languages and show impressive results on challenging tasks like text-to-code generation. Rephrashing: In other words, instead of forcing LLMs of natural language to be fine-tuned on structured commonsense data, an easier way to close the gap between the pre-training data (free-form text) and the task-specific data (commonsense reasoning) would be to adapt LLMs that were pre-trained on code to structured commonsense reasoning.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "CodexDB is enabled by recent advances in the domain of natural language processing. Those advances have been fuelled by two key ideas: a novel neural network architecture, the Transformer [Vaswani2017], and new training paradigms, implementing the idea of transfer learning [ruder2019transfer]. The Transformer is nowadays the dominant architecture in the domain of language processing [Wolf2020]. Among other advantages, it lends itself better to parallelization than prior methods. This has, in part, enabled the creation of very large, pre-trained language models. Such models are pre-trained on tasks for which large amounts of training data are easily available, e.g. predicting the next word in text snippets. While pre-training is very expensive, the resulting models can be easily specialized for new tasks via different methods. Fine-tuning describes a process in which pre-trained models are used as a starting point for further training on more specialized tasks (reducing the amount of training samples and computational overheads by orders of magnitude via pre-training [Houlsby2019]). Until recently, fine-tuning has been the primary method of exploiting pre-trained language models. The latest generation of pre-trained models, most notably OpenAI's Generative Pre-Trained Transformer (GPT) version 3, unlocks new possibilities. It turns out that sufficiently large models can oftentimes solve new tasks without specialized training (“zero-shot learning”), based on inputs describing the task in natural language alone [Brown2020]. Precision increases if the input integrates few (i.e., typically less than ten) examples pairing tasks of the same type with solutions (“few-shot learning”). This is the method currently used by CodexDB. The final development that enabled this paper is the emergence of the Codex variant of GPT-3 [Chen2021, OpenAI2021]. The primary difference between GPT-3 Codex and the original GPT-3 model lies in the data used for pre-training. GPT-3 Codex is trained on code and technical documentation. This results in a model whose primary use case is the translation of natural language commands into code.\n",
      "---\n",
      "Existing work in NLP primarily investigates using explanations for labeled outputs to classification tasks. In contrast, we do not assume access to gold-labeled outputs, and we study the more general text generation setting, which classification tasks can be formulated as [Radford2019LanguageMA,raffel2020exploring,brown2020language]. Explanations describe why a labeled output is correct, while feedback describes how to improve a candidate output. Prior work explores ways of using explanations to train text classification models, with mixed results [camburu2018snli,stacey2021natural,pruthi2021evaluating,wiegreffe-etal-2021-measuring,hase2021can, lampinen2022can]. A few prior works also learn from language feedback, for the purpose of ranking candidate outputs rather than generating outputs [weston2016dialog, li2016dialogue, hancock2019learning,li2022using]. [matiana2021cut] learn text embeddings of language feedback, where improvements could benefit the refinement-scoring step of our algorithm.\n",
      "---\n",
      "Negation. Despite improvements of LLMs in many NLP tasks such as natural language understanding, reading comprehension, zero-shot text generation, negation remains a challenge for pre-trained LLMs [kassner2019negated,hosseini-etal-2021-understanding]. Data augmentation has been used to tackle negation in the NLP literature. For example, modification of the natural language understanding corpora by adding negation to the verb or adjective and reversing the labels was proposed in [hossain-etal-2020-analysis], and an unlikelihood loss for  the span corruption pre-training tasks was proposed in [hosseini-etal-2021-understanding]. Negation is also addressed in the meta-learning literature [murty-etal-2021-dreca], where it is treated as one of the reasoning categories that requires additional few-shot classification tasks to augment the original task distribution.\n",
      "---\n",
      "Past work has characterized what kinds of functions ICL can learn [Garg2022WhatCT, laskin2022context, muller2021transformers] and the distributional properties of pretraining that can elicit in-context learning [xie2021explanation, chan2022data]. But how ICL learns these functions has remained unclear. What learning algorithms (if any) are implementable by deep network models? Which algorithms are actually discovered in the course of training?  This paper takes first steps toward answering these questions, focusing on a widely used model architecture (the transformer) and an extremely well-understood class of learning problems (linear regression).\n",
      "---\n",
      "A recent body of work has investigated this problem by training auxiliary models or differentiable representations of the prompt [qin2021learning,deng2022rlprompt]. However, such works assume access to internal state variables of the LLM  [shin2020autoprompt,lester2021power] while practitioners often communicate with LLMs through an API. Other work applies discrete manipulations to prompts via Reinforcement Learning or LLM-based feedback [zhang2023tempera,zhou2022large]. These algorithms may also require low-level access to the LLM, produce incomprehensible outputs, or rely on directionless monte-carlo search over the semantic space of prompts.Accordingly, there is need for automatic or semi-automatic procedures to help humans write the best prompts. This would help reduce manual effort, improve task performance, and produce interpretable descriptions of a cognitive decision process.Large Language Models (LLMs) trained on web-scale text have recently demonstrated unprecedented abilities across a variety of NLP tasks [gpt4,bubeck2023sparks].  These LLMs use prompt inputs to follow human instructions. Writing these natural language prompts remains a manual trial-and-error process requiring significant human effort [jiang2022promptmaker] and expertise [reynolds2021prompt,zamfirescu2023johnny].We propose Automatic Prompt Optimization (APO), a general purpose and nonparametric prompt optimization algorithm that connects these two bodies of research by applying discrete improvements to prompts in a directed way.\n",
      "---\n",
      "Besides our  for one-sentence classification tasks[One exception: MPQA  consists of incomplete sentences, so we adopt manual template without engineering.] and NLI tasks mentioned in Section <ref>, we also design more , always a simple concatenation of input fields and the  token, for other complicated tasks. For BoolQ,  is “x_1? , x_2”. For COPA,  is “x_1 x_2? x_3? , x_4”. For MultiRC,  is “x_2 , x_3 x_1”. For WiC,  is “x_1 x_2 `x_3' ”. Since ReCoRD and WSC can be easily and intuitively transformed into fill-in-the-blank tasks, we follow [schick2020size] and do not process the automatic label sequence generation for them. To make the input closer to  pre-training, we refer to [gao2021making] for the implementation details of prompts.Table <ref> gives the results of using only manual label words with engineering and no templates (so the mask token is concatenated the same way as AutoSeq).This can be seen as the null prompts from  [logan2021cutting].Our results further validate that null prompts perform comparably or even better to manual prompts in most cases.Table <ref> demonstrates all the manual templates and label words adopted by us. We basically follow [logan2021cutting] for these prompts. For the tasks that are not covered by [logan2021cutting], we manually write one prompt for each of them, using only our intuition.Table <ref> gives all the manual templates and label words with careful engineering (gao2021making for GLUE and schick2020size for SuperGLUE) that we use in our experiments.Table <ref> compares the full results of manual prompts with engineering to our AutoSeq. Overall, AutoSeq performs comparably or even better compared with manual prompts, particularly for tasks where developing solid manual prompts is less instinctive (e.g., TREC, QNLI, QQP and COPA).We demonstrate the top 1 automatically generated label sequences before and after re-ranking with contrastive probability for all tasks in Table <ref>. It can be observed that our contrastive probability draws a strong distinction between different classes, especially for those multi-classification tasks like SST-5 and TREC, in which our beam search tends to find the same sequence whatever the class is.Generally speaking, the generated results after re-ranking conform with our intuition in a majority of single and two-sentence tasks. For more complicated ones, such as COPA and WiC, the generated label sequences can be counterintuitive, calling for a more elegant solution in the future.\n",
      "---\n",
      "Increasing transparency. With more transparency, the general public  can be better engaged into  the global efforts to address the growing water challenges. For example, water-conscious users may prefer to use the inference services of AI models during water-efficient hours, but  currently, the lack of transparency about AI models' detailed water efficiency prohibits them from doing so.  Additionally, being informed of the data center's runtime water efficiency, AI model developers can better schedule their model training and choose locations for the deployment of trained models.We recommend AI model developers and data center operators be more transparent. For example, what are the runtime (say, hourly) on-site WUE and off-site WUE? When and where are the AI models trained? What about the AI models trained and/or deployed in third-party colocation data centers or public clouds? Such information will be of great value to the research community and the general public.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 109, in run\n",
      "    await self.visit(self.parse_tree, VariableStack([self.program._variables], self))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 524, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_assistant.py\", line 13, in assistant\n",
      "    return await role(role_name=\"assistant\", hidden=hidden, _parser_context=_parser_context, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_role.py\", line 17, in role\n",
      "    new_content += await parser.visit(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in visit\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in <listcomp>\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 379, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_gen.py\", line 140, in gen\n",
      "    gen_obj = await parser.llm_session(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 665, in __call__\n",
      "    out = await self.llm.caller(**call_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 370, in _library_call\n",
      "    out = await openai.ChatCompletion.acreate(**kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 728, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 9738 tokens. Please reduce the length of the messages.\n",
      "\n",
      "Error in program:  This model's maximum context length is 8192 tokens. However, your messages resulted in 9738 tokens. Please reduce the length of the messages.\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the relationship between large language models and neurosymbolic reasoning?\n",
      "--------------------------------------------------\n",
      "Answer A ($21.78):\n",
      "On our CMATH dataset, we conduct evaluation for a variety of popular LLMs, accessible via commercial API or released model weights. We discover that  GPT-4 is the only model that achieves success (accuracy ≥ 60%) across all six elementary school grades. We also examine the robustness of LLMs against the distracting information. It turns out that GPT-4 is again the sole model that maintains robustness, while other models are easily misled by the presence of distracting information.They require understanding natural language questions and translating them into mathematical expressions/equations. This tests a model's linguistic abilities.They require applying mathematical knowledge and reasoning logically through multiple steps. This tests a model's symbolic manipulation and reasoning skills.They often involve commonsense knowledge about the real world. This tests a model's knowledge grounding and understanding.They come in a range of difficulties, allowing us to probe model abilities at different levels of depth. From grade 1 to 6, the problems become increasingly complex and abstract.* How well can state-of-the-art Chinese language models solve elementary school math word problems? What are their limitations? \n",
      "  * What types of reasoning or math abilities do current models struggle with most? Logical, linguistic, commonsense or mathematical? \n",
      "  * How can we develop models with stronger reasoning and problem-solving skills? What abilities are most important to improve?We release CMATH, the first Chinese dataset for evaluating math reasoning abilities of language models. CMATH contains 1,000 elementary school level math word problems with detailed annotations.We conduct the first systematic evaluation of popular Chinese language models on math word problems. We show how CMATH can reveal differences in the math reasoning skills of language models at various ability levels.We find that although elementary school word problems seem simple, solving them requires sophisticated linguistics, logical and commonsense reasoning that current language models do not possess. CMATH provides a challenging testbed for continued progress in these directions.Our work establishes a first framework for evaluating and enhancing the math reasoning skills of Chinese language models. We release CMATH as a public benchmark to support future work in this direction.0.85! \n",
      "    < g r a p h i c s >\n",
      "  Sample problems along with their English translations (not part of the dataset) and human annotations. The column title “#Steps” and “#Digits” stand for “number of reasoning steps” and “number of digits” respectively.This work is motivated by the following question:  To what grade level of elementary school math do the abilities of popular LLMs correspond?  We create the CMATH dataset in order to  answer this question. We believe that the evaluation results of LLMs should be presented in an intuitive manner, making them easily understandable for the general public.\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "What are the benefits and drawbacks of the datasets we used? Using tasks from [bigbench] allowed us to collect a set of diverse, challenging tasks within a similar format. These tasks cover skills which may not be well-represented in standard datasets. However, these idiosyncratic adversarially-sampled tasks may therefore not be representative of tasks to which LMs are usually applied. This difference could potentially amplify or suppress our effects. Explanations might be uniquely helpful in challenging tasks where the relationship between questions and answers is unusual. Alternatively, explanations might be more effective in more standard settings where they could relate more directly to common reasoning patterns. The breadth of tasks we explored, together with statistical analyses that explicitly model task differences, partially ameliorate these concerns. Nevertheless, future work should explore the benefits of explanations beyond BIG-Bench.\n",
      "---\n",
      "Q1: How does the deductive reasoning ability (DRA) of large language models evolve with their growing size? We find that with the training setup and model architecture kept constant, larger model size leads to higher DRA, but also that this effect can often be compensated by choices made in the training configuration. Previous work has shown that the DRA of models fine-tuned for deduction decreases on data that requires longer chains of reasoning that have been seen in training. <Ref> shows that this does not seem to be the case for foundation models as we evaluated them, that the performance of many models on deeper questions is similar to that on shallower questions, and that it sometimes even increases, especially for OpenAI GPT-1/2 models.Q2: How does the DRA of general-purpose generative large language models depend the number of deductive steps required to reach the conclusions? For depths considered in our study, it remains largely similar throughout, except for the OpenAI GPT-3 and 3.5 models, for whom the performance decreases as seen in <Ref>.\n",
      "---\n",
      "With the advancement of highly parallelizable transformer architectures, pre-trained language models (PLMs) have demonstrated remarkable capabilities in comprehending, generating, and manipulating natural language [qiu2020pre, min2021recent]. These models were pre-trained on a large corpora of unlabeled text data and commonly subsequently fine-tuned for specific downstream tasks. Shortly, the scaled-up PLMs, known as large language models (LLMs) [raffel2020exploring, brown2020language, ouyang2022training, chowdhery2022palm, zhang2022opt, touvron2023llama], encompassed a substantially greater number of parameters and leverage vast amounts of training data. Consequently, LLMs exhibited enhanced capacity for learning intricate language patterns and structures, along with a notable reasoning ability. This results in superior performance across diverse natural language processing tasks [brown2020language, touvron2023llama]. Apart from the above superiority, LLMs may occasionally produce seemingly plausible yet inaccurate predictions and face challenges when addressing problems that require specialized domain expertise [mialon2023augmented, shen2023hugginggpt]. Consequently, the emerging field of Augmented Language Models (ALMs) focuses on addressing the limitations of conventional LLMs [chung2022scaling,chowdhery2022palm,brown2020language] by equipping them with enhanced reasoning capabilities and the ability to employ external resources [mialon2023augmented]. The process of reasoning involves breaking down intricate assignments into smaller, more manageable subtasks that can be independently or collaboratively tackled by LLMs with the assistance of tools. What's more, LLMs can also invoke external tools or models to accomplish the relevant tasks. For exmaple, ToolFormer [shen2023hugginggpt] introduces external API tags within text sequences, facilitating LLMs’ access to external tools. Visual ChatGPT [wu2023visual] is a new model that combines ChatGPT with Visual Foundation Models (VFMs) such as Transformers, ControlNet, and Stable Diffusion, which acts as a bridge between users, allowing them to communicate via chat and generate visuals. HuggingGPT [schick2023toolformer] integrates the Hugging Face hub with task-specific models around ChatGPT to tackle generalized AI tasks.  Augmented language models may use these enhancements separately or joint them in a specific order to finish the specific task, which ultimately results in superior generalization capabilities.\n",
      "---\n",
      "Q3: Do the specifics of a training setup, such as the model variant, dataset multilingualism, or dataset size have a decisive influence on the DRA? Yes. As noted for Q1, the differences between model training setups in many cases play a larger role than the size of the model. An example outlier is Bloom-560m, which comfortably outperforms all larger Bloom models, and Bloom setting as a whole, which in turn outperforms much larger OPT and GPT-2 XL models. This is perhaps due to the multi-lingual training setup (including multiple programming languages) of Bloom models, which is in contrast to natural language English used for GPT-1/2 and OPT models.In summary, we conclude that all our initial hypotheses were partially incorrect, and that the subject of deductive reasoning ability in large language models warrants further examination from a wider set of angles.Our work only considers the token probabilities directly as provided by the generative models. Since the models are trained with generation of patches of text in mind, it can be argued that the model might in fact be trying to output a verbose answer. We attempt to to mitigate this by priming the model, but higher accuracy could perhaps be achieved by fine-tuning the language modelling head.acl_natbib\n",
      "--------------------------------------------------\n",
      "Answer B ($87.11):\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "What are the benefits and drawbacks of the datasets we used? Using tasks from [bigbench] allowed us to collect a set of diverse, challenging tasks within a similar format. These tasks cover skills which may not be well-represented in standard datasets. However, these idiosyncratic adversarially-sampled tasks may therefore not be representative of tasks to which LMs are usually applied. This difference could potentially amplify or suppress our effects. Explanations might be uniquely helpful in challenging tasks where the relationship between questions and answers is unusual. Alternatively, explanations might be more effective in more standard settings where they could relate more directly to common reasoning patterns. The breadth of tasks we explored, together with statistical analyses that explicitly model task differences, partially ameliorate these concerns. Nevertheless, future work should explore the benefits of explanations beyond BIG-Bench.\n",
      "---\n",
      "With the advancement of highly parallelizable transformer architectures, pre-trained language models (PLMs) have demonstrated remarkable capabilities in comprehending, generating, and manipulating natural language [qiu2020pre, min2021recent]. These models were pre-trained on a large corpora of unlabeled text data and commonly subsequently fine-tuned for specific downstream tasks. Shortly, the scaled-up PLMs, known as large language models (LLMs) [raffel2020exploring, brown2020language, ouyang2022training, chowdhery2022palm, zhang2022opt, touvron2023llama], encompassed a substantially greater number of parameters and leverage vast amounts of training data. Consequently, LLMs exhibited enhanced capacity for learning intricate language patterns and structures, along with a notable reasoning ability. This results in superior performance across diverse natural language processing tasks [brown2020language, touvron2023llama]. Apart from the above superiority, LLMs may occasionally produce seemingly plausible yet inaccurate predictions and face challenges when addressing problems that require specialized domain expertise [mialon2023augmented, shen2023hugginggpt]. Consequently, the emerging field of Augmented Language Models (ALMs) focuses on addressing the limitations of conventional LLMs [chung2022scaling,chowdhery2022palm,brown2020language] by equipping them with enhanced reasoning capabilities and the ability to employ external resources [mialon2023augmented]. The process of reasoning involves breaking down intricate assignments into smaller, more manageable subtasks that can be independently or collaboratively tackled by LLMs with the assistance of tools. What's more, LLMs can also invoke external tools or models to accomplish the relevant tasks. For exmaple, ToolFormer [shen2023hugginggpt] introduces external API tags within text sequences, facilitating LLMs’ access to external tools. Visual ChatGPT [wu2023visual] is a new model that combines ChatGPT with Visual Foundation Models (VFMs) such as Transformers, ControlNet, and Stable Diffusion, which acts as a bridge between users, allowing them to communicate via chat and generate visuals. HuggingGPT [schick2023toolformer] integrates the Hugging Face hub with task-specific models around ChatGPT to tackle generalized AI tasks.  Augmented language models may use these enhancements separately or joint them in a specific order to finish the specific task, which ultimately results in superior generalization capabilities.\n",
      "---\n",
      "Outlier Features in Language Models. Large magnitude outlier features in language models have been studied before [timkey2021all,bondarenko2021understanding, wei2022outlier,luo-etal-2021-positional]. Previous work proved the theoretical relationship between outlier appearance in transformers and how it relates to layer normalization and the token frequency distribution [gao2019representation]. Similarly, [kovaleva2021bert] attribute the appearance of outliers in BERT model family to LayerNorm, and [puccetti2022outliers] show empirically that outlier emergence is related to the frequency of tokens in the training distribution. We extend this work further by showing how the scale of autoregressive models relates to the emergent properties of these outlier features, and showing how appropriately modeling outliers is critical to effective quantization.\n",
      "---\n",
      "Q1: How does the deductive reasoning ability (DRA) of large language models evolve with their growing size? We find that with the training setup and model architecture kept constant, larger model size leads to higher DRA, but also that this effect can often be compensated by choices made in the training configuration. Previous work has shown that the DRA of models fine-tuned for deduction decreases on data that requires longer chains of reasoning that have been seen in training. <Ref> shows that this does not seem to be the case for foundation models as we evaluated them, that the performance of many models on deeper questions is similar to that on shallower questions, and that it sometimes even increases, especially for OpenAI GPT-1/2 models.Q2: How does the DRA of general-purpose generative large language models depend the number of deductive steps required to reach the conclusions? For depths considered in our study, it remains largely similar throughout, except for the OpenAI GPT-3 and 3.5 models, for whom the performance decreases as seen in <Ref>.\n",
      "---\n",
      "Q3: Do the specifics of a training setup, such as the model variant, dataset multilingualism, or dataset size have a decisive influence on the DRA? Yes. As noted for Q1, the differences between model training setups in many cases play a larger role than the size of the model. An example outlier is Bloom-560m, which comfortably outperforms all larger Bloom models, and Bloom setting as a whole, which in turn outperforms much larger OPT and GPT-2 XL models. This is perhaps due to the multi-lingual training setup (including multiple programming languages) of Bloom models, which is in contrast to natural language English used for GPT-1/2 and OPT models.In summary, we conclude that all our initial hypotheses were partially incorrect, and that the subject of deductive reasoning ability in large language models warrants further examination from a wider set of angles.Our work only considers the token probabilities directly as provided by the generative models. Since the models are trained with generation of patches of text in mind, it can be argued that the model might in fact be trying to output a verbose answer. We attempt to to mitigate this by priming the model, but higher accuracy could perhaps be achieved by fine-tuning the language modelling head.acl_natbib\n",
      "---\n",
      "What do our results imply about LMs' abilities for in-context learning? The process by which LMs adapt to a prompt is debated. For example, [min2022rethinking] make the interesting observation that GPT-3 can perform nearly as well on common tasks like entailment even with random answers in the prompt, and therefore question whether the models are really “learning” in context. Comparisons between our explanations and matched controls allow us to test whether superficial features of the explanations drive their benefits.\n",
      "---\n",
      "*           We experiment with  [cobbe2021training] (1319 samples).          The dataset contains math word problems geared toward an average middle-school curriculum.          \n",
      "  *           We use date understanding (, 349 samples) and sports understanding (, 980 samples) as representative tasks for commonsense reasoning, both derived from [date:sport].          \n",
      "  *           We experiment with sorting (, 500 samples) a list of single-digit integers.           We do not associate explicit instruction (sort these numbers) with the questions.          Instead, we frame the questions as a challenging setup in which the model should figure out the task and the requisite information to solve it.In this work, we neither train any of the \\palm models, nor performs finetuning.We solely perform inference on \\palm variants using TPU v4~[tpuv4,jouppi2017datacenter].For \\mpalm, we use 4$\\times$4$\\times$4 TPU v4 configuration, whereas, for \\lpalm we use 4$\\times$4$\\times$16 mesh configuration.To account for the variation in results caused by the order of examples in the prompt, we conduct each experiment three times, each with different seeds, and report the average task solve rate.Following [wei2022chain], we evaluate each task using accuracy \\ie fraction of examples where the output matched the expected result.\\niparagraph{Public large language models.}We use OpenAI API\\footnote{\\href{https://openai.com/blog/openai-api/}{https://openai.com/blog/openai-api/}, v0.23.0} to conduct experiments with \\gptt (\\bench{text-davinci-002}) and \\codex (\\bench{code-davinci-002}).\\input{tables/task_examples}Based on our initial findings, this section explores the idea of engineering effective prompts tailored to remove ineffectual tokens.The benefit of such tailored design is multi-fold: (1) “reducing noise”:reducing noise from prompts could potentially lessen the confusion for the model and lead to a higher task solve rate and(2) “potential energy savings”:the reduced number of tokens in the prompts instruct the model to generate less number of tokens per question.Less number of generated tokens directly translates to reduced runtime and carbon footprint per inference [strubell2019energy].This is especially crucial because of the significant compute cost of large language models.Our proposed prompts, dubbed as , distill the essence of our hypothesis by supplying minimal requisite information to drive the model to factual conclusions.<Ref> compares and across different tasks and four variants of s.In all tasks, except , we use identical examples in the prompt and only rephrased the thoughts to use less number of tokens.For , we could not find a systematic mechanism to shorten the thoughts.Instead, we randomly harvest questions from the training set whose thoughts are shorter than .Overall, outperforms while employing prompts with fewer tokens.The task solve rate of remains relatively high as we scale the model to , highlighting the efficiency of .Appendix-<Ref> compares the average number of input/output tokens between and .On average, (our approach) reduces the number of input (1.39×) and output tokens (1.58×).This work evaluates the capacity of to elevate complex reasoning in three state-of-the-arts s, , , and .We systematically assembled a series of controlled counterfactual experiments.Our results show the initial inklings of connection between text, patterns, and reasoning in s.Our study indicates that the symbiosis of text and patterns bears more weight in the chain of thought reasoning process.In addition, we assert that text is a channel to extract semantic patterns, unlocking the ability of these models to mold correct answers.Large language models.To facilitate conducting an exhaustive number of experiments, we center the main analysis of this paper around [The results in [wei2022chain] reinforce that is the smallest model with significant gains across a range of reasoning tasks.].For reproducibility, we also conduct the experiments on publicly available models such as and .We present results from in the main body and defer the results from other models to <Ref>.Nonetheless, our findings concur across the studied s.Reasoning tasks.We focus on reasoning tasks for which presents ample improvements over prompting [wei2022chain], namely  ( [cobbe2021training]),  (date and sports understanding [date:sport]), and  ()—details in <Ref> and Appendix-<Ref>.This work intends to tease apart the major semantic components of a prompt that play a critical role in the efficacy of .To achieve this, we identify and systematically construe three key semantic components of a prompt, listed as follows (See Appendix-<Ref> for the examples to which we refer in the definitions):* are sequences of tokens in the prompt, about which the model reasons to solve a task.For and , symbols are numerical quantities (5, 4, 2, 13).Similarly for the dataset, we categorize players and activities as symbols. We define the symbols in the dataset as date and time indicating expressions.* are either composition of symbols and operators or a structure of prompt that reinforces task understanding.The isolation of patterns within a prompt is evident in datasets like (equations), (person is a sport, activity is a sport), and (1 less than 2).However, for the dataset, the pattern is semantically implicit but consistent.Each thought contains two parts: (a) ⟨calculation⟩ in which the information from the input is restated (“Today is 06/02/1943”) and intermediate results are generated (“One day after 06/01/1943 is 06/02/1943”) through mathematical calculations and (b) ⟨output⟩ in which the final requisite answer is generated using the intermediate results (“10 days before today is 05/23/1943”).* are tokens that are neither symbols, nor part of patterns.Specifically, text in prompts assists in eitheroutlining the target task (is the sentence plausible),connecting patterns to symbols (John is left with 4 - 2 = 2), or contextualizing symbols (4 toys).In a nutshell, text is the conceptual glue that binds different parts of a prompt.Relying on our analysis, we conjecture and hypothesize about the effects of each semantic components on the outcomes of prompting.We discuss detailed results, including statistical significance tests in <Ref>.The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence [lake2017building].This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data.Recently, large language models (s) [palm:arxiv,brown2020language,codex] have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning.Few-shot learning has shown promising applications for a wide range of tasks [gem,flan,bigscience,lamda,liu_what_2021,reif2021recipe,wang2020generalizing,chen_evaluating_2021,minerva,wu2022autoformalization].While beneficial, this setting requires meticulous design of prompts [le2021many,liu2021pre,mishra_reframing_2021].[ling2017program] pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning.Recently, [wei2022chain] proposed chain of thought () prompting, showing that the few-shot setting in s similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks [ling2017program,cobbe2021training,svamp,date:sport].Despite its wide-range usage, the rationale behind the success of remains unclear.Recent work draws [ling2017program,wei2022chain] parallels to human thinking.Humans often think about a problem before deducing a solution.Akin to this process, it is argued that models should also be able to employ a similar mechanism.While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate.Ultimately, s are trained to estimate the next token distribution for a given context.Therefore, there is presumably a systematic rationale behind their successes and failures.In this work, we undertake initial steps towards understanding the mechanism behind .Contributions and findings.We construct a series of tailored counterfactual prompts [counterfactual], deliberately sketched as controlled studies.First, we identify key components of an example in few-shot prompting as follows: Symbols, Patterns, and Text.Next, we perform counterfactual prompting—keeping all but one component fixed (replacing symbols (numbers) with Greek alphabets).Finally, we elicit meaningful findings via conducting a systematic and qualitative analysis of the performance divergence between different prompt queries.Our experiments on four diverse reasoning tasks and across three large language models—, , and , reveal several surprising findings:1 We find that the exact type of symbols in the prompt virtually does not affect the model performance.In addition, our results and analysis demonstrate counterintuitive phenomena.For example, we identify that the correctness of symbols and patterns is immaterial to the task solve rate.2We learn that patterns contribute chiefly as a venue to reinforce task understanding [ouyang2022training] and prompt the model to attain correct outputs.3Most importantly, we find that text and patterns form a symbiotic relationship that plays a vital role in the success of .Text helps generate useful patterns (by extracting commonsense knowledge), and patterns help reinforce task understanding, enabling the language model to generate text that helps solve the task.Overall, we argue that one of the primary reasons behind the success of is this interplay between text and patterns—helps a language model in imitating the prompt and generating the right tokens for the task—and is conceivably less related to their reasoning abilities.Finally, as indicated by applications such as -Saycan [ahn2022can], we posit that techniques like will play a key role in enabling the success of s on diverse use cases.Thus, designing efficient prompts informed by a set of key design principles is an important challenge.To this end, we distill our findings to create concise prompting, dubbed .prunes the prompt (20%↓) to only retain indispensable tokens without negative repercussions on the task solve rate.§ ETHICS STATEMENTDisseminating reasoning into machines has numerous benefits and applications, from algorithmic reasoning [alphacode] to code generation [chen_evaluating_2021,poesia2021synchromesh] and formal verification [wu2022autoformalization].While this research does not directly enhance the reasoning capabilities of large language models, it identifies several systematic behavioral patterns in the functioning of few-shot models.Similar to any technological advances, this work has risks of detrimental societal impact.However, anticipating potential future downsides of such methods is challenging.More than ever, the research community's utmost responsibility is to acknowledge these risks candidly and reflect on practices and strategies to prevent potential harm.Environmental impact.Training large language models devour a nontrivial amount of compute resources, a limiting factor for frequent training.Few-shot prompting is an appealing solution for mitigating the unfavorable environmental impact of large language models by evading additional iterations of training and dataset collection.Ours and similar studies may lead to more effective prompting techniques and bring technological innovation to the architecture of large language models, especially regarding their reasoning capabilities.Therefore, we hope that the significant compute used in this work can help promote positive environmental outcomes.Finally, aligned with the credible concerns of the research community, we recognize the longer-term risk of Excellence in artificial intelligence, primarily when it boils down to human reasoning.While distilling comparable human reasoning to machines offers many benefits, undisciplined and uncontrolled progress in this area could be alarming, especially in the presence of bad actors.Effective reasoning in machines as a result of our study, even though not directly, can lead to algorithmic advances that may facilitate bad actors in developing malicious software and systems with human-level capabilities.We also want to acknowledge a large body of researchers that has greeted innovations in large language models and steady scaling of models with skepticism, questioning the connection between human reasoning and large language models [han2022human,cog:gpt3].Additionally, recent efforts have started exploring the relationship between the structure of contemporary language models and the human mind [schrimpf2021neural,tang2021sensory,whittington2021relating].Despite their progress, the rationale behind 's mimicking human reasoning, and any potential connection with linguistics is so far an uncharted territory.The primary objective of our study is to understand through counterfactual prompting and empirically establish the underpinnings of the reasoning ability of s in the presence of .Each counterfactual prompt C_f(p) alters only one particular aspect of the in-context examples ⟨ x_k · t_k · y_k⟩ in a .For example, consider a sample thought for the math world problems in the dataset (See <Ref>).A symbolic counterfactual prompt, C_symbolic(p), may simply replace all the numbers in the thoughts with symbols (X1).Such analysis enables us to ask: “what would the performance of the model have been, if all the numbers in the prompt were replaced with symbols?”.Analyzing the performance disparity of a on C_symbolic(p) vs. p can thus indicate the role that using actual numbers plays in the success or failure of a task[Appendix-Table <ref> outlines a set of major “what if” questions that we study in this work.].The ability to successfully complete prompts p for complex reasoning tasks is typically present for at the scale of , , and .Nonetheless, we do not make any assumptions about the underlying model architecture.In summary, our study on reveals that for solving math problems, neither the presence of numbers, nor the credibility of the thoughts is paramount to the success of .Similarly, altering the style and wording of the texts in the thoughts has a modest impact on the model performance.Nevertheless, eradicating either of these components nullifies the efficacy of .Finally, a per-layer analysis of the model reveals that if the model performs similarly for a pair of counterfactual prompts C_f(p) vs. p, then the attention patterns are comparable as well.We defer the detailed background to Appendix <ref>.Limitations of counterfactual prompting.Relying on counterfactual examples could be misleading and precarious [laugel2019dangers,slack2021counterfactual].Nonetheless, counterfactual explanation presents a channel to gain insights into the workings of the model.This approach potentially yields more favorable explanations for state-of-the-art s.Notably, unlike fine-tuned methods, one can readily identify and collect a set of prompts that are critical for the model to generate particular outputs.In particular, few-shot prompting augments the model with an additional dimension to calibrate the accuracy to a discernible degree.Thus, we deduce that the counterfactual examples that exhibit consistent and systematic performance divergence are more prone to reflect credible interpretations of the model.In this work, we neither rely on the results that do not exhibit such characteristics, nor reject prompts that pose contradictory observations. We discuss additional limitations in <Ref>.Broadly, this paper intersects with a growing body of work on prompting and large language model reasoning [brown2020language,palm:arxiv].Below, we review the most relevant work in these directions.\n",
      "---\n",
      "Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in , a Korean-centric GPT-3 model.  From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning,  (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance.\n",
      "---\n",
      "CodexDB is enabled by recent advances in the domain of natural language processing. Those advances have been fuelled by two key ideas: a novel neural network architecture, the Transformer [Vaswani2017], and new training paradigms, implementing the idea of transfer learning [ruder2019transfer]. The Transformer is nowadays the dominant architecture in the domain of language processing [Wolf2020]. Among other advantages, it lends itself better to parallelization than prior methods. This has, in part, enabled the creation of very large, pre-trained language models. Such models are pre-trained on tasks for which large amounts of training data are easily available, e.g. predicting the next word in text snippets. While pre-training is very expensive, the resulting models can be easily specialized for new tasks via different methods. Fine-tuning describes a process in which pre-trained models are used as a starting point for further training on more specialized tasks (reducing the amount of training samples and computational overheads by orders of magnitude via pre-training [Houlsby2019]). Until recently, fine-tuning has been the primary method of exploiting pre-trained language models. The latest generation of pre-trained models, most notably OpenAI's Generative Pre-Trained Transformer (GPT) version 3, unlocks new possibilities. It turns out that sufficiently large models can oftentimes solve new tasks without specialized training (“zero-shot learning”), based on inputs describing the task in natural language alone [Brown2020]. Precision increases if the input integrates few (i.e., typically less than ten) examples pairing tasks of the same type with solutions (“few-shot learning”). This is the method currently used by CodexDB. The final development that enabled this paper is the emergence of the Codex variant of GPT-3 [Chen2021, OpenAI2021]. The primary difference between GPT-3 Codex and the original GPT-3 model lies in the data used for pre-training. GPT-3 Codex is trained on code and technical documentation. This results in a model whose primary use case is the translation of natural language commands into code.\n",
      "---\n",
      "Natural Language Processing (NLP) is a rapidly growing field of study that works on understanding human language. NLP gives machines the ability to learn human language by turning it into numerical data [khurana2023natural]. With the increasing number of digital texts, the need of NLP is growing rapidly[feder2022causal]. In recent years, NlP has provided a large scale analysis and management of text data making sentiment analysis, emotion detection and other complicated tasks possible. Furthermore, with the help of NLP, it is possible to detect mental illness at early stage and provide treatment [zhang2022natural]. Previously the training of NLP models were slow and inefficient [chen2021crossvit]. Especially after 2017, the innovation of transformer architecture has revolutionized the NLP field. The transformers made NLP tasks to be carried out sequentially that gave birth to large language models like ChatGPT. This models are so efficient that it can imitate human behaviour which creates some reasonable concerns.Transformers are one of the most powerful tools for natural language processing [gillioz2020overview]. It can largely be divided into two parts. The encoder and the decoder. The encoder part takes a text sequence as input and produces a sequence of encoded representation. The difference between other architecture is that the encoded sequence is more context aware. When a series of encoders are stacked togather, it is called a BERT [singh2021nlp]. The decoder part, on the other hand, is able to generate an arbitrary length sequence. Stacking decoder blocks produces an architecture named GPT. ChatGPT is also a transformer based architecture that has been trained on a large set of public data in self-supervised fashion. It has more than a billion parameters making it on of the biggest language model available. It is considered a major breakthrough since it's release in November, 2022.\n",
      "---\n",
      "We discover repetitive attention patterns given trained language models. One interesting question that needs to be answered is whether such behavior is a model architecture bias or an unexpected training outcome. For such purpose, we perform the same experiment with a randomly initialized OPT, and compare it against the results presented in Section <ref>. As shown in Figure <ref>, the repetitive attention pattern does not exist in randomly initialized models. Apart from an efficiency deployment perspective, could such repetitive attention patterns contribute to some known problem in language generation such as repetitions? It may be worth investigating the relationship between repetitive attention patterns and undesired generations.R8.9cm     \n",
      "          \n",
      "    < g r a p h i c s >\n",
      "               \n",
      "          \n",
      "    < g r a p h i c s >\n",
      "                                         We plot the attention map corresponding to Section <ref> but with a randomly initialized OPT. We observe no repetitive attention for a randomly initialized model.Due to the limitation of the server in academics, the largest model we can fit is OPT-66B. We try to understand the behavior and verify the generality across the different models and datasets. However, we cannot access the training process and fail to know exactly how an LLM is trained to exhibit such behavior. Experiments with the large model create carbon dioxide emissions. However, our work improves the efficiency of LLM, and we  foresee no negative impacts.Inspired by our intriguing findings that pivotal tokens exert a lasting influence on future steps, we developed  to leverage this observation to reduce the memory usage of KV cache. Our method achieves memory reductions of 5× in the KV cache without compromising the performance of LLMs. Furthermore, we demonstrate the compatibility of  with quantization techniques, opening up the possibility of reducing memory usage in both the representation and sequence length dimensions.§ APPENDIXThe problem of sampling and quantizing kv-cache can naturally cast as approximating the output of attentions. We can analyse quantization and sampling separately. A note on sampling : Given the nature of sampling, a closely related problem is that of reservoir sampling. However, the problem is slightly different as the weights associated with the weighted summation change with addition of new tokens which opens up new avenues for variance reduction. We start with simple abstract sampling design with E(S)=1.1. Any sampling design that ensures E(S_i) = 1 is a good to get almost unbiased estimates of attention2. Variance reduction of denominator ( also numerator)  requires sampling with probability proportional to weights.  While the problem of weighted reservoir sampling is well understood, the problem of dynamic weights is rather understudied (verify) and often ifeasible.3. We give a reservior sampling for dynamic weights with weight evolution assumptions motivated from the problem at hand. (π(k|j)  π(k|j-1).  The algorithm and proof are straightfroward.4. Proving / intuiton as to why the assumption holds. Either jaspers proof can be modified for this or can be used as a intuition.  Also, we can give empirical evidence.We can thus cover the sampling part under standard (modifed) sampling theory and the sections can be made rigorous with notations borrowed from the sampling literature.Quantisation can be analysed separately. Although it deviates from the central storyIn this section we will talk about reduction of kv-cache for a particular attention head given a budget of tokens say n. The problem can be interpreted as sampling problem with the target to get an accurate estimate of the output of attention head at token, say t+1. The computation in attention can be written as,\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 109, in run\n",
      "    await self.visit(self.parse_tree, VariableStack([self.program._variables], self))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 524, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_assistant.py\", line 13, in assistant\n",
      "    return await role(role_name=\"assistant\", hidden=hidden, _parser_context=_parser_context, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_role.py\", line 17, in role\n",
      "    new_content += await parser.visit(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in visit\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in <listcomp>\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 379, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_gen.py\", line 140, in gen\n",
      "    gen_obj = await parser.llm_session(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 665, in __call__\n",
      "    out = await self.llm.caller(**call_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 370, in _library_call\n",
      "    out = await openai.ChatCompletion.acreate(**kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 728, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 9175 tokens. Please reduce the length of the messages.\n",
      "\n",
      "Error in program:  This model's maximum context length is 8192 tokens. However, your messages resulted in 9175 tokens. Please reduce the length of the messages.\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the meaning of \"emergent capabilities\" in the context of large language models?\n",
      "--------------------------------------------------\n",
      "Answer A ($19.18):\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "Stable Diffusion revolutionised image creation from descriptive text. ,  and  demonstrated astonishing performance across a variety of language tasks.  introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to  once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as [The name is inspired by the Generative Adversarial Networks (GAN) literature on mode collapse, where GANs start producing a limited set of outputs that all trick the discriminator.  is a process whereby models eventually converge to a state similar to that of a GAN Mode Collapse. The original version of this paper referred to this effect as `model dementia', but we decided to change this following feedback that it trivialised the medical notion of `dementia' and could cause offence. ] and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.\n",
      "--------------------------------------------------\n",
      "Answer B ($86.77):\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "Stable Diffusion revolutionised image creation from descriptive text. ,  and  demonstrated astonishing performance across a variety of language tasks.  introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to  once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as [The name is inspired by the Generative Adversarial Networks (GAN) literature on mode collapse, where GANs start producing a limited set of outputs that all trick the discriminator.  is a process whereby models eventually converge to a state similar to that of a GAN Mode Collapse. The original version of this paper referred to this effect as `model dementia', but we decided to change this following feedback that it trivialised the medical notion of `dementia' and could cause offence. ] and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.\n",
      "---\n",
      "While large language models fine-tuned for detection - such as the ones we used here - have been criticized as stylometry unsuited for generative models detection [Bhat2020SemanticsNotSyntax,Schuster2020StylometryLimitations], there has so far been no alternative detection method shown to perform better. Notably, methods relying on the factual structure of the text have shown similar vulnerability to prompt selection and fine-tuning [Zhong2020DeepfakesFActualStructure], even before the arrival of factual database augmented generative models [Shu2021FactEnhancedNewsGeneration,SeeKeR2022Facebook].Similarly, evasion detection through prompt selection - that we use to some extent - has been almost entirely neglected until now. However, results from the generative model normativity and privacy fields suggest that well-chosen prompts can lead to highly unexpected and uncharacteristic texts [RedTeamingLanguageModels2022,GPT2LeaksAF], likely leading to a degradation of detection capabilities presented here as a previous SotA.If anything, both of those avenues for evasion detection reinforce our conclusion that a competent attacker can easily evade detection even with relatively small models.§ ETHICS STATEMENTWe essentially provide a blueprint for a competent attacker to create a generative model that would effectively evade detection by most if not all, means available today.While the first version of this paper was prepared in mid-2022, following the release of ChatGPT, we delayed its submission for publication by four months following the public release of ChatGPT to leave time for model developers to improve detection mechanisms. We similarly have contacted some of the entities operating common generative LLM detection endpoints to report our findings, to no avail.Given that prior knowledge of reference \"human\" texts used to train the detector, LLM is a critical component of full evasion described here, and given that it is currently unlikely for most deployed detectors using classification fine-tuned LLMs, we decided in favor of releasing our results publicly.Another factor that contributed to our decision to release these results is an increasing reliance on fundamentally flawed detection tools to make prejudicial decisions. We observed LLM-based detectors for generative LLMs used in the educational setting, meaning that their high false-positive rate led to students being unjustly accused and disciplined, potentially impacting their long-term perspectives.We hope the results presented here will lead to a more rigorous study of alternative ways to detect LLMs, starting with the generated text fingerprinting by major generative LLM providers.§ ACKNOWLEDGEMENTSWe would like to thank the armasuisse - Cyber-Defence (CYD) Campus for the Distinguished Post Doctoral Fellowship supporting AK, as well as Fabien Salvi (EPFL) for the technical support regarding the computational infrastructure organization, and France Faille (EPFL) for the administrative support.splncs04We based our code on the Pytorch implementation of a set of text-generating GANs by <https://github.com/williamSYSU/TextGAN-PyTorch>, including the DPGAN [Xu2018DPGAN].\n",
      "---\n",
      "More recently, a lot of work focus on scaling up the PLMs and developing the large language models (LLMs) [ouyangtraining,chowdhery2022palm,smith2022using,zhang2022opt]. [weiemergent] show that LLMs exhibit emergent abilities, e.g., few-shot and zero-shot learning, when the model sizes are large enough. As a typical LLM, the recently-released ChatGPT has attracted great attention, due to its impressive ability to generate fluent and high-quality responses. There is growing interest in exploring the capabilities, applications, ethics, and failures of ChatGPT [jiao2023chatgpt,bang2023multitask,qin2023chatgpt,zhuo2023exploring,wang23cls]. Along with the research line, we mainly focus on analyzing the understanding ability of ChatGPT in this report, which is important but has been given little attention.\n",
      "---\n",
      "Realistic Pre-training data For the realistic Pooling and Pooling-Attention models, we created a pre-training dataset similar to other projects. We select Java GitHub repositories with 5 stars or more, we clone the main branch of the repository, while only downloading files under 2 megabytes. We then filter the commits that include the words \"error\", \"bug\", \"fix\", \"issue\", \"mistake\", \"incorrect\", \"fault\", \"defect\", \"flaw\", or \"type\", using standard practice in ManySStubs4J project [karampatsis2020often] . Since our models are designed only for single-file bug localization, we take each modified file and apply the labeling procedure described in the paper to generate the examples and labels. We truncate files at 8192 tokens in the same manner as in [codebert]. In total, we get about 195 GB of data to use for pre-training.We publish the training, evaluation, and validation sets for the synthetic data. We also publish the realistic benchmark. These items can be accessed in a Google Cloud Storage bucket at https://console.cloud.google.com/storage/browser/bigissue-research. All materials are released under the MIT License.\n",
      "---\n",
      "To tackle these challenges, we need to operate at a level of abstraction that allows harnessing the intuitive and interactive capabilities offered by LLMs.  Our key insight is to leverage reward functions as an interface that bridges the gap between language and low-level robot actions. This is motivated by the fact that language instructions from humans often tend to describe behavioral outcomes instead of low-level behavioral details (e.g. ``robot standing up'' versus ``applying 15 Nm to hip motor''), and therefore we posit that it would be easier to connect instructions to rewards than low-level actions given the richness of semantics in rewards. In addition, reward terms are usually modular and compositional, which enables concise representations of complex behaviors, goals, and constraints. This modularity further creates an opportunity for the user to interactively steer the robot behavior. However, in many previous works in reinforcement learning (RL) or model predictive control (MPC), manual reward design requires extensive domain expertise [lee2019robust, siekmann2021sim, xia2020relmogen].  While reward design can be automated, these techniques are  sample-inefficient and still requires manual specification of an objective indicator function for each task [chiang2019learning].  This points to a missing link between the reward structures and task specification which is often in natural language. As such, we propose to utilize LLMs to automatically generate rewards, and leverage online optimization techniques to solve them. Concretely, we explore the code-writing capabilities of LLMs to translate task semantics to reward functions, and use MuJoCo MPC, a real-time optimization tool to synthesize robot behavior in real-time~[howell2022].  Thus reward functions generated by LLMs can enable non-technical users to generate and steer novel and intricate robot behaviors without the need for vast amounts of data nor the expertise to engineer low-level primitives.\n",
      "---\n",
      "Reasoning seems an emergent ability of LLMs [wei2022emergent,wei2022chain,suzgun2022challenging] show that  reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters). This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks.  However, the reason for this emergent ability is not yet fully understood. We refer the reader to [wei2022emergent,fu_khot_peng_2022b] for some potential explanations.\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "As [josifoski-etal-2022-genie] noticed, cIE datasets naturally present large imbalances in relation frequency.  Training on such data results in models that perform well only for the few frequent relations and poorly on the rest.  Fixing this issue requires access to more data points for the large amount of rare relations. Such data cannot be easily collected easily from human or distant supervision heuristics [huguet-cabot-navigli-2021-rebel-relation]. In this work, we fix it by generating large amount of synthetic data for all relations. This data scarcity problem is not unique to cIE and our procedure can be applied to other structured NLP tasks, such as entity linking, open information extraction, writing assistants  [schick2022peer] or abstract meaning representation (AMR) parsing [banarescu2013abstract].Several approaches to data augmentation use pretrained language models (PLMs). Early efforts include works that require the preexistence of a dataset, which is then used to finetune the pretrained generator network [anaby2020not, papanikolaou2020dare, yang-etal-2020-generative, mohapatra2020simulated, kumar2020data].\n",
      "---\n",
      "In the context of large-scale pre-training, effectively incorporating multi-modal knowledge of vision and language into sequential decision-making is of utmost importance. While semantic common-sense information plays a critical role in enhancing the efficiency and effectiveness of sequential decision-making for general purposes, ChatGPT [qin2023chatgpt] achieves a remarkable breakthrough as a powerful knowledge base. However, the integration between sequential decision-making and perception modalities still lacks naturalness.  For example, LM-Nav and SayCan manually design the fusion mechanism of multiple outputs from large perception models, but fail to perform joint training.  While Gato performs joint training of multiple modalities, it lacks alignment between modalities in terms of tasks.  It would be interesting to explore the possibility of learning an extra module to splice cross-modal large models together, such as an adapter or an inverse dynamic model.  Furthermore, the emergence of in-context learning and chain-of-thought abilities [wei2022chain] in large language models may be the ingredients for creating general self-improving agents without the need for supervised knowledge from humans.Training Efficiency represents a significant impediment that hinders the development of large decision models, which requires additional efforts and extensive exploration in future research aimed at designing efficient training systems.\n",
      "---\n",
      "Intelligence is a multifaceted and elusive concept that has long challenged psychologists, philosophers, and computer scientists. There is no generally agreed upon definition of intelligence, but one aspect that is broadly accepted is that intelligence is not limited to a specific domain or task, but rather encompasses a broad range of cognitive skills and abilities.     Building an artificial system that exhibits such broad behavior is a long-standing and ambitious goal of AI research.     In early writings, the founders of the modern discipline of artificial intelligence (AI) research called out sets of aspirational goals for understanding intelligence [mccarthy2006proposal].     Over decades, AI researchers have pursued principles of intelligence, including generalizable mechanisms for reasoning (e.g., [newellshawsimonGPS1959], [lindsay1993dendral]) and construction of knowledge bases containing large corpora of commonsense knowledge [lenat1995cyc].     However, many of the more recent successes in AI research can be described as being narrowly focused on well-defined tasks and challenges, such as playing chess or Go, which were mastered by AI systems in 1996 and 2016, respectively. In the late-1990s and into the 2000s, there were increasing calls for developing more general AI systems (e.g., [selman1996challenge]) and scholarship in the field has sought to identify principles that might underly more generally intelligent systems (e.g., [legg2008machine, gershman2015computational]). The phrase, “artificial general intelligence\" (AGI), was popularized in the early-2000s (see [goertzel2014artificial]) to emphasize the aspiration of moving from the “narrow AI\", as demonstrated in the focused, real-world applications being developed, to broader notions of intelligence, harkening back to the long-term aspirations and dreams of earlier AI research.     We use AGI to refer to systems that demonstrate broad capabilities of intelligence, including reasoning, planning, and the ability to learn from experience, and with these capabilities at or above human-level. We discuss other definitions of AGI in the conclusion section.     The most remarkable breakthrough in AI research of the last few years has been the advancement of natural language processing achieved by large language models (LLMs). These neural network models are based on the Transformer architecture [Vas17] and trained on massive corpora of web-text data, using at its core a self-supervised objective of predicting the next word in a partial sentence.     In this paper, we report on evidence that a new LLM developed by OpenAI, which is an early and non-multimodal version of  [gpt4], exhibits many traits of intelligence.     Despite being purely a language model, this early version of  demonstrates remarkable capabilities on a variety of domains and tasks, including abstraction, comprehension, vision, coding, mathematics, medicine, law, understanding of human motives and emotions, and more.      We interacted with  during its early development by OpenAI using purely natural language queries (prompts)[As ’s development continued after our experiments, one should expect different responses from the final version of . In particular, all quantitative results should be viewed as estimates of the model’s potential, rather than definitive numbers. We repeat this caveat throughout the paper to clarify that the experience on the deployed model may differ. Moreover we emphasize that the version we tested was text-only for inputs, but for simplicity we refer to it as  too.].     In Figure <ref>, we display some preliminary examples of outputs from , asking it to write a proof of infinitude of primes in the form of a poem, to draw a unicorn in TiKZ (a language for creating graphics in ), to create a complex animation in Python, and to solve a high-school level mathematical problem. It easily succeeds at all these tasks, and produces outputs that are essentially indistinguishable from (or even better than) what humans could produce. We      also compare 's performance to those of previous LLMs, most notably ChatGPT, which is a fine-tuned version of (an improved) GPT-3 [gpt3]. In Figure <ref>, we display the results of asking ChatGPT for both the infinitude of primes poem and the TikZ unicorn drawing. While the system performs non-trivially on both tasks, there is no comparison with the outputs from . These preliminary observations will repeat themselves throughout the paper, on a great variety of tasks. The combination of the generality of 's capabilities, with numerous abilities spanning a broad swath of domains, and its performance on a wide spectrum of tasks at or beyond human-level, makes us comfortable      with saying that  is a significant step towards AGI.     Our claim that  represents progress towards AGI does not mean that it is perfect at what it does, or that it comes close to being able to do anything that a human can do (which is one of the usual definition of AGI; see the conclusion section for more on this), or that it has inner motivation and goals (another key aspect in some definitions of AGI). In fact it is not fully clear how far  can go along some of those axes of intelligence that we focus on, e.g., planning (see Section <ref>), and arguably it is entirely missing the learning from experience as the model is not continuously updating (although it can learn within a session, see Section <ref> for example).     Overall  still has many limitations, and biases, which we discuss in detail below and that are also covered in OpenAI's report [gpt4]. In particular it still suffers from some of the well-documented shortcomings of LLMs such as the problem of hallucinations [maynez2020faithfulness] (see Figure <ref>) or making basic arithmetic mistakes [cobbe2021training] (see Appendix <ref>), and yet it has also overcome some fundamental obstacles such as acquiring many non-linguistic capabilities (e.g., it solves most of the LLM failure modes described in [mahowald2023dissociating], and it also made great progress on common-sense, see Figure <ref> for a first example and Appendix <ref> for more). This highlights the fact that, while  is at or beyond human-level for many tasks, overall its patterns of intelligence are decidedly not human-like. However,  is almost certainly only a first step towards a series of increasingly generally intelligent systems, and in fact  itself has improved throughout our time testing it, see Figure <ref> for the evolution of the unicorn drawing over the course of a month of training[Note that the improving we refer to here is a slow type of learning, which eventually comes to a halt, as opposed to the fast-paced and real-time learning one would expect from an AGI.]. Even as a first step, however,  challenges a considerable number of widely held assumptions about machine intelligence, and exhibits emergent behaviors and capabilities whose sources and mechanisms are, at this moment, hard to discern precisely (see again the conclusion section for more discussion on this).      Our primary goal in composing this paper is to share our exploration of 's capabilities and limitations in support of our assessment that a technological leap has been achieved. We believe that 's intelligence signals a true paradigm shift in the field of computer science and beyond.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:01,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 109, in run\n",
      "    await self.visit(self.parse_tree, VariableStack([self.program._variables], self))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 524, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_assistant.py\", line 13, in assistant\n",
      "    return await role(role_name=\"assistant\", hidden=hidden, _parser_context=_parser_context, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_role.py\", line 17, in role\n",
      "    new_content += await parser.visit(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in visit\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in <listcomp>\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 379, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_gen.py\", line 140, in gen\n",
      "    gen_obj = await parser.llm_session(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 665, in __call__\n",
      "    out = await self.llm.caller(**call_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 370, in _library_call\n",
      "    out = await openai.ChatCompletion.acreate(**kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 728, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 8737 tokens. Please reduce the length of the messages.\n",
      "\n",
      "Error in program:  This model's maximum context length is 8192 tokens. However, your messages resulted in 8737 tokens. Please reduce the length of the messages.\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "Can in-context learning occur when there's a distribution mismatch between prompts and pretraining data?\n",
      "--------------------------------------------------\n",
      "Answer A ($24.64):\n",
      "[xrlm21] explores the occurrence of in-context learning during pre-training when documents exhibit long-range coherence. The Language Model (LLM) develops the ability to generate coherent next tokens by deducing a latent document-level concept. During testing, in-context learning is observed when the LLM deduces a shared latent concept between examples in a prompt. Through the research conducted, it has been demonstrated that in-context learning happens even when there is a distribution mismatch between prompts and pretraining data, especially in scenarios where the pretraining distribution is a mixture of Hidden Markov Models [bp66]. Theoretically, they show that the error of the in-context predictor is optimal when a distinguishability condition holds.  In cases where this condition does not hold, the expected error still reduces as the length of each example increases. This finding highlights the importance of both input and input-output mapping contributes to in-context learning.\n",
      "---\n",
      "GPT-3 [brown2020gpt3] demonstrates the power of in-context learning, an adaptation method that does not need any parameter optimization. In the adaptation phase, the pretrained language foundation model takes in a prompta sequence of tokens that concatenates input-output examples from the taskfollowed by a test example and simply generates the label of the test example by conditioning on the sequence seen thus far (prompt plus test example).  In other words, there is no explicit training or change to the model parameters.   What is the mechanism by which the model “learns” from the different examples by simply executing with the examples as inputs? The previous modularization does not directly apply because we do not obtain new model parameters during adaptation, but rather we only use the generative capabilities of the foundation model by executing on structurally-designed inputs.  However, the idea of separating pretraining with infinite data and pretraining with finite data can still be useful. For example, a recent work starts with the assumption of infinite pretraining data and sufficient model expressivity to study in-context learning [xie2021incontext]. These assumptions reduce the characterization of in-context learning to a matter of analyzing the pretraining distribution conditioned on in-context learning prompts, which are drawn from a different distribution than the pretraining data. In particular, [xie2021incontext] proposes that in-context learning emerges from long-term coherence structure in the pretraining distribution, which is described by a latent variable generative model with coherence structure.  More broadly, while the modularization proposed in this section provides a nice framework to gain useful theoretical insights into foundation models, it is possible that some emergent behavior like in-context learning and other capabilities yet to be discovered would require going beyond the modularization, by opening the black box of the architecture.theory-tools\n",
      "---\n",
      "There has been an interest in effectively leveraging the in-context learning paradigm [zhao2021calibrate,holtzman-etal-2021-surface,min-etal-2022-noisy,liu-etal-2022-makes,lu-etal-2022-fantastically,rubin-etal-2022-learning,mishra-etal-2022-reframing] ever since its introduction by [brown2020language], but there have been relatively fewer studies toward better understanding the paradigm itself. [xie2021explanation] cast in-context learning as implicit Bayesian inference where the language model implicitly infers a shared concept among in-context examples when making a prediction, showing that this occurs when the pre-training distribution is a mixture of Hidden Markov Models (HMMs) over latent concepts despite a mismatch with the prompt distribution. [min2022rethinking] study the role of the in-context examples themselves, finding that the ground-truth labels are not needed in the examples and that the more important drivers are provision of the label space, the distribution of the input text and the overall format of the sequence. [garg2022can] showcase that Transformer models trained from scratch can in-context learn the class of linear functions with performance comparable to the optimal least squares estimator even under distribution shifts. [razeghi2022impact] showcase that in-context learning performance is correlated strongly with term frequencies in the pre-training corpora used. [olsson2022context] consider an alternate framing of in-context learning as the ability of a language model to better predict tokens later in the context than tokens earlier and hypothesize the existence of induction heads that are responsible for in-context learning. [chan2022transformers] explore training Transformers separately on controlled stimuli and natural language to showcase that Transformers exhibit striking differences in their generalization from in-context vs. in-weights information.\n",
      "---\n",
      "DP-SGD is known to (possibly) incur substantial performance losses compared to non-private learning. To improve the privacy-utility trade-off, several past works have leveraged large-scale publicly pretrained models [yu2022differentially,li2022large,de2022unlocking,mehta2022large]. These works observe that the privacy-utility trade-off improves with the use of larger (and thus better) pretrained models.[While size does not equate quality, there is strong correlation between the two under currently popular pretraining techniques [liu2019roberta,brown2020language]. We're optimistic that future smaller models pretrained with improved techniques can be as performant as current large models [hoffmann2022training].  ] We extend this research and study a second research question   \tCan the privacy-utility trade-off be further improved with even better / larger pretrained models?  To study this, we scale DP fine-tuning to work with one of the largest and most performant pretrained language models to date—the original 175 billion-parameter GPT-3. Weights of this model cannot be hosted in the memory of a single accelerator (e.g., GPU) and must be distributed across multiple devices.  This presents challenges for flat clipping which calls for communicating per-example gradient norms across devices. To bypass these challenges, we turn to per-device clipping, where each device is prescribed a clipping threshold for clipping per-example gradients of the hosted model piece. Per-device clipping incurs no additional communication cost and allowed us to obtain with GPT-3 a private fine-tuning performance at ϵ=1 that is better than what is attainable by non-privately fine-tuning the largest GPT-2 on a challenging summarization task.\n",
      "--------------------------------------------------\n",
      "Answer B ($99.08):\n",
      "[xrlm21] explores the occurrence of in-context learning during pre-training when documents exhibit long-range coherence. The Language Model (LLM) develops the ability to generate coherent next tokens by deducing a latent document-level concept. During testing, in-context learning is observed when the LLM deduces a shared latent concept between examples in a prompt. Through the research conducted, it has been demonstrated that in-context learning happens even when there is a distribution mismatch between prompts and pretraining data, especially in scenarios where the pretraining distribution is a mixture of Hidden Markov Models [bp66]. Theoretically, they show that the error of the in-context predictor is optimal when a distinguishability condition holds.  In cases where this condition does not hold, the expected error still reduces as the length of each example increases. This finding highlights the importance of both input and input-output mapping contributes to in-context learning.\n",
      "---\n",
      "GPT-3 [brown2020gpt3] demonstrates the power of in-context learning, an adaptation method that does not need any parameter optimization. In the adaptation phase, the pretrained language foundation model takes in a prompta sequence of tokens that concatenates input-output examples from the taskfollowed by a test example and simply generates the label of the test example by conditioning on the sequence seen thus far (prompt plus test example).  In other words, there is no explicit training or change to the model parameters.   What is the mechanism by which the model “learns” from the different examples by simply executing with the examples as inputs? The previous modularization does not directly apply because we do not obtain new model parameters during adaptation, but rather we only use the generative capabilities of the foundation model by executing on structurally-designed inputs.  However, the idea of separating pretraining with infinite data and pretraining with finite data can still be useful. For example, a recent work starts with the assumption of infinite pretraining data and sufficient model expressivity to study in-context learning [xie2021incontext]. These assumptions reduce the characterization of in-context learning to a matter of analyzing the pretraining distribution conditioned on in-context learning prompts, which are drawn from a different distribution than the pretraining data. In particular, [xie2021incontext] proposes that in-context learning emerges from long-term coherence structure in the pretraining distribution, which is described by a latent variable generative model with coherence structure.  More broadly, while the modularization proposed in this section provides a nice framework to gain useful theoretical insights into foundation models, it is possible that some emergent behavior like in-context learning and other capabilities yet to be discovered would require going beyond the modularization, by opening the black box of the architecture.theory-tools\n",
      "---\n",
      "There has been an interest in effectively leveraging the in-context learning paradigm [zhao2021calibrate,holtzman-etal-2021-surface,min-etal-2022-noisy,liu-etal-2022-makes,lu-etal-2022-fantastically,rubin-etal-2022-learning,mishra-etal-2022-reframing] ever since its introduction by [brown2020language], but there have been relatively fewer studies toward better understanding the paradigm itself. [xie2021explanation] cast in-context learning as implicit Bayesian inference where the language model implicitly infers a shared concept among in-context examples when making a prediction, showing that this occurs when the pre-training distribution is a mixture of Hidden Markov Models (HMMs) over latent concepts despite a mismatch with the prompt distribution. [min2022rethinking] study the role of the in-context examples themselves, finding that the ground-truth labels are not needed in the examples and that the more important drivers are provision of the label space, the distribution of the input text and the overall format of the sequence. [garg2022can] showcase that Transformer models trained from scratch can in-context learn the class of linear functions with performance comparable to the optimal least squares estimator even under distribution shifts. [razeghi2022impact] showcase that in-context learning performance is correlated strongly with term frequencies in the pre-training corpora used. [olsson2022context] consider an alternate framing of in-context learning as the ability of a language model to better predict tokens later in the context than tokens earlier and hypothesize the existence of induction heads that are responsible for in-context learning. [chan2022transformers] explore training Transformers separately on controlled stimuli and natural language to showcase that Transformers exhibit striking differences in their generalization from in-context vs. in-weights information.\n",
      "---\n",
      "We explore this hypothesis experimentally in Section <ref> by decomposing the performance gap into a reasoning and a representation gap. By observing both these quantities across a variety of LLM classes (GPT-Neo, Pythia, BLOOM) and a suite of binary classification tasks, we conclude that LLMs possess good representations, and the majority of the quality gap (up to 79%) can be attributed to its reasoning ability (see Figure <ref>). Furthermore, we find that task adaptive methods (fine-tuning and adapter heads) improve the base model on both these axes, but majorly improve the task-specific reasoning ability which accounts for 72% of the gained performance on the task.These findings raise an intriguing question: Could it be that task-agnostic reasoning is beyond the capabilities of large language models? While the persistent quality gap between in-context learning and fine-tuning suggests this might be a generic limitation, we argue that the converse is in fact true. As a proof of concept, we propose (see Figure <ref>) which improves this reasoning deficiency in a completely task-agnostic manner.trains a transformer-based inference module using only synthetically generated logistic regression tasks, independent of the underlying task or the base language model. Surprisingly, this inference module can be composed without any additional training with the embeddings of any LLM to improve upon its reasoning abilities. demonstrates how we can compose, or graft, two different transformer models - an LLM trained autoregressively on language data (representation learner) and an inference module trained entirely on synthetic data (reasoning ability).Notably, our method satisfies the desired objectives:  \n",
      "     \n",
      "  * Task agnostic: 's inference module is only trained once using synthetic data.\n",
      "     \n",
      "  * Quality: Outperforms base LLM on all tasks and closes gap to task specific fine tuning methods.\n",
      "     \n",
      "  * Data-scalable: Can accommodate 10x more examples than in-context learning.\n",
      "---\n",
      "DP-SGD is known to (possibly) incur substantial performance losses compared to non-private learning. To improve the privacy-utility trade-off, several past works have leveraged large-scale publicly pretrained models [yu2022differentially,li2022large,de2022unlocking,mehta2022large]. These works observe that the privacy-utility trade-off improves with the use of larger (and thus better) pretrained models.[While size does not equate quality, there is strong correlation between the two under currently popular pretraining techniques [liu2019roberta,brown2020language]. We're optimistic that future smaller models pretrained with improved techniques can be as performant as current large models [hoffmann2022training].  ] We extend this research and study a second research question   \tCan the privacy-utility trade-off be further improved with even better / larger pretrained models?  To study this, we scale DP fine-tuning to work with one of the largest and most performant pretrained language models to date—the original 175 billion-parameter GPT-3. Weights of this model cannot be hosted in the memory of a single accelerator (e.g., GPU) and must be distributed across multiple devices.  This presents challenges for flat clipping which calls for communicating per-example gradient norms across devices. To bypass these challenges, we turn to per-device clipping, where each device is prescribed a clipping threshold for clipping per-example gradients of the hosted model piece. Per-device clipping incurs no additional communication cost and allowed us to obtain with GPT-3 a private fine-tuning performance at ϵ=1 that is better than what is attainable by non-privately fine-tuning the largest GPT-2 on a challenging summarization task.\n",
      "---\n",
      "Generality and Specialization. An important design choice for multimodal foundation models is the degree of specialization, or the  structural sharing between the modules responsible for each modality. Naturally, data of different domains exhibits diverse kinds of structures and propertieswhere, for instance, language is discrete while vision is continuous. At first sight, this variation hints that specialized inductive biases tailored for each modality could be of aid. Yet, as training scales upwards and models are provided with the opportunity to base their learning less on structural priors and more on the data itself, general approaches that maintain only a handful of broad general assumptions prove in fact a lot more successful than task-specific alternatives. And so, as corroborated by recent success of general-purpose models like transformers across different modalitiesboth linguistic [liu2019roberta,albert] and visual [visual_transformer,ganformer], we see that generality is critical for improving AI capabilities.Multimodal Interactions. Another key consideration for multimodal models relates to weight sharing: do the various modalities benefit from using the same or different parameters for their respective components? Prior works have shown that fruitful transfer could certainly occur across modalities, but the ideal degree of sharing remains unclear, so is the existence of principled ways for discovering it.\n",
      "---\n",
      "We have studied the effect of scaling on the gap between compositional ID and OOD generalization. We find that the relative generalization gap follows a decreasing trend as models are scaled up for different model families and for different number of support examples. One factor that limited our study is that in-context learning performance on CFQ and SCAN benchmarks is still very small for almost all publicly available models. One thing worth investigating in future research is why Codex model family, including the smaller Cushman model, is the only family in this study that achieves above 1% ID or OOD performance on CFQ or SCAN datasets. Another interesting future direction is studying the effects of pretraining on code and natural language, rather than natural language alone, on compositional generalization with scaling. Would pretraining on code provide more benefits with increased model scale? Such questions can be answered in the future when the research community has access to more large generative models that are equal in size and amount of training but differ only in data composition.Dataset     Prompt template        CFQ                                          SCAN                                                GeoQuery                                      Templates used for generating the prompts for CFQ, SCAN, and GeoQuery.Our prompts include a prefix string that introduces the task, followed by a number of input-output examples where inputs and outputs have dataset-specific prefixes. The templates used for producing the prompts are illustrated in Table <ref>.\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "Finally, not surprisingly, when BioGPT is fine-tuned on an augmented dataset, it outperforms LLaMA-7B. This is consistent with the previous finding [luo2022biogpt], and highlights the effectiveness of pretraining with domain-specific data, enabling BioGPT to better understand and excel in domain-specific tasks. Leveraging domain-specific knowledge during fine-tuning improves the model's accuracy and contextual relevance, resulting in superior performance for domain-specific questions or tasks.A promising direction for future work is to investigate the application of knowledge distillation, a popular technique that trains a smaller language model to mimic the behavior of a larger language model on medical question-answering tasks.Another potential approach is through contrastive learning. By training an SLM using contrastive learning on medical question-answering data, contrastive loss can help the model learn to identify similarities and differences between different instances of data and improve its ability to generalize to new and unseen data.\n",
      "---\n",
      "Our experiments in Section <ref> focused on automatically identifying factual precision errors in long-form generations by language models. Can these labels be used to actually correct mistakes in the long-form generations? In this section we perform a preliminary exploration of methods to edit long-form LM generations to reflect factually correct information. We assume we have access to the human-annotated set of  labels, and measure how good models are at editing incorrect sentences. In other words, we evaluate our editor models independent of the errors arising from the estimator.We adopt a similar set of methods as Section <ref> for our editing models. All methods below use four exemplar examples for in-context learning which were sampled from our dataset and removed for subsequent analysis. For all methods, we use OpenAI's ChatGPT [chatgpt] as the base language model due to its generative capabilities.No-context LM. We feed language models the prompt  and ask it to edit the text, without any retrieved context.. To assist an editor model, we use a passage retrieval system to find supporting evidence from an external knowledge source (Wikipedia in our case). Our retrieval pipeline is identical to Section <ref>, but uses 3 retrieved passages instead of 5 due to context length restrictions.+ Atomic Facts. Additionally, we explore whether adding atomic facts and their labels assist a model with fine-grained editing. Specifically, after the input sentence we add information to the prompt of the form . This data is also provided in the exemplars.Non-edit baselines. Finally, we add some trivial baselines to lower-bound our editing metrics. Specifically, we measure the performance of input copying (no edits), as well as an editor with random token dropping / replacement on a random 25% subset of tokens.\n",
      "---\n",
      "Starting with 4-8 manually constructed chain-of-thought exemplars, our methods show great success in automatically generating, pruning, and selecting suitable exemplars for each task.  It helps bypass the order selection sensitivity and style sensitivity by humans and reach a better difficulty and diversity trade-off.  After that, we raise a new question: Can we further bypass the effort of writing the initial chain-of-thought exemplars?  Based on current research of Zero-Shot-CoT [kojima2022large], we found it is possible.  Instead of using 4-8 manual-written exemplars to generate the chains, we simply add \"Let's think step by step.\" and let LLMs generate the chains.  The further steps are still the same as before.\n",
      "---\n",
      "In this paper, we evaluate open-source (to ensure reproducibility) and advanced large VLMs, such as UniDiffuser [bao2022one], which uses a diffusion-based framework to jointly model the distribution of image-text pairs and can perform both image-to-text and text-to-image generation; BLIP [li2022blip] is a unified vision-language pretraining framework for learning from noisy image-text pairs; BLIP-2 [li2023blip] adds a querying transformer [vaswani2017attention] and a large LM (T5 [raffel2020exploring]) to improve the image-grounded text generation; Img2Prompt [guoimages] proposes a plug-and-play, LM-agnostic module that provides large LM prompts to enable zero-shot VQA tasks; MiniGPT-4 [zhu2023minigpt] and LLaVA [liu2023llava] have recently scaled up the capacity of large LMs and leveraged Vicuna-13B [chiang2023vicuna] for image-grounded text generation tasks. We note that MiniGPT-4 also exploits a high-quality, well-aligned dataset to further finetune the model with a conversation template, resulting in performance comparable to GPT-4 [gpt4].\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:36, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "In-context learning can occur even when there's a distribution mismatch between prompts and pretraining data, but it may not be as effective. The AI model learns to generalize from the pretraining data and then fine-tunes its responses based on the specific prompts it receives. If the prompts are very different from the pretraining data, the model may struggle to provide accurate or relevant responses. However, advanced AI models are designed to handle a wide range of inputs and can often still learn and adapt to some extent even with a distribution mismatch.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: Looking at the answers from both students, it's clear that they both have a good understanding of the topic. They both mention the research conducted by [xrlm21] and the role of the Language Model (LLM) in in-context learning. They also both discuss the power of in-context learning demonstrated by GPT-3 [brown2020gpt3].\n",
      "\n",
      "Bobby: Yes, but if you look closely, you'll see that student A's answer is more comprehensive. They delve deeper into the topic, discussing the role of the in-context examples themselves, the performance of Transformer models under distribution shifts, and the correlation between in-context learning performance and term frequencies in the pre-training corpora used.\n",
      "\n",
      "Michael: That's true, but student B's answer is more concise and to the point. They directly address the question and provide a clear explanation of how in-context learning can occur even when there's a distribution mismatch between prompts and pretraining data. They also mention the importance of both input and input-output mapping in in-context learning.\n",
      "\n",
      "Bobby: I see your point, but I still think student A's answer is better. They provide more details and references to support their explanation, which shows a deeper understanding of the topic.\n",
      "\n",
      "Michael: I agree that student A's answer is more detailed, but I think student B's answer is clearer and easier to understand. They do a better job at answering the question directly without going off on tangents.\n",
      "\n",
      "Bobby: After considering your points, I can see the merit in student B's answer. It's concise, clear, and directly addresses the question. \n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the name of the architecture that has dominated natural language processing (NLP) benchmarks in recent years?\n",
      "--------------------------------------------------\n",
      "Answer A ($16.28):\n",
      "Natural Language Processing (NLP) is a rapidly growing field of study that works on understanding human language. NLP gives machines the ability to learn human language by turning it into numerical data [khurana2023natural]. With the increasing number of digital texts, the need of NLP is growing rapidly[feder2022causal]. In recent years, NlP has provided a large scale analysis and management of text data making sentiment analysis, emotion detection and other complicated tasks possible. Furthermore, with the help of NLP, it is possible to detect mental illness at early stage and provide treatment [zhang2022natural]. Previously the training of NLP models were slow and inefficient [chen2021crossvit]. Especially after 2017, the innovation of transformer architecture has revolutionized the NLP field. The transformers made NLP tasks to be carried out sequentially that gave birth to large language models like ChatGPT. This models are so efficient that it can imitate human behaviour which creates some reasonable concerns.Transformers are one of the most powerful tools for natural language processing [gillioz2020overview]. It can largely be divided into two parts. The encoder and the decoder. The encoder part takes a text sequence as input and produces a sequence of encoded representation. The difference between other architecture is that the encoded sequence is more context aware. When a series of encoders are stacked togather, it is called a BERT [singh2021nlp]. The decoder part, on the other hand, is able to generate an arbitrary length sequence. Stacking decoder blocks produces an architecture named GPT. ChatGPT is also a transformer based architecture that has been trained on a large set of public data in self-supervised fashion. It has more than a billion parameters making it on of the biggest language model available. It is considered a major breakthrough since it's release in November, 2022.\n",
      "---\n",
      "Performance on natural language processing tasks is dominated by neural end-to-end systems that directly map inputs to outputs [devlins2019bert,liu2019roberta,lewis-etal-2020-bart,T5]. These end-to-end approaches are flexible and easy-to-use while lacking interpretability and robustness. This stands in contrast to symbolic approaches that produce explicit intermediate representations such as logical forms, reasoning paths, or program code, which might then be executed to derive a final output [Zettlemoyer05, gulwani2017program, chen2019neural].  The intermediate form produced by these the resulting execution makes them more robust to input changes. However, their semantic coverage is limited by the affordances of the grammar of the selected symbolic language (e.g., not being able to handle “North America?” in Fig. <ref>), leading to failures on real-world diverse questions, and the intermediate form annotations require expert knowledge and researcher labour.\n",
      "---\n",
      "CodexDB is enabled by recent advances in the domain of natural language processing. Those advances have been fuelled by two key ideas: a novel neural network architecture, the Transformer [Vaswani2017], and new training paradigms, implementing the idea of transfer learning [ruder2019transfer]. The Transformer is nowadays the dominant architecture in the domain of language processing [Wolf2020]. Among other advantages, it lends itself better to parallelization than prior methods. This has, in part, enabled the creation of very large, pre-trained language models. Such models are pre-trained on tasks for which large amounts of training data are easily available, e.g. predicting the next word in text snippets. While pre-training is very expensive, the resulting models can be easily specialized for new tasks via different methods. Fine-tuning describes a process in which pre-trained models are used as a starting point for further training on more specialized tasks (reducing the amount of training samples and computational overheads by orders of magnitude via pre-training [Houlsby2019]). Until recently, fine-tuning has been the primary method of exploiting pre-trained language models. The latest generation of pre-trained models, most notably OpenAI's Generative Pre-Trained Transformer (GPT) version 3, unlocks new possibilities. It turns out that sufficiently large models can oftentimes solve new tasks without specialized training (“zero-shot learning”), based on inputs describing the task in natural language alone [Brown2020]. Precision increases if the input integrates few (i.e., typically less than ten) examples pairing tasks of the same type with solutions (“few-shot learning”). This is the method currently used by CodexDB. The final development that enabled this paper is the emergence of the Codex variant of GPT-3 [Chen2021, OpenAI2021]. The primary difference between GPT-3 Codex and the original GPT-3 model lies in the data used for pre-training. GPT-3 Codex is trained on code and technical documentation. This results in a model whose primary use case is the translation of natural language commands into code.\n",
      "---\n",
      "<Ref> shows that replacing person and activity has negligible impact on the accuracy. However, abstracting out the sports abstract drops the performance below levels (<Ref>).  What explains such a discrepancy? Note that both person and activity are already present in the question, and are copied in the thought. Thus, replacing them even with abstract tokens does not harm the performance, as the extraction pattern is retained. On the other hand, the sport is not present in the question, and has to be extracted. Consider a question: Is the following sentence plausible? “Norman Powell committed a blocking foul.” For abstracted sports prompts, the model generates Norman Powell is a SPORT1 player. Committing a blocking foul is part of SPORT2. So the answer is no. Whereas for the abstract person or the activity prompt, the model correctly infers the pattern (copy the person or the activity, and generate the corresponding sport) and generates: Norman Powell is a basketball player. Being a blocking foul is part of basketball. So the answer is yes. This example also clearly drives the notion of symbiosis:  only generating the right pattern or only having the right text. Both are required for effective .Cases where patterns help.As empirical results show, combining patterns with language reveals their true potential. However, our analysis uncovers interesting reasons for the superiority of patterns only, for the small number of cases.Interestingly, we found a few cases where forcing the model to generate some text leads to errors.Primarily, they were caused by the model trying to make faulty connections. Detailed examples are located in <Ref>.<Ref> presents detailed results, including ablations on various parts of the prompt. These were omitted from the main text due to space constraints.0.99     0          The presence of text is necessary for the model to arrive at the right conclusions.          While the model is sufficiently capable of forming abstractions, employing concordant entities in questions and thoughts is crucial for the success of .          The performance of the model is proportionally sensitive to the degree of variations in the text.We test this hypothesis to determine the role of text in the performance of .Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar.Note that the practicality of the counterfactual experiments for text is contingent on the nature of the target task.In addition, because there is no text in , we did not conduct counterfactual experiments in this dataset.<Ref> summarizes the results and shows sample thought for each counterfactual prompt.Text with discrepant entities text_diff_entities.We first investigate the role of employing identical entities (names and objects) between a question and its associated thought.To achieve this, we simply modify the entities in the thoughts to be discrepant with the ones in the questions.is exclusively relevant for the and datasets for which concrete entities exist: objects and people for and players and activities for .In the dataset, we change the player name and the activity in the thought, while retaining the sport in the question.Samples of generated outputs in <Ref> illustrate that in this scenario, the model appears to disregard the prompt, generating arbitrary thoughts and attaining a similar performance as .The task solve rate for with is noticeably better than , while still fails to match the performance of .We delineate samples of generated outputs in <Ref> to glean more insights.\n",
      "---\n",
      "In the context of prompting LLMs, several previous works have explored their reliability. For example, in the release reports of GPT-3 [GPT3], OPT [Zhang2022OPTOP], Gopher [Rae2021ScalingLM] and PaLM [Chowdhery2022PaLMSL], there are dedicated experiments evaluating these LLMs' representational bias and toxicity. Another line of work has  evaluated calibration [Lin2022TeachingMT,Kadavath2022LanguageM] of prompting-based LLMs on math questions or multiple-choice questions.  We differ from these prior works in two key aspects: (i) We perform a more comprehensive study of four core facets of reliability, serving as a meta-analysis. (ii) We focus particularly on finding prompting strategies that are effective under these reliability facets, rather than just evaluating intrinsic model characteristics (Figure <ref>).Deploying such models in the real world could result in catastrophic harm.generations, false statements [Lin2022TruthfulQAMH],  and outdated information [Chen2021ADF,Kasai2022RealTimeQW].For example, LLMs (including GPT-3) produce biased [Lucy2021GenderAR]However, rising numbers on these evaluations do not ensure LLM reliability.Existing empirical studies investigate GPT-3 on specific tasks such as mathematical reasoning [Hendrycks2021MeasuringMP], multi-hop reasoning [Wei2022ChainOT,Kojima2022LargeLM], and code generation [Codex]..Among all the existing LLMs, GPT-3 [GPT3] is particularly popular due to its flexibility and ease of use from the OpenAI API [By default, we use the code-davinci-002 model (also known as Codex or GPT 3.5) in our experiments unless otherwise specified, because our preliminary results show that this is the most accurate model on most NLP datasets we tried.]These emergent behaviors are accomplished through prompting—a crafted, natural language text to shape predictions or offer relevant information without expensive supervised data.Scaling the model and data size often brings gains on downstream tasks [Scaling,BIG-Bench], allowing what some call emergent abilities [Wei2022EmergentAO].NLP is dominated by large language models (LLMs) — pretrained on large, unlabeled text data — that are then used for downstream tasks [BERT,GPT3].\n",
      "--------------------------------------------------\n",
      "Answer B ($89.89):\n",
      "Natural Language Processing (NLP) is a rapidly growing field of study that works on understanding human language. NLP gives machines the ability to learn human language by turning it into numerical data [khurana2023natural]. With the increasing number of digital texts, the need of NLP is growing rapidly[feder2022causal]. In recent years, NlP has provided a large scale analysis and management of text data making sentiment analysis, emotion detection and other complicated tasks possible. Furthermore, with the help of NLP, it is possible to detect mental illness at early stage and provide treatment [zhang2022natural]. Previously the training of NLP models were slow and inefficient [chen2021crossvit]. Especially after 2017, the innovation of transformer architecture has revolutionized the NLP field. The transformers made NLP tasks to be carried out sequentially that gave birth to large language models like ChatGPT. This models are so efficient that it can imitate human behaviour which creates some reasonable concerns.Transformers are one of the most powerful tools for natural language processing [gillioz2020overview]. It can largely be divided into two parts. The encoder and the decoder. The encoder part takes a text sequence as input and produces a sequence of encoded representation. The difference between other architecture is that the encoded sequence is more context aware. When a series of encoders are stacked togather, it is called a BERT [singh2021nlp]. The decoder part, on the other hand, is able to generate an arbitrary length sequence. Stacking decoder blocks produces an architecture named GPT. ChatGPT is also a transformer based architecture that has been trained on a large set of public data in self-supervised fashion. It has more than a billion parameters making it on of the biggest language model available. It is considered a major breakthrough since it's release in November, 2022.\n",
      "---\n",
      "Performance on natural language processing tasks is dominated by neural end-to-end systems that directly map inputs to outputs [devlins2019bert,liu2019roberta,lewis-etal-2020-bart,T5]. These end-to-end approaches are flexible and easy-to-use while lacking interpretability and robustness. This stands in contrast to symbolic approaches that produce explicit intermediate representations such as logical forms, reasoning paths, or program code, which might then be executed to derive a final output [Zettlemoyer05, gulwani2017program, chen2019neural].  The intermediate form produced by these the resulting execution makes them more robust to input changes. However, their semantic coverage is limited by the affordances of the grammar of the selected symbolic language (e.g., not being able to handle “North America?” in Fig. <ref>), leading to failures on real-world diverse questions, and the intermediate form annotations require expert knowledge and researcher labour.\n",
      "---\n",
      "<Ref> shows that replacing person and activity has negligible impact on the accuracy. However, abstracting out the sports abstract drops the performance below levels (<Ref>).  What explains such a discrepancy? Note that both person and activity are already present in the question, and are copied in the thought. Thus, replacing them even with abstract tokens does not harm the performance, as the extraction pattern is retained. On the other hand, the sport is not present in the question, and has to be extracted. Consider a question: Is the following sentence plausible? “Norman Powell committed a blocking foul.” For abstracted sports prompts, the model generates Norman Powell is a SPORT1 player. Committing a blocking foul is part of SPORT2. So the answer is no. Whereas for the abstract person or the activity prompt, the model correctly infers the pattern (copy the person or the activity, and generate the corresponding sport) and generates: Norman Powell is a basketball player. Being a blocking foul is part of basketball. So the answer is yes. This example also clearly drives the notion of symbiosis:  only generating the right pattern or only having the right text. Both are required for effective .Cases where patterns help.As empirical results show, combining patterns with language reveals their true potential. However, our analysis uncovers interesting reasons for the superiority of patterns only, for the small number of cases.Interestingly, we found a few cases where forcing the model to generate some text leads to errors.Primarily, they were caused by the model trying to make faulty connections. Detailed examples are located in <Ref>.<Ref> presents detailed results, including ablations on various parts of the prompt. These were omitted from the main text due to space constraints.0.99     0          The presence of text is necessary for the model to arrive at the right conclusions.          While the model is sufficiently capable of forming abstractions, employing concordant entities in questions and thoughts is crucial for the success of .          The performance of the model is proportionally sensitive to the degree of variations in the text.We test this hypothesis to determine the role of text in the performance of .Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar.Note that the practicality of the counterfactual experiments for text is contingent on the nature of the target task.In addition, because there is no text in , we did not conduct counterfactual experiments in this dataset.<Ref> summarizes the results and shows sample thought for each counterfactual prompt.Text with discrepant entities text_diff_entities.We first investigate the role of employing identical entities (names and objects) between a question and its associated thought.To achieve this, we simply modify the entities in the thoughts to be discrepant with the ones in the questions.is exclusively relevant for the and datasets for which concrete entities exist: objects and people for and players and activities for .In the dataset, we change the player name and the activity in the thought, while retaining the sport in the question.Samples of generated outputs in <Ref> illustrate that in this scenario, the model appears to disregard the prompt, generating arbitrary thoughts and attaining a similar performance as .The task solve rate for with is noticeably better than , while still fails to match the performance of .We delineate samples of generated outputs in <Ref> to glean more insights.\n",
      "---\n",
      "CodexDB is enabled by recent advances in the domain of natural language processing. Those advances have been fuelled by two key ideas: a novel neural network architecture, the Transformer [Vaswani2017], and new training paradigms, implementing the idea of transfer learning [ruder2019transfer]. The Transformer is nowadays the dominant architecture in the domain of language processing [Wolf2020]. Among other advantages, it lends itself better to parallelization than prior methods. This has, in part, enabled the creation of very large, pre-trained language models. Such models are pre-trained on tasks for which large amounts of training data are easily available, e.g. predicting the next word in text snippets. While pre-training is very expensive, the resulting models can be easily specialized for new tasks via different methods. Fine-tuning describes a process in which pre-trained models are used as a starting point for further training on more specialized tasks (reducing the amount of training samples and computational overheads by orders of magnitude via pre-training [Houlsby2019]). Until recently, fine-tuning has been the primary method of exploiting pre-trained language models. The latest generation of pre-trained models, most notably OpenAI's Generative Pre-Trained Transformer (GPT) version 3, unlocks new possibilities. It turns out that sufficiently large models can oftentimes solve new tasks without specialized training (“zero-shot learning”), based on inputs describing the task in natural language alone [Brown2020]. Precision increases if the input integrates few (i.e., typically less than ten) examples pairing tasks of the same type with solutions (“few-shot learning”). This is the method currently used by CodexDB. The final development that enabled this paper is the emergence of the Codex variant of GPT-3 [Chen2021, OpenAI2021]. The primary difference between GPT-3 Codex and the original GPT-3 model lies in the data used for pre-training. GPT-3 Codex is trained on code and technical documentation. This results in a model whose primary use case is the translation of natural language commands into code.\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "Over the last several years, remarkable progress has been made within the domain of natural language understanding, with machine-learned models able to solve some tasks at near or above human-level performance. This progress has, by and large, been fueled by research centered around 1) better inductive biases, such as the attention-enabled Transformer architecture [vaswani2017attention], 2) the clever leverage of massive corpora of textual data that was historically disregarded as “unlabeled,” usually in the form of pre-training objectives that strive to teach the model the structure of language [radford2019language,devlin2018bert], 3) scaling up model capacity and the methods to support it [shazeer2018adafactor], 4) multi-task learning [raffel2019exploring], and lastly, 5) larger and more diverse datasets along with ever-improving benchmarks that attempt to test the true capabilities of these models. Although these efforts all share the single goal of improving the model's generalization, doing so by explicit changes to the optimization of the loss function has received less attention in comparison.\n",
      "---\n",
      "In the context of prompting LLMs, several previous works have explored their reliability. For example, in the release reports of GPT-3 [GPT3], OPT [Zhang2022OPTOP], Gopher [Rae2021ScalingLM] and PaLM [Chowdhery2022PaLMSL], there are dedicated experiments evaluating these LLMs' representational bias and toxicity. Another line of work has  evaluated calibration [Lin2022TeachingMT,Kadavath2022LanguageM] of prompting-based LLMs on math questions or multiple-choice questions.  We differ from these prior works in two key aspects: (i) We perform a more comprehensive study of four core facets of reliability, serving as a meta-analysis. (ii) We focus particularly on finding prompting strategies that are effective under these reliability facets, rather than just evaluating intrinsic model characteristics (Figure <ref>).Deploying such models in the real world could result in catastrophic harm.generations, false statements [Lin2022TruthfulQAMH],  and outdated information [Chen2021ADF,Kasai2022RealTimeQW].For example, LLMs (including GPT-3) produce biased [Lucy2021GenderAR]However, rising numbers on these evaluations do not ensure LLM reliability.Existing empirical studies investigate GPT-3 on specific tasks such as mathematical reasoning [Hendrycks2021MeasuringMP], multi-hop reasoning [Wei2022ChainOT,Kojima2022LargeLM], and code generation [Codex]..Among all the existing LLMs, GPT-3 [GPT3] is particularly popular due to its flexibility and ease of use from the OpenAI API [By default, we use the code-davinci-002 model (also known as Codex or GPT 3.5) in our experiments unless otherwise specified, because our preliminary results show that this is the most accurate model on most NLP datasets we tried.]These emergent behaviors are accomplished through prompting—a crafted, natural language text to shape predictions or offer relevant information without expensive supervised data.Scaling the model and data size often brings gains on downstream tasks [Scaling,BIG-Bench], allowing what some call emergent abilities [Wei2022EmergentAO].NLP is dominated by large language models (LLMs) — pretrained on large, unlabeled text data — that are then used for downstream tasks [BERT,GPT3].\n",
      "---\n",
      "To the best of our knowledge, this paper is the first to demonstrate how the recently-proposed Sharpness-Aware Minimization can be applied for fine-tuning the ubiquitous text-to-text Transformer (T5) and its multilingual counterpart mT5 on language tasks of broad interest. We thereby corroborate the already-documented success the method has had in the vision domain. Furthermore, we reveal SAM's benefits when data is limited by fine-tuning on subsamples of the original task training split. By approximating the ascent step of the algorithm via fewer samples, we show how large gains can be had across benchmarks and model sizes while adding only around 25% additional compute and wall-clock training time. Our hope is that this work will spur SAM's adoption in the natural language processing community the way it is starting to in the vision one.acl_natbibTable <ref> shows the per-language TyDiQA-GoldP scores. We found that the multilingual mT5 model benefited from a smaller ρ than the vanilla T5 model.Table <ref> shows the full (no subsampling) SuperGLUE results for the Base model for different hyper-parameter choices.\n",
      "---\n",
      "Q1: How does the deductive reasoning ability (DRA) of large language models evolve with their growing size? We find that with the training setup and model architecture kept constant, larger model size leads to higher DRA, but also that this effect can often be compensated by choices made in the training configuration. Previous work has shown that the DRA of models fine-tuned for deduction decreases on data that requires longer chains of reasoning that have been seen in training. <Ref> shows that this does not seem to be the case for foundation models as we evaluated them, that the performance of many models on deeper questions is similar to that on shallower questions, and that it sometimes even increases, especially for OpenAI GPT-1/2 models.Q2: How does the DRA of general-purpose generative large language models depend the number of deductive steps required to reach the conclusions? For depths considered in our study, it remains largely similar throughout, except for the OpenAI GPT-3 and 3.5 models, for whom the performance decreases as seen in <Ref>.\n",
      "---\n",
      "Intelligence is a multifaceted and elusive concept that has long challenged psychologists, philosophers, and computer scientists. There is no generally agreed upon definition of intelligence, but one aspect that is broadly accepted is that intelligence is not limited to a specific domain or task, but rather encompasses a broad range of cognitive skills and abilities.     Building an artificial system that exhibits such broad behavior is a long-standing and ambitious goal of AI research.     In early writings, the founders of the modern discipline of artificial intelligence (AI) research called out sets of aspirational goals for understanding intelligence [mccarthy2006proposal].     Over decades, AI researchers have pursued principles of intelligence, including generalizable mechanisms for reasoning (e.g., [newellshawsimonGPS1959], [lindsay1993dendral]) and construction of knowledge bases containing large corpora of commonsense knowledge [lenat1995cyc].     However, many of the more recent successes in AI research can be described as being narrowly focused on well-defined tasks and challenges, such as playing chess or Go, which were mastered by AI systems in 1996 and 2016, respectively. In the late-1990s and into the 2000s, there were increasing calls for developing more general AI systems (e.g., [selman1996challenge]) and scholarship in the field has sought to identify principles that might underly more generally intelligent systems (e.g., [legg2008machine, gershman2015computational]). The phrase, “artificial general intelligence\" (AGI), was popularized in the early-2000s (see [goertzel2014artificial]) to emphasize the aspiration of moving from the “narrow AI\", as demonstrated in the focused, real-world applications being developed, to broader notions of intelligence, harkening back to the long-term aspirations and dreams of earlier AI research.     We use AGI to refer to systems that demonstrate broad capabilities of intelligence, including reasoning, planning, and the ability to learn from experience, and with these capabilities at or above human-level. We discuss other definitions of AGI in the conclusion section.     The most remarkable breakthrough in AI research of the last few years has been the advancement of natural language processing achieved by large language models (LLMs). These neural network models are based on the Transformer architecture [Vas17] and trained on massive corpora of web-text data, using at its core a self-supervised objective of predicting the next word in a partial sentence.     In this paper, we report on evidence that a new LLM developed by OpenAI, which is an early and non-multimodal version of  [gpt4], exhibits many traits of intelligence.     Despite being purely a language model, this early version of  demonstrates remarkable capabilities on a variety of domains and tasks, including abstraction, comprehension, vision, coding, mathematics, medicine, law, understanding of human motives and emotions, and more.      We interacted with  during its early development by OpenAI using purely natural language queries (prompts)[As ’s development continued after our experiments, one should expect different responses from the final version of . In particular, all quantitative results should be viewed as estimates of the model’s potential, rather than definitive numbers. We repeat this caveat throughout the paper to clarify that the experience on the deployed model may differ. Moreover we emphasize that the version we tested was text-only for inputs, but for simplicity we refer to it as  too.].     In Figure <ref>, we display some preliminary examples of outputs from , asking it to write a proof of infinitude of primes in the form of a poem, to draw a unicorn in TiKZ (a language for creating graphics in ), to create a complex animation in Python, and to solve a high-school level mathematical problem. It easily succeeds at all these tasks, and produces outputs that are essentially indistinguishable from (or even better than) what humans could produce. We      also compare 's performance to those of previous LLMs, most notably ChatGPT, which is a fine-tuned version of (an improved) GPT-3 [gpt3]. In Figure <ref>, we display the results of asking ChatGPT for both the infinitude of primes poem and the TikZ unicorn drawing. While the system performs non-trivially on both tasks, there is no comparison with the outputs from . These preliminary observations will repeat themselves throughout the paper, on a great variety of tasks. The combination of the generality of 's capabilities, with numerous abilities spanning a broad swath of domains, and its performance on a wide spectrum of tasks at or beyond human-level, makes us comfortable      with saying that  is a significant step towards AGI.     Our claim that  represents progress towards AGI does not mean that it is perfect at what it does, or that it comes close to being able to do anything that a human can do (which is one of the usual definition of AGI; see the conclusion section for more on this), or that it has inner motivation and goals (another key aspect in some definitions of AGI). In fact it is not fully clear how far  can go along some of those axes of intelligence that we focus on, e.g., planning (see Section <ref>), and arguably it is entirely missing the learning from experience as the model is not continuously updating (although it can learn within a session, see Section <ref> for example).     Overall  still has many limitations, and biases, which we discuss in detail below and that are also covered in OpenAI's report [gpt4]. In particular it still suffers from some of the well-documented shortcomings of LLMs such as the problem of hallucinations [maynez2020faithfulness] (see Figure <ref>) or making basic arithmetic mistakes [cobbe2021training] (see Appendix <ref>), and yet it has also overcome some fundamental obstacles such as acquiring many non-linguistic capabilities (e.g., it solves most of the LLM failure modes described in [mahowald2023dissociating], and it also made great progress on common-sense, see Figure <ref> for a first example and Appendix <ref> for more). This highlights the fact that, while  is at or beyond human-level for many tasks, overall its patterns of intelligence are decidedly not human-like. However,  is almost certainly only a first step towards a series of increasingly generally intelligent systems, and in fact  itself has improved throughout our time testing it, see Figure <ref> for the evolution of the unicorn drawing over the course of a month of training[Note that the improving we refer to here is a slow type of learning, which eventually comes to a halt, as opposed to the fast-paced and real-time learning one would expect from an AGI.]. Even as a first step, however,  challenges a considerable number of widely held assumptions about machine intelligence, and exhibits emergent behaviors and capabilities whose sources and mechanisms are, at this moment, hard to discern precisely (see again the conclusion section for more discussion on this).      Our primary goal in composing this paper is to share our exploration of 's capabilities and limitations in support of our assessment that a technological leap has been achieved. We believe that 's intelligence signals a true paradigm shift in the field of computer science and beyond.\n",
      "---\n",
      "In the past five years, large sequence models have achieved remarkable successes on solving visual language tasks. In this paper, we take the initial effort to build the connection between multi-agent reinforcement learning (MARL) problems and generic sequence models (SM), with the ambition that MARL researchers can hereafter benefit from the prosperous development on the sequence modeling side.  Specifically, we contribute by unifying a general solution to cooperative MARL problems into a Transformer like encoder-decoder model.    The proposed Multi-Agent Transformer (MAT) leverages the multi-agent advantage decomposition theorem, which essentially transforms the joint policy optimization process into a sequential decision making process that can be simply implemented by an auto-regressive model.    We have demonstrated MAT's  strong empirical performance  on three challenging benchmarks against current state-of-the-art MARL solutions including MAPPO and HAPPO.   Based on the established connection between MARL and SM, in the future, we plan to bring multi-agent learning tasks into large multi-modal SM, chasing for more generally intelligent models as the most recent success of GATO has already demonstrated [gato].§ ACKNOWLEDGMENT\n",
      " The SJTU team is partially supported by “New Generation of AI 2030” Major Project (2018AAA0100900), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), Shanghai Sailing Program (21YF1421900), and National Natural Science Foundation of China (62076161, 62106141).plain\n",
      "---\n",
      "In recent years, sequence models (SM)  have made a substantial progress in natural language processing (NLP) [nadkarni2011natural]. For example,  GPT series [brown2020language] and BERT models [devlin2019bert], built on autoregressive SMs, have demonstrated remarkable performance on a wide range of downstream tasks and achieved strong performance on few-shot  generalization tasks. Although SM are mostly used in language tasks due to its natural fitting with the sequential property of languages, the sequential approaches are not confined to NLP only, but is instead a widely applicable general foundation model [bommasani2021opportunities]. For example, in computer vision (CV), one can split an image into sub-images and align them in a sequence as if they were tokens in NLP tasks [devlin2019bert, dosovitskiy2020image, he2021masked]. Although the idea of solving CV tasks by SM is straightforward, it serves as the foundation to some of the best-performing CV algorithms [touvron2021going, wang2021crossformer, tu2022maxvit]. Furthermore, sequential methods are starting to spawn powerful multi-modal visual language models such as Flamingo [alayrac2022flamingo], DALL-E [ramesh2021zero], and GATO [gato] in the recent past.\n",
      "---\n",
      "Large language models (LLMs) such as GPT-3.x [brown2020language] have emerged as one of the hottest trends in natural language processing over the last few years. With 175 billion parameters, the GPT-3.x language models, which held the record for being the largest neural network ever developed, is an order of magnitude larger than prior language models. Using this massive model architecture, GPT-3.x were trained using almost all accessible data from the Internet, including CommonCrawl [commoncrawl], WebText [kulkarni2009collective], Wikipedia [wikipedia], and a corpus of books. GPT-3.x models surpass the state-of-the-art models in a variety of NLP tasks, including machine translation, question-answering, and close tasks. Furthermore, the GPT-3.x models achieved a significant milestone by showing that unsupervised language models trained with adequate data can multi-task to the same level of fine-tuned models using just a few examples of the new tasks. As a result of its powerful text generation capabilities in new tasks, GPT-3.x are used in a wide range of categories and industries, from productivity and education to creativity and gaming. For instance, GPT-3.x are used to produce creative writing, including blog posts, advertisements, and poetry, that mimics the literary style of well-known writers like Shakespeare.\n",
      "---\n",
      "Scaling Transformer-based models to 100B+ [brown2020language, kaplan2020scaling,rae2021scaling,chinchilla] and later 500B+ parameters [chowdhery2022palm,smith2022using] has led to state of the art results on natural language processing benchmarks. The practical utility of these large language models (LLMs) in a variety of applications makes them compelling for widespread use. While the sequence parallelism of the Transformer architecture enables highly parallel training, efficient deployment of these models is challenging in practice because generative inference proceeds one token at a time and the computation for each token sequentially depends on the previously generated tokens. Thus, models that support efficient training at scales of thousands of chips require careful attention to parallel layout and memory optimizations to unlock the scalability needed for efficient, low-latency inference. This paper focuses on a simple set of engineering principles that enable serving large-scale Transformer-based models efficiently in a variety of challenging production settings.We consider the requirements of downstream applications for LLMs. Some applications, including interactive workloads like chatbots, involve tight latency constraints [thoppilan2022lamda]. Others, including offline inference for scoring or distillation, emphasize high throughput and low cost per token at any latency.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:37,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 109, in run\n",
      "    await self.visit(self.parse_tree, VariableStack([self.program._variables], self))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 524, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_assistant.py\", line 13, in assistant\n",
      "    return await role(role_name=\"assistant\", hidden=hidden, _parser_context=_parser_context, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_role.py\", line 17, in role\n",
      "    new_content += await parser.visit(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in visit\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in <listcomp>\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 379, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_gen.py\", line 140, in gen\n",
      "    gen_obj = await parser.llm_session(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 665, in __call__\n",
      "    out = await self.llm.caller(**call_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 370, in _library_call\n",
      "    out = await openai.ChatCompletion.acreate(**kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 728, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 9777 tokens. Please reduce the length of the messages.\n",
      "\n",
      "Error in program:  This model's maximum context length is 8192 tokens. However, your messages resulted in 9777 tokens. Please reduce the length of the messages.\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "How does reinforcement learning apply to InstructGPT's training procedure?\n",
      "--------------------------------------------------\n",
      "Answer A ($3.57):\n",
      "\\noindentInstruction-tuned Models. Several works have been proposed in the domain of prompt- and instruction-tuning to improve the zero-shot capability of the models.~[zest_weller] introduce a framework, ZEST, to solve a new task after reading its description. The authors formulate the task descriptions as questions to ensure that the model is general enough to apply on the unseen tasks.~[pet_schick] develop a novel pattern exploiting training (PET) scheme to verbalize supervised classification task into cloze question format. The authors show that this pattern leverages the knowledge contained within pretrained language models, resulting in an improvement on few-shot text classification and natural language inference tasks.~[t0_sanh] utilize~[p3_batch] for mapping any natural language tasks into a human-readable prompted form. Thus, the authors deploy a multi-task training approach, where the model is trained to perform multiple tasks simultaneously using task-specific prompts. The model's strong capability on the unseen tasks demonstrates the importance of human-readable prompting for finetuning the downstream tasks.~[flan_wei] finetune language models on a collection of datasets verbalized with natural language instructions. The authors show that the resulting models exhibits impressive generalization capability in zero-shot settings.~[instructgpt_ouyang] propose InstructGPT, where the authors first finetune GPT-3 with labeler-written prompts. Then the authors rank the output with human feedback and further finetune the model with reinforcement learning. The authors show that in the way the output of the model is more aligned with the users' intent. Later, ChatGPT\\footnote{\\href{https://openai.com/blog/chatgpt}{https://openai.com/blog/chatgpt}} followed the same training procedure to develop a conversational agent.\n",
      "---\n",
      "Language model for decision making The strong capability of LLMs has enabled them to perform tasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a policy model for decision making, especially in interactive environments. WebGPT [nakano2021webgpt]  uses an LM to interact with web browsers, navigate through web pages, and infer answers to complicated questions from ELI5 [fan-etal-2019-eli5]. In comparison to , WebGPT does not explicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for reinforcement learning. In conversation modeling, chatbots like BlenderBot [shuster2022blenderbot3] and Sparrow [amelia2022improving] and task-oriented dialogue systems like SimpleTOD [hosseini2020simple] also train LMs to make decision about API calls. Unlike , they do not explicitly consider the reasoning procedure either, and also relies on expensive datasets and human feedback collections for policy learning. In contrast,  learns a policy in a much cheaper way, since the decision making process only requires language description of the reasoning procedure.[Human feedback can also be incorporated in a complementary manner but we leave it for future work.]\n",
      "---\n",
      "The limited improvement of Alpaca might be attributed to its instruction tuning procedure consisting of “a list of user-oriented instructions including email writing, social media, and productivity tools” [alpaca], which differs from reasoning-intensive tasks. We suggest future work to examine different types of instruction tuning (e.g., focused on reasoning procedures or reinforcement learning from human feedback), which might help the model answer more complex types of questions in a step-by-step manner and more accurately. We hypothesize that the different performances in versions of GPT-3 Davinci might be produced by the specific type of instructions used for training, by the reinforcement learning component [ouyang2022instructGPT], or simply by an extension of the language modeling pre-training. It is challenging to pinpoint the exact factor in the training procedure that contributes to this improvement, as specific methodological details are not available.\n",
      "---\n",
      "Building on recent works successfully using Reinforcement Learning (RL) to finetune LLMs for natural language generation tasks [stiennon-etal-2020-learning, ouyang-etal-2022-training, ramamurthy-etal-2022-is], we propose the first study about functional grounding of LLMs through incremental online RL. In particular, we aim at empirically answering the following open scientific questions:   ∙ Q1. Sample efficiency How fast can an LLM adapt and learn to solve various spatial and navigation problems specified in natural language? How does the use of pre-trained knowledge from LLM boosts sample efficiency?∙ Q2. Generalization to new objects: Once functionally grounded, how can an LLM generalize to various kinds of changes about objects, yet staying in trained tasks?∙ Q3. Generalization to new tasks: How can such an interactively trained LLM perform zero-shot generalization to new tasks? How does generalization depend on the kind of new tasks? ∙ Q4. Impact of online interventions: What is the empirical impact of grounding using online RL with incremental interactions in comparison with offline Behavioral Cloning from a dataset of expert trajectories?\n",
      "--------------------------------------------------\n",
      "Answer B ($91.17):\n",
      "\\noindentInstruction-tuned Models. Several works have been proposed in the domain of prompt- and instruction-tuning to improve the zero-shot capability of the models.~[zest_weller] introduce a framework, ZEST, to solve a new task after reading its description. The authors formulate the task descriptions as questions to ensure that the model is general enough to apply on the unseen tasks.~[pet_schick] develop a novel pattern exploiting training (PET) scheme to verbalize supervised classification task into cloze question format. The authors show that this pattern leverages the knowledge contained within pretrained language models, resulting in an improvement on few-shot text classification and natural language inference tasks.~[t0_sanh] utilize~[p3_batch] for mapping any natural language tasks into a human-readable prompted form. Thus, the authors deploy a multi-task training approach, where the model is trained to perform multiple tasks simultaneously using task-specific prompts. The model's strong capability on the unseen tasks demonstrates the importance of human-readable prompting for finetuning the downstream tasks.~[flan_wei] finetune language models on a collection of datasets verbalized with natural language instructions. The authors show that the resulting models exhibits impressive generalization capability in zero-shot settings.~[instructgpt_ouyang] propose InstructGPT, where the authors first finetune GPT-3 with labeler-written prompts. Then the authors rank the output with human feedback and further finetune the model with reinforcement learning. The authors show that in the way the output of the model is more aligned with the users' intent. Later, ChatGPT\\footnote{\\href{https://openai.com/blog/chatgpt}{https://openai.com/blog/chatgpt}} followed the same training procedure to develop a conversational agent.\n",
      "---\n",
      "The limited improvement of Alpaca might be attributed to its instruction tuning procedure consisting of “a list of user-oriented instructions including email writing, social media, and productivity tools” [alpaca], which differs from reasoning-intensive tasks. We suggest future work to examine different types of instruction tuning (e.g., focused on reasoning procedures or reinforcement learning from human feedback), which might help the model answer more complex types of questions in a step-by-step manner and more accurately. We hypothesize that the different performances in versions of GPT-3 Davinci might be produced by the specific type of instructions used for training, by the reinforcement learning component [ouyang2022instructGPT], or simply by an extension of the language modeling pre-training. It is challenging to pinpoint the exact factor in the training procedure that contributes to this improvement, as specific methodological details are not available.\n",
      "---\n",
      "Few- and zero-shot baselines do not match fully finetuned models' performance, but considerably improve over the majority baseline.  For UnifiedQA-v2 in particular, this suggests that some reasoning about implications of negation is acquired during pretraining. Surprisingly, UnifiedQA-v2 few-shot performance is worse than zero-shot.  While this behavior has been reported for in-context learning with GPT-3 [brown2020language, DBLP:conf/iclr/XieRL022], we did not expect to observe this for a finetuned model.[A lower learning rate or less training steps do not help improve UnifiedQA-v2 few-shot performance.]  UnifiedQA-v2-3B finetuned with a few examples is comparable to InstructGPT (; at least 175B parameters) with in-context learning.  Chain-of-thought prompting (CoT) notably improves the performance of InstructGPT, especially in terms of the most challenging metrics: scope and affirmative consistency.  In the zero-shot setting, the 11B version of UnifiedQA-v2 performs the best, while the base version of only 220M parameters is comparable to InstructGPT.  UnifiedQA-v2-11B is also better than FLAN-T5-XXL (a 11B-parameter model as well).  Given that UnifiedQA-v1 [khashabi-etal-2020-unifiedqa] has been effective for tasks beyond QA [DBLP:conf/nips/BraggCLB21, marasovic-etal-2022-shot-correct], this result suggests that UnifiedQA models are strong but overlooked baselines in recent works on large-scale models.\n",
      "---\n",
      "Building on recent works successfully using Reinforcement Learning (RL) to finetune LLMs for natural language generation tasks [stiennon-etal-2020-learning, ouyang-etal-2022-training, ramamurthy-etal-2022-is], we propose the first study about functional grounding of LLMs through incremental online RL. In particular, we aim at empirically answering the following open scientific questions:   ∙ Q1. Sample efficiency How fast can an LLM adapt and learn to solve various spatial and navigation problems specified in natural language? How does the use of pre-trained knowledge from LLM boosts sample efficiency?∙ Q2. Generalization to new objects: Once functionally grounded, how can an LLM generalize to various kinds of changes about objects, yet staying in trained tasks?∙ Q3. Generalization to new tasks: How can such an interactively trained LLM perform zero-shot generalization to new tasks? How does generalization depend on the kind of new tasks? ∙ Q4. Impact of online interventions: What is the empirical impact of grounding using online RL with incremental interactions in comparison with offline Behavioral Cloning from a dataset of expert trajectories?\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "Language model for decision making The strong capability of LLMs has enabled them to perform tasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a policy model for decision making, especially in interactive environments. WebGPT [nakano2021webgpt]  uses an LM to interact with web browsers, navigate through web pages, and infer answers to complicated questions from ELI5 [fan-etal-2019-eli5]. In comparison to , WebGPT does not explicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for reinforcement learning. In conversation modeling, chatbots like BlenderBot [shuster2022blenderbot3] and Sparrow [amelia2022improving] and task-oriented dialogue systems like SimpleTOD [hosseini2020simple] also train LMs to make decision about API calls. Unlike , they do not explicitly consider the reasoning procedure either, and also relies on expensive datasets and human feedback collections for policy learning. In contrast,  learns a policy in a much cheaper way, since the decision making process only requires language description of the reasoning procedure.[Human feedback can also be incorporated in a complementary manner but we leave it for future work.]\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "We leave a more thorough investigation of our approach on  (a) the HumanEval dataset [codex_2021] and (b) real-world benchmarks possibly collected from open-source projects on GitHub.  While Codex [codex_2021] has been shown to be one of the most effective LLM for code generation, we would like to leverage and possibly fine-tune neural models in the future for both code and test generation, including models such as InCoder [incoder_2022], CodeGen [codegen_2022], PolyCoder [polycoder_2022].  We also leave as future work a user study of  in place of the simulated quantitative evaluation to further explore the usefulness of our approach. Finally, we use tests as an instance of a partial specification of intent for a function; we believe one can easily extend the framework to richer forms of formal specifications (e.g., procedure summaries) provided we have a scalable generator and checker of likely specifications.Acknowledgements. We are grateful to Todd Mytkowicz for initial discussions on the idea of simulating user response and initial implementation of the dynamic mutation algorithm, and Johannes Gehrke, Mark Encarnación, Andres Codas, Mei Yang and Rahee Ghosh Peshawaria for fruitful discussions and feedback on this work.We use the sanitized version of the MBPP dataset [google_llm_2021], an academic code generation dataset, to answer RQ1 through RQ3. This version consists of 427 ⟨_p, s_p, h_p, b_p, T_p ⟩ tuples as per Definition <ref> where b_p is the ground truth definition of the corresponding function. One example of such a tuple has been discussed in <ref>.We pose the following research questions in order to evaluate different approaches and techniques for test-driven user-intent formalization problem:  \n",
      "  * RQ1: Does test-driven user-intent formalization workflow improve the accuracy of code suggestions? \n",
      "  * RQ2: How does the quality of generated code and tests vary with the number of user-queries? \n",
      "  * RQ3: How does each of the design decisions affect the metrics (ablation study)?\n",
      "---\n",
      "In this work, we tackle the task of financial sentiment analysis on Reddit with an LLM distilled into a production-friendly student model. With minimal human-annotated data, our classifier performs on par with existing supervised models and generalizes well across other datasets. The application of our model does pose a product challenge:  how can we incorporate the model's output responsibly, delivering value to users without misleading them or inadvertently amplifying malicious behavior? Nevertheless, our investigation highlights the promise of in-context learning with LLMs for textual tasks that are hard for human raters to annotate. Can human raters, instead of simply labeling the data, help design a domain-knowledge-injected prompt teaching the LLM to perform the task, or otherwise \"collaborate\" with the LLM? How can automatic prompt-tuning further optimize the human-engineered prompt? Exploring the answers to these questions would be a compelling direction for future work.\n",
      "---\n",
      "In-context Learning Recently, prompting LMs for few-shot learning has achieved remarkable success via in-context learning [brown2020language]. Despite its empirical success, in-context learning is known to suffer from over-sensitivity to instruction wording [schick-schutze-2021-exploiting, jiang-etal-2020-know, gao-etal-2021-making]Sameer has a recent work that shows ICL performance is related to term frequency in pretraining, which might be relevant. and example ordering [zhao2021calibrate, liu2021makes]. Several methods have been proposed to reduce LM sensitivity. [zhou2022prompt] propose to explicitly fine-tune the LM to output consistent predictions across diverse prompts, so as to achieve better zero-shot accuracy and reduce sensitivity. [chen-etal-2022-meta] propose to meta-train LMs to learn in-context learning, and show that meta-learning effectively reduces LM sensitivity with respect to example ordering and example selection. Despite these efforts, state-of-the-art instruction-tuned LMs such as T0Is this still sota? I thought InstructGPT is better. [sanh2021multitask] are still highly sensitive to the prompt format.\n",
      "---\n",
      "\\begin{abstract} Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs, e.g., making them follow given instructions while keeping them less toxic. However, it requires a significant amount of human demonstrations and feedback. Recently, open-sourced models have attempted to replicate the alignment learning process by distilling data from already aligned LLMs like InstructGPT or ChatGPT. While this process reduces human efforts, constructing these datasets has a heavy dependency on the teacher models. In this work, we propose a novel framework for alignment learning with almost no human labor and no dependency on pre-aligned LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM for simulating high-quality demonstrations to train a supervised policy and for further optimizing the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms open-sourced models, including Alpaca, Dolly, and OpenAssistant, which are trained on the outputs of InstructGPT or human-annotated instructions. Our $7$B-sized model outperforms the $12$-$13$B models in the A/B tests using GPT-4 as the judge with about 75\\% winning rate on average. \\end{abstract}\\onecolumn\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:05, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "InstructGPT, developed by OpenAI, is trained using a two-step process: pretraining and fine-tuning. Reinforcement Learning from Human Feedback (RLHF) is used in the fine-tuning stage.\n",
      "\n",
      "During the fine-tuning process, reinforcement learning is used to optimize the model's responses based on feedback from human AI trainers. These trainers provide rankings for different model-generated responses, which are used to create a reward model. The model is then fine-tuned using Proximal Policy Optimization, a reinforcement learning algorithm, to generate responses that are more likely to receive higher rankings.\n",
      "\n",
      "This process allows the model to learn from its mistakes and improve over time, which is a key aspect of reinforcement learning. It's important to note that while this process helps in improving the model's performance, it doesn't guarantee perfect responses and the model can still make mistakes or generate inappropriate responses.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: Looking at the answers from both students, it's clear that they both have a good understanding of the topic. However, I think student A's answer is more comprehensive and detailed.\n",
      "\n",
      "Bobby: I disagree, Michael. While student A's answer is indeed detailed, it seems to me that it goes off on tangents and includes a lot of information that isn't directly relevant to the question. On the other hand, student B's answer is more concise and directly addresses the question.\n",
      "\n",
      "Michael: I see your point, Bobby. However, I believe that the additional information in student A's answer provides valuable context and depth. It shows a broader understanding of the topic.\n",
      "\n",
      "Bobby: That may be true, but the question specifically asks about the application of reinforcement learning in InstructGPT's training procedure. I think student B does a better job at directly addressing this. Student A's answer, while informative, seems to meander a bit.\n",
      "\n",
      "Michael: You're right, Bobby. Upon reflection, I agree that student B's answer is more focused and directly answers the question. While the additional information in student A's answer is interesting, it doesn't directly address the question.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is reinforcement learning?\n",
      "--------------------------------------------------\n",
      "Answer A ($22.85):\n",
      "What do our results imply about LMs' abilities for in-context learning? The process by which LMs adapt to a prompt is debated. For example, [min2022rethinking] make the interesting observation that GPT-3 can perform nearly as well on common tasks like entailment even with random answers in the prompt, and therefore question whether the models are really “learning” in context. Comparisons between our explanations and matched controls allow us to test whether superficial features of the explanations drive their benefits.\n",
      "---\n",
      "* Can retriever-augmented language models perform multi-step reasoning given supporting statements?     The experimental results show that retriever-augmented language models have some difficulties in performing multi-step entailments or logical reasoning in solving QA and LM examples, especially when facing some distracting statements.     \n",
      "  * What are some shortcomings of retrievers which undermine reasoning performance?               By investigating the shortcomings of retrievers in selecting the required statements for reasoning, we conclude that selecting the statements based on their similarity with the queries is insufficient for the reasoning nature. This approach works promisingly when similar forms of the query containing the answer exist among the supporting statements, while there are sometimes no such statements when reasoning is required to solve a task.     \n",
      "  * What are some shortcomings of language models which undermine reasoning performance?               The results demonstrate that language models cannot consider the relations between the statements, which is necessary for reasoning. Interestingly, we also observe that language models perform much better when a statement containing information from all relevant ones is provided.      \n",
      "  * What is the impact of model size?     Our experimental results using various sizes of Flan-T5 model demonstrate that larger models lead to better performance on reasoning tasks.     \n",
      "  * What is the impact of using multihop retrieval?     The experimental results with different sizes of Flan-T5 model and GPT-3.5 using a recent multihop approach reflect improvement in some, but not all of the experiments which shows there is still significant room for improvement in this area.Augmenting language models with external corpora or memory has been well studied in literature [realm, fid, atlas, knnlm]. Additionally, researchers have recently been interested in eliciting the reasoning abilities of parametric language models in tasks where answering the questions involves multiple computational steps [chain-of-thoughts,flan]. Next, we discuss some of the recent studies in these areas.\n",
      "---\n",
      "Past work has characterized what kinds of functions ICL can learn [Garg2022WhatCT, laskin2022context, muller2021transformers] and the distributional properties of pretraining that can elicit in-context learning [xie2021explanation, chan2022data]. But how ICL learns these functions has remained unclear. What learning algorithms (if any) are implementable by deep network models? Which algorithms are actually discovered in the course of training?  This paper takes first steps toward answering these questions, focusing on a widely used model architecture (the transformer) and an extremely well-understood class of learning problems (linear regression).\n",
      "---\n",
      "On our CMATH dataset, we conduct evaluation for a variety of popular LLMs, accessible via commercial API or released model weights. We discover that  GPT-4 is the only model that achieves success (accuracy ≥ 60%) across all six elementary school grades. We also examine the robustness of LLMs against the distracting information. It turns out that GPT-4 is again the sole model that maintains robustness, while other models are easily misled by the presence of distracting information.They require understanding natural language questions and translating them into mathematical expressions/equations. This tests a model's linguistic abilities.They require applying mathematical knowledge and reasoning logically through multiple steps. This tests a model's symbolic manipulation and reasoning skills.They often involve commonsense knowledge about the real world. This tests a model's knowledge grounding and understanding.They come in a range of difficulties, allowing us to probe model abilities at different levels of depth. From grade 1 to 6, the problems become increasingly complex and abstract.* How well can state-of-the-art Chinese language models solve elementary school math word problems? What are their limitations? \n",
      "  * What types of reasoning or math abilities do current models struggle with most? Logical, linguistic, commonsense or mathematical? \n",
      "  * How can we develop models with stronger reasoning and problem-solving skills? What abilities are most important to improve?We release CMATH, the first Chinese dataset for evaluating math reasoning abilities of language models. CMATH contains 1,000 elementary school level math word problems with detailed annotations.We conduct the first systematic evaluation of popular Chinese language models on math word problems. We show how CMATH can reveal differences in the math reasoning skills of language models at various ability levels.We find that although elementary school word problems seem simple, solving them requires sophisticated linguistics, logical and commonsense reasoning that current language models do not possess. CMATH provides a challenging testbed for continued progress in these directions.Our work establishes a first framework for evaluating and enhancing the math reasoning skills of Chinese language models. We release CMATH as a public benchmark to support future work in this direction.0.85! \n",
      "    < g r a p h i c s >\n",
      "  Sample problems along with their English translations (not part of the dataset) and human annotations. The column title “#Steps” and “#Digits” stand for “number of reasoning steps” and “number of digits” respectively.This work is motivated by the following question:  To what grade level of elementary school math do the abilities of popular LLMs correspond?  We create the CMATH dataset in order to  answer this question. We believe that the evaluation results of LLMs should be presented in an intuitive manner, making them easily understandable for the general public.\n",
      "---\n",
      "Recent advances in large-scale generative models [Brown2020LanguageMA, Nichol2021GLIDETP, Ramesh2021ZeroShotTG] have led to a major paradigm shift towards building general-purpose models, which can perform many new tasks not explicitly trained on. These generative models learn to predict the missing data given the context. Post training, we can directly input a question, optionally with a few contextual question-answer examples, instead of fine-tuning with labeled data.  To give a concrete example, the model can answer a question like, “What is the capital of Japan?” with an example of such a relationship in the context: “The capital of Germany is Berlin. The capital of Japan is”.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "To validate the effectiveness of our MEFT methods, we conduct extensive evaluations on the GLUE benchmark [DBLP:conf/iclr/WangSMHLB19] with BERT [DBLP:conf/naacl/DevlinCLT19], RoBERTa [DBLP:journals/corr/abs-1907-11692] and BART [DBLP:conf/acl/LewisLGGMLSZ20] (<ref>). The experimental results consistently demonstrate that our MEFT methods outperform both full fine-tuning and strong PEFT baselines in terms of parameter and memory efficiency. Remarkably, our methods achieve the same score as full fine-tuning while updating only 0.2% of the parameters and saving up to 84% of the activation memory. Furthermore, we evaluate MEFT on five question-answering tasks with a larger model, OPT [DBLP:journals/corr/abs-2205-01068]. The results show that our approach achieves a comparable score as full fine-tuning while saving 50% of the activation memory and updating only 0.64% of the parameters. Collectively, these experiments establish the effectiveness of MEFT as a powerful parameter- and memory-efficient approach that does not compromise performance.In this section, we aim to provide essential background knowledge by addressing the following questions: (1) Why are existing PEFTs not sufficiently memory-efficient (<ref>)? (2) What is a key factor for the success of PEFT (<ref>)? (3) What challenges does a reversible model have (<ref>)?\n",
      "---\n",
      "To validate the effectiveness of our MEFT methods, we conduct extensive evaluations on the GLUE benchmark [DBLP:conf/iclr/WangSMHLB19] with BERT [DBLP:conf/naacl/DevlinCLT19], RoBERTa [DBLP:journals/corr/abs-1907-11692] and BART [DBLP:conf/acl/LewisLGGMLSZ20] (<ref>). The experimental results consistently demonstrate that our MEFT methods outperform both full fine-tuning and strong PEFT baselines in terms of parameter and memory efficiency. Remarkably, our methods achieve the same score as full fine-tuning while updating only 0.2% of the parameters and saving up to 84% of the activation memory. Furthermore, we evaluate MEFT on five question-answering tasks with a larger model, OPT [DBLP:journals/corr/abs-2205-01068]. The results show that our approach achieves a comparable score as full fine-tuning while saving 50% of the activation memory and updating only 0.64% of the parameters. Collectively, these experiments establish the effectiveness of MEFT as a powerful parameter- and memory-efficient approach that does not compromise performance.In this section, we aim to provide essential background knowledge by addressing the following questions: (1) Why are existing PEFTs not sufficiently memory-efficient (<ref>)? (2) What is a key factor for the success of PEFT (<ref>)? (3) What challenges does a reversible model have (<ref>)?\n",
      "--------------------------------------------------\n",
      "Answer B ($63.04):\n",
      "What do our results imply about LMs' abilities for in-context learning? The process by which LMs adapt to a prompt is debated. For example, [min2022rethinking] make the interesting observation that GPT-3 can perform nearly as well on common tasks like entailment even with random answers in the prompt, and therefore question whether the models are really “learning” in context. Comparisons between our explanations and matched controls allow us to test whether superficial features of the explanations drive their benefits.\n",
      "---\n",
      "* Can retriever-augmented language models perform multi-step reasoning given supporting statements?     The experimental results show that retriever-augmented language models have some difficulties in performing multi-step entailments or logical reasoning in solving QA and LM examples, especially when facing some distracting statements.     \n",
      "  * What are some shortcomings of retrievers which undermine reasoning performance?               By investigating the shortcomings of retrievers in selecting the required statements for reasoning, we conclude that selecting the statements based on their similarity with the queries is insufficient for the reasoning nature. This approach works promisingly when similar forms of the query containing the answer exist among the supporting statements, while there are sometimes no such statements when reasoning is required to solve a task.     \n",
      "  * What are some shortcomings of language models which undermine reasoning performance?               The results demonstrate that language models cannot consider the relations between the statements, which is necessary for reasoning. Interestingly, we also observe that language models perform much better when a statement containing information from all relevant ones is provided.      \n",
      "  * What is the impact of model size?     Our experimental results using various sizes of Flan-T5 model demonstrate that larger models lead to better performance on reasoning tasks.     \n",
      "  * What is the impact of using multihop retrieval?     The experimental results with different sizes of Flan-T5 model and GPT-3.5 using a recent multihop approach reflect improvement in some, but not all of the experiments which shows there is still significant room for improvement in this area.Augmenting language models with external corpora or memory has been well studied in literature [realm, fid, atlas, knnlm]. Additionally, researchers have recently been interested in eliciting the reasoning abilities of parametric language models in tasks where answering the questions involves multiple computational steps [chain-of-thoughts,flan]. Next, we discuss some of the recent studies in these areas.\n",
      "---\n",
      "Past work has characterized what kinds of functions ICL can learn [Garg2022WhatCT, laskin2022context, muller2021transformers] and the distributional properties of pretraining that can elicit in-context learning [xie2021explanation, chan2022data]. But how ICL learns these functions has remained unclear. What learning algorithms (if any) are implementable by deep network models? Which algorithms are actually discovered in the course of training?  This paper takes first steps toward answering these questions, focusing on a widely used model architecture (the transformer) and an extremely well-understood class of learning problems (linear regression).\n",
      "---\n",
      "On our CMATH dataset, we conduct evaluation for a variety of popular LLMs, accessible via commercial API or released model weights. We discover that  GPT-4 is the only model that achieves success (accuracy ≥ 60%) across all six elementary school grades. We also examine the robustness of LLMs against the distracting information. It turns out that GPT-4 is again the sole model that maintains robustness, while other models are easily misled by the presence of distracting information.They require understanding natural language questions and translating them into mathematical expressions/equations. This tests a model's linguistic abilities.They require applying mathematical knowledge and reasoning logically through multiple steps. This tests a model's symbolic manipulation and reasoning skills.They often involve commonsense knowledge about the real world. This tests a model's knowledge grounding and understanding.They come in a range of difficulties, allowing us to probe model abilities at different levels of depth. From grade 1 to 6, the problems become increasingly complex and abstract.* How well can state-of-the-art Chinese language models solve elementary school math word problems? What are their limitations? \n",
      "  * What types of reasoning or math abilities do current models struggle with most? Logical, linguistic, commonsense or mathematical? \n",
      "  * How can we develop models with stronger reasoning and problem-solving skills? What abilities are most important to improve?We release CMATH, the first Chinese dataset for evaluating math reasoning abilities of language models. CMATH contains 1,000 elementary school level math word problems with detailed annotations.We conduct the first systematic evaluation of popular Chinese language models on math word problems. We show how CMATH can reveal differences in the math reasoning skills of language models at various ability levels.We find that although elementary school word problems seem simple, solving them requires sophisticated linguistics, logical and commonsense reasoning that current language models do not possess. CMATH provides a challenging testbed for continued progress in these directions.Our work establishes a first framework for evaluating and enhancing the math reasoning skills of Chinese language models. We release CMATH as a public benchmark to support future work in this direction.0.85! \n",
      "    < g r a p h i c s >\n",
      "  Sample problems along with their English translations (not part of the dataset) and human annotations. The column title “#Steps” and “#Digits” stand for “number of reasoning steps” and “number of digits” respectively.This work is motivated by the following question:  To what grade level of elementary school math do the abilities of popular LLMs correspond?  We create the CMATH dataset in order to  answer this question. We believe that the evaluation results of LLMs should be presented in an intuitive manner, making them easily understandable for the general public.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "To validate the effectiveness of our MEFT methods, we conduct extensive evaluations on the GLUE benchmark [DBLP:conf/iclr/WangSMHLB19] with BERT [DBLP:conf/naacl/DevlinCLT19], RoBERTa [DBLP:journals/corr/abs-1907-11692] and BART [DBLP:conf/acl/LewisLGGMLSZ20] (<ref>). The experimental results consistently demonstrate that our MEFT methods outperform both full fine-tuning and strong PEFT baselines in terms of parameter and memory efficiency. Remarkably, our methods achieve the same score as full fine-tuning while updating only 0.2% of the parameters and saving up to 84% of the activation memory. Furthermore, we evaluate MEFT on five question-answering tasks with a larger model, OPT [DBLP:journals/corr/abs-2205-01068]. The results show that our approach achieves a comparable score as full fine-tuning while saving 50% of the activation memory and updating only 0.64% of the parameters. Collectively, these experiments establish the effectiveness of MEFT as a powerful parameter- and memory-efficient approach that does not compromise performance.In this section, we aim to provide essential background knowledge by addressing the following questions: (1) Why are existing PEFTs not sufficiently memory-efficient (<ref>)? (2) What is a key factor for the success of PEFT (<ref>)? (3) What challenges does a reversible model have (<ref>)?\n",
      "---\n",
      "To validate the effectiveness of our MEFT methods, we conduct extensive evaluations on the GLUE benchmark [DBLP:conf/iclr/WangSMHLB19] with BERT [DBLP:conf/naacl/DevlinCLT19], RoBERTa [DBLP:journals/corr/abs-1907-11692] and BART [DBLP:conf/acl/LewisLGGMLSZ20] (<ref>). The experimental results consistently demonstrate that our MEFT methods outperform both full fine-tuning and strong PEFT baselines in terms of parameter and memory efficiency. Remarkably, our methods achieve the same score as full fine-tuning while updating only 0.2% of the parameters and saving up to 84% of the activation memory. Furthermore, we evaluate MEFT on five question-answering tasks with a larger model, OPT [DBLP:journals/corr/abs-2205-01068]. The results show that our approach achieves a comparable score as full fine-tuning while saving 50% of the activation memory and updating only 0.64% of the parameters. Collectively, these experiments establish the effectiveness of MEFT as a powerful parameter- and memory-efficient approach that does not compromise performance.In this section, we aim to provide essential background knowledge by addressing the following questions: (1) Why are existing PEFTs not sufficiently memory-efficient (<ref>)? (2) What is a key factor for the success of PEFT (<ref>)? (3) What challenges does a reversible model have (<ref>)?\n",
      "---\n",
      "A cornerstone of the recent remarkable advancements in the capabilities of large language models (LLMs) like ChatGPT and GPT-4 is the integration of human feedback [ouyang2022training,openai2023gpt]. The approach to leveraging human feedback often begins with the training of a reward model that encapsulates human preferences, values, and ethical considerations [christiano2017deep,ibarz2018reward, bahdanau2018learning,ziegler2019fine,ganguli2022red]. This is followed by the fine-tuning of the LLMs using reinforcement learning, guided by the reward model. This process, often referred to as reinforcement learning from human feedback (RLHF), has proven effective in aligning LLMs with human intent, substantially enriching the quality of human interaction.\n",
      "---\n",
      "Building on recent works successfully using Reinforcement Learning (RL) to finetune LLMs for natural language generation tasks [stiennon-etal-2020-learning, ouyang-etal-2022-training, ramamurthy-etal-2022-is], we propose the first study about functional grounding of LLMs through incremental online RL. In particular, we aim at empirically answering the following open scientific questions:   ∙ Q1. Sample efficiency How fast can an LLM adapt and learn to solve various spatial and navigation problems specified in natural language? How does the use of pre-trained knowledge from LLM boosts sample efficiency?∙ Q2. Generalization to new objects: Once functionally grounded, how can an LLM generalize to various kinds of changes about objects, yet staying in trained tasks?∙ Q3. Generalization to new tasks: How can such an interactively trained LLM perform zero-shot generalization to new tasks? How does generalization depend on the kind of new tasks? ∙ Q4. Impact of online interventions: What is the empirical impact of grounding using online RL with incremental interactions in comparison with offline Behavioral Cloning from a dataset of expert trajectories?\n",
      "---\n",
      "What are the smallest α and m for which any given class of distributions  of size n is α-learnable with sample complexity m? A seminal work by Yatracos [yatracos85] (also see [devroye1996universally, devroye1997nonasymptotic, Devroye01combinatorial]) shows that any reference class Q of size n is 3-learnable with sample complexity O(log n/^2).  For the case of n=2, Mahalanabis and Stefankovic [Mahalanabis08density] improve the approximation factor, constructing a 2-learner.  This was extended by the recent work of Bousquet, Kane, and Moran [BKM19] to give a 2-approximation for any finite n, using a very different scheme. A matching lower bound of 2 on the approximation factor follows from the work of [Chan14histograms].\n",
      "---\n",
      "Quantum machine learning Quantum machine learning (QML) algorithms have been developed for a wide array of classical ML tasks, such as clustering [h20], boosting [am20], support vector machine [rml14], principal component analysis [lmr14_pca], statistical query learning [agy20]. In addition, quantum algorithms for learning quantum data have increasingly garnered attention in recent years. (See [aa23] and references therein.) Conversely, QML algorithms can also inspire breakthroughs in classical ML algorithms. This was notably illustrated by Tang's algorithm for the recommendation system [t19]. Since then, a long list of quantum-inspired (or so-called “de-quantized”) algorithms have been proposed for tackling various tasks, such as principal component analysis [t18], low-rank approximation [glt18,cgt+20], linear regression [gst20], etc.Roadmap.We have organized our paper as follows. In Section <ref>, we introduce the notations and present some basic mathematical tools used throughout the paper.  In Section <ref>, we present our algorithm, which constructs a sparsity matrix using Grover's Search, and provide a comprehensive analysis of its performance, including time complexity and error analysis. In Section <ref>, we introduce an algorithm where sparsity matrix is constructed using the half-space reporting data structure.Our achievement revolves around the quantum algorithm for rapid attention computation based on sparsity matrix constructed by two methods above, which is presented in Section <ref>. And our main result about quantum attention computation in inference is presented in Section <ref>.1arrowstheoremTheorem lemmaLemma definitionDefinition notationNotationpropositionProposition corollaryCorollary conjectureConjecture assumptionAssumption observationObservation factFact remarkRemark claimClaim exampleExample problemProblem openOpen Problem propertyProperty hypothesisHypothesisIntern Project\n",
      "    [NO \\author GIVEN]\n",
      "    August 28, 2023\n",
      "======================\n",
      "\n",
      "????[NO \\title GIVEN]\n",
      "    [NO \\author GIVEN]\n",
      "    August 28, 2023\n",
      "======================emptyalphaalphaFor any matrix A, we denote the spectral norm of A as A, which is defined as A  := max_x_2 = 1Ax_2. The Frobenius norm of A is denoted as A _F, and the infinity norm is denoted as A _∞. In this notation, A_i,j represents the element in the i-th row and j-th column of matrix A. The determinant of matrix A is represented as (A).For a square and symmetric matrix A ∈ℝ^n × n, we say that A is positive semi-definite (A ≽ 0) if for all vectors x ∈ℝ^n, we have x^⊤ A x ≥ 0.We state a well-known result about the quadratic quantum speedup for the unstructured search using Grover's search algorithm.\n",
      "---\n",
      "Red teaming with automated searches for natural language prompts. Given a specific target behavior of a language model (such as outputting toxic or dishonest text), a red team's job is to find inputs that can elicit that behavior. This is challenging for two reasons. First, embedding and sampling discrete tokens is not differentiable, and second, manual searches are expensive. Several methods have been proposed for efficiently automating prompt search absent the ability to propagate gradients. These include local search [prasad2022grips], gradient-informed searches over token changes [ebrahimi2017hotflip, li2018textbugger, ren2019generating, shin2020autoprompt, jones2023automatically], searches based on Langevin dynamics [shi2022toward, kumar2022gradient], the Gumbel Softmax trick [wallace2019universal, song2020universal, guo2021gradient], rejection sampling at scale [ganguli2022red], projecting soft prompts onto hard prompts [wen2023hard], and reinforcement learning [deng2022rlprompt, perez2022red]. Here, we use reinforcement learning because it is demonstrated to be effective and, unlike most such tools, results in a distribution of prompts that can be sampled as opposed to a single prompt.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:33, 18.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to achieve a goal. The agent is rewarded or penalized (with points, for example) for the actions it takes, with the aim of maximizing the total reward. The agent learns from trial and error, so it needs to balance the exploration of uncharted territory with the exploitation of its current knowledge. This learning process is used in various kinds of applications where autonomous vehicles, game playing, robotics, resource management, and many other areas can be optimized with reinforcement learning.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: Well, Bobby, it seems neither of the students directly answered the question about reinforcement learning. They both provided extensive information on various aspects of machine learning, but they didn't define reinforcement learning as per the question.\n",
      "\n",
      "Bobby: I agree, Michael. However, if we look at the content, student B's answer seems to be slightly more relevant. They mention reinforcement learning and its application in fine-tuning large language models, which is a part of the broader context of reinforcement learning.\n",
      "\n",
      "Michael: That's true, Bobby. But student A also mentioned reinforcement learning in the context of language models and provided a detailed explanation of how it's used in the fine-tuning process. \n",
      "\n",
      "Bobby: Yes, but student B's answer was more concise and to the point. They didn't go off on tangents as much as student A did. \n",
      "\n",
      "Michael: I see your point, Bobby. While neither of the students directly answered the question, student B's answer was more focused and relevant to the topic of reinforcement learning. \n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the purpose of cross-validation in machine learning?\n",
      "--------------------------------------------------\n",
      "Answer A ($23.90):\n",
      "What do our results imply about LMs' abilities for in-context learning? The process by which LMs adapt to a prompt is debated. For example, [min2022rethinking] make the interesting observation that GPT-3 can perform nearly as well on common tasks like entailment even with random answers in the prompt, and therefore question whether the models are really “learning” in context. Comparisons between our explanations and matched controls allow us to test whether superficial features of the explanations drive their benefits.\n",
      "---\n",
      "On our CMATH dataset, we conduct evaluation for a variety of popular LLMs, accessible via commercial API or released model weights. We discover that  GPT-4 is the only model that achieves success (accuracy ≥ 60%) across all six elementary school grades. We also examine the robustness of LLMs against the distracting information. It turns out that GPT-4 is again the sole model that maintains robustness, while other models are easily misled by the presence of distracting information.They require understanding natural language questions and translating them into mathematical expressions/equations. This tests a model's linguistic abilities.They require applying mathematical knowledge and reasoning logically through multiple steps. This tests a model's symbolic manipulation and reasoning skills.They often involve commonsense knowledge about the real world. This tests a model's knowledge grounding and understanding.They come in a range of difficulties, allowing us to probe model abilities at different levels of depth. From grade 1 to 6, the problems become increasingly complex and abstract.* How well can state-of-the-art Chinese language models solve elementary school math word problems? What are their limitations? \n",
      "  * What types of reasoning or math abilities do current models struggle with most? Logical, linguistic, commonsense or mathematical? \n",
      "  * How can we develop models with stronger reasoning and problem-solving skills? What abilities are most important to improve?We release CMATH, the first Chinese dataset for evaluating math reasoning abilities of language models. CMATH contains 1,000 elementary school level math word problems with detailed annotations.We conduct the first systematic evaluation of popular Chinese language models on math word problems. We show how CMATH can reveal differences in the math reasoning skills of language models at various ability levels.We find that although elementary school word problems seem simple, solving them requires sophisticated linguistics, logical and commonsense reasoning that current language models do not possess. CMATH provides a challenging testbed for continued progress in these directions.Our work establishes a first framework for evaluating and enhancing the math reasoning skills of Chinese language models. We release CMATH as a public benchmark to support future work in this direction.0.85! \n",
      "    < g r a p h i c s >\n",
      "  Sample problems along with their English translations (not part of the dataset) and human annotations. The column title “#Steps” and “#Digits” stand for “number of reasoning steps” and “number of digits” respectively.This work is motivated by the following question:  To what grade level of elementary school math do the abilities of popular LLMs correspond?  We create the CMATH dataset in order to  answer this question. We believe that the evaluation results of LLMs should be presented in an intuitive manner, making them easily understandable for the general public.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "What are the benefits and drawbacks of the datasets we used? Using tasks from [bigbench] allowed us to collect a set of diverse, challenging tasks within a similar format. These tasks cover skills which may not be well-represented in standard datasets. However, these idiosyncratic adversarially-sampled tasks may therefore not be representative of tasks to which LMs are usually applied. This difference could potentially amplify or suppress our effects. Explanations might be uniquely helpful in challenging tasks where the relationship between questions and answers is unusual. Alternatively, explanations might be more effective in more standard settings where they could relate more directly to common reasoning patterns. The breadth of tasks we explored, together with statistical analyses that explicitly model task differences, partially ameliorate these concerns. Nevertheless, future work should explore the benefits of explanations beyond BIG-Bench.\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "As for the risk of overfitting associated with synthetic data, a mixed approach utilizing both real and synthetic data for model training could be adopted. This hybrid strategy could leverage the benefits of synthetic data while preserving the capacity to generalize to real-world conditions. Techniques such as regularization, cross-validation, and ensemble methods can also be deployed to counter overfitting. For instance, in the telematics example, a blend of real-world data and synthetic data encompassing various driving conditions could lead to a more robust and generalizable model.To summarize, while synthetic data indeed presents challenges, these are not insurmountable barriers. It's essential to underscore that while all these concerns are valid and warrant attention, the scope of this paper is primarily to initiate the conversation around generating a telematics dataset using ChatGPT. With careful application, continuous improvement, and strategic advancements, synthetic data can prove an invaluable tool in data science, particularly in data-sensitive fields, and further research in this area is meritedChatGPT, a product of OpenAI, represents an advanced language model utilizing machine learning to generate text reminiscent of human discourse. Its potential extends beyond mere text generation, enabling the creation of structured data in response to specific prompts.Leveraging the capabilities of ChatGPT, this paper proposes a practical demonstration involving the generation of a synthetic dataset. The focus will be on crafting a rudimentary telematics dataset. This dataset, while simplified, is intended to serve as an educational resource, encapsulating key data points inherent in telematics analytics.This study emphasizes the creation of a synthetic dataset with composition limited to randomly generated values. While this approach may not replicate the direct correspondence found in real-world data, it serves as a highly conducive environment for exploratory research, analytical processing, and in-depth learning. In fact, the apparent simplicity of this randomly generated data allows for clearer understanding and manipulation, providing an ideal introductory tool in the realm of telematics data.Throughout the process of generating and exploring this synthetic dataset, it is critical to bear in mind the primary objective is not an exact emulation of real-world telematics data. Instead, the focus lies in providing an accessible gateway for individuals aiming to develop a foundational understanding of such complex data. Thus, the value of this synthetic dataset lies not in its replication of real-world data, but in its ability to facilitate learning, discovery, and comprehension of telematics data structure and interpretation.In the generation of synthetic datasets using ChatGPT, the initial stage involves the delineation of the data's anticipated format and structure. When contemplating a scenario involving tabular traffic pattern data, for instance, the structure might comprise elements such as date, time, location, traffic volume, and weather conditions.Following this, the process segues into the formulation of a sequence of prompts which serve to guide ChatGPT about the character of the data intended for generation. Armed with these well-structured prompts, ChatGPT stands primed to assist effectively in the orchestration of a synthetic telematics dataset.Engaging with the process of synthetic data creation entails acknowledging its extensive potential that extends beyond mere research and demonstration. In scenarios where synthetic data are expected to integrate into production or operational environments, the process is not restricted to the act of data generation alone.A vital part of the process is the incorporation of a robust validation and testing phase, essential for validating the quality of the synthetic data and evaluating its suitability for its intended function. This crucial phase could involve statistical comparisons with real data and the employment of synthetic data within various models.Though this paper primarily concentrates on delving into potential applications, rather than preparing for real-world operational use, the importance of a comprehensive validation process should not be diminished for those intending to broaden their exploration of synthetic data. This pivotal step facilitates the unlocking of a synthetic dataset's full potential, thus charting the path towards robust, privacy-conscious, and holistic data solutions.Telematics, the synergistic blend of telecommunications and informatics, plays a pivotal role in today's data-driven era, specifically within the realms of vehicular operation and urban mobility. At its core, telematics is concerned with the collection, processing, and transmission of information pertaining to remote entities, predominantly vehicles, facilitated by telecommunication devices.Consequently, the breadth of telematics data encapsulates numerous informational parameters, including a vehicle's precise location, speed, idle time, instances of harsh acceleration or abrupt braking, fuel consumption, along with other intricate diagnostic data. This wealth of information is typically harvested via a combination of GPS technology, onboard diagnostics, and sensory devices. The richness and scope of telematics data make it a compelling field for the application of synthetic data generation.Telematics data holds the power to deliver transformative insights across diverse sectors, with particular prominence in urban planning and traffic management. A comprehensive understanding of traffic patterns, derived from this data, is an indispensable asset for managing road infrastructure planning and traffic congestion. This knowledge facilitates the pinpointing of peak congestion times and zones, the formulation of strategies for optimizing traffic flow, the planning of enhanced public transportation routes, and the overall improvement of the commuter experience.With the growing ubiquity of Internet of Things (IoT) devices and the enhancement of connectivity, telematics data has emerged as a critical component in smart city initiatives. Real-time traffic data can be harnessed to steer emergency services towards quicker response times, or to inform eco-routing strategies aimed at curtailing carbon emissions. Therefore, the potential and impact of telematics data extend far beyond its conventional applications, leading the charge towards a smarter and more sustainable urban environment.Beyond immediate operational applications, telematics data also has immense potential for research and innovation. Researchers can leverage this data to predict future traffic trends, improve urban planning, and facilitate the development of autonomous vehicles. Given the growing concern for privacy and data security, synthetic telematics data, mirroring real-world data without compromising privacy, has become a significant research area. Synthetic datasets can aid researchers in bypassing privacy-related obstacles and uncovering new insights in urban mobility.The process of synthesizing a telematics dataset with ChatGPT involves prompt experimentation and iterative refinement, with gradual increases in prompt complexity. The initial step starts with a simple open-ended prompt such as, “Can you create a telematics dataset for me?” This step provides an insight into ChatGPT's interpretation and structuring of telematics data.This initial, basic prompt serves as our first insight into how ChatGPT understands and structures telematics data. It forms the foundation upon which we build, enhancing our prompts iteratively for better and more specific outputs.The beauty of this approach lies in its progressive nature, reminiscent of a teaching or training process where we start with the basics and gradually introduce more complex elements.From our first prompt, \"Can you create a telematics dataset for me?\", we glean several key observations. Firstly, ChatGPT demonstrates an understanding of what telematics data is, signifying that it is capable of generating data relevant to our use case.However, the output is quite short, consisting of only five rows, which indicates that we may need to provide clearer instructions about the size of the dataset we desire.Moreover, while the model generates telematics data, the output format is not readily usable. The data is not in a format, such as a CSV or Pandas' DataFrame, that is amenable to further analysis or processing.These crucial insights inform us that while ChatGPT understands the concept of a 'telematics dataset', it requires more specific instructions to generate the type and format of data we're targeting. Consequently, these revelations guide us as we craft more complex prompts in our subsequent iterations.Moving forward in our data generation journey, our second prompt brings more specificity to the table, directly addressing some limitations we noticed with our initial, more open-ended prompt. Our refined prompt reads, \"Can you create a telematics dataset for me that has 100,000 rows and is exportable into CSV format or a Pandas DataFrame?\"This expanded prompt offers a few key enhancements:    \n",
      "  * Dataset Size: Explicitly requesting 100,000 rows sets a clear expectation for the scale of the dataset, influencing potential data generation methods and potential optimizations for handling larger datasets.   \n",
      "  * Format: The prompt's specification for an exportable CSV format or a Pandas' DataFrame offers clear guidance on the dataset's structure.The second attempt underscores the value of providing explicit instructions in the prompts. Precise requirements can minimize the likelihood of needing multiple iterations and can enhance the chances of producing a synthetic dataset aligned with research needs from the outset.Although the second prompt exhibits marked improvement over the initial attempt, the refinement process is iterative, offering continual opportunities for enhancement. The objective is not to produce a flawless synthetic telematics dataset at the first instance, but to understand the capabilities and limitations of the ChatGPT model more deeply, and to learn how to steer it towards producing the needed data.With this insight, the focus shifts to the creation of the third prompt. Building upon the improvements achieved so far, the objective becomes to strike an optimal balance between specificity and data diversity. This approach aims to more effectively guide ChatGPT in the generation of a synthetic dataset that is both usable and better aligned with the requirements.Venturing further into the exploration of synthetic dataset creation, the third prompt incorporates an additional layer of complexity: context. This step involves asking ChatGPT to envision a real-life scenario, providing a more tangible backdrop for the data generation process.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "--------------------------------------------------\n",
      "Answer B ($81.93):\n",
      "What do our results imply about LMs' abilities for in-context learning? The process by which LMs adapt to a prompt is debated. For example, [min2022rethinking] make the interesting observation that GPT-3 can perform nearly as well on common tasks like entailment even with random answers in the prompt, and therefore question whether the models are really “learning” in context. Comparisons between our explanations and matched controls allow us to test whether superficial features of the explanations drive their benefits.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "What are the benefits and drawbacks of the datasets we used? Using tasks from [bigbench] allowed us to collect a set of diverse, challenging tasks within a similar format. These tasks cover skills which may not be well-represented in standard datasets. However, these idiosyncratic adversarially-sampled tasks may therefore not be representative of tasks to which LMs are usually applied. This difference could potentially amplify or suppress our effects. Explanations might be uniquely helpful in challenging tasks where the relationship between questions and answers is unusual. Alternatively, explanations might be more effective in more standard settings where they could relate more directly to common reasoning patterns. The breadth of tasks we explored, together with statistical analyses that explicitly model task differences, partially ameliorate these concerns. Nevertheless, future work should explore the benefits of explanations beyond BIG-Bench.\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "<Ref> shows that replacing person and activity has negligible impact on the accuracy. However, abstracting out the sports abstract drops the performance below levels (<Ref>).  What explains such a discrepancy? Note that both person and activity are already present in the question, and are copied in the thought. Thus, replacing them even with abstract tokens does not harm the performance, as the extraction pattern is retained. On the other hand, the sport is not present in the question, and has to be extracted. Consider a question: Is the following sentence plausible? “Norman Powell committed a blocking foul.” For abstracted sports prompts, the model generates Norman Powell is a SPORT1 player. Committing a blocking foul is part of SPORT2. So the answer is no. Whereas for the abstract person or the activity prompt, the model correctly infers the pattern (copy the person or the activity, and generate the corresponding sport) and generates: Norman Powell is a basketball player. Being a blocking foul is part of basketball. So the answer is yes. This example also clearly drives the notion of symbiosis:  only generating the right pattern or only having the right text. Both are required for effective .Cases where patterns help.As empirical results show, combining patterns with language reveals their true potential. However, our analysis uncovers interesting reasons for the superiority of patterns only, for the small number of cases.Interestingly, we found a few cases where forcing the model to generate some text leads to errors.Primarily, they were caused by the model trying to make faulty connections. Detailed examples are located in <Ref>.<Ref> presents detailed results, including ablations on various parts of the prompt. These were omitted from the main text due to space constraints.0.99     0          The presence of text is necessary for the model to arrive at the right conclusions.          While the model is sufficiently capable of forming abstractions, employing concordant entities in questions and thoughts is crucial for the success of .          The performance of the model is proportionally sensitive to the degree of variations in the text.We test this hypothesis to determine the role of text in the performance of .Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar.Note that the practicality of the counterfactual experiments for text is contingent on the nature of the target task.In addition, because there is no text in , we did not conduct counterfactual experiments in this dataset.<Ref> summarizes the results and shows sample thought for each counterfactual prompt.Text with discrepant entities text_diff_entities.We first investigate the role of employing identical entities (names and objects) between a question and its associated thought.To achieve this, we simply modify the entities in the thoughts to be discrepant with the ones in the questions.is exclusively relevant for the and datasets for which concrete entities exist: objects and people for and players and activities for .In the dataset, we change the player name and the activity in the thought, while retaining the sport in the question.Samples of generated outputs in <Ref> illustrate that in this scenario, the model appears to disregard the prompt, generating arbitrary thoughts and attaining a similar performance as .The task solve rate for with is noticeably better than , while still fails to match the performance of .We delineate samples of generated outputs in <Ref> to glean more insights.\n",
      "---\n",
      "As for the risk of overfitting associated with synthetic data, a mixed approach utilizing both real and synthetic data for model training could be adopted. This hybrid strategy could leverage the benefits of synthetic data while preserving the capacity to generalize to real-world conditions. Techniques such as regularization, cross-validation, and ensemble methods can also be deployed to counter overfitting. For instance, in the telematics example, a blend of real-world data and synthetic data encompassing various driving conditions could lead to a more robust and generalizable model.To summarize, while synthetic data indeed presents challenges, these are not insurmountable barriers. It's essential to underscore that while all these concerns are valid and warrant attention, the scope of this paper is primarily to initiate the conversation around generating a telematics dataset using ChatGPT. With careful application, continuous improvement, and strategic advancements, synthetic data can prove an invaluable tool in data science, particularly in data-sensitive fields, and further research in this area is meritedChatGPT, a product of OpenAI, represents an advanced language model utilizing machine learning to generate text reminiscent of human discourse. Its potential extends beyond mere text generation, enabling the creation of structured data in response to specific prompts.Leveraging the capabilities of ChatGPT, this paper proposes a practical demonstration involving the generation of a synthetic dataset. The focus will be on crafting a rudimentary telematics dataset. This dataset, while simplified, is intended to serve as an educational resource, encapsulating key data points inherent in telematics analytics.This study emphasizes the creation of a synthetic dataset with composition limited to randomly generated values. While this approach may not replicate the direct correspondence found in real-world data, it serves as a highly conducive environment for exploratory research, analytical processing, and in-depth learning. In fact, the apparent simplicity of this randomly generated data allows for clearer understanding and manipulation, providing an ideal introductory tool in the realm of telematics data.Throughout the process of generating and exploring this synthetic dataset, it is critical to bear in mind the primary objective is not an exact emulation of real-world telematics data. Instead, the focus lies in providing an accessible gateway for individuals aiming to develop a foundational understanding of such complex data. Thus, the value of this synthetic dataset lies not in its replication of real-world data, but in its ability to facilitate learning, discovery, and comprehension of telematics data structure and interpretation.In the generation of synthetic datasets using ChatGPT, the initial stage involves the delineation of the data's anticipated format and structure. When contemplating a scenario involving tabular traffic pattern data, for instance, the structure might comprise elements such as date, time, location, traffic volume, and weather conditions.Following this, the process segues into the formulation of a sequence of prompts which serve to guide ChatGPT about the character of the data intended for generation. Armed with these well-structured prompts, ChatGPT stands primed to assist effectively in the orchestration of a synthetic telematics dataset.Engaging with the process of synthetic data creation entails acknowledging its extensive potential that extends beyond mere research and demonstration. In scenarios where synthetic data are expected to integrate into production or operational environments, the process is not restricted to the act of data generation alone.A vital part of the process is the incorporation of a robust validation and testing phase, essential for validating the quality of the synthetic data and evaluating its suitability for its intended function. This crucial phase could involve statistical comparisons with real data and the employment of synthetic data within various models.Though this paper primarily concentrates on delving into potential applications, rather than preparing for real-world operational use, the importance of a comprehensive validation process should not be diminished for those intending to broaden their exploration of synthetic data. This pivotal step facilitates the unlocking of a synthetic dataset's full potential, thus charting the path towards robust, privacy-conscious, and holistic data solutions.Telematics, the synergistic blend of telecommunications and informatics, plays a pivotal role in today's data-driven era, specifically within the realms of vehicular operation and urban mobility. At its core, telematics is concerned with the collection, processing, and transmission of information pertaining to remote entities, predominantly vehicles, facilitated by telecommunication devices.Consequently, the breadth of telematics data encapsulates numerous informational parameters, including a vehicle's precise location, speed, idle time, instances of harsh acceleration or abrupt braking, fuel consumption, along with other intricate diagnostic data. This wealth of information is typically harvested via a combination of GPS technology, onboard diagnostics, and sensory devices. The richness and scope of telematics data make it a compelling field for the application of synthetic data generation.Telematics data holds the power to deliver transformative insights across diverse sectors, with particular prominence in urban planning and traffic management. A comprehensive understanding of traffic patterns, derived from this data, is an indispensable asset for managing road infrastructure planning and traffic congestion. This knowledge facilitates the pinpointing of peak congestion times and zones, the formulation of strategies for optimizing traffic flow, the planning of enhanced public transportation routes, and the overall improvement of the commuter experience.With the growing ubiquity of Internet of Things (IoT) devices and the enhancement of connectivity, telematics data has emerged as a critical component in smart city initiatives. Real-time traffic data can be harnessed to steer emergency services towards quicker response times, or to inform eco-routing strategies aimed at curtailing carbon emissions. Therefore, the potential and impact of telematics data extend far beyond its conventional applications, leading the charge towards a smarter and more sustainable urban environment.Beyond immediate operational applications, telematics data also has immense potential for research and innovation. Researchers can leverage this data to predict future traffic trends, improve urban planning, and facilitate the development of autonomous vehicles. Given the growing concern for privacy and data security, synthetic telematics data, mirroring real-world data without compromising privacy, has become a significant research area. Synthetic datasets can aid researchers in bypassing privacy-related obstacles and uncovering new insights in urban mobility.The process of synthesizing a telematics dataset with ChatGPT involves prompt experimentation and iterative refinement, with gradual increases in prompt complexity. The initial step starts with a simple open-ended prompt such as, “Can you create a telematics dataset for me?” This step provides an insight into ChatGPT's interpretation and structuring of telematics data.This initial, basic prompt serves as our first insight into how ChatGPT understands and structures telematics data. It forms the foundation upon which we build, enhancing our prompts iteratively for better and more specific outputs.The beauty of this approach lies in its progressive nature, reminiscent of a teaching or training process where we start with the basics and gradually introduce more complex elements.From our first prompt, \"Can you create a telematics dataset for me?\", we glean several key observations. Firstly, ChatGPT demonstrates an understanding of what telematics data is, signifying that it is capable of generating data relevant to our use case.However, the output is quite short, consisting of only five rows, which indicates that we may need to provide clearer instructions about the size of the dataset we desire.Moreover, while the model generates telematics data, the output format is not readily usable. The data is not in a format, such as a CSV or Pandas' DataFrame, that is amenable to further analysis or processing.These crucial insights inform us that while ChatGPT understands the concept of a 'telematics dataset', it requires more specific instructions to generate the type and format of data we're targeting. Consequently, these revelations guide us as we craft more complex prompts in our subsequent iterations.Moving forward in our data generation journey, our second prompt brings more specificity to the table, directly addressing some limitations we noticed with our initial, more open-ended prompt. Our refined prompt reads, \"Can you create a telematics dataset for me that has 100,000 rows and is exportable into CSV format or a Pandas DataFrame?\"This expanded prompt offers a few key enhancements:    \n",
      "  * Dataset Size: Explicitly requesting 100,000 rows sets a clear expectation for the scale of the dataset, influencing potential data generation methods and potential optimizations for handling larger datasets.   \n",
      "  * Format: The prompt's specification for an exportable CSV format or a Pandas' DataFrame offers clear guidance on the dataset's structure.The second attempt underscores the value of providing explicit instructions in the prompts. Precise requirements can minimize the likelihood of needing multiple iterations and can enhance the chances of producing a synthetic dataset aligned with research needs from the outset.Although the second prompt exhibits marked improvement over the initial attempt, the refinement process is iterative, offering continual opportunities for enhancement. The objective is not to produce a flawless synthetic telematics dataset at the first instance, but to understand the capabilities and limitations of the ChatGPT model more deeply, and to learn how to steer it towards producing the needed data.With this insight, the focus shifts to the creation of the third prompt. Building upon the improvements achieved so far, the objective becomes to strike an optimal balance between specificity and data diversity. This approach aims to more effectively guide ChatGPT in the generation of a synthetic dataset that is both usable and better aligned with the requirements.Venturing further into the exploration of synthetic dataset creation, the third prompt incorporates an additional layer of complexity: context. This step involves asking ChatGPT to envision a real-life scenario, providing a more tangible backdrop for the data generation process.\n",
      "---\n",
      "Large Language Models (LLMs) have been a popular topic of research among the academic community [srivastava2022beyond]. The promise of a near-general purpose neural model for a variety of language processing tasks is indeed an attractive one [xu2022systematic]. Deep learning has made significant developments in language tasks such as conversational language understanding [tur2018deep], spoken/text-based dialog systems [celikyilmaz2018deep], and natural language generation from images [he2018deep]. Large Language models can be viewed as the natural progression away from the rigid rule-based systems that we've had since the 1950's [chiticariu2013rule], continuing the empiricist mentality of statistical natural language processing without the potentially costly and context-specific activity of feature engineering [collobert2011natural]. However, with large corporations touting their ever-growing, state-of-the-art models under closed-source code and payment walls, it could be seen that these large language models are becoming less accessible. Some organizations have acknowledged the potential harms that deep learning models could cause by establishing ethical frameworks [ashurst2022ai] [weidinger2022taxonomy], but there are still growing concerns regarding accessibility and the result of false/irreproducible science [kapoor2022leakage].\n",
      "---\n",
      "We discover repetitive attention patterns given trained language models. One interesting question that needs to be answered is whether such behavior is a model architecture bias or an unexpected training outcome. For such purpose, we perform the same experiment with a randomly initialized OPT, and compare it against the results presented in Section <ref>. As shown in Figure <ref>, the repetitive attention pattern does not exist in randomly initialized models. Apart from an efficiency deployment perspective, could such repetitive attention patterns contribute to some known problem in language generation such as repetitions? It may be worth investigating the relationship between repetitive attention patterns and undesired generations.R8.9cm     \n",
      "          \n",
      "    < g r a p h i c s >\n",
      "               \n",
      "          \n",
      "    < g r a p h i c s >\n",
      "                                         We plot the attention map corresponding to Section <ref> but with a randomly initialized OPT. We observe no repetitive attention for a randomly initialized model.Due to the limitation of the server in academics, the largest model we can fit is OPT-66B. We try to understand the behavior and verify the generality across the different models and datasets. However, we cannot access the training process and fail to know exactly how an LLM is trained to exhibit such behavior. Experiments with the large model create carbon dioxide emissions. However, our work improves the efficiency of LLM, and we  foresee no negative impacts.Inspired by our intriguing findings that pivotal tokens exert a lasting influence on future steps, we developed  to leverage this observation to reduce the memory usage of KV cache. Our method achieves memory reductions of 5× in the KV cache without compromising the performance of LLMs. Furthermore, we demonstrate the compatibility of  with quantization techniques, opening up the possibility of reducing memory usage in both the representation and sequence length dimensions.§ APPENDIXThe problem of sampling and quantizing kv-cache can naturally cast as approximating the output of attentions. We can analyse quantization and sampling separately. A note on sampling : Given the nature of sampling, a closely related problem is that of reservoir sampling. However, the problem is slightly different as the weights associated with the weighted summation change with addition of new tokens which opens up new avenues for variance reduction. We start with simple abstract sampling design with E(S)=1.1. Any sampling design that ensures E(S_i) = 1 is a good to get almost unbiased estimates of attention2. Variance reduction of denominator ( also numerator)  requires sampling with probability proportional to weights.  While the problem of weighted reservoir sampling is well understood, the problem of dynamic weights is rather understudied (verify) and often ifeasible.3. We give a reservior sampling for dynamic weights with weight evolution assumptions motivated from the problem at hand. (π(k|j)  π(k|j-1).  The algorithm and proof are straightfroward.4. Proving / intuiton as to why the assumption holds. Either jaspers proof can be modified for this or can be used as a intuition.  Also, we can give empirical evidence.We can thus cover the sampling part under standard (modifed) sampling theory and the sections can be made rigorous with notations borrowed from the sampling literature.Quantisation can be analysed separately. Although it deviates from the central storyIn this section we will talk about reduction of kv-cache for a particular attention head given a budget of tokens say n. The problem can be interpreted as sampling problem with the target to get an accurate estimate of the output of attention head at token, say t+1. The computation in attention can be written as,\n",
      "---\n",
      "We apply to LLaMA-7B and LLaMA-13B [touvron2023llama] and tune it on the Alpaca dataset [alpaca] to compare to Stanford Alpaca, and on the open-source ShareGPT dataset to compare to Vicuna [vicuna2023]. For evaluation, we follow the pipeline in Vicuna [vicuna2023], , we ask questions that span different categories and use GPT-4 [openai2023gpt4] to score the answers provided by the model. Scores are based on the helpfulness, relevance, accuracy, and level of detail of the answers. In Table <ref>, we compare the scores of different models on each category of questions. All the scores are on a scale of 1-10. Overall, has slightly better performance than fine-tuning for each baseline model, while LoRA is less competitive. We observe that is better at providing detailed and informed answers, which makes it a better assistant for questions about general knowledge (Generic, Knowledge, Common). Figure <ref> is an example. Given the question What are the most effective strategies for conflict resolution in the workplace, while the fine-tuned baseline lists several suggestions, is able to provide more detailed explanations for each suggestion. On the other hand, fine-tuning is more helpful when following the instruction for creative questions such as Roleplay and Counterfact. Figure <ref> gives an example of a roleplay question. provides a detailed explanation of what to say to the crews, while the fine-tuned baseline directly creates a short speech which more adheres to the instruction. These observations are consistent with the underlying mechanism of and fine-tuning, , does not modify the pre-trained backbone and thus is able to “remember” all the knowledge learned during pre-training, meanwhile fine-tuning may forget what is learned during pre-training when modifying the weights but is able to better follow the instruction in this way. More examples are shown in Appendix.\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "Prior efforts to improve compositional generalization primarily rely on specialized architectures or training procedures [lake2019compositional,xinyun2020neuralsymbolic,nye2020learning,andreas-2020-good,conklin-etal-2021-meta,ekina2021recombine,liu-etal-2021-learning-algebraic]. Although effective, these can be task-specific. Even more general purpose methods that rely on data augmentation are limited in the class of data it can support [shaw2020compositional,qiu-etal-2022-improving]. Prompting on the other hand is sufficiently flexible and, with recent advancement of large-scale pretrained language models (LLMs), has become an effective and generic approach to address a wide range of language understanding problems [gpt3]. Prompting now performs on-par or better than model finetuning in many cases [Wei2022ChainOT,palm2022,wei2022emergent,kojima2022large,palm2022saycan], and might be suitable for improving language model performance on compositional generalization.Compositionality is a key part of human intelligence as it allows us to understand and produce a potentially infinite number of novel combinations of known components [chomsky1957syntax,Montague1970grammar,lake2017buildingmachines]. In contrast, standard neural sequence models, transformers and recurrent neural networks, often fail to capture the compositional structure of the problem domain and thus fail to generalize compositionally [Keysers2020MeasuringCG,Furrer2020CompositionalGI].\n",
      "---\n",
      "We present our results in Table <ref>.  As the dataset was constructed to be adversarial to BERT, it is not surprising that it performs the worst of three models despite generally outperforming GPT on most other benchmarks.  Comparing GPT and RoBERTa we see that despite more training data, a larger vocabulary, twice the number of parameters and careful construction of robust training, there is only a 8pt performance gain and RoBERTa still falls roughly 18 points short of human performance on this task.  As noted throughout, exploring this gap is precisely the purpose for  existing and which facets of the dataset fool RoBERTa is the focus of the remainder of this paper.In this section, we unpack the results of state-of-the-art models on . In particular, we take a look at the errors made by the top-performing model RoBERTa, as a view towards the physical commonsense knowledge that can be learned through language alone.The setup of  allows us to use it to probe the inner workings of deep pretrained language models, and to determine the extent of their physical knowledge. In this way, our dataset can augment prior work on studying to what extent models such as BERT understand syntax [goldberg2019assessing]. However, while syntax is a well studied problem within linguistics, physical commonsense does not have as rich a literature to borrow from, making its dimensions challenging to pin down.Simple concepts. Understanding the physical world requires a deep understanding of simple concepts, such as “water” or “ketchup,” and their affordances and interactions with respect to other concepts.Though our dataset covers interactions between and with common objects, we can analyze the space of concepts in the dataset by performing a string alignment between solution pairs. Two solution choices that differ by editing a single phrase must by definition test the commonsense understanding of that phrase.\n",
      "---\n",
      "To detect AI generated texts multiple approaches are proposed. Traditionally Sebastian Gehrmann et al. [gehrmann2019gltr] proposed a statistical method to distinguish machine and human generated text. The paper introduces a tool named GLTR. The GLTR tool is a 6-gram character-level statistical language model model, which is trained on a large corpus of text data. The tool uses this model to calculate the probability of each character in the generated text, and then shows any character that has a low probability of occurring in the training corpus. Anton Bakhtin et al. [bakhtin2019real] proposed a Energy-based model (EBM) to discriminate machine generated text. EBM is also a statistical model which finds an energy function from given data. The authors used a comparatively larger dataset collected from human to machine conversation. Eric Mitchell et al.[mitchell2023detectgpt] proposed a zero shot learning method called DetectGPT. This method detects whether text is machine-written or not by calculating log probabilities computed by the model of interest. On text samples generated by the GPT-2, the research team conducted an  study [solaiman2019release]. Atsumu Harada  et al. [harada2021discrimination] gathered two datasets, one with sentences produced by humans, the other with sentences written by both humans and machines. The cosine similarity between sentence pairs was then calculated as a measure of text consistency. Finally, based on the cosine similarity ratings of the sentences, they classified them as either human-written or human and machine-written using machine learning methods. Sandra Mitrovic et al. [mitrovic2023chatgpt] proposed a transformer based model to detect chatgpt generated texts. To determine if a text was produced by ChatGPT or a person, the paper's authors developed a machine learning methodology. The model was trained on a dataset of 10,000 text samples that were classified as either human- or ChatGPT-generated. It is based on a combination of text-based and user-based attributes. Tiziano Fagni et al. [fagni2021tweepfake] proposed a method to detect deepFake tweets. At first tweets were generated by different language models. Then different machine learning methods were used with tf-idf and Bag of Words techniques.Sankar Sadasivan et al. [sadasivan2023can]  evaluated the effectiveness of several existing approaches for detecting AI-generated text including rule-based methods, statistical methods, and machine learning-based methods.They discover that although these techniques can be effective in identifying some sorts of AI-generated text, they are frequently open to adversarial attempts that trick them into thinking the material is human-generated. A lightweight neural network-based paraphraser was developed and applied it to the AI-generated texts.John Kirchenbauer et al. [kirchenbauer2023watermark] introduced a watermarking method. This method add a small amount of noise to the weights of the LLM during training.The noise is made with the intention of encoding a distinct watermark signal that can later be decoded by a watermark detector.The GPT-2 and GPT-3 language models are used to demonstrate the utility of their watermarking technology. The watermark can be found even after fine-tuning the LLM on fresh data and that it is resistant to a variety of attacks, including gradient masking and weight perturbations. In a paper Kalpesh Krishna et al.[krishna2023paraphrasing]  created a substantial amount of AI-generated text samples using a number of cutting-edge language models, such as GPT-3 and T5. The efficiency of several rule-based and machine learning-based text-derived AI detectors is then assessed using the created samples. A retrieval-based defensive method was proposed that depends on determining the text's original author. The suggested technique operates by maintaining a database of known AI-generated text samples and their associated original sources, and by comparing any new text samples against this database to identify probable sources. The authors demonstrate that the suggested retrieval-based defensive mechanism is successful in identifying material that has been paraphrased by AI, with detection a good accuracy. Souradip Chakraborty et al. [chakraborty2023possibilities] proposed multiple possibilities that can detect AI generated texts including some statistical methods and several machine learning algorithms. In most of the papers GPT-2 or previous version of GPT-3 were used. But in our paper we approached with conventional machine learning algorithms but with a dataset that was generated by GPT-3.5 which were more human like.The aim of this research is to differentiate human text from generative model text using machine learning. In Figure <ref>, the high level overview of our process is described. The task initiates by data collection. Section <ref> holds the detailed process of this stage.\n",
      "---\n",
      "Convolutional neural networks (CNNs) are among popular artificial neural networks widely used for image classification and image segmentation purposes in computer visions and computer graphics, due to their capabilities for pattern recognition. In this sense, CNNs can be used for supervised learning of computational mechanics, when the problem solution is a function of geometric parameters [bhatnagar2019prediction,thuerey2020deep]. In this approach, the data must be represented on Cartesian grids (i.e., image-like data). For example, we can design a deep learning framework, where the CNN input is the images of airfoils with different shapes and the CNN output is the velocity and pressure fields. We ask ChatGPT to develop a Python code for designing a CNN-based deep learning framework to predict the flow fields around airfoils.lmss\n",
      "    < g r a p h i c s >\n",
      " Develop a Python code for the prediction of the velocity and pressure fields around airfoils with different geometries in two dimensions. Specifically, use convolutional neural networks to encode geometric features and then use deconvolutional neural networks to decode the outputs. Input is 128 by 128 images, and the output has the same size.lmss\n",
      "    < g r a p h i c s >\n",
      " To predict the velocity and pressure fields around two-dimensional airfoils with different geometries using a combination of convolutional neural networks (CNNs) and deconvolutional neural networks (also known as transposed convolutions), you can follow these steps:* Data preparation: Obtain a dataset with airfoil shapes and corresponding velocity and pressure fields. You can use computational fluid dynamics (CFD) simulations or experimental data for this purpose.* Data preprocessing: Normalize and resize the airfoil images and velocity and pressure fields to a suitable size for the CNN. Split the dataset into training, validation, and test sets.* Model creation: Design a CNN architecture for encoding geometric features and a deconvolutional neural network for decoding the outputs.* Training: Train the combined model using the training and validation datasets.* Testing: Evaluate the model's performance on the test dataset.Here's a sample code using TensorFlow and Keras:CNNs.txtlmss\n",
      "    < g r a p h i c s >\n",
      " This sample code provides a starting point for your project. You'll need to implement the functions to load and preprocess the data, as well as modify the CNN architecture and parameters according to your specific dataset and requirements.There are two notable observations. First, ChatGPT implements the “mean squared error” as the loss function in line 40 of the above code. This is interesting because although this is an image-based prediction problem, ChatGPT realizes that this is, indeed, a regression task. Line 40 of the above code is listed below as well.CNNs_1.txt  Second, ChatGPT implements the last layer of the network with three outputs for the velocity in the x and y directions and the pressure, as can be seen in line 34 of the above code, showing that GPT understands that the velocity field in two dimensions is a vector with two components and the pressure is simply a scalar variable. Line 34 of the above code is listed below as well.CNNs_2.txtIn this subsection, we investigate if ChatGPT can help us debug and improve our codes. As a practical example, we consider the Gauss-Seidel method [wendland2017numerical]. The Gauss-Seidel algorithm is used for obtaining a numerical solution to a system of linear equations Ax⃗ = b⃗. Given an initial guess such as the vector x⃗^(0), the solution can be approximated using the following formula iteratively:where x_i^(k) indicates the i-th component of the solution vector at the k-th iteration, a_ij show the coefficients of the matrix A, and b_i denotes the constants in the vector b⃗. We test ChatGPT with a C code written by the first author (A. Kashefi) that solves the system of Ax⃗ = b⃗. We provide ChatGPT with the code and ask if the code produces the correct results.lmss\n",
      "    < g r a p h i c s >\n",
      " I have written a C++ code for solving a system of equations using the Gauss-Seidel method. The code is as follows. Please let me know if the output of the code is correct.GS_Correct.txtlmss\n",
      "    < g r a p h i c s >\n",
      " The code for solving a system of equations using the Gauss-Seidel method looks fine. However, to check whether the output of the code is correct, we need to compare it with the actual solution.For the given system of equations:GS_Analysis_1.txtlmssThe actual solution is x1 = 1, x2 = 2, x3 = 1, x4 = -1, and x5 = 3.Running the provided code, we get the following output:GS_Analysis_2.txtlmssThe  We can see that the output of the code is very close to the actual solution. The small differences may be due to numerical error, and can be reduced by increasing the number of iterations or using double precision arithmetic.Therefore, the output of the code is correct.ChatGPT could surprisingly successfully recognize the embedded system of equations (Ax⃗ = b⃗) inside our C code (see lines 64 to 76 of the above code). Additionally, it seems that ChatGPT ran our code (using perhaps an inner compiler) as it said in the chat box: “Running the provided code, we get the following output”.For the next step, we intentionally put a bug in our C code. In the loop written in line 18 of the following code, we start indexing from “10” instead of “0”, and we ask the same question from ChatGPT.lmss\n",
      "    < g r a p h i c s >\n",
      " I have written a C++ code for solving a system of equations using the Gauss-Seidel method. The code is as follows. Please let me know if the code works correctly or not!GS_Wrong.txtlmss\n",
      "    < g r a p h i c s >\n",
      " The code appears to be working correctly for solving a system of linear equations using the Gauss-Seidel method. However, there are a few points that can be improved:* Indexing of the loop: The loop indexing for i starts at 10, which does not make sense since n is 5. It should start from 0.* Memory allocation: Memory for the matrix a should be deallocated using the `delete' keyword after the program has finished using it. Also, x and b are not being deallocated in the code, which can lead to memory leaks.* Printing output: The output of the solution x is being printed in a for loop in the `main' function. This can be improved by creating a separate function for printing the solution vector.* Magic number: The value `10-̂6' is used as a threshold for convergence, which is a magic number. It should be declared as a constant variable at the beginning of the program to make the code more readable.Here is a modified version of the code with these improvements:GS_Improved.txtInterestingly, ChatGPT could recognize the minor bug in our code and fix it with some additional improvements.\n",
      "---\n",
      "Exploring the \"why\" forms a critical focus in the novel generation of pattern analysis and machine intelligence [zhang2021survey,xin_wang,yu2019multi,9756301], where causal structure learning is an essential task of recovering causality among the variables of interest from observed data [scanagatta2019survey,jiang2019joint,li2014sparse]. Recent advances in Large Language Model (LLM) have shown significant capability of providing high-quality information to capture domain knowledge causal mechanisms based on conceptual description of variables [kiciman2023causal], which is happening to form a new ladder to the current three-stage of causality research according to Judea Pearl. However, LLM faces challenges of  in providing prior causal knowledge compared to traditional expert ways, producing possible erroneous causal statements while enriching the available prior knowledge [long2023can]. Existing causal structure learning methods necessitate rigorously verified priors in the current expert predominate resources, rarely concerning the prior errors. Therefore, this paper aims to devise a causal structure learning method with a human-independent nature of resisting unreliable priors.\n",
      "---\n",
      "Tom et al. [brown2020language] introduced GPT-3, the third iteration of the Generative Pretrained Transformer model developed by OpenAI. This paper's primary focus is on the few-shot learning capability of language models. The authors demonstrate that language models start exhibiting remarkable few-shot performance when scaled up, essentially learning from a limited number of examples. Lampinen et al. [lampinen2022can] investigated how AI systems interpret, understand, and apply knowledge from explanations provided in various contexts. Specifically, this is an important contribution to AI, particularly in language understanding and knowledge acquisition by machine learning models.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:33, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 109, in run\n",
      "    await self.visit(self.parse_tree, VariableStack([self.program._variables], self))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 524, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_assistant.py\", line 13, in assistant\n",
      "    return await role(role_name=\"assistant\", hidden=hidden, _parser_context=_parser_context, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_role.py\", line 17, in role\n",
      "    new_content += await parser.visit(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in visit\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in <listcomp>\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 379, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_gen.py\", line 140, in gen\n",
      "    gen_obj = await parser.llm_session(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 665, in __call__\n",
      "    out = await self.llm.caller(**call_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 370, in _library_call\n",
      "    out = await openai.ChatCompletion.acreate(**kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 728, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 14700 tokens. Please reduce the length of the messages.\n",
      "\n",
      "Error in program:  This model's maximum context length is 8192 tokens. However, your messages resulted in 14700 tokens. Please reduce the length of the messages.\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the difference between LLaMA-Adapter and LLaMA-Adapter-V2?\n",
      "--------------------------------------------------\n",
      "Answer A ($21.74):\n",
      "* Can retriever-augmented language models perform multi-step reasoning given supporting statements?     The experimental results show that retriever-augmented language models have some difficulties in performing multi-step entailments or logical reasoning in solving QA and LM examples, especially when facing some distracting statements.     \n",
      "  * What are some shortcomings of retrievers which undermine reasoning performance?               By investigating the shortcomings of retrievers in selecting the required statements for reasoning, we conclude that selecting the statements based on their similarity with the queries is insufficient for the reasoning nature. This approach works promisingly when similar forms of the query containing the answer exist among the supporting statements, while there are sometimes no such statements when reasoning is required to solve a task.     \n",
      "  * What are some shortcomings of language models which undermine reasoning performance?               The results demonstrate that language models cannot consider the relations between the statements, which is necessary for reasoning. Interestingly, we also observe that language models perform much better when a statement containing information from all relevant ones is provided.      \n",
      "  * What is the impact of model size?     Our experimental results using various sizes of Flan-T5 model demonstrate that larger models lead to better performance on reasoning tasks.     \n",
      "  * What is the impact of using multihop retrieval?     The experimental results with different sizes of Flan-T5 model and GPT-3.5 using a recent multihop approach reflect improvement in some, but not all of the experiments which shows there is still significant room for improvement in this area.Augmenting language models with external corpora or memory has been well studied in literature [realm, fid, atlas, knnlm]. Additionally, researchers have recently been interested in eliciting the reasoning abilities of parametric language models in tasks where answering the questions involves multiple computational steps [chain-of-thoughts,flan]. Next, we discuss some of the recent studies in these areas.\n",
      "---\n",
      "* We show that generative LLM detection with a discriminator LLM is impossible if the attacker has access to the reference \"human\" dataset used to train the discriminator LLM. Simply fine-tuning the dataset and using prompts from it leads to a complete failure of the discriminator to learn the difference between machine and human-generated texts, even in a setting where all LLM outputs are correctly labeled.   \n",
      "  * We show that reinforcement from critic generalizes significantly better than previously described when paired with the AdamW optimizer rather than the commonly used Adam one and allows well-generalizing model fine-tunes from limited data, matching prior SotA in normativity fine-tuning.   \n",
      "  * We demonstrate a critical weakness on a previously proposed text-generating GAN architecture - DPGAN, and show the connection of this weakness to the difference in representative power of LLMs and RNNs used in text GANs compatible with the in-the-wild detection setting.Our code is based on a pre-existing Pytorch implementation of common text-generating GANs, including DPGAN, available from <https://github.com/williamSYSU/TextGAN-PyTorch>.\n",
      "---\n",
      "We further explore how different task augmentation strategy affects the learning outcome. As shown in Table <ref>(c), we use template-based approach for composing QA pairs and task augmentation, and compare with the fine-tuning results with PaLM-generated QA pairs. The Template-based fine-tuning scores around 10% lower in AUC score compared to PaLM-based fine-tuning. The observed gap can be explained by the increased compound divergence between the training data and the evaluation data. The accuracy difference on the non-negated split is around 7% while the difference on the negated split is around 10% to 12%. Recall that the template-based approach only modifies the noun for negation while Palm-based approach incorporates more variations, which can explain why the template-based fine-tuning performs worse on the negated split. This experiment has highlighted the importance of composing augmented tasks with natural linguistic variations that match human language used in production environment. However, even Template-based task augmentation can improve negation understanding significantly, on average 30% higher than training without task augmentation (Table <ref>(a)).\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "On our CMATH dataset, we conduct evaluation for a variety of popular LLMs, accessible via commercial API or released model weights. We discover that  GPT-4 is the only model that achieves success (accuracy ≥ 60%) across all six elementary school grades. We also examine the robustness of LLMs against the distracting information. It turns out that GPT-4 is again the sole model that maintains robustness, while other models are easily misled by the presence of distracting information.They require understanding natural language questions and translating them into mathematical expressions/equations. This tests a model's linguistic abilities.They require applying mathematical knowledge and reasoning logically through multiple steps. This tests a model's symbolic manipulation and reasoning skills.They often involve commonsense knowledge about the real world. This tests a model's knowledge grounding and understanding.They come in a range of difficulties, allowing us to probe model abilities at different levels of depth. From grade 1 to 6, the problems become increasingly complex and abstract.* How well can state-of-the-art Chinese language models solve elementary school math word problems? What are their limitations? \n",
      "  * What types of reasoning or math abilities do current models struggle with most? Logical, linguistic, commonsense or mathematical? \n",
      "  * How can we develop models with stronger reasoning and problem-solving skills? What abilities are most important to improve?We release CMATH, the first Chinese dataset for evaluating math reasoning abilities of language models. CMATH contains 1,000 elementary school level math word problems with detailed annotations.We conduct the first systematic evaluation of popular Chinese language models on math word problems. We show how CMATH can reveal differences in the math reasoning skills of language models at various ability levels.We find that although elementary school word problems seem simple, solving them requires sophisticated linguistics, logical and commonsense reasoning that current language models do not possess. CMATH provides a challenging testbed for continued progress in these directions.Our work establishes a first framework for evaluating and enhancing the math reasoning skills of Chinese language models. We release CMATH as a public benchmark to support future work in this direction.0.85! \n",
      "    < g r a p h i c s >\n",
      "  Sample problems along with their English translations (not part of the dataset) and human annotations. The column title “#Steps” and “#Digits” stand for “number of reasoning steps” and “number of digits” respectively.This work is motivated by the following question:  To what grade level of elementary school math do the abilities of popular LLMs correspond?  We create the CMATH dataset in order to  answer this question. We believe that the evaluation results of LLMs should be presented in an intuitive manner, making them easily understandable for the general public.\n",
      "--------------------------------------------------\n",
      "Answer B ($91.07):\n",
      "* Can retriever-augmented language models perform multi-step reasoning given supporting statements?     The experimental results show that retriever-augmented language models have some difficulties in performing multi-step entailments or logical reasoning in solving QA and LM examples, especially when facing some distracting statements.     \n",
      "  * What are some shortcomings of retrievers which undermine reasoning performance?               By investigating the shortcomings of retrievers in selecting the required statements for reasoning, we conclude that selecting the statements based on their similarity with the queries is insufficient for the reasoning nature. This approach works promisingly when similar forms of the query containing the answer exist among the supporting statements, while there are sometimes no such statements when reasoning is required to solve a task.     \n",
      "  * What are some shortcomings of language models which undermine reasoning performance?               The results demonstrate that language models cannot consider the relations between the statements, which is necessary for reasoning. Interestingly, we also observe that language models perform much better when a statement containing information from all relevant ones is provided.      \n",
      "  * What is the impact of model size?     Our experimental results using various sizes of Flan-T5 model demonstrate that larger models lead to better performance on reasoning tasks.     \n",
      "  * What is the impact of using multihop retrieval?     The experimental results with different sizes of Flan-T5 model and GPT-3.5 using a recent multihop approach reflect improvement in some, but not all of the experiments which shows there is still significant room for improvement in this area.Augmenting language models with external corpora or memory has been well studied in literature [realm, fid, atlas, knnlm]. Additionally, researchers have recently been interested in eliciting the reasoning abilities of parametric language models in tasks where answering the questions involves multiple computational steps [chain-of-thoughts,flan]. Next, we discuss some of the recent studies in these areas.\n",
      "---\n",
      "* We show that generative LLM detection with a discriminator LLM is impossible if the attacker has access to the reference \"human\" dataset used to train the discriminator LLM. Simply fine-tuning the dataset and using prompts from it leads to a complete failure of the discriminator to learn the difference between machine and human-generated texts, even in a setting where all LLM outputs are correctly labeled.   \n",
      "  * We show that reinforcement from critic generalizes significantly better than previously described when paired with the AdamW optimizer rather than the commonly used Adam one and allows well-generalizing model fine-tunes from limited data, matching prior SotA in normativity fine-tuning.   \n",
      "  * We demonstrate a critical weakness on a previously proposed text-generating GAN architecture - DPGAN, and show the connection of this weakness to the difference in representative power of LLMs and RNNs used in text GANs compatible with the in-the-wild detection setting.Our code is based on a pre-existing Pytorch implementation of common text-generating GANs, including DPGAN, available from <https://github.com/williamSYSU/TextGAN-PyTorch>.\n",
      "---\n",
      "We further explore how different task augmentation strategy affects the learning outcome. As shown in Table <ref>(c), we use template-based approach for composing QA pairs and task augmentation, and compare with the fine-tuning results with PaLM-generated QA pairs. The Template-based fine-tuning scores around 10% lower in AUC score compared to PaLM-based fine-tuning. The observed gap can be explained by the increased compound divergence between the training data and the evaluation data. The accuracy difference on the non-negated split is around 7% while the difference on the negated split is around 10% to 12%. Recall that the template-based approach only modifies the noun for negation while Palm-based approach incorporates more variations, which can explain why the template-based fine-tuning performs worse on the negated split. This experiment has highlighted the importance of composing augmented tasks with natural linguistic variations that match human language used in production environment. However, even Template-based task augmentation can improve negation understanding significantly, on average 30% higher than training without task augmentation (Table <ref>(a)).\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "On our CMATH dataset, we conduct evaluation for a variety of popular LLMs, accessible via commercial API or released model weights. We discover that  GPT-4 is the only model that achieves success (accuracy ≥ 60%) across all six elementary school grades. We also examine the robustness of LLMs against the distracting information. It turns out that GPT-4 is again the sole model that maintains robustness, while other models are easily misled by the presence of distracting information.They require understanding natural language questions and translating them into mathematical expressions/equations. This tests a model's linguistic abilities.They require applying mathematical knowledge and reasoning logically through multiple steps. This tests a model's symbolic manipulation and reasoning skills.They often involve commonsense knowledge about the real world. This tests a model's knowledge grounding and understanding.They come in a range of difficulties, allowing us to probe model abilities at different levels of depth. From grade 1 to 6, the problems become increasingly complex and abstract.* How well can state-of-the-art Chinese language models solve elementary school math word problems? What are their limitations? \n",
      "  * What types of reasoning or math abilities do current models struggle with most? Logical, linguistic, commonsense or mathematical? \n",
      "  * How can we develop models with stronger reasoning and problem-solving skills? What abilities are most important to improve?We release CMATH, the first Chinese dataset for evaluating math reasoning abilities of language models. CMATH contains 1,000 elementary school level math word problems with detailed annotations.We conduct the first systematic evaluation of popular Chinese language models on math word problems. We show how CMATH can reveal differences in the math reasoning skills of language models at various ability levels.We find that although elementary school word problems seem simple, solving them requires sophisticated linguistics, logical and commonsense reasoning that current language models do not possess. CMATH provides a challenging testbed for continued progress in these directions.Our work establishes a first framework for evaluating and enhancing the math reasoning skills of Chinese language models. We release CMATH as a public benchmark to support future work in this direction.0.85! \n",
      "    < g r a p h i c s >\n",
      "  Sample problems along with their English translations (not part of the dataset) and human annotations. The column title “#Steps” and “#Digits” stand for “number of reasoning steps” and “number of digits” respectively.This work is motivated by the following question:  To what grade level of elementary school math do the abilities of popular LLMs correspond?  We create the CMATH dataset in order to  answer this question. We believe that the evaluation results of LLMs should be presented in an intuitive manner, making them easily understandable for the general public.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "<Ref> shows that replacing person and activity has negligible impact on the accuracy. However, abstracting out the sports abstract drops the performance below levels (<Ref>).  What explains such a discrepancy? Note that both person and activity are already present in the question, and are copied in the thought. Thus, replacing them even with abstract tokens does not harm the performance, as the extraction pattern is retained. On the other hand, the sport is not present in the question, and has to be extracted. Consider a question: Is the following sentence plausible? “Norman Powell committed a blocking foul.” For abstracted sports prompts, the model generates Norman Powell is a SPORT1 player. Committing a blocking foul is part of SPORT2. So the answer is no. Whereas for the abstract person or the activity prompt, the model correctly infers the pattern (copy the person or the activity, and generate the corresponding sport) and generates: Norman Powell is a basketball player. Being a blocking foul is part of basketball. So the answer is yes. This example also clearly drives the notion of symbiosis:  only generating the right pattern or only having the right text. Both are required for effective .Cases where patterns help.As empirical results show, combining patterns with language reveals their true potential. However, our analysis uncovers interesting reasons for the superiority of patterns only, for the small number of cases.Interestingly, we found a few cases where forcing the model to generate some text leads to errors.Primarily, they were caused by the model trying to make faulty connections. Detailed examples are located in <Ref>.<Ref> presents detailed results, including ablations on various parts of the prompt. These were omitted from the main text due to space constraints.0.99     0          The presence of text is necessary for the model to arrive at the right conclusions.          While the model is sufficiently capable of forming abstractions, employing concordant entities in questions and thoughts is crucial for the success of .          The performance of the model is proportionally sensitive to the degree of variations in the text.We test this hypothesis to determine the role of text in the performance of .Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar.Note that the practicality of the counterfactual experiments for text is contingent on the nature of the target task.In addition, because there is no text in , we did not conduct counterfactual experiments in this dataset.<Ref> summarizes the results and shows sample thought for each counterfactual prompt.Text with discrepant entities text_diff_entities.We first investigate the role of employing identical entities (names and objects) between a question and its associated thought.To achieve this, we simply modify the entities in the thoughts to be discrepant with the ones in the questions.is exclusively relevant for the and datasets for which concrete entities exist: objects and people for and players and activities for .In the dataset, we change the player name and the activity in the thought, while retaining the sport in the question.Samples of generated outputs in <Ref> illustrate that in this scenario, the model appears to disregard the prompt, generating arbitrary thoughts and attaining a similar performance as .The task solve rate for with is noticeably better than , while still fails to match the performance of .We delineate samples of generated outputs in <Ref> to glean more insights.\n",
      "---\n",
      "While toxicity is an umbrella term, and its evaluation in LMs comes with challenges [xu2021detoxifying,welbl2021challenges], automatic classifier scores can provide an indication for the levels of harmful text that a LM generates.  [rae2021gopher] found that improving language modelling loss by increasing the number of model parameters has only a negligible effect on toxic text generation (unprompted); here we analyze whether the same holds true for a lower LM loss achieved via more compute-optimal training. Similar to the protocol of [rae2021gopher], we generate 25,000 unprompted samples from , and compare their PerspectiveAPI toxicity score distribution to that of -generated samples. Several summary statistics indicate an absence of major differences: the mean (median) toxicity score for is 0.081 (0.064), compared to 0.087 (0.066) for , and the 95^th percentile scores are 0.230 for , compared to 0.238 for .That is, the large majority of generated samples are classified as non-toxic, and the difference between the models is negligible.In line with prior findings [rae2021gopher], this suggests that toxicity levels in unconditional text generation are largely independent of the model quality (measured in language modelling loss), i.e. that better models of the training dataset are not necessarily more toxic.\n",
      "---\n",
      "Manually “engineering” prompts for large language models (LLMs) have been shown to lead to tremendous performance gains and have been a subject of intense study in recent years   [schick2021exploiting,reynolds2021prompt,mishra2022reframing]. However, the task of prompt engineering can be challenging due to the difficulty in determining the effectiveness of a prompt solely based on its raw text form. Consequently, this process is typically carried out manually, which can be laborious and time-intensive. In particular, LLMs may produce vastly different predictive distributions for two seemingly comparable prompts, despite their semantic similarity [mishra2022reframing]. This phenomenon results in an unexpectedly high level of variability. [DBLP:journals/tacl/JiangXAN20,perez2021true,DBLP:journals/tacl/ElazarKRRHSG21].In response to such difficulties, recent works propose metrics for automatic prompt selection.  Notably, [sorensen2022information] introduces Mutual Information (MI) to quantify the shared information between prediction and inputs. Further, [chen2022relation] introduces Sensitivity () to quantify model receptiveness to textual perturbations of the input prompts.  Despite such metrics' empirical effectiveness, the underlying principles that enable them are not well-understood.This motivates the following questions:        -0.6em      \n",
      "  * What makes the existing methods for prompt selection effective?      \n",
      "  * How are these existing methods connected?     \n",
      "  * Are there any new metrics complementary to the existing ones?To address the questions above, we study existing methods from an optimization perspective.The objective ℒ(p, , θ_0)  quantifies the performance of an LLM (parameterized by θ_0)  on\n",
      "---\n",
      "In-context learning. A PLM is primed with labeled examples in its prompt. It classifies the example included at the end of the prompt by predicting the classification conditioned on the priming. GPT-3 [brown2020gpt3] used in-context learning to achieve promising results on a variety of classification tasks. UniFew [bragg2021flex] similarly achieved strong results on classification tasks via in-context learning, converting classification tasks into a multiple-choice question answer format for prompting.Prompt-based fine-tuning. A PLM is fine-tuned with masked-language modeling objectives to learn from few examples. This is also known as Pattern-exploiting training (PET) [schick2021exploiting]. While PET requires task-specific prompts, it achieves better performance than GPT-3 in-context with smaller models [schick2021its]. LM-BFF [gao2021making] improves prompt-based fine-tuning by dynamically constructing prompts.The most closely related few-shot NLP benchmarks are FLEX [bragg2021flex], FewGLUE [schick2021its], CrossFit [ye2021crossfit], and NaturalInstructions [mishra2021natural]. Each of these benchmarks includes at least some classification tasks with meaningful textual labels.These benchmarks are designed to study transfer between tasks [bragg2021flex, ye2021crossfit], pinpoint where NLP models fall short [schick2021its], and evaluate ability of models to follow instructions [mishra2021natural], whereas \\name{} is designed to be representative of real-world classification tasks. This difference in goals is reflected in selection of tasks and evaluation:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:11, 20.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "I'm sorry, but as of my current knowledge update, there's no specific information available about LLaMA-Adapter and LLaMA-Adapter-V2. It's possible that you're referring to a specific software, tool, or technology that has been updated or revised. Typically, a \"V2\" or version 2 of a software or tool indicates an updated version with improvements, bug fixes, or new features compared to the original version. However, without specific context or information about these terms, it's difficult to provide a precise difference. Please provide more context or check the documentation or source where you found these terms for more accurate information.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: Well, Bobby, it seems we have a bit of a problem here. Neither of the students actually answered the question about the difference between LLaMA-Adapter and LLaMA-Adapter-V2. They both provided extensive information on various topics, but none of it seems to directly address the question.\n",
      "\n",
      "Bobby: I agree, Michael. It's clear that both students have a deep understanding of various topics related to machine learning and language models, but they've missed the mark on this particular question. However, we still need to determine which answer is better.\n",
      "\n",
      "Michael: True, Bobby. Let's consider the clarity and relevance of the information they provided. Student A's answer is quite detailed and covers a wide range of topics, but it's also quite complex and difficult to follow. It seems to go off on several tangents, which makes it hard to understand.\n",
      "\n",
      "Bobby: I see your point, Michael. On the other hand, Student B's answer is almost identical to Student A's. It seems they've copied the same information, which doesn't really help us determine which answer is better.\n",
      "\n",
      "Michael: That's a good point, Bobby. However, considering that neither answer directly addresses the question, I think we should consider the effort and understanding demonstrated by the students. Even though Student B's answer is a copy of Student A's, Student A at least demonstrated some effort and understanding by providing original content.\n",
      "\n",
      "Bobby: I agree, Michael. Even though neither answer is ideal, Student A at least demonstrated some initiative and understanding. Therefore, I think we should consider Student A's answer as the better one.\n",
      "\n",
      "VERDICT: Better answer from student A.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What are some ways to ensure that machine learning models are robust and reliable?\n",
      "--------------------------------------------------\n",
      "Answer A ($15.95):\n",
      "Foundation models have drastically changed the research and practice of NLP. Foundation models have given rise to many new research directions for the community: understanding generation as a fundamental aspect of language, studying how to best use and understand foundation models, understanding the ways in which foundation models may increase inequities in NLP,  examining whether foundation models can satisfactorily encompass linguistic variation and diversity, and finding ways to draw on human language learning dynamics.  Most of the complex NLP tasks that the research community focused on before foundation models are now best handled, to an almost-human level, using one of a few publicly released foundation models. Nevertheless, there remain significant gaps between this performance and the needs for useful and safe deployment of foundation models in complex downstream settings.philosophyChristopher Potts, Thomas Icard, Eva Portelance, Dallas Card, Kaitlyn Zhou, John EtchemendyWhat could a foundation model come to understand about the data it is trained on? An answer to this question would be extremely informative about the overall capacity of foundation models to contribute to intelligent systems. In this section, we focus on the case of natural language, since language use is a hallmark of human intelligence and central to the human experience.The best foundation models at present can consume and produce language with striking fluency, but they invariably lapse into the sort of incoherence that suggests they are merely “stochastic parrots” [bender2021]. Are these lapses evidence of inherent limitations, or might future foundation models truly come to understand the symbols they process?\n",
      "---\n",
      "Recent innovations on hardware (e.g. Nvidia A100) have motivated learning N:M structured sparsity masks from scratch for fast model inference. However, state-of-the-art learning recipes in this regime (e.g. SR-STE) are proposed for non-adaptive optimizers like momentum SGD, while incurring non-trivial accuracy drop for Adam-trained models like attention-based LLMs.     In this paper, we first demonstrate such gap origins from poorly estimated second moment (i.e. variance) in Adam states given by the masked weights. We conjecture that learning N:M masks with Adam should take the critical regime of variance estimation into account. In light of this, we propose , an Adam-aware recipe that learns N:M masks with two phases: first,  calculates a reliable variance estimate (precondition phase) and subsequently, the variance remains fixed and is used as a precondition to learn N:M masks (mask-learning phase).  automatically identifies the switching point of two phases by dynamically sampling variance changes over the training trajectory and testing the sample concentration. Empirically, we evaluate  and other baselines such as ASP and SR-STE on multiple tasks including CIFAR classification, machine translation and LLM fine-tuning (BERT-Base, GPT-2). We show  mitigates the accuracy drop of baseline recipes and is robust to aggressive structured sparsity ratios.\n",
      "--------------------------------------------------\n",
      "Answer B ($83.09):\n",
      "Foundation models have drastically changed the research and practice of NLP. Foundation models have given rise to many new research directions for the community: understanding generation as a fundamental aspect of language, studying how to best use and understand foundation models, understanding the ways in which foundation models may increase inequities in NLP,  examining whether foundation models can satisfactorily encompass linguistic variation and diversity, and finding ways to draw on human language learning dynamics.  Most of the complex NLP tasks that the research community focused on before foundation models are now best handled, to an almost-human level, using one of a few publicly released foundation models. Nevertheless, there remain significant gaps between this performance and the needs for useful and safe deployment of foundation models in complex downstream settings.philosophyChristopher Potts, Thomas Icard, Eva Portelance, Dallas Card, Kaitlyn Zhou, John EtchemendyWhat could a foundation model come to understand about the data it is trained on? An answer to this question would be extremely informative about the overall capacity of foundation models to contribute to intelligent systems. In this section, we focus on the case of natural language, since language use is a hallmark of human intelligence and central to the human experience.The best foundation models at present can consume and produce language with striking fluency, but they invariably lapse into the sort of incoherence that suggests they are merely “stochastic parrots” [bender2021]. Are these lapses evidence of inherent limitations, or might future foundation models truly come to understand the symbols they process?\n",
      "---\n",
      "A long-standing goal of AI systems is to act reliably and learn complex tasks efficiently like human beings.  In the process of reliable decision making, humans follow an explicit chain-of-thought (CoT) reasoning process that is typically expressed as an explanation.  However, machine learning models are trained mostly using a large number of input-output examples to perform a specific task.  These black-box models only generate the final decision without reliably revealing the underlying reasoning process.  Not surprisingly, it is unclear if they understand the task and can generalize even though they perform well on the benchmark.  On the other hand, humans are able to learn from instructions or explanations from past experience and generalize them to novel and unseen problems.  This helps them learn more quickly with fewer data. In this work, we explore if machines can be endowed with such reasoning abilities in the context of science-based question answering.\n",
      "---\n",
      "Methods     Knowledge Domain     Downstream Task Indicator     What to  Pre-Train    How to  Pre-Train     How to Use Pre-Trained Model    Xland [team2021open]     Online tasks                      Predicates           Policy     RL     Zero-shot; Finetune    MIA [IL-Representation]       Offline human demo                    Text         Policy     BC     Zero-shot; Finetune     Gato [reed2022generalist]     Offline expert demo;  Multi-modal data           Prompt                Policy     BC     Zero-shot; Finetune    SayCan [saycan]     Pre-trained LM                     Text     Perception     SL; RL     zero-shot    Minedojo [fan2022minedojo]      Internet video; Pre-trained LVM;      Text      Reward     SL     Online RL    VPT [baker2022vpt]      Internet video;  Manual annotation      -                 Policy;  World Model     BC     FinetuneLM-Nav [shah2022lmNav]     Pre-trained LVM; Pre-trained LM                       Text              Perception     SL      Search method    Inner Mono. [huang2022inner]     Pre-trained LM; Pre-trained VM      Text      Perception      SL; BC     Zero-shotThe essential differences between prediction and sequential decision-making problems make the current success of large sequence models in NLP or CV cannot be directly transferred to the latter. Because the sequential decision-making process involves a feedback loop, subtle changes in behavior would lead to severe data distribution shifts. Therefore, new algorithms are demanded to learn stable representation, mitigate distribution shifts, and improve data efficiency.We cannot expect that pre-training a single model would lead to strong generalization ability in all out-of-distribution tasks.  Therefore, how to learn a universal and consistent representation for all the downstream tasks and minimize the distance between the training data distribution and the evaluation data distribution are the major issues that remain unsolved for effective large decision models with a reliable theoretical guarantee.\n",
      "---\n",
      "Stable Diffusion revolutionised image creation from descriptive text. ,  and  demonstrated astonishing performance across a variety of language tasks.  introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to  once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as [The name is inspired by the Generative Adversarial Networks (GAN) literature on mode collapse, where GANs start producing a limited set of outputs that all trick the discriminator.  is a process whereby models eventually converge to a state similar to that of a GAN Mode Collapse. The original version of this paper referred to this effect as `model dementia', but we decided to change this following feedback that it trivialised the medical notion of `dementia' and could cause offence. ] and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.\n",
      "---\n",
      "Increasing transparency. With more transparency, the general public  can be better engaged into  the global efforts to address the growing water challenges. For example, water-conscious users may prefer to use the inference services of AI models during water-efficient hours, but  currently, the lack of transparency about AI models' detailed water efficiency prohibits them from doing so.  Additionally, being informed of the data center's runtime water efficiency, AI model developers can better schedule their model training and choose locations for the deployment of trained models.We recommend AI model developers and data center operators be more transparent. For example, what are the runtime (say, hourly) on-site WUE and off-site WUE? When and where are the AI models trained? What about the AI models trained and/or deployed in third-party colocation data centers or public clouds? Such information will be of great value to the research community and the general public.\n",
      "---\n",
      "Past work has characterized what kinds of functions ICL can learn [Garg2022WhatCT, laskin2022context, muller2021transformers] and the distributional properties of pretraining that can elicit in-context learning [xie2021explanation, chan2022data]. But how ICL learns these functions has remained unclear. What learning algorithms (if any) are implementable by deep network models? Which algorithms are actually discovered in the course of training?  This paper takes first steps toward answering these questions, focusing on a widely used model architecture (the transformer) and an extremely well-understood class of learning problems (linear regression).\n",
      "---\n",
      "To facilitate such research, we create a new reading comprehension dataset, , which contains 8,678 paragraph-question pairs, each with four candidate answers. Our dataset is sourced from publically available logical examination papers for reading comprehension, which are designed by domain experts for evaluating the logical reasoning ability and test participants. Thus the quality and topic coverage of the questions are reliable. We manually select problems from the original dataset, filtering out problems that involve figures, charts, or those that are heavy of mathematics, and ensuring a wide coverage of logical reasoning types, including categorical reasoning, conditional reasoning, disjunctive reasoning and conjunctive reasoning [concise-logic].To establish baseline performances on , we explore several state-of-the-art neural models developed for reading comprehension. Experimental results demonstrate a significant gap between machine (35.31% accuracy) and human ceiling performance (96.00%). We provide detailed analysis to give insights into potentially promising research directions.Existing Datasets For Reading Comprehension\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "We proposed a Linear Optimal Partial Transport (LOPT) technique that allows us to embed distributions with different masses into a fixed dimensional space in which several calculations are significantly simplified. We show how to implement this for real data distributions allowing us to reduce the computational cost in applications that would benefit from the use of optimal (partial) transport. We finally provide comparisons with previous techniques and show some concrete applications. In particular, we show that LOPT is more robust or computationally efficient in the presence of noise than previous methods. For future work, we will continue to investigate the comparison of LHK and LOPT, and the potential applications of LOPT in other machine learning and data science tasks, such as Barycenter problems, graph embedding, task similarity measurement in transfer learning, and so on.icml2023AppendixWe refer to the main text for references.\n",
      "---\n",
      "In this paper, we presented , a novel benchmark for evaluating advanced reasoning capabilities in large language models. Our dataset is composed of various problems from the sciences and law, sourced from graduate-level exams and professional resources.  Despite advancements in current LLMs, their performance remains very low on the quantitative subjects, in 's tasks.  We also proposed a rubric-based self-evaluation method, enabling LLMs to grade their own reasoning.  This method is not yet reliable enough to replace human grading.  We hope this method can be extended to more reliable and cheap testing of complex model outputs.As with all other benchmarks that are not created anew and kept secret, it is possible there is data contamination. For example, the MCAT books are not available for free in most jurisdictions, but it certainly possible that some model creators have trained on it anyway.Finally, the benchmark does not remotely cover all aspects of human ability; a model solving this benchmark perfectly could still be much worse than most educated people in many aspects. Nevertheless, we hope that increasing the difficulty standards helps the research community ground the performance of increasingly powerful models more accurately.We thank Jeffrey Deng for developing and documenting the API, and building the project website.  We would also like to thank Raunak Chowdhuri for helpful comments, and Zhangir Azerbayev for useful discussions early on in the project. TS is supported by NSF grant 1745583.plainnatWe present the data card, following the format proposed by [2022arXiv220401075P].Dataset Owners. .Data overview.\n",
      "---\n",
      "While the intersection of DP and LLMs is fairly novel, the prominent approach to introduce DP into machine learning models has been to modify the training process (e.g., Differentially Private Stochastic Gradient Descent (DP-SGD) [song2013stochastic,Abadi2016DPSGD]).  Intuitively, these modifications ensure that the trained model does not depend too much on certain training data points thereby guaranteeing privacy; albeit the privacy gains come at significant costs in computation, training time, and model performance. Recent works [li2021large, yu2021differentially, dupuy2021efficient] address the aforementioned challenges, but DP training still remains an active area of interest as no conclusive method satisfactorily balances all aspects. In a real-world commercial setting, the cost considerations are even magnified: retraining production-scale LLMs from scratch, without a noticeable loss in performance, would entail a massive time and monetary expense and is therefore an unappealing option.\n",
      "---\n",
      "Our method enables the deployment of high-quality LLMs in the 7-13B parameters range to memory-limited devices such as laptops and phones. With our method, it is possible to develop specialized 7B LLMs in hassle-free 16-bit and then enable the deployment of such LLMs to phones by applying SpQR. Since SpQR is practically lossless, this ensures a reliable performance level for deployed LLMs which is important for consumer applications. Since mobile phones are ubiquitous and LLMs powerful general-purpose tools, SpQR might have a wide-reaching effect on how LLMs are used by the general population to complete useful tasks.LLMs are inherently a dual-use technology that can bring both significant benefits and serious harm. The ethical and societal risks of LLMs range from deliberate malicious use (e.g. generating spam) and accidental misuse to adverse economic side-effects [weidinger2021ethical].However, we believe that the marginal impact of SpQR will be positive or neutral since the LLMs we use are already openly available. Better quantization algorithms like SpQR let users with low-end devices run larger and generally more accurate language models. In other words, our algorithm does not create models with new capabilities (and risks): it only makes existing models more accessible.Following the request in this year's call for papers, we describe the use of large language models in our paper. We used two different chat-based language models: ChatGPT and Claude+. We used these models to accelerate the process of writing LaTeX code in Alg. <ref> and Figure <ref> (via Tikz). We also used these LLMs to provide slight improvements to the table design throughout the paper.In addition to this, we use ChatGPT to generate some prompts for Appendix <ref>. Finally, we used Claude+ to produce possible formulations for the outlier criterion in Alg. <ref>. In all these cases, we used LLMs through chat-based user interfaces, instructing them to generate code (LaTeX) or suggest improvements. If the suggested changes would not work as expected, we reported them to the model in natural language, using the same chat-based interface.\n",
      "---\n",
      "Stability on Nonsymmetric Semantic Relations. In order to test whether the responses arise from associative fortuity [pearl2009causality] in the data we tested Davinci and ChatGPT sensitivity to argument reversal for nonsymmetric relations (e.g. Original: Does a missing wheel stop a car from running; Modified: Does a car stop a missing wheel from running?) and seeing if this changed Davinci's response. Results of this on 9 semantic relations with nonsymmetric items (i.e., Cause and effect, Order of intensity, Order of size, Has part, Default inheritance, Precondition, Contained in, Has member, Troponym)  indicated that both Davinci and ChatGPT responses are stable under 7 relations but exhibit potential signs of associative behavior for Cause and effect and Troponym with GPT-3 and ChatGPT failing (e.g., replying yes for both of the examples provided above) on 70% and 50% of the cases, respectively. Combining LLMs with symbolic [nye2021improving] or physical world [ahn2022can] grounding are promising directions to address these side effects of associative learning.This work builds on foundations that are already firmly established in different fields: the importance of lexical semantic relationships for language processing [miller1995wordnet], the importance of conceptual modeling based on basic ontological principles [smith2012ontology], and prior evidence that establishes the characteristics and norms of human reasoning [stanovich1998individual].We have questioned therefore the strong empirical assumption of much prior work in NLP about the ability of LLMs to represent physical common sense using language data alone [lake2017building] and identified some promising areas for algorithmic investigation and theory formation. Going forward we hope that the discussion in this paper will be a useful adjunct for intrinsic evaluation of natural language generation models and further to LLMs that aim to combine machine learning from text with nature-inspired cognitive capabilities.apacite\n",
      "---\n",
      "The development of LLMs is quite involved and requires masses of training data. Anecdotally, some powerful recent models are trained using scrapes of much of the Internet, then further fine-tuned with reinforcement learning from human feedback (RLHF) [NIPS2013_e034fb6b,openai2023gpt4]. This further boosts the effective dataset size. Yet while current LLMs [devlin2018bert, liu2019roberta, brown2020language, zhang2022opt], including , were trained on predominantly human-generated text, this may change in the future. If most future models' training data is also scraped from the web, then they will inevitably come to train on data produced by their predecessors. In this paper, we investigate what happens when text produced,  by a version of , forms most of the training dataset of following models. What happens to  versions  as generation n increases?[This is not limited to text models; one can also consider what happens when music created by human composers and played by human musicians trains models whose output trains other models.]\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [02:55, 27.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "1. Quality Data: The quality of the data used to train the model is crucial. The data should be clean, relevant, and diverse to avoid biases and overfitting.\n",
      "\n",
      "2. Cross-Validation: This technique is used to assess how the results of a model will generalize to an independent data set. It helps in understanding the robustness of the machine learning model.\n",
      "\n",
      "3. Regularization: This technique helps to solve the overfitting problem, which is a situation where a machine learning model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.\n",
      "\n",
      "4. Ensemble Methods: These methods combine the decisions from multiple models to improve the overall performance. This can result in a more robust and stable model.\n",
      "\n",
      "5. Hyperparameter Tuning: This involves adjusting the parameters of the machine learning algorithm to improve the model's performance.\n",
      "\n",
      "6. Feature Selection: This involves selecting the most useful features to train the model. Irrelevant or partially relevant features can negatively impact model performance.\n",
      "\n",
      "7. Model Evaluation: Use appropriate metrics to evaluate the performance of the model. The choice of metrics depends on the task (classification, regression, ranking, etc.).\n",
      "\n",
      "8. Monitoring: After deployment, the model should be monitored to ensure it is performing as expected. If the model's performance drops, it may need to be retrained with new data.\n",
      "\n",
      "9. Updating: Machine learning models may need to be updated as new data becomes available. This can help the model stay relevant and perform well.\n",
      "\n",
      "10. Explainability: Understanding why a model makes certain decisions can help improve its reliability. Techniques like SHAP, LIME can be used for this purpose.\n",
      "\n",
      "11. Testing: Just like software, machine learning models should be tested thoroughly. This includes unit tests, integration tests, and system tests to ensure the model works as expected.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: Looking at the answers from both students, it's clear that neither of them directly addressed the question. The question asked for ways to ensure that machine learning models are robust and reliable, but both students went off on tangents.\n",
      "\n",
      "Bobby: I agree, Michael. However, if we have to choose, I'd say student B's answer is slightly better. While it doesn't directly answer the question, it does touch on the importance of reasoning abilities in machine learning models, which is a crucial aspect of robustness and reliability.\n",
      "\n",
      "Michael: I see your point, Bobby. But I think student A's answer is more relevant. It talks about the use of hardware innovations and learning masks for fast model inference, which can be seen as a way to ensure robustness. It's not a direct answer, but it's closer than student B's.\n",
      "\n",
      "Bobby: That's a fair point, Michael. But I still think that student B's focus on reasoning abilities is more relevant to the question. After all, a machine learning model's ability to reason is a key factor in its robustness and reliability.\n",
      "\n",
      "Michael: I see where you're coming from, Bobby. But I still believe that student A's answer is more relevant. The use of hardware innovations and learning masks can directly contribute to a model's robustness and reliability.\n",
      "\n",
      "Bobby: I understand your perspective, Michael. But considering the question asked for ways to ensure robustness and reliability, I believe that student B's focus on reasoning abilities is more relevant. \n",
      "\n",
      "Michael: I respect your opinion, Bobby. But I still think that student A's answer is more relevant to the question. \n",
      "\n",
      "Bobby: I see your point, Michael. But I still believe that student B's answer is more relevant to the question. \n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What are the potential consequences of using LLMs in high-stakes applications without proper interpretability?\n",
      "--------------------------------------------------\n",
      "Answer A ($24.05):\n",
      "Large language models (LLMs) have demonstrated remarkable predictive performance across a growing range of diverse tasks [brown2020language,devlin2018bert]. However, the inability to effectively interpret these models has led them to be characterized as black boxes. This opaqueness has debilitated their use in high-stakes applications such as medicine [Kornblith2022] and policy-making [brennan2013emergence], and raised issues related to fairness [dwork2012fairness], regulatory pressure [goodman2016european], safety [amodei2016concrete], and alignment [gabriel2020artificial]. This lack of interpretability is particularly detrimental in scientific fields, where trustworthy interpretation itself is the end goal [angermueller2016deep].\n",
      "---\n",
      "Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness.  We propose , a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls.  achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while only uses dozens of annotations as in-context exemplars without any training. Our code is available at <https://github.com/hkunlp/binder>[More resources at <https://lm-code-binder.github.io/>.].experiments are mainly based on OpenAI Codex (code-davinci-002) API[<https://beta.openai.com/>]. We provide (1) the input prompt templates we use in Appendix <ref>, (2) Codex hyper-parameters for each dataset we adopt in Appendix <ref>, (3) more implementation details in Appendix <ref>. Besides, we also upload our source code in the materials making our results easy to be reproduced.\n",
      "---\n",
      "Following this line of research, recent studies indicate it is critical (and also possible) to learn these N:M masks from scratch, without additional training or finetuning steps. Representative methods in this domain include SR-STE [zhou2021learning], DominoSearch [sun2021dominosearch] and Decaying Mask [kao2022training], which sparsify the model weights during each forward pass in training to compute gradients, and update them to models. While these methods demonstrate promising results with momentum SGD, their performance over adaptive optimizers, such as Adam, is less satisfactory (Section <ref>).  This implies the benefits of sparsity are largely traded-off by adaptivity in training, leading to slow convergence on many state-of-the-art models [zhang2020adaptive]. In light of this, in this paper we answer the question: \n",
      "Can we learn N:M structured sparsity masks with Adam, without model degradation?\n",
      " Motivated by the insights from recent studies on critical learning regime of Adam in a distributed learning environment [tang20211,lu2022maximizing], we first hypothesize that with masked weights, the back propagation leads to noisy gradients and gives a poorly estimated variance (running average of second moment gradients) in the Adam states. It essentially breaks the proper scaling of the coordinate-wise learning rate.\n",
      "---\n",
      "Past work has characterized what kinds of functions ICL can learn [Garg2022WhatCT, laskin2022context, muller2021transformers] and the distributional properties of pretraining that can elicit in-context learning [xie2021explanation, chan2022data]. But how ICL learns these functions has remained unclear. What learning algorithms (if any) are implementable by deep network models? Which algorithms are actually discovered in the course of training?  This paper takes first steps toward answering these questions, focusing on a widely used model architecture (the transformer) and an extremely well-understood class of learning problems (linear regression).\n",
      "---\n",
      "What are the benefits and drawbacks of the datasets we used? Using tasks from [bigbench] allowed us to collect a set of diverse, challenging tasks within a similar format. These tasks cover skills which may not be well-represented in standard datasets. However, these idiosyncratic adversarially-sampled tasks may therefore not be representative of tasks to which LMs are usually applied. This difference could potentially amplify or suppress our effects. Explanations might be uniquely helpful in challenging tasks where the relationship between questions and answers is unusual. Alternatively, explanations might be more effective in more standard settings where they could relate more directly to common reasoning patterns. The breadth of tasks we explored, together with statistical analyses that explicitly model task differences, partially ameliorate these concerns. Nevertheless, future work should explore the benefits of explanations beyond BIG-Bench.\n",
      "--------------------------------------------------\n",
      "Answer B ($81.75):\n",
      "Large language models (LLMs) have demonstrated remarkable predictive performance across a growing range of diverse tasks [brown2020language,devlin2018bert]. However, the inability to effectively interpret these models has led them to be characterized as black boxes. This opaqueness has debilitated their use in high-stakes applications such as medicine [Kornblith2022] and policy-making [brennan2013emergence], and raised issues related to fairness [dwork2012fairness], regulatory pressure [goodman2016european], safety [amodei2016concrete], and alignment [gabriel2020artificial]. This lack of interpretability is particularly detrimental in scientific fields, where trustworthy interpretation itself is the end goal [angermueller2016deep].\n",
      "---\n",
      "We set out a number of uses for LLMs in the language learning domain, relating to content creation and calibration, automated assessment of written texts, and personalised feedback. In each case the general principles of the approach are well established thanks to previous work with pre-existing LLMs such as BERT [bert] – language models with millions of parameters have existed for several years already. We look at the opportunities presented by the recent and rapid steps taken by OpenAI in releasing new variants from the `generative pre-training' (GPT) model series, along with some newly published pre-prints relating to LLMs and the language learning research field. We refer to some LLM-driven language learning applications already in use, and outline the variety of LLMs available besides GPT. It is a fast evolving research field, one being driven by industry developments. We perceive some possible risks in this research trajectory, which include but are not limited to the absence of proper safeguards on education technology, the lack of public understanding as to how LLMs are trained and how they can confidently assert incorrect information, and the harm to the advancement of education technology as a whole if it is considered `solved' by investors and research councils – not to mention the ethical issues that are already well known, such as data protection [carlini2021extracting], examination malpractice [fyfe2022cheat, ventayen2023openai][Note that the text-matching tool  [Mphahlele2019], which is commonly used to detect plagiarism, has developed a module to detect the use of AI in essays: <https://www.turnitin.com/blog/the-launch-of-turnitins-ai-writing-detector-and-the-road-ahead>], environmental impact [strubell-etal-2019-energy,dodge2022], and internet addiction [chou2005review,bhargava2021ethics,gioia2021problematic], among others.\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "Past work has characterized what kinds of functions ICL can learn [Garg2022WhatCT, laskin2022context, muller2021transformers] and the distributional properties of pretraining that can elicit in-context learning [xie2021explanation, chan2022data]. But how ICL learns these functions has remained unclear. What learning algorithms (if any) are implementable by deep network models? Which algorithms are actually discovered in the course of training?  This paper takes first steps toward answering these questions, focusing on a widely used model architecture (the transformer) and an extremely well-understood class of learning problems (linear regression).\n",
      "---\n",
      "The training of large language models (LLMs) – also known as pre-trained language models or foundation models – has had a transformative effect on the fields of natural language processing (NLP) and artificial intelligence (AI) more broadly. LLMs are `large' because they are neural networks made up of billions or trillions of parameters. The networks are Transformers [vaswani] trained on huge swathes of text from the World Wide Web, using language modelling objectives such as predicting omitted (or, `masked') words and sentence pairs [bert], or predicting the next token in a sequence [radford-gpt1]. Furthermore, in the few-shot learning paradigm, LLMs can be directed towards new tasks without large quantities of task-specific data [radford2019language,brown-gpt3], the collection of which tends to be time-consuming and costly. Overall, LLMs also offer great potential for educational applications. One previous paper has already provided an overview of some of the possible applications of LLMs to educational technology as a whole [kasneci2023chatgpt], across subjects. Our distinct contribution is to focus on the language learning and assessment domain specifically. In this paper, we describe some of the uses for LLMs in the context of language learning, discuss the state of the art or work in progress, and consider practical, societal and ethical implications.\n",
      "---\n",
      "What are the benefits and drawbacks of the datasets we used? Using tasks from [bigbench] allowed us to collect a set of diverse, challenging tasks within a similar format. These tasks cover skills which may not be well-represented in standard datasets. However, these idiosyncratic adversarially-sampled tasks may therefore not be representative of tasks to which LMs are usually applied. This difference could potentially amplify or suppress our effects. Explanations might be uniquely helpful in challenging tasks where the relationship between questions and answers is unusual. Alternatively, explanations might be more effective in more standard settings where they could relate more directly to common reasoning patterns. The breadth of tasks we explored, together with statistical analyses that explicitly model task differences, partially ameliorate these concerns. Nevertheless, future work should explore the benefits of explanations beyond BIG-Bench.\n",
      "---\n",
      "These models are often based on the attention mechanism [bahdanauNeuralMachineTranslation2016], a key component of the transformer architecture [vaswaniAttentionAllYou2017]. Besides providing substantial performance benefit, attention weights have been used to provide interpretability of neural models [linStructuredSelfAttentiveSentence2017, vashishthAttentionInterpretabilityNLP2019a,paltenghiThinkingDeveloperComparing2021]. In particular, [wanWhatTheyCapture2022] and [vigAnalyzingStructureAttention2019] have shown how the attention weights contain important syntactic information on both the Abstract Syntax Tree (AST) of source code and Part of Speech (POS) tags in natural language. Moreover, [wanWhatTheyCapture2022] showed how using attention weights to infer the distance between two tokens outperformed techniques using hidden representations. In a similar direction, [zhangWhatDoesTransformer2022] has shown how a novel graph representation of source code derived solely from attention weights achieved comparable performance on the VarMisuse dataset [allamanisLearningRepresentPrograms2018b] to that of a hand-crafted graph representation based on control flow and data flow  [hellendoornGlobalRelationalModels2020].The work cited above suggests the attention mechanism reflects or encodes objective properties of the source code processed by the model. We argue, that just as software developers consider different locations in the code individually and follow precise connections between them, so the self-attention of transformers connects and creates information flow between similar and linked code locations. If those relations are indeed comparable, this raises the possibility: can the knowledge about source code conveyed by the attention weights of neural models be leveraged to support human code exploration?There are datasets tracking developers' visual attention while looking at code, but they do not seem suitable to this task. The largest ones either put the developers in an unnatural (and thus possibly biasing) environment where most of the vision is blurred  [paltenghiThinkingDeveloperComparing2021], or they contain few and very specific code comprehension tasks [bednarikEMIPEyeMovements2020a] on code snippets too short to exhibit any interesting code navigation pattern.\n",
      "---\n",
      "What do our results imply about LMs' abilities for in-context learning? The process by which LMs adapt to a prompt is debated. For example, [min2022rethinking] make the interesting observation that GPT-3 can perform nearly as well on common tasks like entailment even with random answers in the prompt, and therefore question whether the models are really “learning” in context. Comparisons between our explanations and matched controls allow us to test whether superficial features of the explanations drive their benefits.\n",
      "---\n",
      "\\begin{abstract}     Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks.     However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability.     Here, we ask whether we can automatically obtain natural language explanations for black box text modules.     A text module is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region.     Black box indicates that we only have access to the module's inputs/outputs.          We introduce \\methodlongunderlineds (\\method), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is.     We study \\methods in 3 contexts.     First, we evaluate \\methods on synthetic modules and find that it often recovers ground truth explanations.     Second, we use \\methods to explain modules found within a pre-trained BERT model, enabling inspection of the model's internals.     Finally, we show that \\methods can generate explanations for the response of individual fMRI voxels to language stimuli, with potential applications to fine-grained brain mapping.     All code for using \\methods and reproducing results is made available on Github.\\footnote{Scikit-learn-compatible API available at \\href{https://github.com/csinva/imodelsX}{\\faGithub~github.com/csinva/imodelsX} and code for experiments along with all generated explanations is available at \\href{https://github.com/microsoft/automated-explanations}{\\faGithub~github.com/microsoft/automated-explanations}.} \\end{abstract}\n",
      "---\n",
      "<Ref> shows that replacing person and activity has negligible impact on the accuracy. However, abstracting out the sports abstract drops the performance below levels (<Ref>).  What explains such a discrepancy? Note that both person and activity are already present in the question, and are copied in the thought. Thus, replacing them even with abstract tokens does not harm the performance, as the extraction pattern is retained. On the other hand, the sport is not present in the question, and has to be extracted. Consider a question: Is the following sentence plausible? “Norman Powell committed a blocking foul.” For abstracted sports prompts, the model generates Norman Powell is a SPORT1 player. Committing a blocking foul is part of SPORT2. So the answer is no. Whereas for the abstract person or the activity prompt, the model correctly infers the pattern (copy the person or the activity, and generate the corresponding sport) and generates: Norman Powell is a basketball player. Being a blocking foul is part of basketball. So the answer is yes. This example also clearly drives the notion of symbiosis:  only generating the right pattern or only having the right text. Both are required for effective .Cases where patterns help.As empirical results show, combining patterns with language reveals their true potential. However, our analysis uncovers interesting reasons for the superiority of patterns only, for the small number of cases.Interestingly, we found a few cases where forcing the model to generate some text leads to errors.Primarily, they were caused by the model trying to make faulty connections. Detailed examples are located in <Ref>.<Ref> presents detailed results, including ablations on various parts of the prompt. These were omitted from the main text due to space constraints.0.99     0          The presence of text is necessary for the model to arrive at the right conclusions.          While the model is sufficiently capable of forming abstractions, employing concordant entities in questions and thoughts is crucial for the success of .          The performance of the model is proportionally sensitive to the degree of variations in the text.We test this hypothesis to determine the role of text in the performance of .Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar.Note that the practicality of the counterfactual experiments for text is contingent on the nature of the target task.In addition, because there is no text in , we did not conduct counterfactual experiments in this dataset.<Ref> summarizes the results and shows sample thought for each counterfactual prompt.Text with discrepant entities text_diff_entities.We first investigate the role of employing identical entities (names and objects) between a question and its associated thought.To achieve this, we simply modify the entities in the thoughts to be discrepant with the ones in the questions.is exclusively relevant for the and datasets for which concrete entities exist: objects and people for and players and activities for .In the dataset, we change the player name and the activity in the thought, while retaining the sport in the question.Samples of generated outputs in <Ref> illustrate that in this scenario, the model appears to disregard the prompt, generating arbitrary thoughts and attaining a similar performance as .The task solve rate for with is noticeably better than , while still fails to match the performance of .We delineate samples of generated outputs in <Ref> to glean more insights.\n",
      "---\n",
      "Firstly, more widespread adoption of automated techniques, e.g., NAS eliminates the need for laborious trials and error for manual design of Transformer architectures, freeing up hundreds of hours of man-power as well as computational resources. Secondly, automating architecture design can trigger the generation of new models with superior performance, which benefits the ever-growing applications of NLP in the everyday life. Finally, by making the search algorithm efficient, we ensure it can be accessible to the general scientific public without need for any expensive mode training, thereby minimizing the unwanted byproducts of the Deep Learning era such as the carbon footprint, and power consumption. While the benefits of automation in NLP are plenty, it can lead to potential side effects that have not been yet fully unveiled. Since our work advances the use of NAS in the NLP design pipeline, there is need for scrutiny of the models which have been automatically designed with respect to aspects such as bias, misinformation, and nefarious activity, to name a few.\n",
      "---\n",
      "We apply to LLaMA-7B and LLaMA-13B [touvron2023llama] and tune it on the Alpaca dataset [alpaca] to compare to Stanford Alpaca, and on the open-source ShareGPT dataset to compare to Vicuna [vicuna2023]. For evaluation, we follow the pipeline in Vicuna [vicuna2023], , we ask questions that span different categories and use GPT-4 [openai2023gpt4] to score the answers provided by the model. Scores are based on the helpfulness, relevance, accuracy, and level of detail of the answers. In Table <ref>, we compare the scores of different models on each category of questions. All the scores are on a scale of 1-10. Overall, has slightly better performance than fine-tuning for each baseline model, while LoRA is less competitive. We observe that is better at providing detailed and informed answers, which makes it a better assistant for questions about general knowledge (Generic, Knowledge, Common). Figure <ref> is an example. Given the question What are the most effective strategies for conflict resolution in the workplace, while the fine-tuned baseline lists several suggestions, is able to provide more detailed explanations for each suggestion. On the other hand, fine-tuning is more helpful when following the instruction for creative questions such as Roleplay and Counterfact. Figure <ref> gives an example of a roleplay question. provides a detailed explanation of what to say to the crews, while the fine-tuned baseline directly creates a short speech which more adheres to the instruction. These observations are consistent with the underlying mechanism of and fine-tuning, , does not modify the pre-trained backbone and thus is able to “remember” all the knowledge learned during pre-training, meanwhile fine-tuning may forget what is learned during pre-training when modifying the weights but is able to better follow the instruction in this way. More examples are shown in Appendix.\n",
      "---\n",
      "Generative models (e.g., GANs, diffusion models) learn the underlying data distribution in an unsupervised manner.  However, many applications of interest require sampling from a particular region of the output space or sampling evenly over a range of characteristics. For efficient sampling in these scenarios, we propose Generative Visual Prompt (), a framework for distributional control over pre-trained generative models by incorporating knowledge of other off-the-shelf models. defines control as energy-based models (EBMs) and samples images in a feed-forward manner by approximating the EBM with invertible neural networks, avoiding optimization at inference. Our experiments demonstrate how can efficiently sample from several unconditional generative models (e.g., StyleGAN2, StyleNeRF, diffusion autoencoder, NVAE) in a controlled or/and de-biased manner using various off-the-shelf models: (1) with the CLIP model as control, can sample images guided by text,  (2) with image classifiers as control, can de-bias generative models across a set of attributes or attribute combinations, and (3) with inverse graphics models as control, can sample images of the same identity in different poses. (4) Finally, reveals that the CLIP model shows a “reporting bias” when used as control, and can further de-bias this controlled distribution in an iterative manner.[The code is available at . ]The authors would like to thank the anonymous reviewers, Zoltán Ádám Milacski, Jianchun Chen, and Shubhra Aich for their valuable feedback on drafts of this paper.plain§ CHECKLIST* For all authors...    \n",
      "  * Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?        \n",
      "  * Did you describe the limitations of your work?       In Section <ref> and Appendix <ref>.   \n",
      "  * Did you discuss any potential negative societal impacts of your work?       In Section <ref>.   \n",
      "  * Have you read the ethics review guidelines and ensured that your paper conforms to them?* If you are including theoretical results...    \n",
      "  * Did you state the full set of assumptions of all theoretical results?       In Section <ref> and Appendix <ref>. \t\n",
      "  * Did you include complete proofs of all theoretical results?       In Section <ref> and Appendix <ref>.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "Stability on Nonsymmetric Semantic Relations. In order to test whether the responses arise from associative fortuity [pearl2009causality] in the data we tested Davinci and ChatGPT sensitivity to argument reversal for nonsymmetric relations (e.g. Original: Does a missing wheel stop a car from running; Modified: Does a car stop a missing wheel from running?) and seeing if this changed Davinci's response. Results of this on 9 semantic relations with nonsymmetric items (i.e., Cause and effect, Order of intensity, Order of size, Has part, Default inheritance, Precondition, Contained in, Has member, Troponym)  indicated that both Davinci and ChatGPT responses are stable under 7 relations but exhibit potential signs of associative behavior for Cause and effect and Troponym with GPT-3 and ChatGPT failing (e.g., replying yes for both of the examples provided above) on 70% and 50% of the cases, respectively. Combining LLMs with symbolic [nye2021improving] or physical world [ahn2022can] grounding are promising directions to address these side effects of associative learning.This work builds on foundations that are already firmly established in different fields: the importance of lexical semantic relationships for language processing [miller1995wordnet], the importance of conceptual modeling based on basic ontological principles [smith2012ontology], and prior evidence that establishes the characteristics and norms of human reasoning [stanovich1998individual].We have questioned therefore the strong empirical assumption of much prior work in NLP about the ability of LLMs to represent physical common sense using language data alone [lake2017building] and identified some promising areas for algorithmic investigation and theory formation. Going forward we hope that the discussion in this paper will be a useful adjunct for intrinsic evaluation of natural language generation models and further to LLMs that aim to combine machine learning from text with nature-inspired cognitive capabilities.apacite\n",
      "---\n",
      "We present , a new framework for multi-robot collaboration that leverages large language models (LLMs) for robot coordination and planning. We introduce , a 6-task benchmark for multi-robot manipulation to be open-sourced to the broader research community. We empirically demonstrate the generality of our approach and many desirable properties such as few-shot adaptation to varying task semantics, while identifying limitations and room for improvement. Our work falls in line with recent literature that explores harnessing the power of LLMs for robotic applications, and points to many exciting opportunities for future research in this direction.LLM-query Efficiency. We rely on querying pre-trained LLMs for generating every single response in an agent's dialog, which can be cost expensive and dependant on the LLM's reaction time. Response delay from LLM querying is not desirable for tasks that are dynamic or speed sensitive.Open-loop execution. The motion trajectories from our planner are executed by robots in an open-loop fashion and lead to potential errors. Due to the layer of abstraction in scene and action descriptions, LLMs can't recognize or find means to handle such execution-level errors.Oracle state information in simulation.  assumes perception (e.g., object detection, pose estimation and collision-checking) is accurate. This assumption makes our method prone to failure in cases where perfect perception is not available: this is reflected in our real-world experiments, where the pre-trained object detection produces errors that can cause planning mistakes.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [02:56, 19.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 109, in run\n",
      "    await self.visit(self.parse_tree, VariableStack([self.program._variables], self))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 524, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_assistant.py\", line 13, in assistant\n",
      "    return await role(role_name=\"assistant\", hidden=hidden, _parser_context=_parser_context, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_role.py\", line 17, in role\n",
      "    new_content += await parser.visit(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in visit\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in <listcomp>\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 379, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_gen.py\", line 140, in gen\n",
      "    gen_obj = await parser.llm_session(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 665, in __call__\n",
      "    out = await self.llm.caller(**call_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 370, in _library_call\n",
      "    out = await openai.ChatCompletion.acreate(**kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 728, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, you requested 8763 tokens (7739 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\n",
      "\n",
      "Error in program:  This model's maximum context length is 8192 tokens. However, you requested 8763 tokens (7739 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the difference between a BERT and a GPT?\n",
      "--------------------------------------------------\n",
      "Answer A ($12.52):\n",
      "BERT v.s. GPT. Traditional language models [kim2016character, verwimp2017character, miyamoto2016gated] mainly focused on a particular task and could not be transferred to other tasks. Transfer learning is a common approach for alleviating this issue by pretraining a foundation model [zhou2023comprehensive], which can then be finetuned on various downstream tasks. Based on the architecture, there are three classes: encoder-decoder [lewis2019bart,song2019mass,qi2020prophetnet,raffel2020exploring], encoder-only [devlin2018bert,lan2019albert,liu2019roberta,clark2020electra], decoder-only [radford2018improving,radford2019language,brown2020language,openai2023gpt4]. Out of numerous large language models, encoder-only BERT [devlin2018bert] and decoder-only GPT [radford2018improving] are arguably the two most popular ones. A comparison of them is summarized in Table <ref>. Both of them use attention-based Transformer [vaswani2017attention] with self-supervised learning to learn from textual datasets without labels. After pretraining, both BERT and GPT can be finetuned and show competitive performance in downstream tasks. A  core difference between BERT and GPT lies in their pretraining strategy: masked modeling (see [zhang2022survey] for a complete survey on masked autoencoder) and autoregressive modeling. With masked modeling, BERT predicts masked language tokens from unmasked ones. A major advantage of BERT is that it can utilize bidirectional text information, which makes it compatible with sentiment analysis tasks. Due to the discrepancy between the mask-then-predict pertaining task and downstream tasks, BERT is rarely used for the downstream task without finetuning. By contrast, autoregressive modeling methods (represented by GPT) show competitive performance for few-shot or zero-shot text generation. In the following, we summarize the development path of GPT from v1 to v4, which is shown in <ref>.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "* We show that generative LLM detection with a discriminator LLM is impossible if the attacker has access to the reference \"human\" dataset used to train the discriminator LLM. Simply fine-tuning the dataset and using prompts from it leads to a complete failure of the discriminator to learn the difference between machine and human-generated texts, even in a setting where all LLM outputs are correctly labeled.   \n",
      "  * We show that reinforcement from critic generalizes significantly better than previously described when paired with the AdamW optimizer rather than the commonly used Adam one and allows well-generalizing model fine-tunes from limited data, matching prior SotA in normativity fine-tuning.   \n",
      "  * We demonstrate a critical weakness on a previously proposed text-generating GAN architecture - DPGAN, and show the connection of this weakness to the difference in representative power of LLMs and RNNs used in text GANs compatible with the in-the-wild detection setting.Our code is based on a pre-existing Pytorch implementation of common text-generating GANs, including DPGAN, available from <https://github.com/williamSYSU/TextGAN-PyTorch>.\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "--------------------------------------------------\n",
      "Answer B ($97.38):\n",
      "BERT v.s. GPT. Traditional language models [kim2016character, verwimp2017character, miyamoto2016gated] mainly focused on a particular task and could not be transferred to other tasks. Transfer learning is a common approach for alleviating this issue by pretraining a foundation model [zhou2023comprehensive], which can then be finetuned on various downstream tasks. Based on the architecture, there are three classes: encoder-decoder [lewis2019bart,song2019mass,qi2020prophetnet,raffel2020exploring], encoder-only [devlin2018bert,lan2019albert,liu2019roberta,clark2020electra], decoder-only [radford2018improving,radford2019language,brown2020language,openai2023gpt4]. Out of numerous large language models, encoder-only BERT [devlin2018bert] and decoder-only GPT [radford2018improving] are arguably the two most popular ones. A comparison of them is summarized in Table <ref>. Both of them use attention-based Transformer [vaswani2017attention] with self-supervised learning to learn from textual datasets without labels. After pretraining, both BERT and GPT can be finetuned and show competitive performance in downstream tasks. A  core difference between BERT and GPT lies in their pretraining strategy: masked modeling (see [zhang2022survey] for a complete survey on masked autoencoder) and autoregressive modeling. With masked modeling, BERT predicts masked language tokens from unmasked ones. A major advantage of BERT is that it can utilize bidirectional text information, which makes it compatible with sentiment analysis tasks. Due to the discrepancy between the mask-then-predict pertaining task and downstream tasks, BERT is rarely used for the downstream task without finetuning. By contrast, autoregressive modeling methods (represented by GPT) show competitive performance for few-shot or zero-shot text generation. In the following, we summarize the development path of GPT from v1 to v4, which is shown in <ref>.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "* We show that generative LLM detection with a discriminator LLM is impossible if the attacker has access to the reference \"human\" dataset used to train the discriminator LLM. Simply fine-tuning the dataset and using prompts from it leads to a complete failure of the discriminator to learn the difference between machine and human-generated texts, even in a setting where all LLM outputs are correctly labeled.   \n",
      "  * We show that reinforcement from critic generalizes significantly better than previously described when paired with the AdamW optimizer rather than the commonly used Adam one and allows well-generalizing model fine-tunes from limited data, matching prior SotA in normativity fine-tuning.   \n",
      "  * We demonstrate a critical weakness on a previously proposed text-generating GAN architecture - DPGAN, and show the connection of this weakness to the difference in representative power of LLMs and RNNs used in text GANs compatible with the in-the-wild detection setting.Our code is based on a pre-existing Pytorch implementation of common text-generating GANs, including DPGAN, available from <https://github.com/williamSYSU/TextGAN-PyTorch>.\n",
      "---\n",
      "BERT [devlin-etal-2019-bert] BERT is pretrained with masked language modeling (MLM) and a next-sentence prediction objective.  Since a majority of the questions have Yes/No/Don't know as the answer, we finetune BERT and other BERT-like models (see below) in a multi-class classification setting.  We train all BERT-like models in this fashion.  In our experiments, we BERT-Large.   We train with a learning rate of 1e-5 for 10 epochs.RoBERTa [liu2019roberta] RoBERTa is a more robustly pretrained version of BERT.  In our experiments, we use RoBERTa-Large.DeBERTa [he2020deberta, he2021debertav3] DeBERTa has a disentangled attention mechanism and it is pretrained with a version of MLM objective that uses the content and position of the context words.  In our experiments, we use DeBERTa-v2-XLarge and DeBERTa-v3-Large.\n",
      "---\n",
      "<Ref> shows that replacing person and activity has negligible impact on the accuracy. However, abstracting out the sports abstract drops the performance below levels (<Ref>).  What explains such a discrepancy? Note that both person and activity are already present in the question, and are copied in the thought. Thus, replacing them even with abstract tokens does not harm the performance, as the extraction pattern is retained. On the other hand, the sport is not present in the question, and has to be extracted. Consider a question: Is the following sentence plausible? “Norman Powell committed a blocking foul.” For abstracted sports prompts, the model generates Norman Powell is a SPORT1 player. Committing a blocking foul is part of SPORT2. So the answer is no. Whereas for the abstract person or the activity prompt, the model correctly infers the pattern (copy the person or the activity, and generate the corresponding sport) and generates: Norman Powell is a basketball player. Being a blocking foul is part of basketball. So the answer is yes. This example also clearly drives the notion of symbiosis:  only generating the right pattern or only having the right text. Both are required for effective .Cases where patterns help.As empirical results show, combining patterns with language reveals their true potential. However, our analysis uncovers interesting reasons for the superiority of patterns only, for the small number of cases.Interestingly, we found a few cases where forcing the model to generate some text leads to errors.Primarily, they were caused by the model trying to make faulty connections. Detailed examples are located in <Ref>.<Ref> presents detailed results, including ablations on various parts of the prompt. These were omitted from the main text due to space constraints.0.99     0          The presence of text is necessary for the model to arrive at the right conclusions.          While the model is sufficiently capable of forming abstractions, employing concordant entities in questions and thoughts is crucial for the success of .          The performance of the model is proportionally sensitive to the degree of variations in the text.We test this hypothesis to determine the role of text in the performance of .Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar.Note that the practicality of the counterfactual experiments for text is contingent on the nature of the target task.In addition, because there is no text in , we did not conduct counterfactual experiments in this dataset.<Ref> summarizes the results and shows sample thought for each counterfactual prompt.Text with discrepant entities text_diff_entities.We first investigate the role of employing identical entities (names and objects) between a question and its associated thought.To achieve this, we simply modify the entities in the thoughts to be discrepant with the ones in the questions.is exclusively relevant for the and datasets for which concrete entities exist: objects and people for and players and activities for .In the dataset, we change the player name and the activity in the thought, while retaining the sport in the question.Samples of generated outputs in <Ref> illustrate that in this scenario, the model appears to disregard the prompt, generating arbitrary thoughts and attaining a similar performance as .The task solve rate for with is noticeably better than , while still fails to match the performance of .We delineate samples of generated outputs in <Ref> to glean more insights.\n",
      "---\n",
      "Stable Diffusion revolutionised image creation from descriptive text. ,  and  demonstrated astonishing performance across a variety of language tasks.  introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to  once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as [The name is inspired by the Generative Adversarial Networks (GAN) literature on mode collapse, where GANs start producing a limited set of outputs that all trick the discriminator.  is a process whereby models eventually converge to a state similar to that of a GAN Mode Collapse. The original version of this paper referred to this effect as `model dementia', but we decided to change this following feedback that it trivialised the medical notion of `dementia' and could cause offence. ] and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "In recent years, sequence models (SM)  have made a substantial progress in natural language processing (NLP) [nadkarni2011natural]. For example,  GPT series [brown2020language] and BERT models [devlin2019bert], built on autoregressive SMs, have demonstrated remarkable performance on a wide range of downstream tasks and achieved strong performance on few-shot  generalization tasks. Although SM are mostly used in language tasks due to its natural fitting with the sequential property of languages, the sequential approaches are not confined to NLP only, but is instead a widely applicable general foundation model [bommasani2021opportunities]. For example, in computer vision (CV), one can split an image into sub-images and align them in a sequence as if they were tokens in NLP tasks [devlin2019bert, dosovitskiy2020image, he2021masked]. Although the idea of solving CV tasks by SM is straightforward, it serves as the foundation to some of the best-performing CV algorithms [touvron2021going, wang2021crossformer, tu2022maxvit]. Furthermore, sequential methods are starting to spawn powerful multi-modal visual language models such as Flamingo [alayrac2022flamingo], DALL-E [ramesh2021zero], and GATO [gato] in the recent past.\n",
      "---\n",
      "To use GPT-3.5 for deductive coding, we wrote a series of prompts that instructed the LLM to perform deductive coding given the codebook and a text (Appendix <ref>). In some cases, the codebook instructions were updated to clarify the code definitions and standardize format.  The prompt design is inspired by techniques such as role prompting [Schulhoff_Learn_Prompting_2022], where a role for the AI is assigned (e.g., “you are a qualitative coder performing deductive coding”), and chain-of-thought prompting [wei_chain–thought_2023], where a complex reasoning task is broken down into sub-tasks and the model is asked to perform the sub-tasks serially.  Outside of examples provided in the existing codebooks, additional few-shot examples for in-context learning [dong2022survey] were not included, making the majority of coding tasks zero-shot.  One design choice of note is the difference in how we approached mutually exclusive vs. non-mutually exclusive coding schemes.  For mutually exclusive code sets, we used a prompt that requests a single code and coding explanation. However, for non-mutually exclusive code sets, we used a prompt that iterates across each code individually, to provide a code and coding explanation for each code in the set separately.Four publicly available data sets were used to demonstrate LACA and assess the deductive coding performance of GPT-3.5 (Table <ref>). Data sets were considered for inclusion if they contained the original text, coded data, and a codebook, though in some instances, a rudimentary codebook was derived from the category names. The original and revised codebook text for each dataset are available in Appendix <ref>.\n",
      "---\n",
      "Past work has characterized what kinds of functions ICL can learn [Garg2022WhatCT, laskin2022context, muller2021transformers] and the distributional properties of pretraining that can elicit in-context learning [xie2021explanation, chan2022data]. But how ICL learns these functions has remained unclear. What learning algorithms (if any) are implementable by deep network models? Which algorithms are actually discovered in the course of training?  This paper takes first steps toward answering these questions, focusing on a widely used model architecture (the transformer) and an extremely well-understood class of learning problems (linear regression).\n",
      "---\n",
      "Set Debiasing: In Figure <ref>.a, we present the performance of the ERM baseline and our EPVT on six biased ISIC2019 datasets. Each point on the graph represents an algorithm that is trained and tested on a specific bias degree split. The graph shows that the ERM baseline performs better than our EPVT when the bias is low (0 and 0.3). However, this is because ERM relies heavily on spurious correlations between artifacts and class labels, leading to overfitting on the training set. As the bias degree increases, the correlation between artifacts and class labels decreases, and overfitting the train set causes the performance of ERM to drop dramatically on the test set with a significant distribution difference. In contrast, our EPVT exhibits greater robustness to different bias levels. Notably, our EPVT outperforms the ERM baseline by 9.4% on the bias 1 dataset.\n",
      "---\n",
      "We begin by formulating the task of instruction induction. Given a sequence of n demonstrations {x_k, y_k}_k ∈{1,…,n}, the goal is to generate a single natural language instruction, such that for each x_k, following the instruction results in y_k. This format is similar to in-context learning [gpt3], only here the desired output is an instruction describing the relation between the inputs and outputs of the demonstrations. We require models to perform this in a zero-shot setting, without any fine-tuning on labeled data. Figure <ref> illustrates the difference between standard in-context prompting and instruction-induction prompting.To elicit models to generate instructions, we consider prompts that would elicit humans to do so. We design a meta-prompt presenting instruction induction as a challenge puzzle and verify its clarity in a human study (<ref>). The prompt is presented in Figure <ref> (right side, in pink).[We found this prompt informative for both humans and models in preliminary experiments.]While prior work already shows that large LMs are often able to infer a latent task from a given set of demonstrations, this has been largely based on their ability to execute the task on a held-out example. Instruction induction requires that the model describe the underlying task in natural language.We evaluate on 24 tasks, listed in Table <ref>. We select these tasks as they vary in difficulty and represent different aspects of language understanding, ranging from surface-level spelling to sentence similarity and causality detection.[See Appendix <ref> for the full details of each task.]We review the dataset's format, the annotation and verification processes we conducted to ensure that the tasks are viable, and finally discuss a theoretical limitation of this setup.\n",
      "---\n",
      "The development of LLMs is quite involved and requires masses of training data. Anecdotally, some powerful recent models are trained using scrapes of much of the Internet, then further fine-tuned with reinforcement learning from human feedback (RLHF) [NIPS2013_e034fb6b,openai2023gpt4]. This further boosts the effective dataset size. Yet while current LLMs [devlin2018bert, liu2019roberta, brown2020language, zhang2022opt], including , were trained on predominantly human-generated text, this may change in the future. If most future models' training data is also scraped from the web, then they will inevitably come to train on data produced by their predecessors. In this paper, we investigate what happens when text produced,  by a version of , forms most of the training dataset of following models. What happens to  versions  as generation n increases?[This is not limited to text models; one can also consider what happens when music created by human composers and played by human musicians trains models whose output trains other models.]\n",
      "---\n",
      "Large sequence models (SM) such as GPT series and BERT have displayed outstanding performance and generalization capabilities in natural language process, vision and recently reinforcement learning. A natural follow-up question is how to abstract multi-agent decision making also as an sequence modeling problem and benefit from the prosperous development of the SMs. In this paper, we introduce a novel architecture named Multi-Agent Transformer (MAT) that effectively casts cooperative multi-agent reinforcement learning (MARL) into SM problems wherein the objective is to map agents'  observation sequences to agents' optimal action sequences. Our goal is to build the bridge between MARL and SMs so that the modeling power of modern sequence models can be unleashed for MARL. Central to our MAT is an encoder-decoder architecture which leverages the multi-agent advantage decomposition theorem to transform the joint policy search problem into a sequential decision making process; this renders only linear time complexity for multi-agent problems and, most importantly, endows MAT with monotonic performance improvement guarantee. Unlike prior arts such as Decision Transformer fit only pre-collected offline data, MAT is trained by online trial and error from the  environment in an on-policy fashion.  To validate MAT, we conduct extensive experiments on StarCraftII, Multi-Agent MuJoCo, Dexterous Hands Manipulation, and Google Research Football benchmarks. Results demonstrate that MAT achieves superior performance and data efficiency compared to  strong baselines including MAPPO and HAPPO. Furthermore, we demonstrate that MAT is an excellent few-short learner on unseen tasks regardless of changes in the number of agents. See our project page at <https://sites.google.com/view/multi-agent-transformer>[^†Corresponding to yaodong.yang@pku.edu.cn. The source code could be accessed directly with this link <https://github.com/PKU-MARL/Multi-Agent-Transformer>.].\n",
      "---\n",
      "In this technical report, we investigate for the first time the capability of ChatGPT for programming numerical algorithms in different programming languages. Particularly, we test the ability of this artificial intelligence model by considering various scientific problems such as the two-dimensional Poisson equation, the two-dimensional diffusion equation, the incompressible Navier-Stokes equations in two dimensions, compressible inviscid flow in one dimension, finding the maximum eigenvalue of matrices, solving linear systems of equations, storing sparse matrices, etc. We also provide examples of scientific machine learning such as convolutional neural networks for supervised learning and physics-informed neural networks for weakly supervised learning in computational mechanics. We analyze the capacity of GhatGPT to debug and enhance user-written codes, complete missing numerical code segments, rewrite existing codes in different programming languages, and parallelize serial codes. Furthermore, we test ChatGPT's ability to determine if given codes were created by humans or machines. Our goal is to explore the successes, failures, and challenges of ChatGPT through these test cases.The results of our exploration provide insight into the capability of ChatGPT for programming numerical algorithms and the potential of such models for automating the programming of complex scientific simulations. Moreover, this technical report can contribute to the development of new tools and techniques for programming numerical algorithms by the means of natural language processing.We start with a simple and basic numerical algorithm. There are various methods for finding the roots of a function. Among them, the secant method [moin2010fundamentals] is an iterative numerical method used to find the root of a function f(x) by approximating the derivative of the function with a finite difference method. The mathematical formulation of the secant method is expressed aswhere x_k and x_k-1 are the last two approximations to the root. x_k+1 is the new approximation to the root. f(x_k) and f(x_k-1) are the function values at x_k and x_k-1, respectively. We ask ChatGPT to write a Python code for the secant method. The gray boxes below show the input prompt and output from ChatGPT.lmss\n",
      "    < g r a p h i c s >\n",
      " Write a Python code for root finding using the secant method.lmss\n",
      "    < g r a p h i c s >\n",
      " Sure! Here is a Python code for root finding using the secant method:Secant1.pylmssHere is an example of how to use this function to find a root of a function:Secant2.pylmssThis will output:Secant3.pylmssNote that the secant method is not guaranteed to converge, so it's important to check for convergence and to set a maximum number of iterations to avoid an infinite loop. In this implementation, if the method fails to converge within the maximum number of iterations, a `ValueError' is raised.As can be seen from this example, the answer provided by ChatGPT is very comprehensive. The comment made by ChatGPT about the convergence of the secant method is valuable.\n",
      "---\n",
      "Models Our evaluation focuses on the models accessible via the OpenAI API: GPT-3 (in the ascending ada, babbage, curie and davinci sizes) and Codex (in the ascending cushman-codex and davinci-codex sizes)[See <Ref> for a discussion on parameter counts.]. These are generative language models which perform next-token prediction during training and inference; GPT-3 is trained on a diverse set of sources from the internet, and Codex is further finetuned on code from GitHub. We compare GPT-3 and Codex against  methods from [shaw-etal-2021-compositional] using the T5 encoder-decoder model. Starting from public checkpoints pretrained on Common Crawl, the T5 model is finetuned on Spider to predict the output SQL, conditioned on the question and schema. The 3B parameter T5 model is currently the state-of-the-art on Spider when combined with constrained inference using the PICARD algorithm [scholak2021picard]. We also compare to BRIDGE v2 [lin2020bridging], a sequence-to-sequence model based on BERT.\n",
      "---\n",
      "We continue by reviewing the literature utilizing reinforcement learning as a guide for language generation. Reinforcement Learning for language generation Policy gradient-based algorithms and their variants have been widely used in text generation to optimize sequence-level metrics [ranzato2015sequence, shen2015minimum, norouzi2016reward, pasunuru2018multi]. Off-policy Reinforcement Learning (RL) is also commonly used in dialogue applications where online interaction with users is expensive [serban2017deep, jaques2019way]. The main difference in our work is that we take advantage of demonstrations and design generic reward functions for generation tasks. We extend this concept to educational contexts by employing reinforcement learning to guide the generation of AI teacher responses in educational dialogues. We focus on optimizing the responses of fine-tuned generative models based on a reward system designed to enhance the pedagogical quality of the generated responses. Recently, Ramamurthy et al. [ramamurthy2022reinforcement] explored the efficacy of using RL to optimize language models in several natural language processing tasks, including text classification, sentiment analysis, and language generation. They developed a library, RL4LMs, which provides a generic framework for deploying RL-based language models for various tasks. We build on top of the RL4LMs framework by adding a new task to its existing array of tasks which we hope can be added as a standard for any future RLHF benchmark.The shared task for BEA 2023 is based on the Teacher-Student Chatroom Corpus (TSCC) [caines2020teacher]. This corpus comprises data collected from 102 chatrooms where English as a Second Language (ESL) teachers interact with students to work on language exercises and assess the students' English language proficiency.From each dialogue in the TSCC, several shorter passages were extracted. Each passage is at most 100 tokens long, consisting of several sequential teacher-student turns (i.e., the preceding dialogue context) and ending with a teacher utterance (i.e., the reference response). These short passages are the data samples used in this shared task.The data samples are formatted using a JSON structure inspired by the ConvoKit [chang2020convokit]. Each training sample is represented as a JSON object with three fields:\n",
      "---\n",
      "Are inconsistencies just due to randomness? Stochastic models can be inconsistent due to randomness alone. However, our tests show inconsistency far beyond the variance in model outputs (even with temperature zero, OpenAI's models exhibit some stochasticity [openai_temp_zero]). To verify this, we run a self-consistency version of our Paraphrasing experiment, where we query the exact same question four times. We find that stochasticity accounts for less than 20% of all the “strong” (ε = 0.2) violations we find.  For details, and additional experiments with temperature 0.5, see <Ref>.Reaching decisions on complex legal cases can be long and costly, and the “correctness” of decisions is often contested (e.g., as evidenced by appeal courts).   ML has been explored both to automate the processing of legal information [chalkidis2020legal, legal_judgment_prediction_survey] and even to reduce human biases in legal decisions [kleinberg2018human].The difficulties in assessing the correctness or fairness of human legal decisions extend to AI tools that are used to assist or automate legal decisions. In this section, we show how to reveal clear logical inconsistencies in two different language models used for predicting legal verdicts: (1) a BERT model that evaluates violations of the European Convention of Human Rights; (2) GPT-3.5-turbo and GPT-4 models prompted to predict bail decisions given a defendant's criminal record.Paraphrasing:   We test whether changing the phrasing of a legal case changes the model's decision.Partial ordering:  While the “correctness” of legal decisions is hard to assess, there can still be clear ways of “ranking” different  outcomes. We consider an extreme example here, where we test whether a bail-decision model could favorably switch its decision if the defendant commits more crimes.\n",
      "---\n",
      "Code generation models can overcome some of these challenges  Despite these struggles, the recent success of large-language models of code [chen2021evaluating,austin2021program,wang2021codet5,xu2022systematic,fried2022incoder] show that the formulation of structured commonsense generation as a text-to-text problem is indeed promising. LLMs of code are able to perform complex reasoning in structured text, if pre-trained on the right data. A key difference in the two setups is the availability of a large pre-training corpus:  while structured commonsense generation models are fine-tuned on relatively small training data for isolated tasks, code generation models leverage a large open-source corpus of code for pre-training. Combined with a large model architecture, code generation models trained in this fashion learn to generate code in various languages and show impressive results on challenging tasks like text-to-code generation. Rephrashing: In other words, instead of forcing LLMs of natural language to be fine-tuned on structured commonsense data, an easier way to close the gap between the pre-training data (free-form text) and the task-specific data (commonsense reasoning) would be to adapt LLMs that were pre-trained on code to structured commonsense reasoning.\n",
      "---\n",
      "[NO \\title GIVEN]\n",
      "    [NO \\author GIVEN]\n",
      "    August 28, 2023\n",
      "======================\n",
      "\n",
      " We solve MIT's Linear Algebra 18.06 course and Columbia University's Computational Linear Algebra COMS3251 courses with perfect accuracy by interactive program synthesis. This surprisingly strong result is achieved by turning the course questions into programming tasks and then running the programs to produce the correct answers. We use OpenAI Codex with zero-shot learning, without providing any examples in the prompts, to synthesize code from questions. We quantify the difference between the original question text and the transformed question text that yields a correct answer. Since all COMS3251 questions are not available online the model is not overfitting. We go beyond just generating code for questions with numerical answers by interactively generating code that also results visually pleasing plots as output. Finally, we automatically generate new questions given a few sample questions which may be used as new course content. This work is a significant step forward in solving quantitative math problems and opens the door for solving many university level STEM courses by machine.Language models have vastly improved in recent years, with the advent of large-scale Transformer models such as GPT-3  [brown2020language] that perform well on question answering tasks. However, when it comes to answering quantitative problems such as word problems in mathematics or deduction from formal logic, these models show poor performance achieving accuracies close to random baselines [hendrycks2020measuring], failing on even the most simple questions such as computing the length of a vector.\n",
      "---\n",
      "According to ETR, the same dynamics of reason underly common sense thinking in both success and failure. This would lead us to expect that if we were to train a system to approximate the dynamics of common sense thinking based on human-produced text, the familiar fallacious judgment patterns should emerge as well. On the ETR view, this would be the case even if the text used as a training basis vastly over-represents cases of correct common-sense judgment. This is relevant since we expect that GPT would have been trained somewhat judiciously to approximate high-quality output and to solve problems with the hope of objective correctness. Yet, on the ETR view, it would not be a a surprise if a better approximation of good common-sense reasoning also yields a better approximation of bad common-sense reasoning. The ETR view of how the successes and failures of reason are linked contrasts with views that might take mistakes to be the result of special purpose heuristics. In addition, insofar as judgment errors are made as a result of approximating patterns of common-sense judgment as envisaged by ETR, we would further expect that prompts to take on board broader questions should reduce the occurrence of fallacious judgments.To investigate these hypotheses, we studied the outputs of GPT with key examples from a recent book-length treatment of ETR as inputs. We then examined whether ETR-inspired prompt engineering can reduce the incidence of certain fallacious judgments that the GPT systems produced.\n",
      "---\n",
      "Concurrent to our work, a small number of other works propose methods focused on CoT student–teacher knowledge distillation. [ho2022reasoningteachers] and [li2022explanations] also explore knowledge distillation with the difference of proposing diverse sampling and rationalization prompting, respectively. In contrast to their work, our work explores more teacher models and demonstrates both the effects of dataset and model size on accuracy. We also achieve a higher accuracy on common datasets, such as GSM8K, than [ho2022reasoningteachers]. In contrast to our work, [shridhar2022distilling] focus on training two models, one for problem decomposition and one for solving. Yet differently, the focus of [eisenstein2022honest] relies on producing markup-and-mask explanations for open-book question answering. Lastly, [huang2022large] present one related experiment, however, we present a more in-depth exploration on more datasets. To the best of our knowledge, our work is the first to extensively explore the improvement of the reasoning ability of small LMs via knowledge distillation across multiple model architectures, and observing the effects of student model size and dataset size on accuracy.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [02:56, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 109, in run\n",
      "    await self.visit(self.parse_tree, VariableStack([self.program._variables], self))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 524, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_assistant.py\", line 13, in assistant\n",
      "    return await role(role_name=\"assistant\", hidden=hidden, _parser_context=_parser_context, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_role.py\", line 17, in role\n",
      "    new_content += await parser.visit(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in visit\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in <listcomp>\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 379, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_gen.py\", line 140, in gen\n",
      "    gen_obj = await parser.llm_session(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 665, in __call__\n",
      "    out = await self.llm.caller(**call_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 370, in _library_call\n",
      "    out = await openai.ChatCompletion.acreate(**kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 728, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 9955 tokens. Please reduce the length of the messages.\n",
      "\n",
      "Error in program:  This model's maximum context length is 8192 tokens. However, your messages resulted in 9955 tokens. Please reduce the length of the messages.\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "How many days after its release did ChatGPT attract over a million users?\n",
      "--------------------------------------------------\n",
      "Answer A ($22.58):\n",
      "\"ChatGPT, am I dying? What does this medical report mean? Can you explain it to me like I'm five?\" With the latest release of OpenAI's Large Language Model (LLM) ChatGPT [chatgpt], algorithmic language modeling has reached a new milestone in generating human-like responses to user text inputs. Just five days after its release, ChatGPT had already attracted over a million users and gained a significant amount of media attention [nyt, wapo, bbc, guardian]. Given the level of popularity and widespread access to the public, the question as to how people will use such models arises and which opportunities and challenges are associated with them. In our experience, the text output of ChatGPT was astonishingly convincing for a variety of tasks, such that we expect disruptive change across numerous domains and industries in a very short time.\n",
      "---\n",
      "We compare our instruction-tuned language models with other models that are instruction-tuned on different datasets. These models are based on either T5-11B [JMLR:v21:20-074_raffel_t5, lester-etal-2021-power] or LLaMA-7B [touvron2023llama]. Additionally, we compare our models with a raw large language model, OPT-30B [zhang2022opt]. Notably, all models have the same or higher number of parameters than our  models. \t \tOPT [zhang2022opt] is an open-sourced decoder-only transformer language model, trained on diverse English corpora. Due to hardware limitations that make long text inference from LLMs time-consuming, we use the 30B variant of OPT. \t \tT0++ [sanh2022multitask] employs a language-model-adapted variant of T5-11B that has been adapted with instruction tuning. The model is finetuned on the PromptSource dataset [bach-etal-2022-promptsource], which consists of over 12 million examples from existing datasets. \t \tTk-Instruct [wang-etal-2022-super] is another instruction-tuned model based on LM-adapted version T5-11B. The training data is a subsample of the NIv2 dataset, which includes 757 tasks and over 75,000 examples. As we are performing zero-shot long text generation via instruction, we use the definition-only version of Tk-Instruct. \t \tFlan-T5 [chung2022scaling] is an instruction-tuned model built on the LM-adapted version of T5-11B. The training data includes a variety of tasks and prompts from multiple sources, including Muffin [wei2022finetuned], T0-SF [sanh2022multitask], and NIv2 [wang-etal-2022-super], comprising more than 14 million examples in total. \t \tAlpaca is an instruction-tuned model built on LLaMA-7B [touvron2023llama] by using a variation of Self-Instruct [wang2022selfinstruct]. As they did not release their finetuned model, we finetune LLaMA-7B with their dataset of Self-Instruct variation and report the results.[<https://github.com/tatsu-lab/stanford_alpaca/>]\n",
      "---\n",
      "The release of GPT-3 in 2020 [GPT3] demonstrated the powerful benefits of training very large auto-regressive language models (LLMs).  GPT-3 had 175 billion parameters, a hundredfold increase over the previous GPT-2 model, and did remarkably well across a wide range of now popular LLM tasks, including reading comprehension, open-ended question answering, and code generation.  This performance has been replicated across several other models [chowdhery2022palm,bloom,opt-zhang]. Furthermore, evidence suggests that large models exhibit emergent behaviors; growth allows them to acquire abilities not present in smaller models [emergent-abilities]. A notable example of emergent behavior is the ability to perform tasks via few-shot prompting, where a model can learn a task from just a few examples. This ability improves well-above random as we increase the size of language models. Broadly speaking, few-shot prompting dramatically expands the range of tasks supported by models and lowers the barrier to entry for users seeking automation for new language tasks.\n",
      "--------------------------------------------------\n",
      "Answer B ($75.12):\n",
      "\"ChatGPT, am I dying? What does this medical report mean? Can you explain it to me like I'm five?\" With the latest release of OpenAI's Large Language Model (LLM) ChatGPT [chatgpt], algorithmic language modeling has reached a new milestone in generating human-like responses to user text inputs. Just five days after its release, ChatGPT had already attracted over a million users and gained a significant amount of media attention [nyt, wapo, bbc, guardian]. Given the level of popularity and widespread access to the public, the question as to how people will use such models arises and which opportunities and challenges are associated with them. In our experience, the text output of ChatGPT was astonishingly convincing for a variety of tasks, such that we expect disruptive change across numerous domains and industries in a very short time.\n",
      "---\n",
      "We compare our instruction-tuned language models with other models that are instruction-tuned on different datasets. These models are based on either T5-11B [JMLR:v21:20-074_raffel_t5, lester-etal-2021-power] or LLaMA-7B [touvron2023llama]. Additionally, we compare our models with a raw large language model, OPT-30B [zhang2022opt]. Notably, all models have the same or higher number of parameters than our  models. \t \tOPT [zhang2022opt] is an open-sourced decoder-only transformer language model, trained on diverse English corpora. Due to hardware limitations that make long text inference from LLMs time-consuming, we use the 30B variant of OPT. \t \tT0++ [sanh2022multitask] employs a language-model-adapted variant of T5-11B that has been adapted with instruction tuning. The model is finetuned on the PromptSource dataset [bach-etal-2022-promptsource], which consists of over 12 million examples from existing datasets. \t \tTk-Instruct [wang-etal-2022-super] is another instruction-tuned model based on LM-adapted version T5-11B. The training data is a subsample of the NIv2 dataset, which includes 757 tasks and over 75,000 examples. As we are performing zero-shot long text generation via instruction, we use the definition-only version of Tk-Instruct. \t \tFlan-T5 [chung2022scaling] is an instruction-tuned model built on the LM-adapted version of T5-11B. The training data includes a variety of tasks and prompts from multiple sources, including Muffin [wei2022finetuned], T0-SF [sanh2022multitask], and NIv2 [wang-etal-2022-super], comprising more than 14 million examples in total. \t \tAlpaca is an instruction-tuned model built on LLaMA-7B [touvron2023llama] by using a variation of Self-Instruct [wang2022selfinstruct]. As they did not release their finetuned model, we finetune LLaMA-7B with their dataset of Self-Instruct variation and report the results.[<https://github.com/tatsu-lab/stanford_alpaca/>]\n",
      "---\n",
      "The release of GPT-3 in 2020 [GPT3] demonstrated the powerful benefits of training very large auto-regressive language models (LLMs).  GPT-3 had 175 billion parameters, a hundredfold increase over the previous GPT-2 model, and did remarkably well across a wide range of now popular LLM tasks, including reading comprehension, open-ended question answering, and code generation.  This performance has been replicated across several other models [chowdhery2022palm,bloom,opt-zhang]. Furthermore, evidence suggests that large models exhibit emergent behaviors; growth allows them to acquire abilities not present in smaller models [emergent-abilities]. A notable example of emergent behavior is the ability to perform tasks via few-shot prompting, where a model can learn a task from just a few examples. This ability improves well-above random as we increase the size of language models. Broadly speaking, few-shot prompting dramatically expands the range of tasks supported by models and lowers the barrier to entry for users seeking automation for new language tasks.\n",
      "---\n",
      "There is a wide spectrum of pre-trained model options for NLP researchers and practitioners these days [devlin2018bert,brown2020language,raffel2019exploring,Radford2019,liu2019roberta,yang2019xlnet, thoppilan2022lamda,fedus2021switch,du2021glam,chowdhery2022palm]. When faced with the question of what model should one use, the answer is often it depends, followed by on what task?Answering this can be overwhelming, comprising of a number of fine-grained follow-up questions like, `encoder-only or encoder-decoder?', `span corruption or language model?'. Pressing further, the answer always seems to depend on the target downstream task. This paper questions and rethinks this thought process, specifically answering the questions of why should the choice of the pre-trained LM depend on the downstream task? and how can we pre-train models that work universally well across many tasks?.This paper proposes a step towards making a universally applicable language model possible. We present a framework for Unifying Language Learning Paradigms or UL2 in short, that is consistently effective across a very diverse set of tasks and setups. Figure <ref> shows an example of how UL2 can perform universally well, unlike other models that often have to make a trade-off.The appeal of a universal model is clear, i.e., as this not only allows concentrated effort in improving and scaling a single model, instead of diversifying resources across N models. Moreover, under resource constrained settings where only a few models can be served (e.g., on device), it would be preferable to have a single pretrained model that can perform well on many types of tasks.\n",
      "---\n",
      "Large-scale Transformer-based language models have achieved great success in many natural language processing tasks [vaswani2017attention,bert]. Among them, large-scale autoregressive models, such as GPT-3 [gpt3], have attracted lots of attention due to their superior performance on zero-shot generalization, i.e., they can perform a wide range of tasks that they are not explicitly trained on. However, pre-training GPT models raises huge challenges on training efficiency and less-discussed training instability issues. On the efficiency side, as the model size continues to grow from a few hundreds of millions (e.g., GPT [gpt]), to billion-scale parameters (1.5B GPT-2 [gpt2]), and to more recent hundreds of billions of parameters (175B GPT-3 [gpt3]), the training cost also increases exponentially: it requires 9.2 days on 512 V100 GPUs to train a 8.3B GPT-2 [megatron], and 47.8 days on 2240 A100 GPUs to train a 530B GPT-3-style model [mt-nlg]. Such a long training time makes it almost infeasible for most research labs and practitioners to reproduce these models. Various solutions have been proposed to reduce the training wall clock time of these large models [megatron,micikevicius2017mixed,rajbhandari2020zero]. However, many solutions require using more GPUs or sophisticated system techniques.This paper was previously titled “Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training” in an early arxiv preprint version[li2021curriculum].\n",
      "---\n",
      "On our CMATH dataset, we conduct evaluation for a variety of popular LLMs, accessible via commercial API or released model weights. We discover that  GPT-4 is the only model that achieves success (accuracy ≥ 60%) across all six elementary school grades. We also examine the robustness of LLMs against the distracting information. It turns out that GPT-4 is again the sole model that maintains robustness, while other models are easily misled by the presence of distracting information.They require understanding natural language questions and translating them into mathematical expressions/equations. This tests a model's linguistic abilities.They require applying mathematical knowledge and reasoning logically through multiple steps. This tests a model's symbolic manipulation and reasoning skills.They often involve commonsense knowledge about the real world. This tests a model's knowledge grounding and understanding.They come in a range of difficulties, allowing us to probe model abilities at different levels of depth. From grade 1 to 6, the problems become increasingly complex and abstract.* How well can state-of-the-art Chinese language models solve elementary school math word problems? What are their limitations? \n",
      "  * What types of reasoning or math abilities do current models struggle with most? Logical, linguistic, commonsense or mathematical? \n",
      "  * How can we develop models with stronger reasoning and problem-solving skills? What abilities are most important to improve?We release CMATH, the first Chinese dataset for evaluating math reasoning abilities of language models. CMATH contains 1,000 elementary school level math word problems with detailed annotations.We conduct the first systematic evaluation of popular Chinese language models on math word problems. We show how CMATH can reveal differences in the math reasoning skills of language models at various ability levels.We find that although elementary school word problems seem simple, solving them requires sophisticated linguistics, logical and commonsense reasoning that current language models do not possess. CMATH provides a challenging testbed for continued progress in these directions.Our work establishes a first framework for evaluating and enhancing the math reasoning skills of Chinese language models. We release CMATH as a public benchmark to support future work in this direction.0.85! \n",
      "    < g r a p h i c s >\n",
      "  Sample problems along with their English translations (not part of the dataset) and human annotations. The column title “#Steps” and “#Digits” stand for “number of reasoning steps” and “number of digits” respectively.This work is motivated by the following question:  To what grade level of elementary school math do the abilities of popular LLMs correspond?  We create the CMATH dataset in order to  answer this question. We believe that the evaluation results of LLMs should be presented in an intuitive manner, making them easily understandable for the general public.\n",
      "---\n",
      "In this work, we tackle the task of financial sentiment analysis on Reddit with an LLM distilled into a production-friendly student model. With minimal human-annotated data, our classifier performs on par with existing supervised models and generalizes well across other datasets. The application of our model does pose a product challenge:  how can we incorporate the model's output responsibly, delivering value to users without misleading them or inadvertently amplifying malicious behavior? Nevertheless, our investigation highlights the promise of in-context learning with LLMs for textual tasks that are hard for human raters to annotate. Can human raters, instead of simply labeling the data, help design a domain-knowledge-injected prompt teaching the LLM to perform the task, or otherwise \"collaborate\" with the LLM? How can automatic prompt-tuning further optimize the human-engineered prompt? Exploring the answers to these questions would be a compelling direction for future work.\n",
      "---\n",
      "We found that English-only finetuning suffices for a multilingual pretrained large language model to generalize to tasks in other pretrained languages. However, finetuning on multiple languages using xP3 provided even better performance. We have further observed finetuned models to be capable of generalization to new tasks in languages they have never intentionally seen. We investigated multilingual prompting and found performance after finetuning on English prompts only to be poor. However, finetuning on a corpus with machine-translated prompts (xP3mt) lead to significantly better performance on human-written non-English prompts. Comparing models from 560 million up to 176 billion parameters revealed that the performance gap between only pretraining and finetuning widens as parameters increase. Lastly, we found multitask finetuning on billions of short targets biases models to produce short answers, which can hurt performance on generative tasks. We proposed a simple workaround by forcing a minimum generation length at inference.To contribute to future progress on improving zero-shot generalization, we release all datasets and models introduced in this work.We highlight several limitations of our work:\n",
      "---\n",
      "Reasoning and decision-making Self-Refine [madaan2023self] employs an iterative framework for self-refinement to autonomously improve generation through self-evaluation. These self-evaluation and self-improvement steps are conditioned on given task constraints, such as \"How can this generation be written in a more positive way\". Self-Refine is effective but is limited to single-generation reasoning tasks. [pryzant2023automatic] performs a similar semantic prompt-writing optimization, but is also limited to single-generation tasks. [paul2023refiner] fine-tune critic models to provide intermediate feedback within trajectories to improve reasoning responses. [xie2023decomposition] use stochastic beam search over actions to perform a more efficient decision-making search strategy which allows the agent to use foresight advantage due to its self-evaluation component. [yoran2023answering] and [nair2023dera] use decider models to reason over several generations. [kim2023language] use a retry pattern over a fixed number of steps without an evaluation step. [goodman2023meta] perform a qualitative evaluation step that proposes optimizations to the previous generation. In this paper, we show that several of these concepts can be enhanced with self-reflection to build a persisting memory of self-reflective experiences which allows an agent to identify its own errors and self-suggest lessons to learn from its mistakes over time.\n",
      "---\n",
      "The emergence of large language models (LLM) has gained much popularity in recent years.  At the time of this writing, some consider OpenAI's GPT 3.5 series models as the state-of-the art [gptAbility].  In particular, a variant tuned for natural dialogue known as ChatGPT [chatGptBlog], released in November 2022 by OpenAI, has gathered much popular interest, gaining over one million users in a single week [cgptUsers].  However, in terms of accuracy, LLMs are known to have performance issues, specifically when reasoning tasks are involved [gptAbility,hoffmann_training_2022].  This issue, combined with the ubiquity of such models has led to work on prompt generation and other aspects of the input [wei_chain–thought_nodate,wang_self-consistency_2022].  Other areas of machine learning, such as meta-learning [hospedales_meta-learning_2022,zhou_domain_2022] and introspection [daftry_introspective_2016,ramanagopal_failing_2018] attempt to predict when a model will succeed or fail for a given input.  An introspective tool, especially for certain tasks, could serve as a front-end to an LLM in a given application.\n",
      "---\n",
      "Generative AI (AIGC, a.k.a AI-generated content) has made headlines with intriguing tools like ChatGPT or DALL-E [ramesh2021zero], suggesting a new era of AI is coming. Under such overwhelming media coverage, the general public are offered many opportunities to have a glimpse of AIGC. However, the content in the media report tends to be biased or sometimes misleading. Moreover, impressed by the powerful capability of ChatGPT, many people are wondering about its limits. Very recently, OpenAI released GPT-4 [OpenAI2023] which demonstrates remarkable performance improvement over the previous variant GPT-3 as well multimodal generation capability like understanding images. Impressed by the powerful capability of GPT-4 powered by AIGC, many are wondering about its limits: can GPT-5 (or other GPT variants) help next-generation ChatGPT unify all AIGC tasks? Therefore, a comprehensive review of generative AI serves as a groundwork to respond to the inevitable trend of AI-powered content creation.  More importantly, our work comes to fill this gap in a timely manner.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [03:10, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "As an AI, I don't have real-time data or the ability to access up-to-date information. As of the last update, specific data about how many days it took for ChatGPT to attract over a million users after its release is not available. For the most accurate information, please refer to the official communications from OpenAI.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: Well, Bobby, both students A and B have provided the same answer to the question. They both state that ChatGPT attracted over a million users five days after its release. \n",
      "\n",
      "Bobby: Yes, Michael, but if we look closely, we can see that student A's answer is a bit more verbose and includes a lot of additional information that isn't directly related to the question. \n",
      "\n",
      "Michael: I agree, Bobby. Student A's answer does go off on a tangent, discussing other models and their parameters, which isn't relevant to the question. \n",
      "\n",
      "Bobby: On the other hand, student B's answer is more concise and to the point. It directly addresses the question and doesn't include any unnecessary information. \n",
      "\n",
      "Michael: Yes, Bobby, I agree. While both students provided the correct answer, student B's answer was more clear and direct. \n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the purpose of model selection in machine learning?\n",
      "--------------------------------------------------\n",
      "Answer A ($24.67):\n",
      "This section presents the methodology used to address the common stages of the machine learning life-cycle referring to our specific STLF case in Portugal. These stages include the following tasks: (i) data collection, wrangling and transformations; (ii) exploratory analysis of the data set; (iii) selection and description of utilized DL architectures; (iv) training and model validation (hyperparameter tuning); (iv) forecast evaluation. Subsequently, the MLR-based framework used for explaining the forecasting performance of the DL models is presented. The experimental process took place using an automated machine learning operations (MLOps) pipeline developed with MLflow [Alla2021], building up to the one described by [Pelekis2022InPerformance].\n",
      "---\n",
      "What do our results imply about LMs' abilities for in-context learning? The process by which LMs adapt to a prompt is debated. For example, [min2022rethinking] make the interesting observation that GPT-3 can perform nearly as well on common tasks like entailment even with random answers in the prompt, and therefore question whether the models are really “learning” in context. Comparisons between our explanations and matched controls allow us to test whether superficial features of the explanations drive their benefits.\n",
      "---\n",
      "<Ref> shows that replacing person and activity has negligible impact on the accuracy. However, abstracting out the sports abstract drops the performance below levels (<Ref>).  What explains such a discrepancy? Note that both person and activity are already present in the question, and are copied in the thought. Thus, replacing them even with abstract tokens does not harm the performance, as the extraction pattern is retained. On the other hand, the sport is not present in the question, and has to be extracted. Consider a question: Is the following sentence plausible? “Norman Powell committed a blocking foul.” For abstracted sports prompts, the model generates Norman Powell is a SPORT1 player. Committing a blocking foul is part of SPORT2. So the answer is no. Whereas for the abstract person or the activity prompt, the model correctly infers the pattern (copy the person or the activity, and generate the corresponding sport) and generates: Norman Powell is a basketball player. Being a blocking foul is part of basketball. So the answer is yes. This example also clearly drives the notion of symbiosis:  only generating the right pattern or only having the right text. Both are required for effective .Cases where patterns help.As empirical results show, combining patterns with language reveals their true potential. However, our analysis uncovers interesting reasons for the superiority of patterns only, for the small number of cases.Interestingly, we found a few cases where forcing the model to generate some text leads to errors.Primarily, they were caused by the model trying to make faulty connections. Detailed examples are located in <Ref>.<Ref> presents detailed results, including ablations on various parts of the prompt. These were omitted from the main text due to space constraints.0.99     0          The presence of text is necessary for the model to arrive at the right conclusions.          While the model is sufficiently capable of forming abstractions, employing concordant entities in questions and thoughts is crucial for the success of .          The performance of the model is proportionally sensitive to the degree of variations in the text.We test this hypothesis to determine the role of text in the performance of .Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar.Note that the practicality of the counterfactual experiments for text is contingent on the nature of the target task.In addition, because there is no text in , we did not conduct counterfactual experiments in this dataset.<Ref> summarizes the results and shows sample thought for each counterfactual prompt.Text with discrepant entities text_diff_entities.We first investigate the role of employing identical entities (names and objects) between a question and its associated thought.To achieve this, we simply modify the entities in the thoughts to be discrepant with the ones in the questions.is exclusively relevant for the and datasets for which concrete entities exist: objects and people for and players and activities for .In the dataset, we change the player name and the activity in the thought, while retaining the sport in the question.Samples of generated outputs in <Ref> illustrate that in this scenario, the model appears to disregard the prompt, generating arbitrary thoughts and attaining a similar performance as .The task solve rate for with is noticeably better than , while still fails to match the performance of .We delineate samples of generated outputs in <Ref> to glean more insights.\n",
      "---\n",
      "Training a WNN entails learning logical functions in its component RAM nodes. Both supervised [10.5555/284803.284804] and unsupervised [unsupervised_wnn] learning techniques have been explored for this purpose. Many training techniques for WNNs directly set values in the RAM nodes. The mutual independence between nodes when LUT entries are changed means that each input in the training set only needs to be presented to the network once. By contrast, most DNN training techniques involve iteratively adjusting weights, and many epochs of training may be needed before a model converges. By leveraging one-shot training techniques, WNNs can be trained up to four orders of magnitude faster than DNNs and other well-known computational intelligence models such as SVM [cluswisard].Algorithmic and hardware improvements, combined with widespread research efforts, drove rapid and substantial increases in DNN accuracies during the 2010s. The ability to rapidly train large networks on powerful GPUs and the availability of big data fueled an AI revolution which is still taking place.\n",
      "---\n",
      "What are the benefits and drawbacks of the datasets we used? Using tasks from [bigbench] allowed us to collect a set of diverse, challenging tasks within a similar format. These tasks cover skills which may not be well-represented in standard datasets. However, these idiosyncratic adversarially-sampled tasks may therefore not be representative of tasks to which LMs are usually applied. This difference could potentially amplify or suppress our effects. Explanations might be uniquely helpful in challenging tasks where the relationship between questions and answers is unusual. Alternatively, explanations might be more effective in more standard settings where they could relate more directly to common reasoning patterns. The breadth of tasks we explored, together with statistical analyses that explicitly model task differences, partially ameliorate these concerns. Nevertheless, future work should explore the benefits of explanations beyond BIG-Bench.\n",
      "---\n",
      "To detect AI generated texts multiple approaches are proposed. Traditionally Sebastian Gehrmann et al. [gehrmann2019gltr] proposed a statistical method to distinguish machine and human generated text. The paper introduces a tool named GLTR. The GLTR tool is a 6-gram character-level statistical language model model, which is trained on a large corpus of text data. The tool uses this model to calculate the probability of each character in the generated text, and then shows any character that has a low probability of occurring in the training corpus. Anton Bakhtin et al. [bakhtin2019real] proposed a Energy-based model (EBM) to discriminate machine generated text. EBM is also a statistical model which finds an energy function from given data. The authors used a comparatively larger dataset collected from human to machine conversation. Eric Mitchell et al.[mitchell2023detectgpt] proposed a zero shot learning method called DetectGPT. This method detects whether text is machine-written or not by calculating log probabilities computed by the model of interest. On text samples generated by the GPT-2, the research team conducted an  study [solaiman2019release]. Atsumu Harada  et al. [harada2021discrimination] gathered two datasets, one with sentences produced by humans, the other with sentences written by both humans and machines. The cosine similarity between sentence pairs was then calculated as a measure of text consistency. Finally, based on the cosine similarity ratings of the sentences, they classified them as either human-written or human and machine-written using machine learning methods. Sandra Mitrovic et al. [mitrovic2023chatgpt] proposed a transformer based model to detect chatgpt generated texts. To determine if a text was produced by ChatGPT or a person, the paper's authors developed a machine learning methodology. The model was trained on a dataset of 10,000 text samples that were classified as either human- or ChatGPT-generated. It is based on a combination of text-based and user-based attributes. Tiziano Fagni et al. [fagni2021tweepfake] proposed a method to detect deepFake tweets. At first tweets were generated by different language models. Then different machine learning methods were used with tf-idf and Bag of Words techniques.Sankar Sadasivan et al. [sadasivan2023can]  evaluated the effectiveness of several existing approaches for detecting AI-generated text including rule-based methods, statistical methods, and machine learning-based methods.They discover that although these techniques can be effective in identifying some sorts of AI-generated text, they are frequently open to adversarial attempts that trick them into thinking the material is human-generated. A lightweight neural network-based paraphraser was developed and applied it to the AI-generated texts.John Kirchenbauer et al. [kirchenbauer2023watermark] introduced a watermarking method. This method add a small amount of noise to the weights of the LLM during training.The noise is made with the intention of encoding a distinct watermark signal that can later be decoded by a watermark detector.The GPT-2 and GPT-3 language models are used to demonstrate the utility of their watermarking technology. The watermark can be found even after fine-tuning the LLM on fresh data and that it is resistant to a variety of attacks, including gradient masking and weight perturbations. In a paper Kalpesh Krishna et al.[krishna2023paraphrasing]  created a substantial amount of AI-generated text samples using a number of cutting-edge language models, such as GPT-3 and T5. The efficiency of several rule-based and machine learning-based text-derived AI detectors is then assessed using the created samples. A retrieval-based defensive method was proposed that depends on determining the text's original author. The suggested technique operates by maintaining a database of known AI-generated text samples and their associated original sources, and by comparing any new text samples against this database to identify probable sources. The authors demonstrate that the suggested retrieval-based defensive mechanism is successful in identifying material that has been paraphrased by AI, with detection a good accuracy. Souradip Chakraborty et al. [chakraborty2023possibilities] proposed multiple possibilities that can detect AI generated texts including some statistical methods and several machine learning algorithms. In most of the papers GPT-2 or previous version of GPT-3 were used. But in our paper we approached with conventional machine learning algorithms but with a dataset that was generated by GPT-3.5 which were more human like.The aim of this research is to differentiate human text from generative model text using machine learning. In Figure <ref>, the high level overview of our process is described. The task initiates by data collection. Section <ref> holds the detailed process of this stage.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "Large Language Models (LLMs) have been a popular topic of research among the academic community [srivastava2022beyond]. The promise of a near-general purpose neural model for a variety of language processing tasks is indeed an attractive one [xu2022systematic]. Deep learning has made significant developments in language tasks such as conversational language understanding [tur2018deep], spoken/text-based dialog systems [celikyilmaz2018deep], and natural language generation from images [he2018deep]. Large Language models can be viewed as the natural progression away from the rigid rule-based systems that we've had since the 1950's [chiticariu2013rule], continuing the empiricist mentality of statistical natural language processing without the potentially costly and context-specific activity of feature engineering [collobert2011natural]. However, with large corporations touting their ever-growing, state-of-the-art models under closed-source code and payment walls, it could be seen that these large language models are becoming less accessible. Some organizations have acknowledged the potential harms that deep learning models could cause by establishing ethical frameworks [ashurst2022ai] [weidinger2022taxonomy], but there are still growing concerns regarding accessibility and the result of false/irreproducible science [kapoor2022leakage].\n",
      "---\n",
      "For this purpose, the GPT-3 models  was selected for all data samples generated for the submission version of the model. After the submission, we additionally performed a comparative test to find out how the model  (better known as ChatGPT) compares to the original choice. While  was released after  and is better tuned to give concise answers in a chat-like manner, it is impossible to declare one generally more performative than the other. The two models have a similar overall performance, but their robustness varies depending on the given task [ye2023comprehensive]. This makes it important to evaluate the differences in the generated outputs and evaluate the respective strengths for this particular use case.\n",
      "--------------------------------------------------\n",
      "Answer B ($99.96):\n",
      "This section presents the methodology used to address the common stages of the machine learning life-cycle referring to our specific STLF case in Portugal. These stages include the following tasks: (i) data collection, wrangling and transformations; (ii) exploratory analysis of the data set; (iii) selection and description of utilized DL architectures; (iv) training and model validation (hyperparameter tuning); (iv) forecast evaluation. Subsequently, the MLR-based framework used for explaining the forecasting performance of the DL models is presented. The experimental process took place using an automated machine learning operations (MLOps) pipeline developed with MLflow [Alla2021], building up to the one described by [Pelekis2022InPerformance].\n",
      "---\n",
      "Why prompt learning? We use pretrained Text-To-Text Transfer Transformer (T5) language models [raffel2020exploring] for both modules. We use prompt-learning because we do not want to distort the pretrained weights too much.  It is well known that pretrained language models have already captured substantial amounts of commonsense knowledge such as hypernymy (A is a type of B) and meronymy (A is part of B) [richardson-sabharwal-2020-qa]; we would like to keep such knowledge to benefit our settings.<ref> gives a detailed explanation for how our inference method works. When D = 0, it is the naive method. When D ≥ 1, it is the inference with explicit planning.  During selection, we constrain the model to only select two premises for a more controllable behavior.  When we compute the proof score we only consider the newly generated deductions for convenience. Its effect to results is negligible since later deductions tend to more directly prove the goal.<ref> is designed to select a set of statements from the current theory T, with the goal of inferring x⃗_0. We fix the size of the selection set to 2 in our experiments, but in principle this restriction can be removed.<ref> applies the standard beam search method to the generation of new deduction. B_ded deductions with top scores are selected.\n",
      "---\n",
      "What do our results imply about LMs' abilities for in-context learning? The process by which LMs adapt to a prompt is debated. For example, [min2022rethinking] make the interesting observation that GPT-3 can perform nearly as well on common tasks like entailment even with random answers in the prompt, and therefore question whether the models are really “learning” in context. Comparisons between our explanations and matched controls allow us to test whether superficial features of the explanations drive their benefits.\n",
      "---\n",
      "<Ref> shows that replacing person and activity has negligible impact on the accuracy. However, abstracting out the sports abstract drops the performance below levels (<Ref>).  What explains such a discrepancy? Note that both person and activity are already present in the question, and are copied in the thought. Thus, replacing them even with abstract tokens does not harm the performance, as the extraction pattern is retained. On the other hand, the sport is not present in the question, and has to be extracted. Consider a question: Is the following sentence plausible? “Norman Powell committed a blocking foul.” For abstracted sports prompts, the model generates Norman Powell is a SPORT1 player. Committing a blocking foul is part of SPORT2. So the answer is no. Whereas for the abstract person or the activity prompt, the model correctly infers the pattern (copy the person or the activity, and generate the corresponding sport) and generates: Norman Powell is a basketball player. Being a blocking foul is part of basketball. So the answer is yes. This example also clearly drives the notion of symbiosis:  only generating the right pattern or only having the right text. Both are required for effective .Cases where patterns help.As empirical results show, combining patterns with language reveals their true potential. However, our analysis uncovers interesting reasons for the superiority of patterns only, for the small number of cases.Interestingly, we found a few cases where forcing the model to generate some text leads to errors.Primarily, they were caused by the model trying to make faulty connections. Detailed examples are located in <Ref>.<Ref> presents detailed results, including ablations on various parts of the prompt. These were omitted from the main text due to space constraints.0.99     0          The presence of text is necessary for the model to arrive at the right conclusions.          While the model is sufficiently capable of forming abstractions, employing concordant entities in questions and thoughts is crucial for the success of .          The performance of the model is proportionally sensitive to the degree of variations in the text.We test this hypothesis to determine the role of text in the performance of .Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar.Note that the practicality of the counterfactual experiments for text is contingent on the nature of the target task.In addition, because there is no text in , we did not conduct counterfactual experiments in this dataset.<Ref> summarizes the results and shows sample thought for each counterfactual prompt.Text with discrepant entities text_diff_entities.We first investigate the role of employing identical entities (names and objects) between a question and its associated thought.To achieve this, we simply modify the entities in the thoughts to be discrepant with the ones in the questions.is exclusively relevant for the and datasets for which concrete entities exist: objects and people for and players and activities for .In the dataset, we change the player name and the activity in the thought, while retaining the sport in the question.Samples of generated outputs in <Ref> illustrate that in this scenario, the model appears to disregard the prompt, generating arbitrary thoughts and attaining a similar performance as .The task solve rate for with is noticeably better than , while still fails to match the performance of .We delineate samples of generated outputs in <Ref> to glean more insights.\n",
      "---\n",
      "What are the benefits and drawbacks of the datasets we used? Using tasks from [bigbench] allowed us to collect a set of diverse, challenging tasks within a similar format. These tasks cover skills which may not be well-represented in standard datasets. However, these idiosyncratic adversarially-sampled tasks may therefore not be representative of tasks to which LMs are usually applied. This difference could potentially amplify or suppress our effects. Explanations might be uniquely helpful in challenging tasks where the relationship between questions and answers is unusual. Alternatively, explanations might be more effective in more standard settings where they could relate more directly to common reasoning patterns. The breadth of tasks we explored, together with statistical analyses that explicitly model task differences, partially ameliorate these concerns. Nevertheless, future work should explore the benefits of explanations beyond BIG-Bench.\n",
      "---\n",
      "To detect AI generated texts multiple approaches are proposed. Traditionally Sebastian Gehrmann et al. [gehrmann2019gltr] proposed a statistical method to distinguish machine and human generated text. The paper introduces a tool named GLTR. The GLTR tool is a 6-gram character-level statistical language model model, which is trained on a large corpus of text data. The tool uses this model to calculate the probability of each character in the generated text, and then shows any character that has a low probability of occurring in the training corpus. Anton Bakhtin et al. [bakhtin2019real] proposed a Energy-based model (EBM) to discriminate machine generated text. EBM is also a statistical model which finds an energy function from given data. The authors used a comparatively larger dataset collected from human to machine conversation. Eric Mitchell et al.[mitchell2023detectgpt] proposed a zero shot learning method called DetectGPT. This method detects whether text is machine-written or not by calculating log probabilities computed by the model of interest. On text samples generated by the GPT-2, the research team conducted an  study [solaiman2019release]. Atsumu Harada  et al. [harada2021discrimination] gathered two datasets, one with sentences produced by humans, the other with sentences written by both humans and machines. The cosine similarity between sentence pairs was then calculated as a measure of text consistency. Finally, based on the cosine similarity ratings of the sentences, they classified them as either human-written or human and machine-written using machine learning methods. Sandra Mitrovic et al. [mitrovic2023chatgpt] proposed a transformer based model to detect chatgpt generated texts. To determine if a text was produced by ChatGPT or a person, the paper's authors developed a machine learning methodology. The model was trained on a dataset of 10,000 text samples that were classified as either human- or ChatGPT-generated. It is based on a combination of text-based and user-based attributes. Tiziano Fagni et al. [fagni2021tweepfake] proposed a method to detect deepFake tweets. At first tweets were generated by different language models. Then different machine learning methods were used with tf-idf and Bag of Words techniques.Sankar Sadasivan et al. [sadasivan2023can]  evaluated the effectiveness of several existing approaches for detecting AI-generated text including rule-based methods, statistical methods, and machine learning-based methods.They discover that although these techniques can be effective in identifying some sorts of AI-generated text, they are frequently open to adversarial attempts that trick them into thinking the material is human-generated. A lightweight neural network-based paraphraser was developed and applied it to the AI-generated texts.John Kirchenbauer et al. [kirchenbauer2023watermark] introduced a watermarking method. This method add a small amount of noise to the weights of the LLM during training.The noise is made with the intention of encoding a distinct watermark signal that can later be decoded by a watermark detector.The GPT-2 and GPT-3 language models are used to demonstrate the utility of their watermarking technology. The watermark can be found even after fine-tuning the LLM on fresh data and that it is resistant to a variety of attacks, including gradient masking and weight perturbations. In a paper Kalpesh Krishna et al.[krishna2023paraphrasing]  created a substantial amount of AI-generated text samples using a number of cutting-edge language models, such as GPT-3 and T5. The efficiency of several rule-based and machine learning-based text-derived AI detectors is then assessed using the created samples. A retrieval-based defensive method was proposed that depends on determining the text's original author. The suggested technique operates by maintaining a database of known AI-generated text samples and their associated original sources, and by comparing any new text samples against this database to identify probable sources. The authors demonstrate that the suggested retrieval-based defensive mechanism is successful in identifying material that has been paraphrased by AI, with detection a good accuracy. Souradip Chakraborty et al. [chakraborty2023possibilities] proposed multiple possibilities that can detect AI generated texts including some statistical methods and several machine learning algorithms. In most of the papers GPT-2 or previous version of GPT-3 were used. But in our paper we approached with conventional machine learning algorithms but with a dataset that was generated by GPT-3.5 which were more human like.The aim of this research is to differentiate human text from generative model text using machine learning. In Figure <ref>, the high level overview of our process is described. The task initiates by data collection. Section <ref> holds the detailed process of this stage.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "One of the most exciting aspects of LLMs is zero, one- or few-shot training. In this line of work, the LLM is not subject to conventional fine-tuning (as is most typical with BERT, T5, RoBERTA, etc [devlin2018bert,raffel2019exploring,liu2019roberta]) using a sizeable number of on-task training examples (typically in the range of 100 -100,000 examples); rather it is given a prefix, comprising just a handful of input-input pairs, and then is prompted with a query input (sans output). In this (highly sample-efficient) regime, LLMs are known to perform surprisingly well.  Most remarkably, few-shot training does not require any weight adjustment whatsoever. Rather, the LLM leverages the information in the first part of the prompt to condition itself to perform the task reflected in the few examples. This works because the massive capacity (billions of parameters!) of the model allows it to condition its generative behaviour on the given prompt in extremely varied, subtle & flexible ways.  An example two-shot training prompt, for the task of English-German translation, might be, for example:   The sentence \"how are you?\" in German is \"wie geht es?\". The sentence \"See you later!\" in German is \"Bis Bald!\". The sentence \"How much is that apple?\" in German issubmit  If prompted with this, when one hits the submit button, GPT3 responds \"Wie viel kostet diese Apfel?\", which is a good translation[Actual output from the GPT3 showcase, obtained from the text-DaVinci-002 model, at <https://beta.openai.com/playground>]. Likewise, LLMs are known to be capable of few-shot learning on a wide range of tasks, including question-answering, natural language inference, summarization, etc.  It should be noted that few-shot learning is very challenging indeed, and the aptitude of LLMs to learn to perform different tasks in this regime is quite phenomenal[See <https://www.nytimes.com/2022/04/15/magazine/ai-language.html>].  Interestingly, few-shot learning has a peculiar and interesting salience for software engineering: for dealing with project-specific linguistic phenomena.\n",
      "---\n",
      "For this purpose, the GPT-3 models  was selected for all data samples generated for the submission version of the model. After the submission, we additionally performed a comparative test to find out how the model  (better known as ChatGPT) compares to the original choice. While  was released after  and is better tuned to give concise answers in a chat-like manner, it is impossible to declare one generally more performative than the other. The two models have a similar overall performance, but their robustness varies depending on the given task [ye2023comprehensive]. This makes it important to evaluate the differences in the generated outputs and evaluate the respective strengths for this particular use case.\n",
      "---\n",
      "Large Language Models (LLMs) have been a popular topic of research among the academic community [srivastava2022beyond]. The promise of a near-general purpose neural model for a variety of language processing tasks is indeed an attractive one [xu2022systematic]. Deep learning has made significant developments in language tasks such as conversational language understanding [tur2018deep], spoken/text-based dialog systems [celikyilmaz2018deep], and natural language generation from images [he2018deep]. Large Language models can be viewed as the natural progression away from the rigid rule-based systems that we've had since the 1950's [chiticariu2013rule], continuing the empiricist mentality of statistical natural language processing without the potentially costly and context-specific activity of feature engineering [collobert2011natural]. However, with large corporations touting their ever-growing, state-of-the-art models under closed-source code and payment walls, it could be seen that these large language models are becoming less accessible. Some organizations have acknowledged the potential harms that deep learning models could cause by establishing ethical frameworks [ashurst2022ai] [weidinger2022taxonomy], but there are still growing concerns regarding accessibility and the result of false/irreproducible science [kapoor2022leakage].\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "We apply to LLaMA-7B and LLaMA-13B [touvron2023llama] and tune it on the Alpaca dataset [alpaca] to compare to Stanford Alpaca, and on the open-source ShareGPT dataset to compare to Vicuna [vicuna2023]. For evaluation, we follow the pipeline in Vicuna [vicuna2023], , we ask questions that span different categories and use GPT-4 [openai2023gpt4] to score the answers provided by the model. Scores are based on the helpfulness, relevance, accuracy, and level of detail of the answers. In Table <ref>, we compare the scores of different models on each category of questions. All the scores are on a scale of 1-10. Overall, has slightly better performance than fine-tuning for each baseline model, while LoRA is less competitive. We observe that is better at providing detailed and informed answers, which makes it a better assistant for questions about general knowledge (Generic, Knowledge, Common). Figure <ref> is an example. Given the question What are the most effective strategies for conflict resolution in the workplace, while the fine-tuned baseline lists several suggestions, is able to provide more detailed explanations for each suggestion. On the other hand, fine-tuning is more helpful when following the instruction for creative questions such as Roleplay and Counterfact. Figure <ref> gives an example of a roleplay question. provides a detailed explanation of what to say to the crews, while the fine-tuned baseline directly creates a short speech which more adheres to the instruction. These observations are consistent with the underlying mechanism of and fine-tuning, , does not modify the pre-trained backbone and thus is able to “remember” all the knowledge learned during pre-training, meanwhile fine-tuning may forget what is learned during pre-training when modifying the weights but is able to better follow the instruction in this way. More examples are shown in Appendix.\n",
      "---\n",
      "Form the results, it is clearly visible that extra tree classifier outperforms all the classifiers with an accuracy of 77%. Moreover, it also has the highest MCC score that is the most informative evaluation matrix compared. The results also demonstrate that some well known classifier such as K-Nearest Neighbor and Decision Tree classifier performs poorly on this dataset. For deep learning based Artificial Neural Network and Long Short-Term Memory, it was trained on 15 epoch. Although it had a high training accuracy, it performed poorly on testing. Some regularization techniques may improve the performance. For further investigation of the result, Figure and <ref> presents roc curve. These results show that the model is not biased to a particular class.In this research, we proposed a model that can differentiate between the text generated from human and ChatGPT. Since generating AI has become very advanced, it is difficult to distinguish between human text and machine generated text. However, in this paper, we have presented a machine learning based approach that can effectively identify two types of text. With the continuing research in this area  we expect to see more sophisticated models for solving this problem that will ensure transparency and accountability in the day to day life.IEEEtran\n",
      "---\n",
      "Tom et al. [brown2020language] introduced GPT-3, the third iteration of the Generative Pretrained Transformer model developed by OpenAI. This paper's primary focus is on the few-shot learning capability of language models. The authors demonstrate that language models start exhibiting remarkable few-shot performance when scaled up, essentially learning from a limited number of examples. Lampinen et al. [lampinen2022can] investigated how AI systems interpret, understand, and apply knowledge from explanations provided in various contexts. Specifically, this is an important contribution to AI, particularly in language understanding and knowledge acquisition by machine learning models.\n",
      "---\n",
      "We discover repetitive attention patterns given trained language models. One interesting question that needs to be answered is whether such behavior is a model architecture bias or an unexpected training outcome. For such purpose, we perform the same experiment with a randomly initialized OPT, and compare it against the results presented in Section <ref>. As shown in Figure <ref>, the repetitive attention pattern does not exist in randomly initialized models. Apart from an efficiency deployment perspective, could such repetitive attention patterns contribute to some known problem in language generation such as repetitions? It may be worth investigating the relationship between repetitive attention patterns and undesired generations.R8.9cm     \n",
      "          \n",
      "    < g r a p h i c s >\n",
      "               \n",
      "          \n",
      "    < g r a p h i c s >\n",
      "                                         We plot the attention map corresponding to Section <ref> but with a randomly initialized OPT. We observe no repetitive attention for a randomly initialized model.Due to the limitation of the server in academics, the largest model we can fit is OPT-66B. We try to understand the behavior and verify the generality across the different models and datasets. However, we cannot access the training process and fail to know exactly how an LLM is trained to exhibit such behavior. Experiments with the large model create carbon dioxide emissions. However, our work improves the efficiency of LLM, and we  foresee no negative impacts.Inspired by our intriguing findings that pivotal tokens exert a lasting influence on future steps, we developed  to leverage this observation to reduce the memory usage of KV cache. Our method achieves memory reductions of 5× in the KV cache without compromising the performance of LLMs. Furthermore, we demonstrate the compatibility of  with quantization techniques, opening up the possibility of reducing memory usage in both the representation and sequence length dimensions.§ APPENDIXThe problem of sampling and quantizing kv-cache can naturally cast as approximating the output of attentions. We can analyse quantization and sampling separately. A note on sampling : Given the nature of sampling, a closely related problem is that of reservoir sampling. However, the problem is slightly different as the weights associated with the weighted summation change with addition of new tokens which opens up new avenues for variance reduction. We start with simple abstract sampling design with E(S)=1.1. Any sampling design that ensures E(S_i) = 1 is a good to get almost unbiased estimates of attention2. Variance reduction of denominator ( also numerator)  requires sampling with probability proportional to weights.  While the problem of weighted reservoir sampling is well understood, the problem of dynamic weights is rather understudied (verify) and often ifeasible.3. We give a reservior sampling for dynamic weights with weight evolution assumptions motivated from the problem at hand. (π(k|j)  π(k|j-1).  The algorithm and proof are straightfroward.4. Proving / intuiton as to why the assumption holds. Either jaspers proof can be modified for this or can be used as a intuition.  Also, we can give empirical evidence.We can thus cover the sampling part under standard (modifed) sampling theory and the sections can be made rigorous with notations borrowed from the sampling literature.Quantisation can be analysed separately. Although it deviates from the central storyIn this section we will talk about reduction of kv-cache for a particular attention head given a budget of tokens say n. The problem can be interpreted as sampling problem with the target to get an accurate estimate of the output of attention head at token, say t+1. The computation in attention can be written as,\n",
      "---\n",
      "Imbalanced data poses a significant challenge in machine learning, often leading to models biased towards overrepresented classes. Synthetic data offers a resolution by generating additional data for the underrepresented classes, thus creating a more balanced dataset that improves learning and facilitates fairer predictions. For instance, in telematics, certain infrequent driving behaviors can be artificially generated, producing a more balanced dataset for model training and leading to advancements in driver safety and vehicle performance. Building upon this notion, not just the creation, but also the augmentation of synthetic data using large language models (LLMs) could emerge as one of the most crucial use cases. When a sample of original data is available, it is feasible to map its distributions onto the synthetic data, which enhances the authenticity and richness of the synthetic dataset. This specific application, which encompasses both creation and augmentation of synthetic data with LLMs, represents a crucial frontier in machine learning. Given its immense potential to reshape our approach to data imbalances, it justifiably necessitates extensive, focused research.While synthetic data provides a multitude of possible benefits, it is not without its inherent challenges. A primary concern is the potential lack of full complexity and variability that characterizes real-world data. Being algorithmically generated, synthetic data might fail to fully encapsulate the intricate details, correlations, and unpredictability found in its real-world counterparts. This shortfall may result in models that perform well with synthetic data but do not generalize effectively to real-world scenarios.To illustrate this point, consider telematics. A synthetic dataset might simulate a range of driving conditions, but it might not account for subtle, influential factors such as driver fatigue, distractions, or even the nuanced effects of billboard advertisements along a route. These omissions could significantly impact the performance of predictive models when applied to real-world data.Another inherent challenge of synthetic data is its potential to propagate the biases embedded in the model used for its generation. If the model creating the synthetic data contains biases, whether from the original training data or algorithmic bias, these biases can be transferred to the synthetic data, leading to skewed results and potentially biased conclusions.For instance, suppose a model generating synthetic telematics data was primarily trained on urban driving conditions. In that case, it might inadvertently over-represent these conditions in the synthetic data, leading to an under-representation of rural or off-road driving scenarios. Such biases could potentially skew the conclusions drawn from the analysis or predictions based on this data, thereby affecting the validity of decisions made using these insights.The exclusive use of synthetic data for model training can lead to overfitting—a situation where a model exhibits high performance on the training data but struggles to generalize to new, unseen data. This issue can arise if the synthetic data does not adequately represent the variability inherent in real-world scenarios.In the domain of telematics, consider a model trained on synthetic data to predict engine failure based on various parameters, such as driving style, vehicle load, and environmental conditions. If the synthetic data does not capture the full spectrum of real-world variations and complexities, the model might become excessively specialized to the synthetic dataset, leading to overfitting. Consequently, the model's predictive performance may decline when applied to real-world dataThe objections raised towards synthetic data, although reasonable, are not insurmountable. With strategic advancements and diligent application, these concerns can be reduced.The challenge of fully capturing the complexity and variability of real-world data can be approached by refining the data generation algorithms and including a wider set of influencing factors and conditions. In the context of telematics, this could mean extending the generation process to factor in additional elements such as driver fatigue or distractions. While this introduces an added layer of complexity, it holds the potential for enhancing the practicality and precision of synthetic data.The potential risk of inherited biases in synthetic data can be mitigated through balanced and careful training of the generative models. This involves incorporating diverse and representative datasets during the training phase to prevent over- or under-representation of specific conditions or classes. Continuous monitoring and correction of any observable bias in the generated synthetic data through iterative improvements in the data generation process is also essential.\n",
      "---\n",
      "While our model demonstrates effectiveness, it is important to acknowledge the risks associated with relying solely on clinical outcome prediction models. There are crucial pieces of information that can be found beyond the scope of clinical notes. Considering the potential impact on patient health outcomes, it is crucial to exercise caution when utilising these clinical LLMs. Therefore, we propose that the PEFT adapter generated by our framework, in conjunction with the pretrained LLM, should be used as an aid rather than a replacement for trained clinical professionals.AcknowledgementsAG was supported by the United Kingdom Research and Innovation (grant EP/S02431X/1), UKRI Centre for Doctoral Training in Biomedical AI at the University of Edinburgh, School of Informatics.PM was partially funded by the European Union’s Horizon 2020 research and innovation programme under grant agreement no. 875160, ELIAI (The Edinburgh Laboratory for Integrated Artificial Intelligence) EPSRC (grant no. EP/W002876/1), an industry grant from Cisco, and a donation from Accenture LLP; and is grateful to NVIDIA for the GPU donations.BA was partially funded by Legal and General PLC as part of the Advanced Care Research Centre and by the Artificial Intelligence and Multimorbidity: Clustering in Individuals, Space and Clinical Context (AIM-CISC) grant NIHR202639.For the purpose of open access, AG has applied a creative commons attribution (CC BY) licence to any author-accepted manuscript version arising.This work was supported by the Edinburgh International Data Facility (EIDF) and the Data-Driven Innovation Programme at the University of Edinburgh.acl_natbibHyperparameter     Value           Learning rate     3e-4           Warmup steps ratio     0.06           Maximum sequence length     128           Gradient accumulation step     4           Batch size     10                Fixed model hyperparameters for language modelling pretraining. These hyperparameters remain unchanged to fit LLaMA into a single GPU.\n",
      "---\n",
      "[raffel2020exploring] introduce the concept of non-causal decoder in the form of a Prefix-LM with favorable performance over causal decoders after fine-tuning on down-stream tasks. The performance in few-shot generative tasks was not evaluated. [wang2022language] conduct an extensive ablation study over architectures and objectives with the conclusion that decoder-only models with causal language modeling exhibit the strongest zero-shot generalization. Therefore, we limit our investigation to causal and non-causal decoders. [tay2022unifying] compare encoder-decoder, decoder-only, and Prefix-LM architectures and report the beneficial performance of encoder-decoder models, while zero-shot generation tasks are not evaluated. The authors later adopt Prefix-LM instead of encoder-decoder in [tay2022transcending].Data MixturesLaMDA [thoppilan2022lamda] was trained on a mixture of various data sources including dialogues, code documents, Q&A data, tutorials, and, Wikipedia. However, the impact of this mixture and the specific sources are unclear. [xie2023data] introduces a data selection method based on importance resampling which allows to mix datasets of various sizes, however, the evaluation only covers encoder-only models.In this Section, requirements for our goals are defined along with relevant components for ablation.We aim to render the training of LLMs for program synthesis more efficient by providing a unification of both learning methods and model architectures, while maintaining (or improving) performance of individual tasks under identical compute budget. The set of tasks is as follows:\n",
      "---\n",
      "One goal of  is to study the comparison of performance across formal systems. In this section we reported the performance of the same methodology (GPT-f [polu2020generative]) applied to both Lean and Metamath. Both models are pre-trained on WebMath [polu2020generative] and respectively trained on datasets extracted from Lean [han2021proof] and Metamath [polu2020generative]. The overall compute deployed at training is comparable in both setup and exactly equivalent at test-time, yet the achieved performance appears drastically superior when applied to Lean. We hypothesize that this is mainly explained by the model's access to high-level tactics when applied to Lean, enabling the model to learn how to guide Lean's automation in an effective way.An example of this high-level guidance behavior is well exemplified by the following proof of the statement  where the model heavily relies on Lean's  solver but provides it with essential premises to successfully guide the search.(The statement above () requires to prove the assertion ∀ a, b ∈ℝ, a^2 + b^2 = 2 → a · b ≤ 1).In Metamath, GPT-f fails to find a proof as it requires a very large number of steps to appropriately rewrite the goal in a way that is amenable to the use of set.mm's existing theorems. The  baseline also fails to find a proof of that statement as  is not capable of solving the goal without being passed extraneous premises.These results motivate the use of neural theorem proving with formal systems that expose powerful high level tactics and also suggest the potential of a closer collaboration between formal systems and machine learning practitioners. It also motivates the use of generative models in that setup as the arguments required by high-level tactics to succeed on non trivial problems generally do not exist in the context of the statement and therefore have to be generated ex-nihilo.\n",
      "---\n",
      "Performance of differnet classifiers          Model     Accuracy     Precision     Recall     F1-Score     MCC    Logistic Regression     0.74     0.73     0.73     0.73     0.48    Support Vector Machines     0.75     0.75     0.71     0.73     0.50    Decision Tree     0.63     0.75     0.79     0.67     0.29    K- Nearest Neighbor     0.69     0.67     0.68     0.67     0.37    Random Forest     0.76     0.73     0.81     0.76     0.53    AdaBoost     0.71     0.68     0.74     0.71     0.43    Bagging Classifier     0.74     0.71     0.75     0.73     0.47    Gradient Boosting     0.71     0.66     0.78     0.72     0.42    Multi-layer Perceptron     0.72     0.73     0.72     0.72     0.43    Long Short-Term Memory     0.73     0.73     0.77     0.75     0.46   Extremely Randomized Trees     0.77     0.74     0.78     0.76     0.54The experiment was carried out on a jupyter notebook and the machine was equipped with a CPU of ryzen 5 5600G. The CPU has an integrated Graphics Processing Unit (GPU) for carrying out deep learning tasks. Moreover, the machine also had 16 GB RAMs. Python was used as programming language along with four libraries named Numpy, Pandas, SKlearn and Tensorflow.Evaluation matrices are used to measure the performance of the model. Different evaluation matrix provides different perspective of the result. In this paper, we have used five matrices namely accuracy, precision, recall,F1 score and Matthews correlation coefficient (MCC). Accuracy calculates the percentage of correctly predicted instances. As shown in Equation <ref>, it is calculated by the total number of correctly predicted samples divided by the total number of samples.While precision calculates the true positive predictions over all positive predictions, recall measures the true positive predictions out of all actual positive instances. The formula of precision and recall are demonstrated in Equation <ref> and Equation <ref> respectively.As shown in Equation <ref>, F1 score is the mean of precision and recall. This provides a more balanced measurement than precision and recall.Finally, the Matthews Correlation Coefficient (MCC) is a more stable matrix that takes all four coefficients into account. This matrix is more significant than all other matrices mentioned above [chicco2020advantages]. The formula for calculating MCC is mentioned in Equation <ref>.We have tested the performance against nine machine learning classifiers along with an MLP and LSTM model with different hyper-parameters. Table <ref> holds the detailed performance analysis of different models. Figure <ref> shows a diagrammatic accuracy comparison of different classifiers.\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "In this work, we present a novel approach CEMAL to use large language models to facilitate knowledge distillation in math word problem solving. Our method first generates an exercise book to evaluate student models and then provides additional training exercises that are customized to the learning needs of the student model, thereby improving the student solver's ability to solve MWP. Our extensive experimental results demonstrate that our proposed approach outperforms all fine-tuned and knowledge distillation baselines on all datasets, while achieving competitive performance against LLMs with significantly fewer parameters. Additionally, we explore different selection generation strategies, revealing that our proposed customized generation strategy is the most effective method, especially in the in-distribution setting. In our future work, we aim to extend this approach to other NLP tasks to evaluate its generalization capability.§ LIMITATIONSDespite the great performance achieved by the student model with the incorporation of our proposed technique, certain limitations remain. Firstly, our approach necessitates meticulous prompt design to generate exercises, which inevitably entails human intervention. This aspect could introduce potential bias or variability and may not scale efficiently.Secondly, we have not explicitly addressed the quality and correctness of the generated problems. Our current filtering process only eliminates problems with incorrect formatting. Thus, there exists a significant opportunity for enhancing the effectiveness of our approach by incorporating mechanisms for evaluating and ensuring the quality and correctness of the generated exercises.Lastly, the current framework relies on a source problem for exercise generation. Future research could explore the feasibility of generating exercises without direct reference problems, potentially utilizing only abstract knowledge components or keywords. Such an exploration could lead to better flexibility and robustness in the generation process.acl_natbib2|good_colorc|Good Generated Problem Cases\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [03:11,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 109, in run\n",
      "    await self.visit(self.parse_tree, VariableStack([self.program._variables], self))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 524, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_assistant.py\", line 13, in assistant\n",
      "    return await role(role_name=\"assistant\", hidden=hidden, _parser_context=_parser_context, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_role.py\", line 17, in role\n",
      "    new_content += await parser.visit(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in visit\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in <listcomp>\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 379, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_gen.py\", line 140, in gen\n",
      "    gen_obj = await parser.llm_session(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 665, in __call__\n",
      "    out = await self.llm.caller(**call_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 370, in _library_call\n",
      "    out = await openai.ChatCompletion.acreate(**kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 728, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 12934 tokens. Please reduce the length of the messages.\n",
      "\n",
      "Error in program:  This model's maximum context length is 8192 tokens. However, your messages resulted in 12934 tokens. Please reduce the length of the messages.\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "How do the performances of different language models compare on benchmarks that measure toxic language harms?\n",
      "--------------------------------------------------\n",
      "Answer A ($21.00):\n",
      "Instruction finetuning improves usability and mitigates some potential harms. Using a pretrained checkpoint directly can be challenging for non-experts because a model trained on the next token prediction objective alone does not know when to stop generating, and can make mistakes such as continuing the user's input rather than responding to it. In <ref>, we saw that on a set of open-ended evaluations, outputs from  had substantially better human ratings compared to outputs from , especially for CoT tasks like complex reasoning, planning, and explanation.  outperforms PaLM on several Responsible AI benchmarks, particularly benchmarks measuring toxic language harms.  These results are consistent with findings from InstructGPT [ouyang2022training], which showed that finetuned models produce outputs that are better aligned with human preferences. The zero-shot usability of models is important for wider adoption of language models that do not require prompt engineering or require few-shot exemplars. A model card [mitchell2019model] is included in the appendix.\n",
      "---\n",
      "We set out to answer three research questions:      \n",
      "  * How does the deductive reasoning ability (DRA) of large language models evolve with their growing size?     \n",
      "  * How does the DRA of general-purpose generative large language models depend on the number of deductive steps required to reach the conclusions?     \n",
      "  * Do the specifics of a training setup, such as the model variant, dataset multilingualism, or dataset size have a decisive influence on the DRA?  We measure DRA by binary accuracy on the answers of the model; in other words, by the proportion of the instances in which the model answered the question about the theory given as context correctly. A detailed account of our experimental setup is presented in <Ref>.\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "--------------------------------------------------\n",
      "Answer B ($53.46):\n",
      "Instruction finetuning improves usability and mitigates some potential harms. Using a pretrained checkpoint directly can be challenging for non-experts because a model trained on the next token prediction objective alone does not know when to stop generating, and can make mistakes such as continuing the user's input rather than responding to it. In <ref>, we saw that on a set of open-ended evaluations, outputs from  had substantially better human ratings compared to outputs from , especially for CoT tasks like complex reasoning, planning, and explanation.  outperforms PaLM on several Responsible AI benchmarks, particularly benchmarks measuring toxic language harms.  These results are consistent with findings from InstructGPT [ouyang2022training], which showed that finetuned models produce outputs that are better aligned with human preferences. The zero-shot usability of models is important for wider adoption of language models that do not require prompt engineering or require few-shot exemplars. A model card [mitchell2019model] is included in the appendix.\n",
      "---\n",
      "We set out to answer three research questions:      \n",
      "  * How does the deductive reasoning ability (DRA) of large language models evolve with their growing size?     \n",
      "  * How does the DRA of general-purpose generative large language models depend on the number of deductive steps required to reach the conclusions?     \n",
      "  * Do the specifics of a training setup, such as the model variant, dataset multilingualism, or dataset size have a decisive influence on the DRA?  We measure DRA by binary accuracy on the answers of the model; in other words, by the proportion of the instances in which the model answered the question about the theory given as context correctly. A detailed account of our experimental setup is presented in <Ref>.\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "On our CMATH dataset, we conduct evaluation for a variety of popular LLMs, accessible via commercial API or released model weights. We discover that  GPT-4 is the only model that achieves success (accuracy ≥ 60%) across all six elementary school grades. We also examine the robustness of LLMs against the distracting information. It turns out that GPT-4 is again the sole model that maintains robustness, while other models are easily misled by the presence of distracting information.They require understanding natural language questions and translating them into mathematical expressions/equations. This tests a model's linguistic abilities.They require applying mathematical knowledge and reasoning logically through multiple steps. This tests a model's symbolic manipulation and reasoning skills.They often involve commonsense knowledge about the real world. This tests a model's knowledge grounding and understanding.They come in a range of difficulties, allowing us to probe model abilities at different levels of depth. From grade 1 to 6, the problems become increasingly complex and abstract.* How well can state-of-the-art Chinese language models solve elementary school math word problems? What are their limitations? \n",
      "  * What types of reasoning or math abilities do current models struggle with most? Logical, linguistic, commonsense or mathematical? \n",
      "  * How can we develop models with stronger reasoning and problem-solving skills? What abilities are most important to improve?We release CMATH, the first Chinese dataset for evaluating math reasoning abilities of language models. CMATH contains 1,000 elementary school level math word problems with detailed annotations.We conduct the first systematic evaluation of popular Chinese language models on math word problems. We show how CMATH can reveal differences in the math reasoning skills of language models at various ability levels.We find that although elementary school word problems seem simple, solving them requires sophisticated linguistics, logical and commonsense reasoning that current language models do not possess. CMATH provides a challenging testbed for continued progress in these directions.Our work establishes a first framework for evaluating and enhancing the math reasoning skills of Chinese language models. We release CMATH as a public benchmark to support future work in this direction.0.85! \n",
      "    < g r a p h i c s >\n",
      "  Sample problems along with their English translations (not part of the dataset) and human annotations. The column title “#Steps” and “#Digits” stand for “number of reasoning steps” and “number of digits” respectively.This work is motivated by the following question:  To what grade level of elementary school math do the abilities of popular LLMs correspond?  We create the CMATH dataset in order to  answer this question. We believe that the evaluation results of LLMs should be presented in an intuitive manner, making them easily understandable for the general public.\n",
      "---\n",
      "Q1: How does the deductive reasoning ability (DRA) of large language models evolve with their growing size? We find that with the training setup and model architecture kept constant, larger model size leads to higher DRA, but also that this effect can often be compensated by choices made in the training configuration. Previous work has shown that the DRA of models fine-tuned for deduction decreases on data that requires longer chains of reasoning that have been seen in training. <Ref> shows that this does not seem to be the case for foundation models as we evaluated them, that the performance of many models on deeper questions is similar to that on shallower questions, and that it sometimes even increases, especially for OpenAI GPT-1/2 models.Q2: How does the DRA of general-purpose generative large language models depend the number of deductive steps required to reach the conclusions? For depths considered in our study, it remains largely similar throughout, except for the OpenAI GPT-3 and 3.5 models, for whom the performance decreases as seen in <Ref>.\n",
      "---\n",
      "Bias and Fairness. Language models producing toxic or biased content can cause severe harm especially to the groups being biased against [Bender2021OnTD]. A series of benchmarks have been developed to show that LLMs can generate toxic outputs [Gehman2020RealToxicityPromptsEN], contain gender biases [Rudinger2018GenderBI,Zhao2018GenderBI] and other categories of social biases [Nangia2020CrowSPairsAC,Nadeem2021StereoSetMS,BBQ], perform poorly against minority demographic groups [Koh2021WILDSAB,Harris2022ExploringTR] or dialectical variations [Ziems2022VALUEUD,Tan2020ItsMT]. Ideally, LLMs should not exhibit biased behaviors and not discriminate against any group. While many of these evaluations focus on evaluating the internal representation of LLMs in a zero-shot setting or evaluating the biases on specific downstream applications in a supervised setting, it remains unclear how these biases change under different prompting schemes in the few-shot setting, which will be the focus of our analysis. A closely related work is [Lucy2021GenderAR] which study representation biases in GPT-3 generated stories. We instead evaluate on the downstream tasks of coreferece resolution and question answering. Apart from few-shot prompting, [Solaiman2021ProcessFA] proposed a general method to align language models with human values, but it involves expensive iterative training.\n",
      "---\n",
      "Remarkably, by simply refocusing attention, achieves state-of-the-art results on various transfer learning benchmarks. Compared to fully fine-tuning, LoRA, and VPT, significantly improves the performances on FGVC fine-grained classification (, 5% improvement over fully fine-tuning on average accuracy), and obtains the best performance on 11 out of 18 tasks on VTAB benchmark [zhai2019large]. Beyond visual recognition, can adapt large language models such as LLaMA-7B [touvron2023llama] for instruction-following language generation, resulting in more detailed and informed answers and outperforming fully fine-tuned Alpaca [alpaca]. We also explore , a more parameter-efficient version of that tunes a similar number of parameters as LoRA while reaching higher performances. These observations strengthen our idea that refocusing attention is key to transfer learning and sheds light on future exploration in the field.\n",
      "---\n",
      "We conduct a set of transfer learning experiments to understand the degree of overlap in terms of necessary knowledge for solving problems in our dataset and existing datasets. In particular, we first fine-tune the RoBERTa model on a source dataset, before fine-tuning the model on . If the required knowledge is similar, the model performance is expected to increase. RACE and COSMOS are adopted as the source datasets. The former tests English reading skills while the latter tests commonsense knowledge. As shown in Table <ref>, the RoBERTa model trained only on either source dataset gives significantly lower accuracies on  test set compared with the RoBERTa model trained on . The performance of RoBERTa trained on RACE is even close to the random guess baseline. In addition, further fine-tuning on  leads to improvements over the source-trained baselines, but the resulting models do not outperform a model trained only on . The observation is different from most other datasets [cosmos,Social-IQa], which demonstrates that  contains highly different challenges compared with existing datasets.We measure the accuracy of RoBERTa against the input size. In particular, the number of words in the paragraph, the question and the candidate answers are added together as the length of a test instance. The statistics and performances are all shown in Table <ref>. Interestingly, the model performances are not negatively associated with the input size, which is different from most NLP benchmarks. This shows that the level of challenge in logical reasoning can be independent of the input verbosity.We aim to understand a bias of models in selecting the candidate answers that have the best surface matching with the paragraph. To this end, we calculate the unigram overlap between each candidate answer and the given paragraph for each problem, and mark the best-matching candidate.\n",
      "---\n",
      "Large language models of code (Code LLMs) power coding assistants that are rapidly reshaping how programmers write code. Researchers have studied their impact on programmer productivity [vaithilingamExpectation2022,ziegler2022productivity,barkeHuman2022], identified real concerns about potential harms [dakhelGithub2022,Mozannar2022ReadingBT,sandoval2023lost,pearce2021asleep,aghakhani2023trojanpuzzle], and considered how they could help students learn [leinonenHuman2022,finnieRobots2022,jayagopal2022exploring]. Fundamental to these studies, and to tool adoption, is the assurance that the underlying models work effectively and consistently.Code LLMs are commonly evaluated using benchmark suites that cover a wide variety of problems. Popular benchmarks such as HumanEval [chen2021evaluating] and MBPP [austin2021program] consist of many problems from varying areas of computing, accompanied by a single expert-written prompt. Achieving good performance on these benchmarks indicates that a model will perform well across many programming tasks, assuming that the user can write prompts equally as well as the expert.\n",
      "---\n",
      "Beyond the NLP benchmarks, language models are also capable of generating long-form answers to open-ended requests. Standard NLP benchmarks and the automatic metrics used to evaluate them are not sufficient to measure human preferences among these open-form responses [ouyang2022training]. Hence, we conduct a manual evaluation that investigates the effect of instruction finetuning on the ability for models to give open-ended responses to challenging inputs. To do this, we created an evaluation set of 190 examples. This evaluation set includes questions posed in a zero-shot manner to the model across five challenging categories of 20 questions each: creativity, reasoning over contexts, complex reasoning, planning, and explanation. For 60 of these examples (from the complex reasoning, planning, and explanation categories) we create a variant with a chain-of-thought trigger phrase (e.g., “let's think step-by-step”), as another evaluation of whether finetuning on CoT enables zero-shot, which was quantitatively evaluated in <ref>. In addition to the above 160 zero-shot inputs, we include 30 inputs testing few-shot capabilities, which strong language models without instruction finetuning have been shown to do well on [chowdhery2022palm].\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [03:39, 15.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "As an AI, I don't have real-time data or the ability to conduct research. However, I can tell you that the performance of different language models on benchmarks that measure toxic language harms can vary significantly. This is due to differences in their training data, architecture, and the strategies used to mitigate such harms.\n",
      "\n",
      "For instance, models trained on larger, more diverse datasets may be better at identifying a wider range of toxic language. However, they may also be more prone to generating such language themselves if not properly controlled. On the other hand, models trained on smaller, more specific datasets may be less likely to generate toxic language, but they may also be less effective at identifying it.\n",
      "\n",
      "Moreover, some models may use strategies like toxicity filters or reinforcement learning from human feedback to reduce toxic language harms. These strategies can be effective, but they also have limitations and can sometimes lead to false positives or negatives.\n",
      "\n",
      "In general, it's important to note that no model is perfect and all models have room for improvement when it comes to mitigating toxic language harms. Ongoing research and development is crucial to improve their performance on these benchmarks.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: Looking at the answers from both students, it's clear that neither of them directly addressed the question about comparing the performances of different language models on benchmarks that measure toxic language harms.\n",
      "\n",
      "Bobby: I agree, Michael. However, if we look closely, student A's answer does mention the topic of toxic language harms in the context of Responsible AI benchmarks. They also mention the use of finetuning to mitigate potential harms, which is relevant to the question.\n",
      "\n",
      "Michael: That's true, Bobby. But the answer from student A also includes a lot of irrelevant information. They go off on tangents about deductive reasoning ability, language modeling for code, and other topics that have nothing to do with the question.\n",
      "\n",
      "Bobby: I see your point, Michael. But if we look at student B's answer, it's essentially a copy of student A's answer. They didn't add anything new or relevant to the discussion.\n",
      "\n",
      "Michael: You're right, Bobby. Both answers are far from perfect, but at least student A made an attempt to address the question, even if they didn't do it very well. Student B, on the other hand, simply copied student A's answer without adding anything of value.\n",
      "\n",
      "Bobby: I agree, Michael. Given the circumstances, I think we have to give the edge to student A.\n",
      "\n",
      "VERDICT: Better answer from student A.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the significance of the date November 30th, 2022, in the development of language models?\n",
      "--------------------------------------------------\n",
      "Answer A ($23.23):\n",
      "Speech language models built upon these speech tokens have exhibited remarkable capabilities. Generative Spoken Language Model (GSLM) [lakhotia2021generative], prosody-aware Generative Spoken Language Model (pGSLM) [DBLP:conf/acl/KharitonovLPACL22], and Unit mBart [popuri2022enhanced] have emerged as notable speech language models in the field, signaling the imminent development of advanced speech language models. GSLM specializes in performing language modeling on discrete units that encode phonetic information extracted by self-supervised speech models such as CPC, wav2vec2, and HuBERT.  Building upon GSSM, pGSLM enhances the model's capabilities by incorporating prosody information spoken language modeling.  Additionally, Unit mBART extends the scope of speech language modeling to encompass multi-lingual scenarios, facilitating the generation of speech across different languages. It is worthy noting that these models are pre-trained on speech tokens without any text supervision.In addition to the “textless\" speech language models, there exist models that incorporate text to enhance the generation expressiveness. Models like VALL-E and Spear-TTS perform text-to-speech (TTS) in a generative language modeling manner based on neural codes, such as soundstream and Encodec.In this paper, our primary focus lies on speech language models that conduct pre-training without text supervision. The reason for this emphasis stems from that textless speech generation is of great significance, given the challenges posed by acquiring labeled text-speech pairs and the absence of text in certain languages.\n",
      "---\n",
      "*           We experiment with  [cobbe2021training] (1319 samples).          The dataset contains math word problems geared toward an average middle-school curriculum.          \n",
      "  *           We use date understanding (, 349 samples) and sports understanding (, 980 samples) as representative tasks for commonsense reasoning, both derived from [date:sport].          \n",
      "  *           We experiment with sorting (, 500 samples) a list of single-digit integers.           We do not associate explicit instruction (sort these numbers) with the questions.          Instead, we frame the questions as a challenging setup in which the model should figure out the task and the requisite information to solve it.In this work, we neither train any of the \\palm models, nor performs finetuning.We solely perform inference on \\palm variants using TPU v4~[tpuv4,jouppi2017datacenter].For \\mpalm, we use 4$\\times$4$\\times$4 TPU v4 configuration, whereas, for \\lpalm we use 4$\\times$4$\\times$16 mesh configuration.To account for the variation in results caused by the order of examples in the prompt, we conduct each experiment three times, each with different seeds, and report the average task solve rate.Following [wei2022chain], we evaluate each task using accuracy \\ie fraction of examples where the output matched the expected result.\\niparagraph{Public large language models.}We use OpenAI API\\footnote{\\href{https://openai.com/blog/openai-api/}{https://openai.com/blog/openai-api/}, v0.23.0} to conduct experiments with \\gptt (\\bench{text-davinci-002}) and \\codex (\\bench{code-davinci-002}).\\input{tables/task_examples}Based on our initial findings, this section explores the idea of engineering effective prompts tailored to remove ineffectual tokens.The benefit of such tailored design is multi-fold: (1) “reducing noise”:reducing noise from prompts could potentially lessen the confusion for the model and lead to a higher task solve rate and(2) “potential energy savings”:the reduced number of tokens in the prompts instruct the model to generate less number of tokens per question.Less number of generated tokens directly translates to reduced runtime and carbon footprint per inference [strubell2019energy].This is especially crucial because of the significant compute cost of large language models.Our proposed prompts, dubbed as , distill the essence of our hypothesis by supplying minimal requisite information to drive the model to factual conclusions.<Ref> compares and across different tasks and four variants of s.In all tasks, except , we use identical examples in the prompt and only rephrased the thoughts to use less number of tokens.For , we could not find a systematic mechanism to shorten the thoughts.Instead, we randomly harvest questions from the training set whose thoughts are shorter than .Overall, outperforms while employing prompts with fewer tokens.The task solve rate of remains relatively high as we scale the model to , highlighting the efficiency of .Appendix-<Ref> compares the average number of input/output tokens between and .On average, (our approach) reduces the number of input (1.39×) and output tokens (1.58×).This work evaluates the capacity of to elevate complex reasoning in three state-of-the-arts s, , , and .We systematically assembled a series of controlled counterfactual experiments.Our results show the initial inklings of connection between text, patterns, and reasoning in s.Our study indicates that the symbiosis of text and patterns bears more weight in the chain of thought reasoning process.In addition, we assert that text is a channel to extract semantic patterns, unlocking the ability of these models to mold correct answers.Large language models.To facilitate conducting an exhaustive number of experiments, we center the main analysis of this paper around [The results in [wei2022chain] reinforce that is the smallest model with significant gains across a range of reasoning tasks.].For reproducibility, we also conduct the experiments on publicly available models such as and .We present results from in the main body and defer the results from other models to <Ref>.Nonetheless, our findings concur across the studied s.Reasoning tasks.We focus on reasoning tasks for which presents ample improvements over prompting [wei2022chain], namely  ( [cobbe2021training]),  (date and sports understanding [date:sport]), and  ()—details in <Ref> and Appendix-<Ref>.This work intends to tease apart the major semantic components of a prompt that play a critical role in the efficacy of .To achieve this, we identify and systematically construe three key semantic components of a prompt, listed as follows (See Appendix-<Ref> for the examples to which we refer in the definitions):* are sequences of tokens in the prompt, about which the model reasons to solve a task.For and , symbols are numerical quantities (5, 4, 2, 13).Similarly for the dataset, we categorize players and activities as symbols. We define the symbols in the dataset as date and time indicating expressions.* are either composition of symbols and operators or a structure of prompt that reinforces task understanding.The isolation of patterns within a prompt is evident in datasets like (equations), (person is a sport, activity is a sport), and (1 less than 2).However, for the dataset, the pattern is semantically implicit but consistent.Each thought contains two parts: (a) ⟨calculation⟩ in which the information from the input is restated (“Today is 06/02/1943”) and intermediate results are generated (“One day after 06/01/1943 is 06/02/1943”) through mathematical calculations and (b) ⟨output⟩ in which the final requisite answer is generated using the intermediate results (“10 days before today is 05/23/1943”).* are tokens that are neither symbols, nor part of patterns.Specifically, text in prompts assists in eitheroutlining the target task (is the sentence plausible),connecting patterns to symbols (John is left with 4 - 2 = 2), or contextualizing symbols (4 toys).In a nutshell, text is the conceptual glue that binds different parts of a prompt.Relying on our analysis, we conjecture and hypothesize about the effects of each semantic components on the outcomes of prompting.We discuss detailed results, including statistical significance tests in <Ref>.The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence [lake2017building].This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data.Recently, large language models (s) [palm:arxiv,brown2020language,codex] have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning.Few-shot learning has shown promising applications for a wide range of tasks [gem,flan,bigscience,lamda,liu_what_2021,reif2021recipe,wang2020generalizing,chen_evaluating_2021,minerva,wu2022autoformalization].While beneficial, this setting requires meticulous design of prompts [le2021many,liu2021pre,mishra_reframing_2021].[ling2017program] pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning.Recently, [wei2022chain] proposed chain of thought () prompting, showing that the few-shot setting in s similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks [ling2017program,cobbe2021training,svamp,date:sport].Despite its wide-range usage, the rationale behind the success of remains unclear.Recent work draws [ling2017program,wei2022chain] parallels to human thinking.Humans often think about a problem before deducing a solution.Akin to this process, it is argued that models should also be able to employ a similar mechanism.While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate.Ultimately, s are trained to estimate the next token distribution for a given context.Therefore, there is presumably a systematic rationale behind their successes and failures.In this work, we undertake initial steps towards understanding the mechanism behind .Contributions and findings.We construct a series of tailored counterfactual prompts [counterfactual], deliberately sketched as controlled studies.First, we identify key components of an example in few-shot prompting as follows: Symbols, Patterns, and Text.Next, we perform counterfactual prompting—keeping all but one component fixed (replacing symbols (numbers) with Greek alphabets).Finally, we elicit meaningful findings via conducting a systematic and qualitative analysis of the performance divergence between different prompt queries.Our experiments on four diverse reasoning tasks and across three large language models—, , and , reveal several surprising findings:1 We find that the exact type of symbols in the prompt virtually does not affect the model performance.In addition, our results and analysis demonstrate counterintuitive phenomena.For example, we identify that the correctness of symbols and patterns is immaterial to the task solve rate.2We learn that patterns contribute chiefly as a venue to reinforce task understanding [ouyang2022training] and prompt the model to attain correct outputs.3Most importantly, we find that text and patterns form a symbiotic relationship that plays a vital role in the success of .Text helps generate useful patterns (by extracting commonsense knowledge), and patterns help reinforce task understanding, enabling the language model to generate text that helps solve the task.Overall, we argue that one of the primary reasons behind the success of is this interplay between text and patterns—helps a language model in imitating the prompt and generating the right tokens for the task—and is conceivably less related to their reasoning abilities.Finally, as indicated by applications such as -Saycan [ahn2022can], we posit that techniques like will play a key role in enabling the success of s on diverse use cases.Thus, designing efficient prompts informed by a set of key design principles is an important challenge.To this end, we distill our findings to create concise prompting, dubbed .prunes the prompt (20%↓) to only retain indispensable tokens without negative repercussions on the task solve rate.§ ETHICS STATEMENTDisseminating reasoning into machines has numerous benefits and applications, from algorithmic reasoning [alphacode] to code generation [chen_evaluating_2021,poesia2021synchromesh] and formal verification [wu2022autoformalization].While this research does not directly enhance the reasoning capabilities of large language models, it identifies several systematic behavioral patterns in the functioning of few-shot models.Similar to any technological advances, this work has risks of detrimental societal impact.However, anticipating potential future downsides of such methods is challenging.More than ever, the research community's utmost responsibility is to acknowledge these risks candidly and reflect on practices and strategies to prevent potential harm.Environmental impact.Training large language models devour a nontrivial amount of compute resources, a limiting factor for frequent training.Few-shot prompting is an appealing solution for mitigating the unfavorable environmental impact of large language models by evading additional iterations of training and dataset collection.Ours and similar studies may lead to more effective prompting techniques and bring technological innovation to the architecture of large language models, especially regarding their reasoning capabilities.Therefore, we hope that the significant compute used in this work can help promote positive environmental outcomes.Finally, aligned with the credible concerns of the research community, we recognize the longer-term risk of Excellence in artificial intelligence, primarily when it boils down to human reasoning.While distilling comparable human reasoning to machines offers many benefits, undisciplined and uncontrolled progress in this area could be alarming, especially in the presence of bad actors.Effective reasoning in machines as a result of our study, even though not directly, can lead to algorithmic advances that may facilitate bad actors in developing malicious software and systems with human-level capabilities.We also want to acknowledge a large body of researchers that has greeted innovations in large language models and steady scaling of models with skepticism, questioning the connection between human reasoning and large language models [han2022human,cog:gpt3].Additionally, recent efforts have started exploring the relationship between the structure of contemporary language models and the human mind [schrimpf2021neural,tang2021sensory,whittington2021relating].Despite their progress, the rationale behind 's mimicking human reasoning, and any potential connection with linguistics is so far an uncharted territory.The primary objective of our study is to understand through counterfactual prompting and empirically establish the underpinnings of the reasoning ability of s in the presence of .Each counterfactual prompt C_f(p) alters only one particular aspect of the in-context examples ⟨ x_k · t_k · y_k⟩ in a .For example, consider a sample thought for the math world problems in the dataset (See <Ref>).A symbolic counterfactual prompt, C_symbolic(p), may simply replace all the numbers in the thoughts with symbols (X1).Such analysis enables us to ask: “what would the performance of the model have been, if all the numbers in the prompt were replaced with symbols?”.Analyzing the performance disparity of a on C_symbolic(p) vs. p can thus indicate the role that using actual numbers plays in the success or failure of a task[Appendix-Table <ref> outlines a set of major “what if” questions that we study in this work.].The ability to successfully complete prompts p for complex reasoning tasks is typically present for at the scale of , , and .Nonetheless, we do not make any assumptions about the underlying model architecture.In summary, our study on reveals that for solving math problems, neither the presence of numbers, nor the credibility of the thoughts is paramount to the success of .Similarly, altering the style and wording of the texts in the thoughts has a modest impact on the model performance.Nevertheless, eradicating either of these components nullifies the efficacy of .Finally, a per-layer analysis of the model reveals that if the model performs similarly for a pair of counterfactual prompts C_f(p) vs. p, then the attention patterns are comparable as well.We defer the detailed background to Appendix <ref>.Limitations of counterfactual prompting.Relying on counterfactual examples could be misleading and precarious [laugel2019dangers,slack2021counterfactual].Nonetheless, counterfactual explanation presents a channel to gain insights into the workings of the model.This approach potentially yields more favorable explanations for state-of-the-art s.Notably, unlike fine-tuned methods, one can readily identify and collect a set of prompts that are critical for the model to generate particular outputs.In particular, few-shot prompting augments the model with an additional dimension to calibrate the accuracy to a discernible degree.Thus, we deduce that the counterfactual examples that exhibit consistent and systematic performance divergence are more prone to reflect credible interpretations of the model.In this work, we neither rely on the results that do not exhibit such characteristics, nor reject prompts that pose contradictory observations. We discuss additional limitations in <Ref>.Broadly, this paper intersects with a growing body of work on prompting and large language model reasoning [brown2020language,palm:arxiv].Below, we review the most relevant work in these directions.\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "--------------------------------------------------\n",
      "Answer B ($79.25):\n",
      "Speech language models built upon these speech tokens have exhibited remarkable capabilities. Generative Spoken Language Model (GSLM) [lakhotia2021generative], prosody-aware Generative Spoken Language Model (pGSLM) [DBLP:conf/acl/KharitonovLPACL22], and Unit mBart [popuri2022enhanced] have emerged as notable speech language models in the field, signaling the imminent development of advanced speech language models. GSLM specializes in performing language modeling on discrete units that encode phonetic information extracted by self-supervised speech models such as CPC, wav2vec2, and HuBERT.  Building upon GSSM, pGSLM enhances the model's capabilities by incorporating prosody information spoken language modeling.  Additionally, Unit mBART extends the scope of speech language modeling to encompass multi-lingual scenarios, facilitating the generation of speech across different languages. It is worthy noting that these models are pre-trained on speech tokens without any text supervision.In addition to the “textless\" speech language models, there exist models that incorporate text to enhance the generation expressiveness. Models like VALL-E and Spear-TTS perform text-to-speech (TTS) in a generative language modeling manner based on neural codes, such as soundstream and Encodec.In this paper, our primary focus lies on speech language models that conduct pre-training without text supervision. The reason for this emphasis stems from that textless speech generation is of great significance, given the challenges posed by acquiring labeled text-speech pairs and the absence of text in certain languages.\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "It's been well-known right from the outset that language modeling for code has to deal with project-specific phenomena  [hindle2012naturalness, tu2014localness, hellendoorn2017deep]. The sticking point here, however, is that project-specific data, especially early-on in a project's history, may  be quite limited in volume; older deep-learning models, require O(10^4) or even O(10^5) samples that are specific to a project or domain to learn the local features.  Even BERT-style foundation models require a  lot of training examples. Such examples may be hard to find on a project-specific basis, even early in the history of a project.  Even if enough examples exist,  retraining  a big model for each new project can be cumbersome, but also necessary (thanks to the “catastrophic  forgetting\" problem [french1999catastrophic]).The few-shot learning capacity of very-large language models offers a work-around. These models can make do with just a handful of training examples; furthermore retraining is not really cumbersome, one can just change the prompt. In addition, the very limited training requirement suggests that we might (in the future) localize to even just a file, or even just a method.  We therefore believe that the few-shot setting has tremendous potential to be useful in project-specific settings in software engineering.In this paper, we primarily focus on comment synthesis. This application has the advantage of being both quite useful, and also well-studied. There has been quite bit of work on investigating various kinds of models : RNNs, Transformers, Foundation Models, etc, and there are  good benchmarks available. We therefore use this problem as a test-bed  to investigate the following questions.* Does the few-shot learning capacity of large language models extend to the task of code summarization? \n",
      "  * Can this few-shot learning capacity be extended to same-project learning on this same task?  \n",
      "  * How does the performance of LLMs in the above two settings compare with that of state-of-the-art  models?acmcopyright 2022 2022 rightsretained 37th IEEE/ACM International Conference onAutomated Software EngineeringOctober 10–14, 2022Rochester, MI, USA 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10–14, 2022, Rochester, MI, USA 10.1145/3551349.3559555 978-1-4503-9475-8/22/10mybox =University of California, Davis      Davis   California   USA   95616 tfahmed@ucdavis.eduUniversity of California, Davis      Davis   California   USA   95616 ptdevanbu@ucdavis.edu\n",
      "---\n",
      "Natural Language Processing (NLP) is a rapidly growing field of study that works on understanding human language. NLP gives machines the ability to learn human language by turning it into numerical data [khurana2023natural]. With the increasing number of digital texts, the need of NLP is growing rapidly[feder2022causal]. In recent years, NlP has provided a large scale analysis and management of text data making sentiment analysis, emotion detection and other complicated tasks possible. Furthermore, with the help of NLP, it is possible to detect mental illness at early stage and provide treatment [zhang2022natural]. Previously the training of NLP models were slow and inefficient [chen2021crossvit]. Especially after 2017, the innovation of transformer architecture has revolutionized the NLP field. The transformers made NLP tasks to be carried out sequentially that gave birth to large language models like ChatGPT. This models are so efficient that it can imitate human behaviour which creates some reasonable concerns.Transformers are one of the most powerful tools for natural language processing [gillioz2020overview]. It can largely be divided into two parts. The encoder and the decoder. The encoder part takes a text sequence as input and produces a sequence of encoded representation. The difference between other architecture is that the encoded sequence is more context aware. When a series of encoders are stacked togather, it is called a BERT [singh2021nlp]. The decoder part, on the other hand, is able to generate an arbitrary length sequence. Stacking decoder blocks produces an architecture named GPT. ChatGPT is also a transformer based architecture that has been trained on a large set of public data in self-supervised fashion. It has more than a billion parameters making it on of the biggest language model available. It is considered a major breakthrough since it's release in November, 2022.\n",
      "---\n",
      "*           We experiment with  [cobbe2021training] (1319 samples).          The dataset contains math word problems geared toward an average middle-school curriculum.          \n",
      "  *           We use date understanding (, 349 samples) and sports understanding (, 980 samples) as representative tasks for commonsense reasoning, both derived from [date:sport].          \n",
      "  *           We experiment with sorting (, 500 samples) a list of single-digit integers.           We do not associate explicit instruction (sort these numbers) with the questions.          Instead, we frame the questions as a challenging setup in which the model should figure out the task and the requisite information to solve it.In this work, we neither train any of the \\palm models, nor performs finetuning.We solely perform inference on \\palm variants using TPU v4~[tpuv4,jouppi2017datacenter].For \\mpalm, we use 4$\\times$4$\\times$4 TPU v4 configuration, whereas, for \\lpalm we use 4$\\times$4$\\times$16 mesh configuration.To account for the variation in results caused by the order of examples in the prompt, we conduct each experiment three times, each with different seeds, and report the average task solve rate.Following [wei2022chain], we evaluate each task using accuracy \\ie fraction of examples where the output matched the expected result.\\niparagraph{Public large language models.}We use OpenAI API\\footnote{\\href{https://openai.com/blog/openai-api/}{https://openai.com/blog/openai-api/}, v0.23.0} to conduct experiments with \\gptt (\\bench{text-davinci-002}) and \\codex (\\bench{code-davinci-002}).\\input{tables/task_examples}Based on our initial findings, this section explores the idea of engineering effective prompts tailored to remove ineffectual tokens.The benefit of such tailored design is multi-fold: (1) “reducing noise”:reducing noise from prompts could potentially lessen the confusion for the model and lead to a higher task solve rate and(2) “potential energy savings”:the reduced number of tokens in the prompts instruct the model to generate less number of tokens per question.Less number of generated tokens directly translates to reduced runtime and carbon footprint per inference [strubell2019energy].This is especially crucial because of the significant compute cost of large language models.Our proposed prompts, dubbed as , distill the essence of our hypothesis by supplying minimal requisite information to drive the model to factual conclusions.<Ref> compares and across different tasks and four variants of s.In all tasks, except , we use identical examples in the prompt and only rephrased the thoughts to use less number of tokens.For , we could not find a systematic mechanism to shorten the thoughts.Instead, we randomly harvest questions from the training set whose thoughts are shorter than .Overall, outperforms while employing prompts with fewer tokens.The task solve rate of remains relatively high as we scale the model to , highlighting the efficiency of .Appendix-<Ref> compares the average number of input/output tokens between and .On average, (our approach) reduces the number of input (1.39×) and output tokens (1.58×).This work evaluates the capacity of to elevate complex reasoning in three state-of-the-arts s, , , and .We systematically assembled a series of controlled counterfactual experiments.Our results show the initial inklings of connection between text, patterns, and reasoning in s.Our study indicates that the symbiosis of text and patterns bears more weight in the chain of thought reasoning process.In addition, we assert that text is a channel to extract semantic patterns, unlocking the ability of these models to mold correct answers.Large language models.To facilitate conducting an exhaustive number of experiments, we center the main analysis of this paper around [The results in [wei2022chain] reinforce that is the smallest model with significant gains across a range of reasoning tasks.].For reproducibility, we also conduct the experiments on publicly available models such as and .We present results from in the main body and defer the results from other models to <Ref>.Nonetheless, our findings concur across the studied s.Reasoning tasks.We focus on reasoning tasks for which presents ample improvements over prompting [wei2022chain], namely  ( [cobbe2021training]),  (date and sports understanding [date:sport]), and  ()—details in <Ref> and Appendix-<Ref>.This work intends to tease apart the major semantic components of a prompt that play a critical role in the efficacy of .To achieve this, we identify and systematically construe three key semantic components of a prompt, listed as follows (See Appendix-<Ref> for the examples to which we refer in the definitions):* are sequences of tokens in the prompt, about which the model reasons to solve a task.For and , symbols are numerical quantities (5, 4, 2, 13).Similarly for the dataset, we categorize players and activities as symbols. We define the symbols in the dataset as date and time indicating expressions.* are either composition of symbols and operators or a structure of prompt that reinforces task understanding.The isolation of patterns within a prompt is evident in datasets like (equations), (person is a sport, activity is a sport), and (1 less than 2).However, for the dataset, the pattern is semantically implicit but consistent.Each thought contains two parts: (a) ⟨calculation⟩ in which the information from the input is restated (“Today is 06/02/1943”) and intermediate results are generated (“One day after 06/01/1943 is 06/02/1943”) through mathematical calculations and (b) ⟨output⟩ in which the final requisite answer is generated using the intermediate results (“10 days before today is 05/23/1943”).* are tokens that are neither symbols, nor part of patterns.Specifically, text in prompts assists in eitheroutlining the target task (is the sentence plausible),connecting patterns to symbols (John is left with 4 - 2 = 2), or contextualizing symbols (4 toys).In a nutshell, text is the conceptual glue that binds different parts of a prompt.Relying on our analysis, we conjecture and hypothesize about the effects of each semantic components on the outcomes of prompting.We discuss detailed results, including statistical significance tests in <Ref>.The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence [lake2017building].This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data.Recently, large language models (s) [palm:arxiv,brown2020language,codex] have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning.Few-shot learning has shown promising applications for a wide range of tasks [gem,flan,bigscience,lamda,liu_what_2021,reif2021recipe,wang2020generalizing,chen_evaluating_2021,minerva,wu2022autoformalization].While beneficial, this setting requires meticulous design of prompts [le2021many,liu2021pre,mishra_reframing_2021].[ling2017program] pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning.Recently, [wei2022chain] proposed chain of thought () prompting, showing that the few-shot setting in s similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks [ling2017program,cobbe2021training,svamp,date:sport].Despite its wide-range usage, the rationale behind the success of remains unclear.Recent work draws [ling2017program,wei2022chain] parallels to human thinking.Humans often think about a problem before deducing a solution.Akin to this process, it is argued that models should also be able to employ a similar mechanism.While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate.Ultimately, s are trained to estimate the next token distribution for a given context.Therefore, there is presumably a systematic rationale behind their successes and failures.In this work, we undertake initial steps towards understanding the mechanism behind .Contributions and findings.We construct a series of tailored counterfactual prompts [counterfactual], deliberately sketched as controlled studies.First, we identify key components of an example in few-shot prompting as follows: Symbols, Patterns, and Text.Next, we perform counterfactual prompting—keeping all but one component fixed (replacing symbols (numbers) with Greek alphabets).Finally, we elicit meaningful findings via conducting a systematic and qualitative analysis of the performance divergence between different prompt queries.Our experiments on four diverse reasoning tasks and across three large language models—, , and , reveal several surprising findings:1 We find that the exact type of symbols in the prompt virtually does not affect the model performance.In addition, our results and analysis demonstrate counterintuitive phenomena.For example, we identify that the correctness of symbols and patterns is immaterial to the task solve rate.2We learn that patterns contribute chiefly as a venue to reinforce task understanding [ouyang2022training] and prompt the model to attain correct outputs.3Most importantly, we find that text and patterns form a symbiotic relationship that plays a vital role in the success of .Text helps generate useful patterns (by extracting commonsense knowledge), and patterns help reinforce task understanding, enabling the language model to generate text that helps solve the task.Overall, we argue that one of the primary reasons behind the success of is this interplay between text and patterns—helps a language model in imitating the prompt and generating the right tokens for the task—and is conceivably less related to their reasoning abilities.Finally, as indicated by applications such as -Saycan [ahn2022can], we posit that techniques like will play a key role in enabling the success of s on diverse use cases.Thus, designing efficient prompts informed by a set of key design principles is an important challenge.To this end, we distill our findings to create concise prompting, dubbed .prunes the prompt (20%↓) to only retain indispensable tokens without negative repercussions on the task solve rate.§ ETHICS STATEMENTDisseminating reasoning into machines has numerous benefits and applications, from algorithmic reasoning [alphacode] to code generation [chen_evaluating_2021,poesia2021synchromesh] and formal verification [wu2022autoformalization].While this research does not directly enhance the reasoning capabilities of large language models, it identifies several systematic behavioral patterns in the functioning of few-shot models.Similar to any technological advances, this work has risks of detrimental societal impact.However, anticipating potential future downsides of such methods is challenging.More than ever, the research community's utmost responsibility is to acknowledge these risks candidly and reflect on practices and strategies to prevent potential harm.Environmental impact.Training large language models devour a nontrivial amount of compute resources, a limiting factor for frequent training.Few-shot prompting is an appealing solution for mitigating the unfavorable environmental impact of large language models by evading additional iterations of training and dataset collection.Ours and similar studies may lead to more effective prompting techniques and bring technological innovation to the architecture of large language models, especially regarding their reasoning capabilities.Therefore, we hope that the significant compute used in this work can help promote positive environmental outcomes.Finally, aligned with the credible concerns of the research community, we recognize the longer-term risk of Excellence in artificial intelligence, primarily when it boils down to human reasoning.While distilling comparable human reasoning to machines offers many benefits, undisciplined and uncontrolled progress in this area could be alarming, especially in the presence of bad actors.Effective reasoning in machines as a result of our study, even though not directly, can lead to algorithmic advances that may facilitate bad actors in developing malicious software and systems with human-level capabilities.We also want to acknowledge a large body of researchers that has greeted innovations in large language models and steady scaling of models with skepticism, questioning the connection between human reasoning and large language models [han2022human,cog:gpt3].Additionally, recent efforts have started exploring the relationship between the structure of contemporary language models and the human mind [schrimpf2021neural,tang2021sensory,whittington2021relating].Despite their progress, the rationale behind 's mimicking human reasoning, and any potential connection with linguistics is so far an uncharted territory.The primary objective of our study is to understand through counterfactual prompting and empirically establish the underpinnings of the reasoning ability of s in the presence of .Each counterfactual prompt C_f(p) alters only one particular aspect of the in-context examples ⟨ x_k · t_k · y_k⟩ in a .For example, consider a sample thought for the math world problems in the dataset (See <Ref>).A symbolic counterfactual prompt, C_symbolic(p), may simply replace all the numbers in the thoughts with symbols (X1).Such analysis enables us to ask: “what would the performance of the model have been, if all the numbers in the prompt were replaced with symbols?”.Analyzing the performance disparity of a on C_symbolic(p) vs. p can thus indicate the role that using actual numbers plays in the success or failure of a task[Appendix-Table <ref> outlines a set of major “what if” questions that we study in this work.].The ability to successfully complete prompts p for complex reasoning tasks is typically present for at the scale of , , and .Nonetheless, we do not make any assumptions about the underlying model architecture.In summary, our study on reveals that for solving math problems, neither the presence of numbers, nor the credibility of the thoughts is paramount to the success of .Similarly, altering the style and wording of the texts in the thoughts has a modest impact on the model performance.Nevertheless, eradicating either of these components nullifies the efficacy of .Finally, a per-layer analysis of the model reveals that if the model performs similarly for a pair of counterfactual prompts C_f(p) vs. p, then the attention patterns are comparable as well.We defer the detailed background to Appendix <ref>.Limitations of counterfactual prompting.Relying on counterfactual examples could be misleading and precarious [laugel2019dangers,slack2021counterfactual].Nonetheless, counterfactual explanation presents a channel to gain insights into the workings of the model.This approach potentially yields more favorable explanations for state-of-the-art s.Notably, unlike fine-tuned methods, one can readily identify and collect a set of prompts that are critical for the model to generate particular outputs.In particular, few-shot prompting augments the model with an additional dimension to calibrate the accuracy to a discernible degree.Thus, we deduce that the counterfactual examples that exhibit consistent and systematic performance divergence are more prone to reflect credible interpretations of the model.In this work, we neither rely on the results that do not exhibit such characteristics, nor reject prompts that pose contradictory observations. We discuss additional limitations in <Ref>.Broadly, this paper intersects with a growing body of work on prompting and large language model reasoning [brown2020language,palm:arxiv].Below, we review the most relevant work in these directions.\n",
      "---\n",
      "The emergence of large language models (LLM) has gained much popularity in recent years.  At the time of this writing, some consider OpenAI's GPT 3.5 series models as the state-of-the art [gptAbility].  In particular, a variant tuned for natural dialogue known as ChatGPT [chatGptBlog], released in November 2022 by OpenAI, has gathered much popular interest, gaining over one million users in a single week [cgptUsers].  However, in terms of accuracy, LLMs are known to have performance issues, specifically when reasoning tasks are involved [gptAbility,hoffmann_training_2022].  This issue, combined with the ubiquity of such models has led to work on prompt generation and other aspects of the input [wei_chain–thought_nodate,wang_self-consistency_2022].  Other areas of machine learning, such as meta-learning [hospedales_meta-learning_2022,zhou_domain_2022] and introspection [daftry_introspective_2016,ramanagopal_failing_2018] attempt to predict when a model will succeed or fail for a given input.  An introspective tool, especially for certain tasks, could serve as a front-end to an LLM in a given application.\n",
      "---\n",
      "ChatGPT has been released and introduced by OpenAI in November 2022. The primary goal of designing ChatGPT is to generate natural language text for a wide range of applications (e.g., see Refs. [shahriar2023let,chen2023chatgpt]). Now as researchers in the area of computational physics and mathematics, if we think about “codes’’ as a specific type of text, some questions that come to mind are if ChatGPT can successfully accomplish the task of programming numerical algorithms and if the automation of programming numerical methods can be fully handled by a trained language model such as ChatGPT. ChatGPT bas been examined for its capacity to develop software [white2023chatgpt,ahmad2023towards] and automatically fix bugs in codes [sobania2023analysis]. Additionally, the categorization of failures by ChatGPT has been reported in Ref. [borji2023categorical]. These research papers [sobania2023analysis,borji2023categorical,white2023chatgpt] studied the ChatGPT performance for coding and programming from a general software engineering point of view. Nevertheless, we carry out the performance of ChatGPT specifically for programming numerical methods in the current report.Numerical algorithms play a critical role in scientific simulations and computational modeling. These algorithms are usually developed and implemented by human programmers using programming languages such as C, C, Python, MATLAB, etc.  Nevertheless, implementing and programming sophisticated numerical algorithms could be potentially challenging tasks.\n",
      "---\n",
      "Recent LLMs, for instance, PaLM [chowdhery2022palm] (released in 2022), are tested only on elementary-level mathematical reasoning datasets, such as the MathQA or GSM8K datasets [amini2019mathqa,cobbe2021training].  We suspect that this is due to a lack of advanced-level natural language mathematics datasets.  Moreover, the results obtained indicate that the models at that time had difficulty with much simpler datasets than ours. For example, the version of PaLM with 540 billion parameters only correctly solves 58% of the problems of the GSM8K dataset, even with chain-of-thought prompting and access to an external calculator [chowdhery2022palm]. This model nonetheless outperforms GPT-3 [brown2020fewshot], which only achieves 54% on the same dataset. Variations of BERT [piekos2021measuring] have been shown to only solve between 28% and 37% of the problems when fine-tuned and tested on the Algebra Question Answering with Rationales () dataset [aquarat2017ling], which is the direct predecessor of MathQA. For some models, such as BLOOM [Scao2022BLOOMAI] or the LaMDA model [thoppilan2022lamda] (both released in 2022), an evaluation of the mathematical reasoning capability is entirely missing. An up-to-date survey on mathematical datasets and the performance of various LLMs can be found in [lu2022survey].\n",
      "---\n",
      "These growing capabilities of large pre-trained language models \"(LLMs)\", so we can use this term later for generating text have enabled their successful application on a diverse set of tasks, including summarization, translation, question-answering, \"and also in \", to emphasize the distinction between NL-LMs and Code-LMs code [chen2021evaluating,austin2021program,xu2022systematic], and graph generation  I would remove \"graph generation\", because it's not as obvious and well known as the other applications. Subsequently, there is now a growing interest in tackling traditionally discriminative tasks in a generation setup. This is more of a technical detail that I'm not sure that is required this early A popular example of this shift appears in commonsense reasoning, where structured commonsense generation is getting increasingly popular. Unlike traditional commonsense reasoning tasks involving reading comprehension or question answering, structured commonsense generation aims to extract structures from the given input. Some examples of structured commonsense tasks include generating event graphs, reasoning graphs, scripts given a goal, and argument explanation graphs.Structured commonsense generationAdd citations everywhereThe key ideas isare this:      \n",
      "  * Structured commonsense generation     \n",
      "  * How are these models typically trained?     \n",
      "  * What are the shortcomings of these models     \n",
      "  * Code generation models can help     \n",
      "  * Our main research question     \n",
      "  * Contributions\n",
      "---\n",
      "CodexDB is enabled by recent advances in the domain of natural language processing. Those advances have been fuelled by two key ideas: a novel neural network architecture, the Transformer [Vaswani2017], and new training paradigms, implementing the idea of transfer learning [ruder2019transfer]. The Transformer is nowadays the dominant architecture in the domain of language processing [Wolf2020]. Among other advantages, it lends itself better to parallelization than prior methods. This has, in part, enabled the creation of very large, pre-trained language models. Such models are pre-trained on tasks for which large amounts of training data are easily available, e.g. predicting the next word in text snippets. While pre-training is very expensive, the resulting models can be easily specialized for new tasks via different methods. Fine-tuning describes a process in which pre-trained models are used as a starting point for further training on more specialized tasks (reducing the amount of training samples and computational overheads by orders of magnitude via pre-training [Houlsby2019]). Until recently, fine-tuning has been the primary method of exploiting pre-trained language models. The latest generation of pre-trained models, most notably OpenAI's Generative Pre-Trained Transformer (GPT) version 3, unlocks new possibilities. It turns out that sufficiently large models can oftentimes solve new tasks without specialized training (“zero-shot learning”), based on inputs describing the task in natural language alone [Brown2020]. Precision increases if the input integrates few (i.e., typically less than ten) examples pairing tasks of the same type with solutions (“few-shot learning”). This is the method currently used by CodexDB. The final development that enabled this paper is the emergence of the Codex variant of GPT-3 [Chen2021, OpenAI2021]. The primary difference between GPT-3 Codex and the original GPT-3 model lies in the data used for pre-training. GPT-3 Codex is trained on code and technical documentation. This results in a model whose primary use case is the translation of natural language commands into code.\n",
      "---\n",
      "Q3: Do the specifics of a training setup, such as the model variant, dataset multilingualism, or dataset size have a decisive influence on the DRA? Yes. As noted for Q1, the differences between model training setups in many cases play a larger role than the size of the model. An example outlier is Bloom-560m, which comfortably outperforms all larger Bloom models, and Bloom setting as a whole, which in turn outperforms much larger OPT and GPT-2 XL models. This is perhaps due to the multi-lingual training setup (including multiple programming languages) of Bloom models, which is in contrast to natural language English used for GPT-1/2 and OPT models.In summary, we conclude that all our initial hypotheses were partially incorrect, and that the subject of deductive reasoning ability in large language models warrants further examination from a wider set of angles.Our work only considers the token probabilities directly as provided by the generative models. Since the models are trained with generation of patches of text in mind, it can be argued that the model might in fact be trying to output a verbose answer. We attempt to to mitigate this by priming the model, but higher accuracy could perhaps be achieved by fine-tuning the language modelling head.acl_natbib\n",
      "---\n",
      "We set out to answer three research questions:      \n",
      "  * How does the deductive reasoning ability (DRA) of large language models evolve with their growing size?     \n",
      "  * How does the DRA of general-purpose generative large language models depend on the number of deductive steps required to reach the conclusions?     \n",
      "  * Do the specifics of a training setup, such as the model variant, dataset multilingualism, or dataset size have a decisive influence on the DRA?  We measure DRA by binary accuracy on the answers of the model; in other words, by the proportion of the instances in which the model answered the question about the theory given as context correctly. A detailed account of our experimental setup is presented in <Ref>.\n",
      "---\n",
      "There has been significant interest in scaling of language models [rae2021scaling,chowdhery2022palm,brown2020language]. Scaling has inspired new research across multiple fronts, e.g., scaling laws [kaplan2020scaling,hoffmann2022training,tay2022scaling], emergent abilities [wei2022emergent,ganguli2022predictability], reasoning capabilities [wei2022chain,lewkowycz2022solving], inter alia. Generally, scaling laws predict a continued improvement in language model quality as we continue to scale up the computational budget (e.g., bigger models or more data). To date, most large language models that form the basis of scaling law research are trained almost exclusively as left-to-right causal language models [kaplan2020scaling,hoffmann2022training].\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [03:40, 11.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 109, in run\n",
      "    await self.visit(self.parse_tree, VariableStack([self.program._variables], self))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 524, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_assistant.py\", line 13, in assistant\n",
      "    return await role(role_name=\"assistant\", hidden=hidden, _parser_context=_parser_context, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_role.py\", line 17, in role\n",
      "    new_content += await parser.visit(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in visit\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in <listcomp>\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 379, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_gen.py\", line 140, in gen\n",
      "    gen_obj = await parser.llm_session(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 665, in __call__\n",
      "    out = await self.llm.caller(**call_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 370, in _library_call\n",
      "    out = await openai.ChatCompletion.acreate(**kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 728, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 11758 tokens. Please reduce the length of the messages.\n",
      "\n",
      "Error in program:  This model's maximum context length is 8192 tokens. However, your messages resulted in 11758 tokens. Please reduce the length of the messages.\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "How do biases and fairness issues arise in LLMs, and what can be done to mitigate them?\n",
      "--------------------------------------------------\n",
      "Answer A ($13.38):\n",
      "On our CMATH dataset, we conduct evaluation for a variety of popular LLMs, accessible via commercial API or released model weights. We discover that  GPT-4 is the only model that achieves success (accuracy ≥ 60%) across all six elementary school grades. We also examine the robustness of LLMs against the distracting information. It turns out that GPT-4 is again the sole model that maintains robustness, while other models are easily misled by the presence of distracting information.They require understanding natural language questions and translating them into mathematical expressions/equations. This tests a model's linguistic abilities.They require applying mathematical knowledge and reasoning logically through multiple steps. This tests a model's symbolic manipulation and reasoning skills.They often involve commonsense knowledge about the real world. This tests a model's knowledge grounding and understanding.They come in a range of difficulties, allowing us to probe model abilities at different levels of depth. From grade 1 to 6, the problems become increasingly complex and abstract.* How well can state-of-the-art Chinese language models solve elementary school math word problems? What are their limitations? \n",
      "  * What types of reasoning or math abilities do current models struggle with most? Logical, linguistic, commonsense or mathematical? \n",
      "  * How can we develop models with stronger reasoning and problem-solving skills? What abilities are most important to improve?We release CMATH, the first Chinese dataset for evaluating math reasoning abilities of language models. CMATH contains 1,000 elementary school level math word problems with detailed annotations.We conduct the first systematic evaluation of popular Chinese language models on math word problems. We show how CMATH can reveal differences in the math reasoning skills of language models at various ability levels.We find that although elementary school word problems seem simple, solving them requires sophisticated linguistics, logical and commonsense reasoning that current language models do not possess. CMATH provides a challenging testbed for continued progress in these directions.Our work establishes a first framework for evaluating and enhancing the math reasoning skills of Chinese language models. We release CMATH as a public benchmark to support future work in this direction.0.85! \n",
      "    < g r a p h i c s >\n",
      "  Sample problems along with their English translations (not part of the dataset) and human annotations. The column title “#Steps” and “#Digits” stand for “number of reasoning steps” and “number of digits” respectively.This work is motivated by the following question:  To what grade level of elementary school math do the abilities of popular LLMs correspond?  We create the CMATH dataset in order to  answer this question. We believe that the evaluation results of LLMs should be presented in an intuitive manner, making them easily understandable for the general public.\n",
      "---\n",
      "Given the previously mentioned issues, this lack of accessibility could affect the education of NLP methods. If students do not have access to code of LLMs, it could be difficult for them to learn to implement complex language model code of their own and learn to keep up with the state of the art. A lack of reproducibility could also be disenfranchising to a young, empircist NLP researcher, leading them to pursue nativist approaches. These issues could reinforce the use of statistical, pre-deep learning techniques in the classroom, but it is difficult to argue that publication venues are interested in shallow neural network experimentation at this time.These issues combine to form an uneven playing field for students to study NLP in empiricist and hybrid forms. After studying NLP formally, they may be inclined to commit to nativist methods or even reinforce the popularity of them at scale.We ask that publication venues merit open source LLM experiments significantly higher than they do currently. We believe that this would mitigate the issues discussed previously in this work. There seem to be developments occuring now in the deep learning publication space to help implement this in a proper form of governance [ashurst2022ai].In this work, we provided a comprehensive history of natural language processing methodologies over roughly the past century. We then used this narrative to lead into today's deep learning practices used in language processing, and current issues in an excessive closed sourcing of code for LLMs. It is our hope that this work inspires researchers and reviewers to champion open source language model code in order to pave the way for a more balanced research space.acl_natbib\n",
      "---\n",
      "Large language models (LLMs) have demonstrated remarkable predictive performance across a growing range of diverse tasks [brown2020language,devlin2018bert]. However, the inability to effectively interpret these models has led them to be characterized as black boxes. This opaqueness has debilitated their use in high-stakes applications such as medicine [Kornblith2022] and policy-making [brennan2013emergence], and raised issues related to fairness [dwork2012fairness], regulatory pressure [goodman2016european], safety [amodei2016concrete], and alignment [gabriel2020artificial]. This lack of interpretability is particularly detrimental in scientific fields, where trustworthy interpretation itself is the end goal [angermueller2016deep].\n",
      "---\n",
      "Are inconsistencies just due to randomness? Stochastic models can be inconsistent due to randomness alone. However, our tests show inconsistency far beyond the variance in model outputs (even with temperature zero, OpenAI's models exhibit some stochasticity [openai_temp_zero]). To verify this, we run a self-consistency version of our Paraphrasing experiment, where we query the exact same question four times. We find that stochasticity accounts for less than 20% of all the “strong” (ε = 0.2) violations we find.  For details, and additional experiments with temperature 0.5, see <Ref>.Reaching decisions on complex legal cases can be long and costly, and the “correctness” of decisions is often contested (e.g., as evidenced by appeal courts).   ML has been explored both to automate the processing of legal information [chalkidis2020legal, legal_judgment_prediction_survey] and even to reduce human biases in legal decisions [kleinberg2018human].The difficulties in assessing the correctness or fairness of human legal decisions extend to AI tools that are used to assist or automate legal decisions. In this section, we show how to reveal clear logical inconsistencies in two different language models used for predicting legal verdicts: (1) a BERT model that evaluates violations of the European Convention of Human Rights; (2) GPT-3.5-turbo and GPT-4 models prompted to predict bail decisions given a defendant's criminal record.Paraphrasing:   We test whether changing the phrasing of a legal case changes the model's decision.Partial ordering:  While the “correctness” of legal decisions is hard to assess, there can still be clear ways of “ranking” different  outcomes. We consider an extreme example here, where we test whether a bail-decision model could favorably switch its decision if the defendant commits more crimes.\n",
      "---\n",
      "We advocate for a cautious approach to the incorporation of LLMs in EdTech for language learning, in which the training process, performance and limitations, and pathway to delivery are well documented and the risks of misapplication of such technology are understood. There are general concerns about AI for NLP and education which are recorded in the literature and continue to be relevant, perhaps more so, as LLMs come to the fore. Firstly there is a bias towards English, and specific genres of English, due to a combination of commercial pressures, training data availability, and data sourcing from the World Wide Web: even though several models have been trained in multilingual ways, the general trend with LLMs has exacerbated this pre-existing bias [gururangan-etal-2022-whose,sogaard-2022-ban,ramesh-etal-2023-fairness]. As LLMs grow, so does their climate impact: an issue which interacts with societal and infrastructure complexities but which we should nevertheless bear in mind and attempt to mitigate [strubell-etal-2019-energy,dodge2022]. In addition, LLMs are known to exhibit certain biases [barocas-et-al] – both representational (language use around demographic groups) [blodgett-etal-2020-language,stochastic-parrots] and allocational (how a system distributes resources or opportunities) [suresh-guttag,blodgett-etal-2020-language] – which need to be debiased or otherwise controlled [kaneko-bollegala-2021-debiasing,lalor-etal-2022-benchmarking].\n",
      "---\n",
      "Bias and Fairness. Language models producing toxic or biased content can cause severe harm especially to the groups being biased against [Bender2021OnTD]. A series of benchmarks have been developed to show that LLMs can generate toxic outputs [Gehman2020RealToxicityPromptsEN], contain gender biases [Rudinger2018GenderBI,Zhao2018GenderBI] and other categories of social biases [Nangia2020CrowSPairsAC,Nadeem2021StereoSetMS,BBQ], perform poorly against minority demographic groups [Koh2021WILDSAB,Harris2022ExploringTR] or dialectical variations [Ziems2022VALUEUD,Tan2020ItsMT]. Ideally, LLMs should not exhibit biased behaviors and not discriminate against any group. While many of these evaluations focus on evaluating the internal representation of LLMs in a zero-shot setting or evaluating the biases on specific downstream applications in a supervised setting, it remains unclear how these biases change under different prompting schemes in the few-shot setting, which will be the focus of our analysis. A closely related work is [Lucy2021GenderAR] which study representation biases in GPT-3 generated stories. We instead evaluate on the downstream tasks of coreferece resolution and question answering. Apart from few-shot prompting, [Solaiman2021ProcessFA] proposed a general method to align language models with human values, but it involves expensive iterative training.\n",
      "---\n",
      "Methods     Knowledge Domain     Downstream Task Indicator     What to  Pre-Train    How to  Pre-Train     How to Use Pre-Trained Model    Xland [team2021open]     Online tasks                      Predicates           Policy     RL     Zero-shot; Finetune    MIA [IL-Representation]       Offline human demo                    Text         Policy     BC     Zero-shot; Finetune     Gato [reed2022generalist]     Offline expert demo;  Multi-modal data           Prompt                Policy     BC     Zero-shot; Finetune    SayCan [saycan]     Pre-trained LM                     Text     Perception     SL; RL     zero-shot    Minedojo [fan2022minedojo]      Internet video; Pre-trained LVM;      Text      Reward     SL     Online RL    VPT [baker2022vpt]      Internet video;  Manual annotation      -                 Policy;  World Model     BC     FinetuneLM-Nav [shah2022lmNav]     Pre-trained LVM; Pre-trained LM                       Text              Perception     SL      Search method    Inner Mono. [huang2022inner]     Pre-trained LM; Pre-trained VM      Text      Perception      SL; BC     Zero-shotThe essential differences between prediction and sequential decision-making problems make the current success of large sequence models in NLP or CV cannot be directly transferred to the latter. Because the sequential decision-making process involves a feedback loop, subtle changes in behavior would lead to severe data distribution shifts. Therefore, new algorithms are demanded to learn stable representation, mitigate distribution shifts, and improve data efficiency.We cannot expect that pre-training a single model would lead to strong generalization ability in all out-of-distribution tasks.  Therefore, how to learn a universal and consistent representation for all the downstream tasks and minimize the distance between the training data distribution and the evaluation data distribution are the major issues that remain unsolved for effective large decision models with a reliable theoretical guarantee.\n",
      "---\n",
      "Gato [reed2022generalist] and VPT [baker2022vpt] have shown the potential of building large decision models for general purposes in the field of sequential decision-making, like what large sequence models have done for NLP and CV tasks. However, to build a large decision model, some modifications in architecture are significant with increasing data and model size, while naively scaling up models might fail as the number of parameters increases.  That is, with the same volume of data and parameters, the network architecture can be the determining factor to improve the performance of large decision models.  In this section, some important characteristics are listed since they can serve as consultative principles when designing network architecture for large decision models in the future. Noticed that the Transformer and its variants are suggested to be promising candidates recently, but any other model architectures [bai2018empirical,MLP-mixer,Perceiver_io] meeting the requirements below are still worth an exploration.To take full advantage of high-capacity models, how to utilize data from diverse tasks is critical for generalization.  Some techniques in model architecture have been investigated, e.g., transfer learning can be accomplished with the mixture of experts (MoE) [MoE] and modularization [soft-modularization,Pathnet]. Related research can help large models in the sequential decision-making domain attain better general intelligence.\n",
      "---\n",
      "Q1: How does the deductive reasoning ability (DRA) of large language models evolve with their growing size? We find that with the training setup and model architecture kept constant, larger model size leads to higher DRA, but also that this effect can often be compensated by choices made in the training configuration. Previous work has shown that the DRA of models fine-tuned for deduction decreases on data that requires longer chains of reasoning that have been seen in training. <Ref> shows that this does not seem to be the case for foundation models as we evaluated them, that the performance of many models on deeper questions is similar to that on shallower questions, and that it sometimes even increases, especially for OpenAI GPT-1/2 models.Q2: How does the DRA of general-purpose generative large language models depend the number of deductive steps required to reach the conclusions? For depths considered in our study, it remains largely similar throughout, except for the OpenAI GPT-3 and 3.5 models, for whom the performance decreases as seen in <Ref>.\n",
      "--------------------------------------------------\n",
      "Answer B ($88.84):\n",
      "Large language models (LLMs) have demonstrated remarkable predictive performance across a growing range of diverse tasks [brown2020language,devlin2018bert]. However, the inability to effectively interpret these models has led them to be characterized as black boxes. This opaqueness has debilitated their use in high-stakes applications such as medicine [Kornblith2022] and policy-making [brennan2013emergence], and raised issues related to fairness [dwork2012fairness], regulatory pressure [goodman2016european], safety [amodei2016concrete], and alignment [gabriel2020artificial]. This lack of interpretability is particularly detrimental in scientific fields, where trustworthy interpretation itself is the end goal [angermueller2016deep].\n",
      "---\n",
      "On our CMATH dataset, we conduct evaluation for a variety of popular LLMs, accessible via commercial API or released model weights. We discover that  GPT-4 is the only model that achieves success (accuracy ≥ 60%) across all six elementary school grades. We also examine the robustness of LLMs against the distracting information. It turns out that GPT-4 is again the sole model that maintains robustness, while other models are easily misled by the presence of distracting information.They require understanding natural language questions and translating them into mathematical expressions/equations. This tests a model's linguistic abilities.They require applying mathematical knowledge and reasoning logically through multiple steps. This tests a model's symbolic manipulation and reasoning skills.They often involve commonsense knowledge about the real world. This tests a model's knowledge grounding and understanding.They come in a range of difficulties, allowing us to probe model abilities at different levels of depth. From grade 1 to 6, the problems become increasingly complex and abstract.* How well can state-of-the-art Chinese language models solve elementary school math word problems? What are their limitations? \n",
      "  * What types of reasoning or math abilities do current models struggle with most? Logical, linguistic, commonsense or mathematical? \n",
      "  * How can we develop models with stronger reasoning and problem-solving skills? What abilities are most important to improve?We release CMATH, the first Chinese dataset for evaluating math reasoning abilities of language models. CMATH contains 1,000 elementary school level math word problems with detailed annotations.We conduct the first systematic evaluation of popular Chinese language models on math word problems. We show how CMATH can reveal differences in the math reasoning skills of language models at various ability levels.We find that although elementary school word problems seem simple, solving them requires sophisticated linguistics, logical and commonsense reasoning that current language models do not possess. CMATH provides a challenging testbed for continued progress in these directions.Our work establishes a first framework for evaluating and enhancing the math reasoning skills of Chinese language models. We release CMATH as a public benchmark to support future work in this direction.0.85! \n",
      "    < g r a p h i c s >\n",
      "  Sample problems along with their English translations (not part of the dataset) and human annotations. The column title “#Steps” and “#Digits” stand for “number of reasoning steps” and “number of digits” respectively.This work is motivated by the following question:  To what grade level of elementary school math do the abilities of popular LLMs correspond?  We create the CMATH dataset in order to  answer this question. We believe that the evaluation results of LLMs should be presented in an intuitive manner, making them easily understandable for the general public.\n",
      "---\n",
      "Given the previously mentioned issues, this lack of accessibility could affect the education of NLP methods. If students do not have access to code of LLMs, it could be difficult for them to learn to implement complex language model code of their own and learn to keep up with the state of the art. A lack of reproducibility could also be disenfranchising to a young, empircist NLP researcher, leading them to pursue nativist approaches. These issues could reinforce the use of statistical, pre-deep learning techniques in the classroom, but it is difficult to argue that publication venues are interested in shallow neural network experimentation at this time.These issues combine to form an uneven playing field for students to study NLP in empiricist and hybrid forms. After studying NLP formally, they may be inclined to commit to nativist methods or even reinforce the popularity of them at scale.We ask that publication venues merit open source LLM experiments significantly higher than they do currently. We believe that this would mitigate the issues discussed previously in this work. There seem to be developments occuring now in the deep learning publication space to help implement this in a proper form of governance [ashurst2022ai].In this work, we provided a comprehensive history of natural language processing methodologies over roughly the past century. We then used this narrative to lead into today's deep learning practices used in language processing, and current issues in an excessive closed sourcing of code for LLMs. It is our hope that this work inspires researchers and reviewers to champion open source language model code in order to pave the way for a more balanced research space.acl_natbib\n",
      "---\n",
      "Are inconsistencies just due to randomness? Stochastic models can be inconsistent due to randomness alone. However, our tests show inconsistency far beyond the variance in model outputs (even with temperature zero, OpenAI's models exhibit some stochasticity [openai_temp_zero]). To verify this, we run a self-consistency version of our Paraphrasing experiment, where we query the exact same question four times. We find that stochasticity accounts for less than 20% of all the “strong” (ε = 0.2) violations we find.  For details, and additional experiments with temperature 0.5, see <Ref>.Reaching decisions on complex legal cases can be long and costly, and the “correctness” of decisions is often contested (e.g., as evidenced by appeal courts).   ML has been explored both to automate the processing of legal information [chalkidis2020legal, legal_judgment_prediction_survey] and even to reduce human biases in legal decisions [kleinberg2018human].The difficulties in assessing the correctness or fairness of human legal decisions extend to AI tools that are used to assist or automate legal decisions. In this section, we show how to reveal clear logical inconsistencies in two different language models used for predicting legal verdicts: (1) a BERT model that evaluates violations of the European Convention of Human Rights; (2) GPT-3.5-turbo and GPT-4 models prompted to predict bail decisions given a defendant's criminal record.Paraphrasing:   We test whether changing the phrasing of a legal case changes the model's decision.Partial ordering:  While the “correctness” of legal decisions is hard to assess, there can still be clear ways of “ranking” different  outcomes. We consider an extreme example here, where we test whether a bail-decision model could favorably switch its decision if the defendant commits more crimes.\n",
      "---\n",
      "Methods     Knowledge Domain     Downstream Task Indicator     What to  Pre-Train    How to  Pre-Train     How to Use Pre-Trained Model    Xland [team2021open]     Online tasks                      Predicates           Policy     RL     Zero-shot; Finetune    MIA [IL-Representation]       Offline human demo                    Text         Policy     BC     Zero-shot; Finetune     Gato [reed2022generalist]     Offline expert demo;  Multi-modal data           Prompt                Policy     BC     Zero-shot; Finetune    SayCan [saycan]     Pre-trained LM                     Text     Perception     SL; RL     zero-shot    Minedojo [fan2022minedojo]      Internet video; Pre-trained LVM;      Text      Reward     SL     Online RL    VPT [baker2022vpt]      Internet video;  Manual annotation      -                 Policy;  World Model     BC     FinetuneLM-Nav [shah2022lmNav]     Pre-trained LVM; Pre-trained LM                       Text              Perception     SL      Search method    Inner Mono. [huang2022inner]     Pre-trained LM; Pre-trained VM      Text      Perception      SL; BC     Zero-shotThe essential differences between prediction and sequential decision-making problems make the current success of large sequence models in NLP or CV cannot be directly transferred to the latter. Because the sequential decision-making process involves a feedback loop, subtle changes in behavior would lead to severe data distribution shifts. Therefore, new algorithms are demanded to learn stable representation, mitigate distribution shifts, and improve data efficiency.We cannot expect that pre-training a single model would lead to strong generalization ability in all out-of-distribution tasks.  Therefore, how to learn a universal and consistent representation for all the downstream tasks and minimize the distance between the training data distribution and the evaluation data distribution are the major issues that remain unsolved for effective large decision models with a reliable theoretical guarantee.\n",
      "---\n",
      "Gato [reed2022generalist] and VPT [baker2022vpt] have shown the potential of building large decision models for general purposes in the field of sequential decision-making, like what large sequence models have done for NLP and CV tasks. However, to build a large decision model, some modifications in architecture are significant with increasing data and model size, while naively scaling up models might fail as the number of parameters increases.  That is, with the same volume of data and parameters, the network architecture can be the determining factor to improve the performance of large decision models.  In this section, some important characteristics are listed since they can serve as consultative principles when designing network architecture for large decision models in the future. Noticed that the Transformer and its variants are suggested to be promising candidates recently, but any other model architectures [bai2018empirical,MLP-mixer,Perceiver_io] meeting the requirements below are still worth an exploration.To take full advantage of high-capacity models, how to utilize data from diverse tasks is critical for generalization.  Some techniques in model architecture have been investigated, e.g., transfer learning can be accomplished with the mixture of experts (MoE) [MoE] and modularization [soft-modularization,Pathnet]. Related research can help large models in the sequential decision-making domain attain better general intelligence.\n",
      "---\n",
      "Q1: How does the deductive reasoning ability (DRA) of large language models evolve with their growing size? We find that with the training setup and model architecture kept constant, larger model size leads to higher DRA, but also that this effect can often be compensated by choices made in the training configuration. Previous work has shown that the DRA of models fine-tuned for deduction decreases on data that requires longer chains of reasoning that have been seen in training. <Ref> shows that this does not seem to be the case for foundation models as we evaluated them, that the performance of many models on deeper questions is similar to that on shallower questions, and that it sometimes even increases, especially for OpenAI GPT-1/2 models.Q2: How does the DRA of general-purpose generative large language models depend the number of deductive steps required to reach the conclusions? For depths considered in our study, it remains largely similar throughout, except for the OpenAI GPT-3 and 3.5 models, for whom the performance decreases as seen in <Ref>.\n",
      "---\n",
      "Models like are trained on data from the public internet, among other data sources, like carefully curated human instructions used in RL pipelines. These datasets are riddled with various sources of inherent biases [bender2021dangers, blodgett2020language, hovy2016social, bordia2019identifying]. It has been demonstrated by previous research that when used to generate content, make decisions, or assist users, LLMs may perpetuate or amplify existing biases. We have demonstrated throughout the paper that 's capabilities and behaviors represent a “phase transition” in capabilities compared to earlier models and observations on earlier models do not necessarily translate. Therefore, it is important to understand whether and how exhibits biases, and more importantly, how the emerging capabilities of the model can be used as part of mitigation strategies.\n",
      "---\n",
      "Research efforts on principles and applications of human-AI collaboration highlight possibilities on the horizon. Studies and results to date include core principles for guiding the combination of machine and human intellect via real-time inferences about the complementarity of human and machine contributions [mixedinit1999, complementary2007, kamar2012, ramakrishnan2019], shaping machine learning procedures to be of maximal value based on a consideration of human and machine capabilities [wilder2020, bansal2021], identifying ideal timing and content of machine contributions [mozannar2022Copilot], harnessing AI methods to help decision makers navigate large quantities of information [aidisplay1995],  taking human mental models into consideration when AI systems are refined and thus may change in their behavior over time [bansal2019], and designing systems that support human-AI interaction [amershi2019]. The powers demonstrated by language models can open up new dimensions of human and AI collaboration [mixedinitfutures2007], including enhancing human-human collaboration by providing guidance on how to assemble ideal teams of people [singla2015], facilitate team work among teams of people and machines [bohus2009] and developing new approaches to meshing multiple machine and human resources to solve challenging multidimensional problems [Shahaf2010]. The special challenges posed by the potential of LLMs to hallucinate and to generate biased, manipulative, and toxic output highlight the value of developing tools enabling people to work collaboratively with AI systems to provide them with oversight and guidance. Research efforts have demonstrated opportunities to develop special machinery and tools to help people recognize and address blindspots in machine learning [Lakkaraju2017].We have only touched on a few areas of societal influence. Numerous impacts will come to the fore, including those viewed as positive and beneficial and those that are seen as costly and negative. New issues will arise based on the special powers of the models and specific applications and engagements.\n",
      "---\n",
      "We advocate for a cautious approach to the incorporation of LLMs in EdTech for language learning, in which the training process, performance and limitations, and pathway to delivery are well documented and the risks of misapplication of such technology are understood. There are general concerns about AI for NLP and education which are recorded in the literature and continue to be relevant, perhaps more so, as LLMs come to the fore. Firstly there is a bias towards English, and specific genres of English, due to a combination of commercial pressures, training data availability, and data sourcing from the World Wide Web: even though several models have been trained in multilingual ways, the general trend with LLMs has exacerbated this pre-existing bias [gururangan-etal-2022-whose,sogaard-2022-ban,ramesh-etal-2023-fairness]. As LLMs grow, so does their climate impact: an issue which interacts with societal and infrastructure complexities but which we should nevertheless bear in mind and attempt to mitigate [strubell-etal-2019-energy,dodge2022]. In addition, LLMs are known to exhibit certain biases [barocas-et-al] – both representational (language use around demographic groups) [blodgett-etal-2020-language,stochastic-parrots] and allocational (how a system distributes resources or opportunities) [suresh-guttag,blodgett-etal-2020-language] – which need to be debiased or otherwise controlled [kaneko-bollegala-2021-debiasing,lalor-etal-2022-benchmarking].\n",
      "---\n",
      "In this paper, we study to which extent ImageNet, arguably the most popular computer vision dataset, can be replaced by a dataset synthesized by a text-to-image generator. Through an extensive study, we find that one can learn models that exhibit surprisingly good performance on fine-grained classification tasks like and without any class-specific prompting. However, the most important result of this study is the finding that models trained on synthetic data exhibit exceptional generalization capability that rivals with models learned with real images. We see this study as merely a first glimpse of what is now possible with the latest large models in terms of visual representation learning. We envision that similar approaches could be used to fine-tune or adapt models, using those synthetic datasets side-by-side with real ones.This section takes a step back and considers some of the implications from the analysis proposed in this paper.=-1 Applicability beyond ImageNet The process we followed to create ImageNet-SD requires minimal assumptions and can be applied to a wider set of classes. To disambiguate semantics, we only assume access to a short textual description of the class. This is generally easy to acquire even at a larger scale, , in semi-automatic ways from Wikipedia.=-1 Scaling laws for synthetic data Conceptually, there is no reason to restrict our approach to  a finite dataset of synthetic images. We could devise a training process which sees each image only once  [parisi2019continual].Yet, despite this scaling potential, the quality of the resulting classifier is bounded by the expressivity of the generator and the concepts it can reliably reproduce. No matter how intriguing the promise of an “infinite dataset” via data generation might be, practical applications  are bound by costs linked to computation and storage, as well as the moderation of the content fueling this generator. The latter has strong implications we discuss next.Data and model bias Because of its pioneering role as a source of images to train generic models, and all it has done to advance the computer vision field, ImageNet and some of its bias has been under heavy scrutiny [denton2021genealogy,luccioni2022bugs].  Its synthetic counterparts have no reason to be immune to bias.\n",
      "---\n",
      "Motivated by these issues, we present BERTuit, a transformer trained from scratch with text created by native speakers from Twitter. BERTuit has been trained with more than $230$ million Tweets from the Archive Twitter Stream Grab\\footnote{\\href{https://archive.org/details/twitterstream?tab=about}{Twitter Stream Grab About page: https://archive.org/details/twitterstream?tab=about}}, from 2021 to 2018. Using this massive amount of data and BERT-base architecture~[devlin2018bert], we replicate RoBERTa~[liu2019roberta] optimization to perform self-supervised masked language modeling pre-training. The result is a transformer model that accurately inherits leanings, nuances and biases from Spanish Twitter, which later is useful applied to any downstream task in Twitter and informal scenarios, as well as misinformation understanding in particular. We performed an evaluation on several NLP tasks on Spanish Twitter, comparing against the current best alternative from the state-of-the-art, XLM-RoBERTa and alternatively multilingual BERT (M-BERT). In summary, the following contributions can be found in this paper: \\begin{itemize}     \\item A description of a transformer model that reliably outperforms state-of-the-art alternatives on Spanish Twitter problems.     \\item This transformer coupled with an appropriate methodology can enhance the understanding of misinformation on social media. We contribute methods to achieve this.     \\item A powerful approach to represent claims containing misinformation into a 2d space using embeddings from the proposed transformer.     \\item An assessment of the ability of BERTuit to extract relevant language patterns even from small sets of data in the context of author profiling on Twitter. \\end{itemize}\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [03:40,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 109, in run\n",
      "    await self.visit(self.parse_tree, VariableStack([self.program._variables], self))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 524, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_assistant.py\", line 13, in assistant\n",
      "    return await role(role_name=\"assistant\", hidden=hidden, _parser_context=_parser_context, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_role.py\", line 17, in role\n",
      "    new_content += await parser.visit(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in visit\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in <listcomp>\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 379, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_gen.py\", line 140, in gen\n",
      "    gen_obj = await parser.llm_session(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 665, in __call__\n",
      "    out = await self.llm.caller(**call_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 370, in _library_call\n",
      "    out = await openai.ChatCompletion.acreate(**kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 728, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, you requested 8665 tokens (7641 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\n",
      "\n",
      "Error in program:  This model's maximum context length is 8192 tokens. However, you requested 8665 tokens (7641 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the name of the approach that bootstraps the reasoning process by finetuning the model on correct rationales generated by the model itself?\n",
      "--------------------------------------------------\n",
      "Answer A ($3.45):\n",
      "Language model for reasoning Perhaps the most well-known work of using LLMs for reasoning is Chain-of-Thought (CoT) [wei2022chain], which reveals the ability of LLMs to formulate their own “thinking procedure” for problem solving. Several follow-up works have since been performed, including least-to-most prompting for solving complicated tasks [zhou2022least], zero-shot-CoT [kojima2022large], and reasoning with self-consistency [wang2022self-consistency]. Recently, [madaan2022text] systematically studied the formulation and structure of CoT, and observed that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other work has also been extended to more sophisticated reasoning architecture beyond simple prompting. For example Selection-Inference [creswell2022selection] divides the reasoning process into two steps of “selection” and “inference”. STaR [zelikman2022star] bootstraps the reasoning process by finetuning the model on correct rationales generated by the model itself. Faithful reasoning [creswell2022faithful] decomposes multi-step reasoning into three steps, each performed by a dedicated LM respectively. Similar approaches like Scratchpad [nye2021show], which finetunes a LM on intermediate computation steps, also demonstrate improvement on multi-step computation problems. In contrast to these methods,  performs more than just isolated, fixed reasoning, and integrates model actions and their corresponding observations into a coherent stream of inputs for the model to reason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\n",
      "--------------------------------------------------\n",
      "Answer B ($96.02):\n",
      "Language model for reasoning Perhaps the most well-known work of using LLMs for reasoning is Chain-of-Thought (CoT) [wei2022chain], which reveals the ability of LLMs to formulate their own “thinking procedure” for problem solving. Several follow-up works have since been performed, including least-to-most prompting for solving complicated tasks [zhou2022least], zero-shot-CoT [kojima2022large], and reasoning with self-consistency [wang2022self-consistency]. Recently, [madaan2022text] systematically studied the formulation and structure of CoT, and observed that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other work has also been extended to more sophisticated reasoning architecture beyond simple prompting. For example Selection-Inference [creswell2022selection] divides the reasoning process into two steps of “selection” and “inference”. STaR [zelikman2022star] bootstraps the reasoning process by finetuning the model on correct rationales generated by the model itself. Faithful reasoning [creswell2022faithful] decomposes multi-step reasoning into three steps, each performed by a dedicated LM respectively. Similar approaches like Scratchpad [nye2021show], which finetunes a LM on intermediate computation steps, also demonstrate improvement on multi-step computation problems. In contrast to these methods,  performs more than just isolated, fixed reasoning, and integrates model actions and their corresponding observations into a coherent stream of inputs for the model to reason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\n",
      "---\n",
      "<Ref> shows that replacing person and activity has negligible impact on the accuracy. However, abstracting out the sports abstract drops the performance below levels (<Ref>).  What explains such a discrepancy? Note that both person and activity are already present in the question, and are copied in the thought. Thus, replacing them even with abstract tokens does not harm the performance, as the extraction pattern is retained. On the other hand, the sport is not present in the question, and has to be extracted. Consider a question: Is the following sentence plausible? “Norman Powell committed a blocking foul.” For abstracted sports prompts, the model generates Norman Powell is a SPORT1 player. Committing a blocking foul is part of SPORT2. So the answer is no. Whereas for the abstract person or the activity prompt, the model correctly infers the pattern (copy the person or the activity, and generate the corresponding sport) and generates: Norman Powell is a basketball player. Being a blocking foul is part of basketball. So the answer is yes. This example also clearly drives the notion of symbiosis:  only generating the right pattern or only having the right text. Both are required for effective .Cases where patterns help.As empirical results show, combining patterns with language reveals their true potential. However, our analysis uncovers interesting reasons for the superiority of patterns only, for the small number of cases.Interestingly, we found a few cases where forcing the model to generate some text leads to errors.Primarily, they were caused by the model trying to make faulty connections. Detailed examples are located in <Ref>.<Ref> presents detailed results, including ablations on various parts of the prompt. These were omitted from the main text due to space constraints.0.99     0          The presence of text is necessary for the model to arrive at the right conclusions.          While the model is sufficiently capable of forming abstractions, employing concordant entities in questions and thoughts is crucial for the success of .          The performance of the model is proportionally sensitive to the degree of variations in the text.We test this hypothesis to determine the role of text in the performance of .Following the same methodology as symbols and patterns, we employ various tailored counterfactual prompts to retain the entire symbols and patterns, while altering the text or grammar.Note that the practicality of the counterfactual experiments for text is contingent on the nature of the target task.In addition, because there is no text in , we did not conduct counterfactual experiments in this dataset.<Ref> summarizes the results and shows sample thought for each counterfactual prompt.Text with discrepant entities text_diff_entities.We first investigate the role of employing identical entities (names and objects) between a question and its associated thought.To achieve this, we simply modify the entities in the thoughts to be discrepant with the ones in the questions.is exclusively relevant for the and datasets for which concrete entities exist: objects and people for and players and activities for .In the dataset, we change the player name and the activity in the thought, while retaining the sport in the question.Samples of generated outputs in <Ref> illustrate that in this scenario, the model appears to disregard the prompt, generating arbitrary thoughts and attaining a similar performance as .The task solve rate for with is noticeably better than , while still fails to match the performance of .We delineate samples of generated outputs in <Ref> to glean more insights.\n",
      "---\n",
      "We follow standard best practices in adapting these models for two-way classification. We consider the two solution choices independently: for each choice, the model is provided the goal, the solution choice, and a special  token. At the final layer of the transformer, we extract the hidden states corresponding to the positions of each  token. We apply a linear transformation to each hidden state and apply a softmax over the two options: this approximates the probability that the correct solution is option A or B. During finetuning, we train the model using a cross-entropy loss over the two options. For GPT, we follow the original implementation and include an additional language modeling loss, which improved training stability.Generally, we found that finetuning was often unstable with some hyperparameter configurations leading to validation performance around chance, particularly for BERT. We follow best practices in using a grid search over learning rates, batch sizes, and the number of training epochs for each model, and report the best-scoring configuration as was found on the validation set. For all models and experiments, we used the   library and truncated examples at 150 tokens, which affects 1% of the data.Manual inspection of the development errors show that some “mistakes\" are actually correct but required a web-search to verify. Human performance was calculated by a majority vote. Annotators were chosen to participate that achieved ≥90% on the qualification HIT from before. It is therefore, completely reasonable that automated methods trained on large web crawls may eventually surpass human performance here. Human evaluation was performed on development data, and the train, development, and test folds were automatically produced by .\n",
      "---\n",
      "[razeghi2022impact] find that pre-training term frequencies can somewhat explain the success of few-shot methods.  In line with their work, our experiments on also show that prompting method is most suited for easy questions (involving personalities and activities found on the web). Finally, [xie2021explanation] show that in-context learning enables a large model to infer a shared concept between the examples, possibly leading to better task understanding. Our studies on the role of prompt, especially examples where wrong examples lead to better output (for ), add more empirical evidence to this finding. Further, we show that a symbiotic relationship between text and patterns allows a more efficient inference of task instruction.Least to most prompting.[zhou2022least] help the model generate a chain of thought by first asking the model to generate the sub-questions for the given problem.Next, the model is asked to answer the sub-questions, and finally, the sub-questions, along with sub-answers, are combined to generate the final result.This work is closely related to [kojima2022large], the latter distinguished by generating the rationale from a large language model directly.We posit that [zhou2022least] derives its key strengths from its ability to generate useful sub-steps.This resonates with our finding that the key contribution of CoT is the extraction of meaningful sub-steps.Rationale generation as an intermediate stepThe idea of generating rationales as an intermediate output for reasoning and structured generation tasks has shown promising results for fine-tuned models [ling2017program,sun2019improving,rajani2019explain,shwartz2020unsupervised,madaan2021think,nye2021show,gu2022dream].Recently, [wei2022chain] proposed chain-of-thought prompting, which shows that few-shot setups can also be improved by making the model first generate an understanding of the output.As a natural extension to , [wang2022self] seek to improve using over-generation using self-consistency. They sample multiple outputs, and take a plurality vote (most frequently generated answer) to arrive at the final answer. This general idea of enforcing the model outputs to be consistent has also been explored for symbolic-commonsense reasoning [kassner2021beliefbank].The efficacy of this approach is corroborated by [wang2022rationale], who report that taking multiple samples helps a model become robust to settings in a few-shot setup. Our work looks at understanding the efficacy of in the standard setup of generating a single output per input.Prompt selection.Several works have recently explored the design of the prompt—a process often called “prompt engineering” [le2021many,liu2021pre].The methods include dynamically creating prompts based on the question [liu_what_2021,rubin2021learning,poesia2021synchromesh], formatting the prompt as a list or questions [mishra_reframing_2021,rubin2021learning], improving order of examples in the prompt [lu2022fantastically], and providing instructions in the task [ouyang2022training].Unlike these techniques, is relatively robust to minor changes in the prompt design.Thus, the findings of our work might be more generally applicable.Explaining model behavior using counterfactual prompts and attention.As noted by [jacovi2020towards], an explanation of a deep learning system typically serves two different purposes: i) plausibility, which aims to provide an interpretation of system outputs that is convincing for humans, and ii) faithfulness, which aims to capture the actual reasoning process of a model.Our study requires both and uses different means to achieve them.We utilize counterfactual prompts to interpret the system outputs to aid human understanding.This is similar to using posthoc analysis tools [ribeiro2016should,lundberg2017unified,liuexplainaboard], which also focus on analyzing outputs without concern for the details of the model.To get a glimpse of the model's inner workings, we leverage attention [vaswani2017attention], a ubiquitous mechanism in NLP. While the broader question on the utility of attention for posthoc analysis is still open [jain2019attention,pruthi2020learning], there is some evidence to show that attention can act as an explanation [wiegreffe2019attention].Finally, the utility of any explanation mechanism is closely tied to the users and application domain [kaur2020interpreting,burkart2021survey].As our analysis shows, attention adds intuition and insights to the empirical findings.Counterfactual explanations seek to explain the behavior of a model by performing a what if analysis on examples [mothilal2020explaining,stepin2021survey,verma2020counterfactual,poyiadzi2020face,counterfactual].While counterfactuals can be misleading due to artifacts (see [laugel2019dangers,slack2021counterfactual]), they offer a tractable solution for probing large models like and .Notably, unlike fine-tuned methods, the most important examples for generating the model output are readily available.Thus, counterfactual inputs that show a consistent and systematic change in the model performance are more likely to reflect the model's behavior.§ REPRODUCIBILITY STATEMENTWe take the following steps to enable the reproducibility of our work.Controlling for randomness due to the order of examples.We run each experiment with multiple random seeds to control for randomness due to the order of examples in the prompt.We report the average and standard deviation of the results across all the random seeds.Additionally, we conduct statistical significance tests (McNemar's test [mcnemar1947note]) to compare the results across different prompts.Finally, we evaluate the agreement in output generated by different models using Cohen's kappa (κ) metric.Reproducing results.We open sourced the code at <https://github.com/google-research/google-research/tree/master/l2da/learned2design>.In addition, we have provided scripts for one-click reproduction of the results for the publicly available models in the paper.Experiments with publicly available models.We experiment with three different language models: , (text-davinci-002), and (code-davinci-002).is not publicly available as of submission time, but the provided source code is compatible with OpenAI API v0.23.0, and can work with any OpenAI models.Finally, is free to use as of submission time that further helps with the reproducibility of the results.Prompts and outputs.All the prompts are included in the prompts/ directory.The generated outputs from and are provided in the outputs/ directory.Each output file follows a standard naming convention: task_name_model_name_sseed.jsonl.Each line of the output file is a json with three fields:* prompt + test question (“question”), \n",
      "  * generated answer (“generated_answer”), \n",
      "  * true answer (“answer”) . A shortened output example is as follows:\n",
      "---\n",
      "Prompt optimization is a black-box optimization problem, as it can only be evaluated based on the quality of the generated rationales or the correctness of the answers provided by LLM. In this regard, we present our empirical findings on how to design f to update prompting constrained by the limit input sequence length based on two attributes of CoT: correctness and depth. Correctness is a crucial criterion for prompting engineers to update the prompting. Here, we want to ask the question: Can a valid rationale be replaced with an invalid one without performance drop? Depth refers to the number of reasoning steps which can be reflected by the length of CoT (different steps are separated by comma or \\n).  A deeper CoT is longer and typically contains more complex rationales, while a shallower CoT is more straightforward and contains fewer reasoning steps. Here, the question is: Given the similar question, can a deep CoT always be replaced by a shallow CoT?It is non-trivial to discuss about whether prompting should be composed by more similar questions or diversified questions. [autocot] discovers a prompting with diversified demonstrations can outperform prompting with similar demonstrations. However, under the streaming setting\n",
      "---\n",
      "A noteworthy phenomenon is that the inclusion of few-shot prompting during sampling seems to dramatically reduce “drift” where later rationales become increasingly dissimilar from the initial few-shot set of rationales. One benefit of this is that the model may be less constrained by the quality and difficulty of the initial rationales, theoretically allowing it to generalize more. One potentially negative consequence is that the style of the rationales may less-closely match the original prompting style. Another benefit is in terms of computational resources - a shorter prompt length allows for a shorter sequence length when sampling. Technically, the point in training at which we “disable” few-shot prompts is another hyperparameter which we could tune, but we leave this to future work. In addition, by leaving prompts out after the initial outer-loop iteration, the model tends to perform gradually worse at rationalization as it trains for longer periods of time. As a result, it may be necessary to include some hints during training for long periods of time with this approach.Ultimately, the choice to include few-shot prompts in later iterations of training appears to depend on the use-case: when the goal is consistent adherence to a particular prompt style, which may benefit explainability, include few-shot prompts in sampling; when the goal is a faster training loop, one may remove them. Moreover, it is possible that with other datasets or larger models there is an impact on performance, so we encourage this to be generally treated as a hyperparameter.\n",
      "---\n",
      "We propose Faithful CoT, a framework that decomposes complex reasoning into Translation and Problem Solving. During Translation, an LM produces a reasoning chain in the form of interleaved natural and symbolic language. The Problem-Solving stage calls an external solver that executes the reasoning chain and derives the final answer. This process guarantees that the reasoning chain is a faithful explanation of how the model arrives at the answer. We demonstrate the efficacy of our approach on 4 types of complex reasoning problems: Math Word Problems, Multi-hop QA, Planning, and Logical Inference. Our method sets new SOTA performance on 7 of the 10 datasets, while additionally providing a faithful explanation for the final answer. These results give empirical evidence that improving model interpretability, by guaranteeing the faithfulness of an explanation, does not come at the expense of overall performance; in fact, we see a strong synergy in between. Through a comprehensive analysis on the strengths and weaknesses of our method, we show its robustness to the choice of exemplars, the pivotal role of the solver, as well as frequent error patterns where it still struggles.One limitation of our work is that the Translation stage is still opaque, leaving an open question about whether it is possible to improve its faithfulness as well. Moreover, it will be helpful to perform a human evaluation on the correctness of the generated reasoning chains. Finally, the NL comments in the reasoning chain can serve as an interface for users without a programming background to interactively debug the model, which should be explored in future work.§ ACKNOWLEDGEMENTS\n",
      "---\n",
      "In-context Learning Recently, a collection of works has emerged exploring the capacity for large language models to perform in-context learning [brown2020language,wei2021finetuned]. In essence, in-context learning treats few-shot learning as a language modeling problem, by showing a few examples in the context (i.e. prompt), and allowing the model to learn and identify the pattern to apply to new examples. Some have studied in-context learning based on the language modeling objective in terms of Bayesian inference [xie2021explanation] while others have attempted to describe the process more mechanistically in terms of “induction heads” [olsson_elhage]. Moreover, differences in prompt configurations have been known to have dramatic effects on few-shot performance. Some have even found that replacing few-shot prompts with a “soft prompt” which can be optimized in embedding space results in noticeable gains [lester2021power]. Instead of emphasizing the representation of the question, we focus on the model output; in particular, we focus on the model's ability to reason through a problem before coming to a conclusion. \n",
      "\n",
      "  \n",
      "Rationales One of the initial works on the impact of rationales on language model performance was [rajani2019explain], showing that training a language model on a dataset with explicit rationales preceding the answer could improve a model's ability to generate the final answer. However, this required many thousands of training examples to be manually annotated with human reasoning. Recently, [nye2021show] demonstrated that step-by-step “scratchpads” can improve fine-tuned LLM performance and generalization on tasks such as arithmetic, polynomial evaluation, and program evaluation. Similarly, [wei2022chain] used a single few-shot “chain-of-thought” reasoning prompt in order to improve model performance on a collection of tasks, without fine-tuning. Finally, [polu2022formal] showed that a curriculum learning approach could help solve formal math problems, as long as 1) they were translated into Lean (a theorem-proving language [moura2015lean]), 2) one could directly evaluate the validity of the proofs, 3) one could sample numerous potential solutions for each problem, 4) had trained a separate value function model, and 5) started with GPT-f (a model already fine-tuned on a large math dataset [polu2020generative]). We note that there are many domains where these conditions do not all apply. In addition, works have aimed to explain why rationales have this beneficial effect: some have analyzed their impact from the perspective of latent variable models [zhou2020towards] while others have provided formal proofs of the benefit of intermediate task supervision [wies2022sub].\n",
      "---\n",
      "*           We experiment with  [cobbe2021training] (1319 samples).          The dataset contains math word problems geared toward an average middle-school curriculum.          \n",
      "  *           We use date understanding (, 349 samples) and sports understanding (, 980 samples) as representative tasks for commonsense reasoning, both derived from [date:sport].          \n",
      "  *           We experiment with sorting (, 500 samples) a list of single-digit integers.           We do not associate explicit instruction (sort these numbers) with the questions.          Instead, we frame the questions as a challenging setup in which the model should figure out the task and the requisite information to solve it.In this work, we neither train any of the \\palm models, nor performs finetuning.We solely perform inference on \\palm variants using TPU v4~[tpuv4,jouppi2017datacenter].For \\mpalm, we use 4$\\times$4$\\times$4 TPU v4 configuration, whereas, for \\lpalm we use 4$\\times$4$\\times$16 mesh configuration.To account for the variation in results caused by the order of examples in the prompt, we conduct each experiment three times, each with different seeds, and report the average task solve rate.Following [wei2022chain], we evaluate each task using accuracy \\ie fraction of examples where the output matched the expected result.\\niparagraph{Public large language models.}We use OpenAI API\\footnote{\\href{https://openai.com/blog/openai-api/}{https://openai.com/blog/openai-api/}, v0.23.0} to conduct experiments with \\gptt (\\bench{text-davinci-002}) and \\codex (\\bench{code-davinci-002}).\\input{tables/task_examples}Based on our initial findings, this section explores the idea of engineering effective prompts tailored to remove ineffectual tokens.The benefit of such tailored design is multi-fold: (1) “reducing noise”:reducing noise from prompts could potentially lessen the confusion for the model and lead to a higher task solve rate and(2) “potential energy savings”:the reduced number of tokens in the prompts instruct the model to generate less number of tokens per question.Less number of generated tokens directly translates to reduced runtime and carbon footprint per inference [strubell2019energy].This is especially crucial because of the significant compute cost of large language models.Our proposed prompts, dubbed as , distill the essence of our hypothesis by supplying minimal requisite information to drive the model to factual conclusions.<Ref> compares and across different tasks and four variants of s.In all tasks, except , we use identical examples in the prompt and only rephrased the thoughts to use less number of tokens.For , we could not find a systematic mechanism to shorten the thoughts.Instead, we randomly harvest questions from the training set whose thoughts are shorter than .Overall, outperforms while employing prompts with fewer tokens.The task solve rate of remains relatively high as we scale the model to , highlighting the efficiency of .Appendix-<Ref> compares the average number of input/output tokens between and .On average, (our approach) reduces the number of input (1.39×) and output tokens (1.58×).This work evaluates the capacity of to elevate complex reasoning in three state-of-the-arts s, , , and .We systematically assembled a series of controlled counterfactual experiments.Our results show the initial inklings of connection between text, patterns, and reasoning in s.Our study indicates that the symbiosis of text and patterns bears more weight in the chain of thought reasoning process.In addition, we assert that text is a channel to extract semantic patterns, unlocking the ability of these models to mold correct answers.Large language models.To facilitate conducting an exhaustive number of experiments, we center the main analysis of this paper around [The results in [wei2022chain] reinforce that is the smallest model with significant gains across a range of reasoning tasks.].For reproducibility, we also conduct the experiments on publicly available models such as and .We present results from in the main body and defer the results from other models to <Ref>.Nonetheless, our findings concur across the studied s.Reasoning tasks.We focus on reasoning tasks for which presents ample improvements over prompting [wei2022chain], namely  ( [cobbe2021training]),  (date and sports understanding [date:sport]), and  ()—details in <Ref> and Appendix-<Ref>.This work intends to tease apart the major semantic components of a prompt that play a critical role in the efficacy of .To achieve this, we identify and systematically construe three key semantic components of a prompt, listed as follows (See Appendix-<Ref> for the examples to which we refer in the definitions):* are sequences of tokens in the prompt, about which the model reasons to solve a task.For and , symbols are numerical quantities (5, 4, 2, 13).Similarly for the dataset, we categorize players and activities as symbols. We define the symbols in the dataset as date and time indicating expressions.* are either composition of symbols and operators or a structure of prompt that reinforces task understanding.The isolation of patterns within a prompt is evident in datasets like (equations), (person is a sport, activity is a sport), and (1 less than 2).However, for the dataset, the pattern is semantically implicit but consistent.Each thought contains two parts: (a) ⟨calculation⟩ in which the information from the input is restated (“Today is 06/02/1943”) and intermediate results are generated (“One day after 06/01/1943 is 06/02/1943”) through mathematical calculations and (b) ⟨output⟩ in which the final requisite answer is generated using the intermediate results (“10 days before today is 05/23/1943”).* are tokens that are neither symbols, nor part of patterns.Specifically, text in prompts assists in eitheroutlining the target task (is the sentence plausible),connecting patterns to symbols (John is left with 4 - 2 = 2), or contextualizing symbols (4 toys).In a nutshell, text is the conceptual glue that binds different parts of a prompt.Relying on our analysis, we conjecture and hypothesize about the effects of each semantic components on the outcomes of prompting.We discuss detailed results, including statistical significance tests in <Ref>.The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence [lake2017building].This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data.Recently, large language models (s) [palm:arxiv,brown2020language,codex] have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning.Few-shot learning has shown promising applications for a wide range of tasks [gem,flan,bigscience,lamda,liu_what_2021,reif2021recipe,wang2020generalizing,chen_evaluating_2021,minerva,wu2022autoformalization].While beneficial, this setting requires meticulous design of prompts [le2021many,liu2021pre,mishra_reframing_2021].[ling2017program] pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning.Recently, [wei2022chain] proposed chain of thought () prompting, showing that the few-shot setting in s similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks [ling2017program,cobbe2021training,svamp,date:sport].Despite its wide-range usage, the rationale behind the success of remains unclear.Recent work draws [ling2017program,wei2022chain] parallels to human thinking.Humans often think about a problem before deducing a solution.Akin to this process, it is argued that models should also be able to employ a similar mechanism.While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate.Ultimately, s are trained to estimate the next token distribution for a given context.Therefore, there is presumably a systematic rationale behind their successes and failures.In this work, we undertake initial steps towards understanding the mechanism behind .Contributions and findings.We construct a series of tailored counterfactual prompts [counterfactual], deliberately sketched as controlled studies.First, we identify key components of an example in few-shot prompting as follows: Symbols, Patterns, and Text.Next, we perform counterfactual prompting—keeping all but one component fixed (replacing symbols (numbers) with Greek alphabets).Finally, we elicit meaningful findings via conducting a systematic and qualitative analysis of the performance divergence between different prompt queries.Our experiments on four diverse reasoning tasks and across three large language models—, , and , reveal several surprising findings:1 We find that the exact type of symbols in the prompt virtually does not affect the model performance.In addition, our results and analysis demonstrate counterintuitive phenomena.For example, we identify that the correctness of symbols and patterns is immaterial to the task solve rate.2We learn that patterns contribute chiefly as a venue to reinforce task understanding [ouyang2022training] and prompt the model to attain correct outputs.3Most importantly, we find that text and patterns form a symbiotic relationship that plays a vital role in the success of .Text helps generate useful patterns (by extracting commonsense knowledge), and patterns help reinforce task understanding, enabling the language model to generate text that helps solve the task.Overall, we argue that one of the primary reasons behind the success of is this interplay between text and patterns—helps a language model in imitating the prompt and generating the right tokens for the task—and is conceivably less related to their reasoning abilities.Finally, as indicated by applications such as -Saycan [ahn2022can], we posit that techniques like will play a key role in enabling the success of s on diverse use cases.Thus, designing efficient prompts informed by a set of key design principles is an important challenge.To this end, we distill our findings to create concise prompting, dubbed .prunes the prompt (20%↓) to only retain indispensable tokens without negative repercussions on the task solve rate.§ ETHICS STATEMENTDisseminating reasoning into machines has numerous benefits and applications, from algorithmic reasoning [alphacode] to code generation [chen_evaluating_2021,poesia2021synchromesh] and formal verification [wu2022autoformalization].While this research does not directly enhance the reasoning capabilities of large language models, it identifies several systematic behavioral patterns in the functioning of few-shot models.Similar to any technological advances, this work has risks of detrimental societal impact.However, anticipating potential future downsides of such methods is challenging.More than ever, the research community's utmost responsibility is to acknowledge these risks candidly and reflect on practices and strategies to prevent potential harm.Environmental impact.Training large language models devour a nontrivial amount of compute resources, a limiting factor for frequent training.Few-shot prompting is an appealing solution for mitigating the unfavorable environmental impact of large language models by evading additional iterations of training and dataset collection.Ours and similar studies may lead to more effective prompting techniques and bring technological innovation to the architecture of large language models, especially regarding their reasoning capabilities.Therefore, we hope that the significant compute used in this work can help promote positive environmental outcomes.Finally, aligned with the credible concerns of the research community, we recognize the longer-term risk of Excellence in artificial intelligence, primarily when it boils down to human reasoning.While distilling comparable human reasoning to machines offers many benefits, undisciplined and uncontrolled progress in this area could be alarming, especially in the presence of bad actors.Effective reasoning in machines as a result of our study, even though not directly, can lead to algorithmic advances that may facilitate bad actors in developing malicious software and systems with human-level capabilities.We also want to acknowledge a large body of researchers that has greeted innovations in large language models and steady scaling of models with skepticism, questioning the connection between human reasoning and large language models [han2022human,cog:gpt3].Additionally, recent efforts have started exploring the relationship between the structure of contemporary language models and the human mind [schrimpf2021neural,tang2021sensory,whittington2021relating].Despite their progress, the rationale behind 's mimicking human reasoning, and any potential connection with linguistics is so far an uncharted territory.The primary objective of our study is to understand through counterfactual prompting and empirically establish the underpinnings of the reasoning ability of s in the presence of .Each counterfactual prompt C_f(p) alters only one particular aspect of the in-context examples ⟨ x_k · t_k · y_k⟩ in a .For example, consider a sample thought for the math world problems in the dataset (See <Ref>).A symbolic counterfactual prompt, C_symbolic(p), may simply replace all the numbers in the thoughts with symbols (X1).Such analysis enables us to ask: “what would the performance of the model have been, if all the numbers in the prompt were replaced with symbols?”.Analyzing the performance disparity of a on C_symbolic(p) vs. p can thus indicate the role that using actual numbers plays in the success or failure of a task[Appendix-Table <ref> outlines a set of major “what if” questions that we study in this work.].The ability to successfully complete prompts p for complex reasoning tasks is typically present for at the scale of , , and .Nonetheless, we do not make any assumptions about the underlying model architecture.In summary, our study on reveals that for solving math problems, neither the presence of numbers, nor the credibility of the thoughts is paramount to the success of .Similarly, altering the style and wording of the texts in the thoughts has a modest impact on the model performance.Nevertheless, eradicating either of these components nullifies the efficacy of .Finally, a per-layer analysis of the model reveals that if the model performs similarly for a pair of counterfactual prompts C_f(p) vs. p, then the attention patterns are comparable as well.We defer the detailed background to Appendix <ref>.Limitations of counterfactual prompting.Relying on counterfactual examples could be misleading and precarious [laugel2019dangers,slack2021counterfactual].Nonetheless, counterfactual explanation presents a channel to gain insights into the workings of the model.This approach potentially yields more favorable explanations for state-of-the-art s.Notably, unlike fine-tuned methods, one can readily identify and collect a set of prompts that are critical for the model to generate particular outputs.In particular, few-shot prompting augments the model with an additional dimension to calibrate the accuracy to a discernible degree.Thus, we deduce that the counterfactual examples that exhibit consistent and systematic performance divergence are more prone to reflect credible interpretations of the model.In this work, we neither rely on the results that do not exhibit such characteristics, nor reject prompts that pose contradictory observations. We discuss additional limitations in <Ref>.Broadly, this paper intersects with a growing body of work on prompting and large language model reasoning [brown2020language,palm:arxiv].Below, we review the most relevant work in these directions.\n",
      "---\n",
      "Evaluator The Evaluator component of the Reflexion framework plays a crucial role in assessing the quality of the generated outputs produced by the Actor. It takes as input a generated trajectory and computes a reward score that reflects its performance within the given task context. Defining effective value and reward functions that apply to semantic spaces is difficult, so we investigate several variants of the Evaluator model. For reasoning tasks, we explore reward functions based on exact match (EM) grading, ensuring that the generated output aligns closely with the expected solution. In decision-making tasks, we employ pre-defined heuristic functions that are tailored to specific evaluation criteria. Additionally, we experiment with using a different instantiation of an LLM itself as an Evaluator, generating rewards for decision-making and programming tasks. This multi-faceted approach to Evaluator design allows us to examine different strategies for scoring generated outputs, offering insights into their effectiveness and suitability across a range of tasks.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [03:41,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 109, in run\n",
      "    await self.visit(self.parse_tree, VariableStack([self.program._variables], self))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 524, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_assistant.py\", line 13, in assistant\n",
      "    return await role(role_name=\"assistant\", hidden=hidden, _parser_context=_parser_context, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_role.py\", line 17, in role\n",
      "    new_content += await parser.visit(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 559, in visit\n",
      "    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in visit\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 266, in <listcomp>\n",
      "    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/_program_executor.py\", line 379, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/library/_gen.py\", line 140, in gen\n",
      "    gen_obj = await parser.llm_session(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 665, in __call__\n",
      "    out = await self.llm.caller(**call_args)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/guidance/llms/_openai.py\", line 370, in _library_call\n",
      "    out = await openai.ChatCompletion.acreate(**kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 728, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/nasimrahaman/miniconda3/envs/th21310/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 8236 tokens. Please reduce the length of the messages.\n",
      "\n",
      "Error in program:  This model's maximum context length is 8192 tokens. However, your messages resulted in 8236 tokens. Please reduce the length of the messages.\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the primary focus of work on scaling laws in the context of LLMs?\n",
      "--------------------------------------------------\n",
      "Answer A ($24.07):\n",
      "The art of training large artificial neural networks has made extraordinary progress in the last decade, especially after the discovery of the Transformer architecture [Vas17], yet the science behind this success remains limited. Amidst a vast and confusing array of results, a semblance of order emerged around the same time as Transformers were introduced, namely that performance improves somewhat predictably as one scales up either the amount of compute or the size of the network [hestness2017deep], a phenomenon which is now referred to as scaling laws [kaplan2020scaling]. The subsequent exploration of scale in deep learning was guided by these scaling laws [gpt3], and discoveries of variants of these laws led to rapid jump in performances [hoffmann2022an]. In this work, following the footsteps of Eldan and Li [eldan2023tinystories], we explore the improvement that can be obtained along a different axis: the quality of the data. It has long been known that higher quality data leads to better results, e.g., data cleaning is an important part of modern dataset creation [raffel2020exploring], and it can yield other side benefits such as somewhat smaller datasets [longpre2023pretrainer, yu2023selective] or allowing for more passes on the data [muennighoff2023scaling]. The recent work of Eldan and Li on TinyStories (a high quality dataset synthetically generated to teach English to neural networks) showed that in fact the effect of high quality data extends well past this: improving data quality can dramatically change the shape of the scaling laws, potentially allowing to match the performance of large-scale models with much leaner training/models. In this work we go beyond the initial foray of Eldan and Li to show that high quality data can even improve the SOTA of large language models (LLMs), while dramatically reducing the dataset size and training compute. Importantly, smaller models requiring less training can significantly reduce the environmental cost of LLMs [bender2021dangers].\n",
      "---\n",
      "Test-running frameworks Many of the benchmarks presented above use tests either as assistance in bug fixing or as a method of filtering bugs. We do not consider testing frameworks and tests as criteria for whether a commit is a bug or not. Firstly, it was recently shown that unit tests on their own do not guarantee fewer failures inside the code [chioteli2021does] which implies that there are even more bugs inside the code that are not exposed by tests. Secondly, we would be severely limiting the diversity and scope of our benchmark by forcing issues to include an exposing test case.Some samples from the synthetic dataset are presented in Appendix <ref>, and all artifact details can be found in Appendix <ref>.In this Section, we conduct a preliminary analysis of the hardness of the BigIssues benchmark. Since the sequence length exceeds the limitations of most pre-trained language models on code, we recruit mean pooling to construct simple baselines.  We hypothesize that although the realistic data is much harder than the synthetic dataset, using long-context encoders in addition to synthetic pre-training will help increase performance.The proposed BigIssue benchmark contains two variants: (1) synthetic rewrites of real code sampled from a strong generative model, (2) realistic rewrites of real code based on the commits associated with a closed issue in GitHub.Recall, for (1) a recent large language model was recruited as a sampler which, compared to prior art, not only is of significant size under scaling laws, but furthermore alters the causal masking such that future tokens can be taken into account as context. We argue that these synthetic rewrites are non-trivial to detect compared to prior art.\n",
      "---\n",
      "What do our results imply about LMs' abilities for in-context learning? The process by which LMs adapt to a prompt is debated. For example, [min2022rethinking] make the interesting observation that GPT-3 can perform nearly as well on common tasks like entailment even with random answers in the prompt, and therefore question whether the models are really “learning” in context. Comparisons between our explanations and matched controls allow us to test whether superficial features of the explanations drive their benefits.\n",
      "---\n",
      "We focus our attention on LLMs trained for code, and specifically writing simple Python functions from their docstrings as in [humaneval]. The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMs' performance on code. We demonstrate the power of high quality data in breaking existing scaling laws by training a 1.3B-parameter model, which we call phi-1, for roughly 8 passes over 7B tokens (slightly over 50B total tokens seen) followed by finetuning on less than 200M tokens. Roughly speaking we pretrain on “textbook quality” data, both synthetically generated (with GPT-3.5) and filtered from web sources, and we finetune on “textbook-exercise-like” data. Despite being several orders of magnitude smaller than competing models, both in terms of dataset and model size (see Table <ref>), we attain 50.6% pass@1 accuracy on HumanEval and 55.5% pass@1 accuracy on MBPP (Mostly Basic Python Programs), which are one of the best self-reported numbers using only one LLM generation. In Section <ref>, we give some details of our training process, and we discuss evidence for the importance of our data selection process in achieving this result. Moreover, despite being trained on much fewer tokens compared to existing models, phi-1 still displays emergent properties. In Section <ref> we discuss these emergent properties, and in particular we confirm the hypothesis that the number of parameters plays a key role in emergence (see e.g., [stack2022emergent]), by comparing the outputs of phi-1 with those of phi-1-small, a model trained with the same pipeline but with only 350M parameters. The methodology used in this section is reminiscent of the Sparks of AGI paper [sparks] that argued for moving away from static benchmarks to test LLMs' performance. Finally in Section <ref> we discuss alternative benchmarks to evaluate the model and in Section <ref> we study possible contamination of our training data with respect to HumanEval.\n",
      "---\n",
      "CodexDB is enabled by recent advances in the domain of natural language processing. Those advances have been fuelled by two key ideas: a novel neural network architecture, the Transformer [Vaswani2017], and new training paradigms, implementing the idea of transfer learning [ruder2019transfer]. The Transformer is nowadays the dominant architecture in the domain of language processing [Wolf2020]. Among other advantages, it lends itself better to parallelization than prior methods. This has, in part, enabled the creation of very large, pre-trained language models. Such models are pre-trained on tasks for which large amounts of training data are easily available, e.g. predicting the next word in text snippets. While pre-training is very expensive, the resulting models can be easily specialized for new tasks via different methods. Fine-tuning describes a process in which pre-trained models are used as a starting point for further training on more specialized tasks (reducing the amount of training samples and computational overheads by orders of magnitude via pre-training [Houlsby2019]). Until recently, fine-tuning has been the primary method of exploiting pre-trained language models. The latest generation of pre-trained models, most notably OpenAI's Generative Pre-Trained Transformer (GPT) version 3, unlocks new possibilities. It turns out that sufficiently large models can oftentimes solve new tasks without specialized training (“zero-shot learning”), based on inputs describing the task in natural language alone [Brown2020]. Precision increases if the input integrates few (i.e., typically less than ten) examples pairing tasks of the same type with solutions (“few-shot learning”). This is the method currently used by CodexDB. The final development that enabled this paper is the emergence of the Codex variant of GPT-3 [Chen2021, OpenAI2021]. The primary difference between GPT-3 Codex and the original GPT-3 model lies in the data used for pre-training. GPT-3 Codex is trained on code and technical documentation. This results in a model whose primary use case is the translation of natural language commands into code.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "We introduce Cerebras-GPT, our open effort to combine recent LLM efficient scaling techniques to produce compute-optimal pre-trained models and corresponding scaling laws. Cerebras-GPT is a family of GPT-3-like models that we scale from 111M to 13B parameters. We train them on the open-source dataset, the Pile [pile], following DeepMind's Chinchilla scaling rules [hoffmann2022chinchilla]. Cerebras-GPT models show state-of-the-art training efficiency when targeting both upstream Pile evaluations as well as a suite of downstream tasks. Our largest model shows state-of-the-art performance on pre-training and most downstream tasks compared to other comparably-sized public models. We also characterize some of the training stability challenges when scaling Cerebras-GPT. We address the challenges by training models with µP, which shows further accuracy improvements and hyperparameter predictability.\n",
      "---\n",
      "GPT-3 [brown2020gpt3] demonstrates the power of in-context learning, an adaptation method that does not need any parameter optimization. In the adaptation phase, the pretrained language foundation model takes in a prompta sequence of tokens that concatenates input-output examples from the taskfollowed by a test example and simply generates the label of the test example by conditioning on the sequence seen thus far (prompt plus test example).  In other words, there is no explicit training or change to the model parameters.   What is the mechanism by which the model “learns” from the different examples by simply executing with the examples as inputs? The previous modularization does not directly apply because we do not obtain new model parameters during adaptation, but rather we only use the generative capabilities of the foundation model by executing on structurally-designed inputs.  However, the idea of separating pretraining with infinite data and pretraining with finite data can still be useful. For example, a recent work starts with the assumption of infinite pretraining data and sufficient model expressivity to study in-context learning [xie2021incontext]. These assumptions reduce the characterization of in-context learning to a matter of analyzing the pretraining distribution conditioned on in-context learning prompts, which are drawn from a different distribution than the pretraining data. In particular, [xie2021incontext] proposes that in-context learning emerges from long-term coherence structure in the pretraining distribution, which is described by a latent variable generative model with coherence structure.  More broadly, while the modularization proposed in this section provides a nice framework to gain useful theoretical insights into foundation models, it is possible that some emergent behavior like in-context learning and other capabilities yet to be discovered would require going beyond the modularization, by opening the black box of the architecture.theory-tools\n",
      "--------------------------------------------------\n",
      "Answer B ($43.99):\n",
      "The art of training large artificial neural networks has made extraordinary progress in the last decade, especially after the discovery of the Transformer architecture [Vas17], yet the science behind this success remains limited. Amidst a vast and confusing array of results, a semblance of order emerged around the same time as Transformers were introduced, namely that performance improves somewhat predictably as one scales up either the amount of compute or the size of the network [hestness2017deep], a phenomenon which is now referred to as scaling laws [kaplan2020scaling]. The subsequent exploration of scale in deep learning was guided by these scaling laws [gpt3], and discoveries of variants of these laws led to rapid jump in performances [hoffmann2022an]. In this work, following the footsteps of Eldan and Li [eldan2023tinystories], we explore the improvement that can be obtained along a different axis: the quality of the data. It has long been known that higher quality data leads to better results, e.g., data cleaning is an important part of modern dataset creation [raffel2020exploring], and it can yield other side benefits such as somewhat smaller datasets [longpre2023pretrainer, yu2023selective] or allowing for more passes on the data [muennighoff2023scaling]. The recent work of Eldan and Li on TinyStories (a high quality dataset synthetically generated to teach English to neural networks) showed that in fact the effect of high quality data extends well past this: improving data quality can dramatically change the shape of the scaling laws, potentially allowing to match the performance of large-scale models with much leaner training/models. In this work we go beyond the initial foray of Eldan and Li to show that high quality data can even improve the SOTA of large language models (LLMs), while dramatically reducing the dataset size and training compute. Importantly, smaller models requiring less training can significantly reduce the environmental cost of LLMs [bender2021dangers].\n",
      "---\n",
      "Scaling laws of large language models [kaplan2020scaling] investigated scaling laws of Transformer language models and first showed the scaling laws are predictive of future performance. The authors found that model size (and not shape) correlates strongly with model quality, i.e., upstream cross entropy. [tay2021scale] studied the scaling properties of encoder-decoder models and their impact on upstream and downstream finetuning tasks. Generally, [tay2021scale] found that upstream perplexity and downstream quality does not always correlate. As a follow up, [tay2022scaling] studied the scaling laws of different model architectures and found that inductive bias does significantly impact the scaling behavior of the model. Finally, [hoffmann2022training] proposed compute-optimal models that popularized the `chinchilla' scaling laws - an approach that aims to be predictive of the optimal amount of data given the number of model parameters. In this work, we mainly consider scaling laws over downstream performance largely because this is more reflective of a language model's usability. Since downstream performance is more important than upstream cross entropy, we advocate for future scaling studies to always incorporate downstream evaluation (and metrics) as opposed to only using cross entropy loss.Emergent Abilities New behaviors that arise due to scaling language models have been increasingly referred to as emergent abilities [jacobsdefinition,ganguli2022predictability,wei2022emergent].\n",
      "---\n",
      "We focus our attention on LLMs trained for code, and specifically writing simple Python functions from their docstrings as in [humaneval]. The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMs' performance on code. We demonstrate the power of high quality data in breaking existing scaling laws by training a 1.3B-parameter model, which we call phi-1, for roughly 8 passes over 7B tokens (slightly over 50B total tokens seen) followed by finetuning on less than 200M tokens. Roughly speaking we pretrain on “textbook quality” data, both synthetically generated (with GPT-3.5) and filtered from web sources, and we finetune on “textbook-exercise-like” data. Despite being several orders of magnitude smaller than competing models, both in terms of dataset and model size (see Table <ref>), we attain 50.6% pass@1 accuracy on HumanEval and 55.5% pass@1 accuracy on MBPP (Mostly Basic Python Programs), which are one of the best self-reported numbers using only one LLM generation. In Section <ref>, we give some details of our training process, and we discuss evidence for the importance of our data selection process in achieving this result. Moreover, despite being trained on much fewer tokens compared to existing models, phi-1 still displays emergent properties. In Section <ref> we discuss these emergent properties, and in particular we confirm the hypothesis that the number of parameters plays a key role in emergence (see e.g., [stack2022emergent]), by comparing the outputs of phi-1 with those of phi-1-small, a model trained with the same pipeline but with only 350M parameters. The methodology used in this section is reminiscent of the Sparks of AGI paper [sparks] that argued for moving away from static benchmarks to test LLMs' performance. Finally in Section <ref> we discuss alternative benchmarks to evaluate the model and in Section <ref> we study possible contamination of our training data with respect to HumanEval.\n",
      "---\n",
      "What do our results imply about LMs' abilities for in-context learning? The process by which LMs adapt to a prompt is debated. For example, [min2022rethinking] make the interesting observation that GPT-3 can perform nearly as well on common tasks like entailment even with random answers in the prompt, and therefore question whether the models are really “learning” in context. Comparisons between our explanations and matched controls allow us to test whether superficial features of the explanations drive their benefits.\n",
      "---\n",
      "In the second stage, we promote and regulate in-context chain-of-thought reasoning in LLMs to solve complex games.  The reasoning module is a directed acyclic graph (DAG), with questions as nodes and dependencies as edges.  For example, the question ``For each action, are the requirements met?\" depends on the question ``What are the top 5 actions?\", creating  an edge from the latter to the former.  For each environment step, we traverse the DAG computing LLM answers for each node in the topological order of the graph. The final node of the DAG is a question about the best action to take and the LLM answer for the question is directly translated to environment action.Qualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can execute sophisticated trajectories independently in. Quantitatively, \\ourmethod{}'s zero-shot performance with GPT-4 surpassing all state-of-the-art RL algorithms trained for 1M steps (Table~\\ref{table:compare}).\n",
      "---\n",
      "GPT-3 [brown2020gpt3] demonstrates the power of in-context learning, an adaptation method that does not need any parameter optimization. In the adaptation phase, the pretrained language foundation model takes in a prompta sequence of tokens that concatenates input-output examples from the taskfollowed by a test example and simply generates the label of the test example by conditioning on the sequence seen thus far (prompt plus test example).  In other words, there is no explicit training or change to the model parameters.   What is the mechanism by which the model “learns” from the different examples by simply executing with the examples as inputs? The previous modularization does not directly apply because we do not obtain new model parameters during adaptation, but rather we only use the generative capabilities of the foundation model by executing on structurally-designed inputs.  However, the idea of separating pretraining with infinite data and pretraining with finite data can still be useful. For example, a recent work starts with the assumption of infinite pretraining data and sufficient model expressivity to study in-context learning [xie2021incontext]. These assumptions reduce the characterization of in-context learning to a matter of analyzing the pretraining distribution conditioned on in-context learning prompts, which are drawn from a different distribution than the pretraining data. In particular, [xie2021incontext] proposes that in-context learning emerges from long-term coherence structure in the pretraining distribution, which is described by a latent variable generative model with coherence structure.  More broadly, while the modularization proposed in this section provides a nice framework to gain useful theoretical insights into foundation models, it is possible that some emergent behavior like in-context learning and other capabilities yet to be discovered would require going beyond the modularization, by opening the black box of the architecture.theory-tools\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [04:24, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "In the context of LLMs (Lifelong Machine Learning), the primary focus of work on scaling laws is to understand how the performance of machine learning models improves as the amount of data or computational resources increases. This includes studying how the size of the model, the amount of training data, or the computational budget affects the learning efficiency, accuracy, and generalization capabilities of the model. The goal is to optimize these factors to achieve better performance and efficiency in lifelong learning scenarios, where the model continuously learns from new data over time.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: Looking at both answers, I think student A's answer is more comprehensive and detailed. It provides a thorough explanation of the scaling laws in the context of LLMs, including the impact of data quality and the environmental cost of training large models. \n",
      "\n",
      "Bobby: I agree that student A's answer is detailed, but it seems to me that it goes off on a tangent at times. It delves into the specifics of their work, which, while interesting, doesn't directly answer the question. On the other hand, student B's answer is more concise and to the point, focusing on the scaling laws and their impact on model performance.\n",
      "\n",
      "Michael: That's a fair point, Bobby. However, I believe that the additional details provided by student A help to paint a more complete picture of the topic. It's not just about the scaling laws themselves, but also about how they are applied and the effects they have on different aspects of machine learning.\n",
      "\n",
      "Bobby: I see where you're coming from, Michael. But the question asks about the primary focus of work on scaling laws in the context of LLMs. While the additional details are informative, they might not be directly relevant to the primary focus of the work. I think student B's answer does a better job at addressing the question directly.\n",
      "\n",
      "Michael: I understand your point, Bobby. However, I still believe that the depth of student A's answer provides a more comprehensive understanding of the topic. The additional details might not be directly related to the primary focus, but they are still relevant to the overall context of scaling laws in LLMs.\n",
      "\n",
      "Bobby: I see your point, Michael. After considering your arguments, I agree that the depth of student A's answer provides a more comprehensive understanding of the topic. Even though some details might not be directly related to the primary focus, they are still relevant to the overall context of scaling laws in LLMs.\n",
      "\n",
      "VERDICT: Better answer from student A.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "verdicts_bpairs = run_matches_between_budget_pairs(shared_qs, full_df, [[25, 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# _ = dump_dict(verdicts_bpairs, \"/Users/nasimrahaman/Python/info-bazaar/notebooks/assets/verdicts_gpt4_heuristic-budget-25-vs-100.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T14:33:51.803935Z",
     "start_time": "2023-11-15T14:33:51.745697Z"
    }
   },
   "id": "9c29dcb487ad2ed5"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "739a7674-3c6e-4217-94b1-e40c7ea850c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T14:42:03.412338Z",
     "start_time": "2023-11-15T14:42:03.353248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([1 for v in verdicts_bpairs if v[\"verdict\"] == \"B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "9385112c-ae6f-4799-abb6-2866e715c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_dict(verdicts_bpairs, \"./assets/verdicts_gpt4_fup-v3_budgets-50-200.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "de16da09-70d6-467b-a225-b7196d849f40",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What are the potential consequences of using LLMs in high-stakes applications without proper interpretability?\n",
      "--------------------------------------------------\n",
      "Answer A ($24.88):\n",
      "The potential consequences of using LLMs in high-stakes applications without proper interpretability include a lack of transparency and explainability, potential bias and discrimination, and security risks. However, the extent to which these consequences can be generalized to all high-stakes applications that use LLMs is unclear and requires further investigation.\n",
      "\n",
      "To address the concerns of bias and discrimination, it is essential to ensure that the data used to train LLMs is diverse and representative of all groups of people, and that the models are designed with fairness and equity in mind. Additionally, implementing fairness-aware regularization techniques, such as adversarial training, can help reduce biases in the model's predictions. Moreover, using evaluation metrics that measure fairness, such as demographic parity or equalized odds, can help identify biases in the model's performance. Finally, providing contextual information or debiasing techniques in the prompt can help mitigate biases in the model's responses.\n",
      "\n",
      "To address the issue of security risks, it is important to carefully evaluate the performance of LLMs on domain-specific tasks and ensure that they are able to provide accurate and reliable information. This can be achieved through the use of benchmark datasets, such as the one introduced in the first passage, which can help to identify areas where LLMs may struggle and require additional training or fine-tuning. Additionally, it may be necessary to implement additional checks and balances, such as human review or expert input, to ensure that the information provided by LLMs is accurate and trustworthy.\n",
      "\n",
      "To ensure that LLMs are trained on up-to-date and relevant data, it is important to use a combination of techniques to evaluate and improve the quality of the training data. This can include using data from multiple sources, ensuring that the data is relevant to the task at hand, and using techniques such as data augmentation and adversarial training to increase the robustness of the model. Additionally, it is important to regularly update the training data to reflect changes in the domain or task, and to use techniques such as active learning and transfer learning to improve the performance of the model.\n",
      "\n",
      "Finally, to balance the need for accurate and reliable information with the need to protect sensitive information and maintain confidentiality in high-stakes applications of LLMs, it may be necessary to implement measures such as watermarking or encryption to protect sensitive information, while still allowing for the use of LLMs to provide accurate and reliable information.\n",
      "\n",
      "In summary, the refined answer incorporates the insights obtained from the follow-up questions to provide a more comprehensive and nuanced understanding of the potential consequences of the lack of interpretability in LLMs used in high-stakes applications, and highlights potential solutions to address these concerns.\n",
      "--------------------------------------------------\n",
      "Answer B ($15.95):\n",
      "The potential consequences of using LLMs in high-stakes applications without proper interpretability include a lack of transparency and trustworthiness, which can limit their use in certain fields, as well as the risk of unreliable predictions due to their vulnerability to input noises and perturbations. Moreover, there are concerns regarding fairness, regulatory pressure, safety, and alignment.\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "LLMs, or Large Language Models, are powerful tools that can generate human-like text based on the input they receive. However, using them in high-stakes applications without proper interpretability can lead to several potential consequences:\n",
      "\n",
      "1. Misinformation: If the LLM is not properly trained or interpreted, it could generate incorrect or misleading information. This could have serious consequences in high-stakes applications like healthcare, finance, or legal advice.\n",
      "\n",
      "2. Bias: LLMs learn from the data they are trained on. If this data contains biases, the LLM can perpetuate and even amplify these biases. This could lead to unfair or discriminatory outcomes.\n",
      "\n",
      "3. Lack of Transparency: Without proper interpretability, it can be difficult to understand why an LLM is making certain decisions or predictions. This lack of transparency can make it hard to trust the LLM's outputs, especially in high-stakes situations.\n",
      "\n",
      "4. Legal and Ethical Issues: If an LLM makes a decision that leads to harm or damage, it can be difficult to determine who is responsible. This could lead to legal and ethical issues.\n",
      "\n",
      "5. Security Risks: LLMs can be manipulated or exploited by malicious actors. Without proper interpretability, these security risks could go unnoticed until it's too late.\n",
      "\n",
      "6. Over-reliance: Without understanding how an LLM works, people might over-rely on its outputs, ignoring other important factors or inputs.\n",
      "\n",
      "Therefore, it's crucial to ensure proper interpretability when using LLMs, especially in high-stakes applications. This can help mitigate these risks and ensure that the LLM is being used responsibly and effectively.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is more comprehensive. They've addressed the question directly and provided a detailed explanation of the potential consequences of using LLMs without proper interpretability. They've also suggested solutions to mitigate these issues.\n",
      "\n",
      "Bobby: I agree that student A's answer is detailed, but it's also quite verbose and goes off on tangents. Student B's answer, on the other hand, is concise and to the point. They've listed the potential consequences and kept their answer focused on the question.\n",
      "\n",
      "Michael: Yes, but student A's answer provides a more nuanced understanding of the issue. They've not only listed the potential consequences but also explained them in detail. They've also highlighted the importance of fairness, security, and data quality, which aligns with the gold standard answer.\n",
      "\n",
      "Bobby: That's true, but student B's answer is more in line with the gold standard answer in terms of structure and content. They've listed the potential consequences in a clear and concise manner, which makes their answer easier to understand.\n",
      "\n",
      "Michael: I see your point, but I still believe that student A's answer is better. They've provided a more thorough and insightful answer, which shows a deep understanding of the topic.\n",
      "\n",
      "Bobby: I understand where you're coming from, but I think student B's answer is more effective. It's concise, clear, and directly addresses the question, which makes it a better answer in my opinion.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the primary focus of work on scaling laws in the context of LLMs?\n",
      "--------------------------------------------------\n",
      "Answer A ($17.57):\n",
      "The primary focus of work on scaling laws in the context of large language models (LLMs) is to ensure the consistency and reliability of model decisions, particularly in legal predictions. However, the scaling laws also aim to address the challenge of efficiently learning a mapping between the embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, which can boost image generation for tasks that require stronger language context dependence. There are limitations to the generator's expressivity and the potential for bias in synthetic datasets, which must be considered when implementing these laws. Additionally, practical applications are bound by costs linked to computation and storage, as well as the moderation of the content fueling the generator. Therefore, while scaling up LLM backbones, image generation backbones, or visual processing models could be promising directions, it is essential to be aware of these challenges and limitations.\n",
      "\n",
      "\n",
      "The initial answer focused mainly on the consistency of model decisions, but the follow-up questions provided additional information about the specific challenges and limitations of implementing scaling laws in the context of LLMs. The refined answer incorporates this information to provide a more comprehensive understanding of the primary focus of work on scaling laws in this context. It highlights the aim of scaling laws to address the challenge of learning a mapping between embeddings and boost image generation, while also acknowledging the limitations and potential biases associated with synthetic datasets and practical applications.\n",
      "--------------------------------------------------\n",
      "Answer B ($35.02):\n",
      "The primary focus of work on scaling laws in the context of LLMs is to investigate the effectiveness and reliability of LLMs on various tasks, such as using chain-of-thought prompting to improve performance on benchmark tasks and aiding humans in collectively producing written opinions that maximize approval rates among users, and to develop strategies for improving their performance on those tasks.\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "In the context of LLMs (Lifelong Machine Learning), the primary focus of work on scaling laws is to understand how the performance of machine learning models improves as the amount of data or computational resources increases. Scaling laws can help to predict the efficiency and effectiveness of machine learning algorithms as they are exposed to more data or given more computational power. This is particularly important in LLMs, where the system is expected to learn continuously from a stream of data over its entire lifetime. The goal is to develop models that can scale effectively and efficiently, improving their performance over time without requiring excessive resources.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is more comprehensive. They've touched on the challenges and limitations of implementing scaling laws in the context of LLMs, which is a crucial aspect of the question.\n",
      "\n",
      "Bobby: I disagree, Michael. While student A's answer is indeed comprehensive, it seems to veer off the main focus of the question. They've delved into the specifics of image generation and synthetic datasets, which isn't the primary focus of scaling laws in LLMs.\n",
      "\n",
      "Michael: That's a fair point, Bobby. But don't you think that student A's mention of the challenges and limitations provides a more rounded understanding of the topic?\n",
      "\n",
      "Bobby: It does, but the question asks for the primary focus. Student B's answer, while less detailed, directly addresses the question. They've mentioned the investigation of the effectiveness and reliability of LLMs on various tasks, which aligns more with the gold standard answer.\n",
      "\n",
      "Michael: I see your point, Bobby. The gold standard answer does indeed focus on the efficiency and effectiveness of machine learning algorithms as they scale. Student B's answer aligns more with this.\n",
      "\n",
      "Bobby: Exactly, Michael. While student A's answer is informative, it doesn't directly answer the question as well as student B's response.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "Can in-context learning occur when there's a distribution mismatch between prompts and pretraining data?\n",
      "--------------------------------------------------\n",
      "Answer A ($14.41):\n",
      "In-context learning can occur even when there's a distribution mismatch between prompts and pretraining data. ChatGPT's ability to adapt to an updated schema and deduce reasonable facts using a chain-of-thought approach suggests that it can learn from context, even when there's a distribution mismatch. This is because ChatGPT determines what information is relevant and reasonable to deduce when generating text by using a combination of natural language processing (NLP) techniques and machine learning algorithms to analyze the input prompt and generate a response that is contextually appropriate and coherent with the updated information.\n",
      "\n",
      "Furthermore, to ensure that a language model is fine-tuned on a diverse range of topics, styles, and formats, it is important to use a diverse range of training data that reflects the desired range of topics, styles, and formats. This can be achieved by collecting data from various sources, such as books, articles, and websites, that cover a wide range of topics and styles. Additionally, it may be helpful to use techniques such as data augmentation and adversarial training to further diversify the training data and improve the model's ability to generalize.\n",
      "\n",
      "Appropriate evaluation metrics that measure the model's ability to generalize to new and unseen data should also be used. This can include metrics such as accuracy, F1 score, and ROUGE score, as well as more specialized metrics that measure the model's ability to handle diverse topics, styles, and formats.\n",
      "\n",
      "For example, if a language model is fine-tuned on a dataset that includes a variety of medical questions and answers, it can learn to identify similarities and differences between different instances of data and improve its ability to generalize to new and unseen data. When applied to a specific medical question-answering task, the model can use its knowledge to provide accurate and relevant answers, even if the prompts are different from the training data.\n",
      "\n",
      "Therefore, while in-context learning can occur when there's a distribution mismatch between prompts and pretraining data, it is important to use a diverse range of training data and appropriate evaluation metrics to ensure that the language model is fine-tuned on a diverse range of topics, styles, and formats and can generalize to new and unseen data.\n",
      "--------------------------------------------------\n",
      "Answer B ($24.57):\n",
      "In-context learning can occur even when there's a distribution mismatch between prompts and pretraining data. However, the effectiveness of the instructions in improving LLM performance may vary depending on the specific task and the quality of the instructions. It's essential to evaluate the effectiveness of instructions for a given task and consider factors such as representational bias and toxicity when using LLMs for tabular prediction. We can evaluate the model's ability to perform explicit, rigorous, and coherent deductive reasoning, construct and evaluate compositional constraints, use a diverse set of training data and advanced pre-training techniques, and use suitable evaluation metrics to capture the diversity and semantic accuracy of the generated text. We should also identify and mitigate representational bias and toxicity in LLMs, especially when using them for tabular prediction, by evaluating their performance using a nuanced rubric, introducing adversarial testing datasets, adjusting the training data, incorporating debiasing and adversarial training, and implementing careful curation and filtering of the model's responses. By taking a multi-faceted approach that combines human evaluation, adversarial testing, and ethical considerations, we can better identify and mitigate representational bias and toxicity in LLMs, ensuring that they are aligned with human values and fairness.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 12.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "In-context learning can occur even when there's a distribution mismatch between prompts and pretraining data. However, the effectiveness of the learning might be impacted. The AI model might struggle to generalize or make accurate predictions if the prompts it's receiving during fine-tuning are significantly different from the data it was pretrained on. This is because the model learns patterns and structures from the pretraining data, and applies this knowledge to the prompts it receives later. If there's a large mismatch, the model might not have learned the necessary patterns to handle the new prompts effectively.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is better. It directly addresses the question and provides a detailed explanation of how in-context learning can occur even when there's a distribution mismatch between prompts and pretraining data. The student also discusses the importance of using a diverse range of training data and appropriate evaluation metrics, which aligns with the gold standard answer.\n",
      "\n",
      "Bobby: I disagree, Michael. While student A's answer is indeed detailed, it seems to go off on a tangent about fine-tuning language models on diverse topics, styles, and formats. This isn't directly relevant to the question. On the other hand, student B's answer, while also detailed, stays more focused on the question. It discusses the effectiveness of instructions, representational bias, and toxicity, which are all relevant to the question.\n",
      "\n",
      "Michael: I see your point, Bobby. However, I still believe that student A's answer is more coherent and easier to understand. It provides a clear explanation of how in-context learning can occur despite a distribution mismatch, which is the crux of the question.\n",
      "\n",
      "Bobby: I understand where you're coming from, Michael. But I think student B's answer does a better job at addressing the complexities of the question. It acknowledges that while in-context learning can occur, the effectiveness of the learning might be impacted, which aligns more closely with the gold standard answer.\n",
      "\n",
      "Michael: You're right, Bobby. Upon reconsideration, I agree that student B's answer does a better job at addressing the complexities of the question. It provides a more nuanced understanding of the topic.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the difference between LLaMA-Adapter and LLaMA-Adapter-V2?\n",
      "--------------------------------------------------\n",
      "Answer A ($3.40):\n",
      "Based on the information provided in the passages, I cannot confidently answer the question \"What is the difference between LLaMA-Adapter and LLaMA-Adapter-V2?\". The passages do not provide a clear explanation of the differences between the two, and it is not possible to infer the differences from the information provided.\n",
      "\n",
      "However, based on the context of the passages, it seems that LLaMA-Adapter and LLaMA-Adapter-V2 are related to the use of LLMs in natural language processing tasks, specifically in the context of chain-of-thought reasoning and deductive coding. LLaMA-Adapter-V2 is mentioned in Passage 2 as a variant of LLaMA-Adapter that is used for deductive coding, but the exact differences between the two are not explained.\n",
      "\n",
      "Therefore, I would recommend seeking additional information or clarification to answer the question fully.\n",
      "--------------------------------------------------\n",
      "Answer B ($25.03):\n",
      "LLaMA-Adapter and LLaMA-Adapter-V2 are two models with different purposes and capabilities. LLaMA-Adapter is a lightweight adaption method that fine-tunes LLaMA into an instruction-following model with only 1.2M learnable parameters, suitable for open-domain question answering with short-form answers. On the other hand, LLaMA-Adapter-V2 is a stronger chatbot model that can answer questions very well, developed by training LLaMA-Adapter on 80K conversation data. It is designed to provide more accurate and powerful chatbot responses, suitable for conversational tasks that require long-form responses, and is more powerful than LLaMA-Adapter. While LLaMA-Adapter focuses on efficiently adapting LLaMA into an instruction-following model, LLaMA-Adapter V2 is designed to provide more accurate and powerful chatbot responses.\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "I'm sorry, but as of my current knowledge update, I don't have specific information about LLaMA-Adapter and LLaMA-Adapter-V2. These could be specific software, tools, or modules within a certain field, but without more context, it's difficult to provide an accurate answer. It's also possible that these are very new or niche tools that haven't been widely documented yet. I would recommend checking the official documentation or reaching out to the creators for the most accurate information.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is better. They were honest about not having enough information to answer the question and suggested seeking additional information or clarification. \n",
      "\n",
      "Bobby: I disagree, Michael. Student B's answer, although it might not be entirely accurate, shows a clear understanding of the topic and an attempt to differentiate between the two models. They've provided specific details about the purpose and capabilities of each model.\n",
      "\n",
      "Michael: But Bobby, the gold standard answer suggests that there isn't enough information available about these models. Student A's answer aligns more with this. They've acknowledged the lack of information and suggested a way forward.\n",
      "\n",
      "Bobby: That's true, Michael. But the task here is to determine which answer does a better job at answering the question. Student B's answer, although speculative, provides a more direct response to the question. They've made an effort to differentiate between the two models, which is what the question asked for.\n",
      "\n",
      "Michael: I see your point, Bobby. While student A's answer is more cautious and honest, student B's answer does attempt to directly answer the question, even if it's speculative.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the purpose of cross-validation in machine learning?\n",
      "--------------------------------------------------\n",
      "Answer A ($24.33):\n",
      "The purpose of cross-validation in machine learning is to evaluate the performance of a model by testing it on multiple subsets of the available data, while ensuring that the model is not overly specialized to a particular subset. This helps to prevent overfitting, which occurs when a model is trained too well on the training data and performs poorly on new, unseen data. Cross-validation works by splitting the available data into multiple subsets, training the model on one subset while evaluating its performance on the remaining subsets, and repeating this process multiple times to estimate the model's generalization performance.\n",
      "\n",
      "In the context of ChatGPT, cross-validation can help ensure that the model is not overly specialized to a particular subset of the data by testing its performance on a variety of tasks and datasets. This can help prevent overfitting and improve the model's generalization ability, allowing it to adapt to new tasks and datasets more effectively.\n",
      "\n",
      "To use cross-validation effectively, it is important to use a sufficient number of folds, a consistent splitting method, and appropriate evaluation metrics. Moreover, it is essential to consider the dataset's characteristics and avoid overfitting by using techniques such as regularization, early stopping, or dropout.\n",
      "\n",
      "In addition to preventing overfitting, cross-validation can also help handle class imbalance and ensure that the validation set is representative of the entire dataset. For instance, oversampling, undersampling, and synthetic data generation are techniques that can be used to handle class imbalance. Random sampling and techniques such as data augmentation and feature scaling can help ensure that the validation set is representative of the entire dataset.\n",
      "\n",
      "While the passage does not provide a complete answer to the question of differences in cross-validation between neural networks and decision trees, we can explain that cross-validation is a technique that can be applied to various machine learning models, including neural networks and decision trees. The choice of cross-validation method depends on the specific problem and dataset, and there are various ways to implement cross-validation, such as k-fold cross-validation and leave-one-out cross-validation.\n",
      "\n",
      "In summary, cross-validation is a vital technique in machine learning that helps ensure the accuracy and reliability of models, even when working with sensitive data. It can be applied in various industries, and it is important to avoid common pitfalls and ensure that synthetic data generated by ChatGPT is representative and unbiased.\n",
      "--------------------------------------------------\n",
      "Answer B ($25.71):\n",
      "Cross-validation is a technique used in machine learning to evaluate the performance of a model by training and testing it on multiple subsets of the data. The purpose of cross-validation is to prevent overfitting, which occurs when a model is trained too well on the training data and performs poorly on new, unseen data. Cross-validation helps to estimate the model's generalization ability and ensures that the model is not biased towards a particular subset of the data. It also allows for the estimation of the model's performance with a certain level of confidence, as the process can be repeated multiple times with different subsets of the data. Ultimately, cross-validation helps to ensure that the model is robust and reliable, and its performance is not a one-time fluke. Cross-validation prevents overfitting by ensuring that the model attends to all relevant information in the input data, evaluates the model's performance on unseen data, and identifies areas where the model needs improvement. It is a more accurate assessment of the model's performance than other evaluation techniques, such as test sets or hold-out techniques, and helps to refine the model's attention to all relevant tokens or features. Cross-validation works in practice by splitting the dataset into training and validation sets and using the validation set to evaluate the model's performance during training. It provides a more robust estimate of the model's performance by obtaining an average performance metric across different folds of the validation set. Cross-validation is particularly important in medical AI due to the limited availability of labeled data, which makes it crucial to assess the model's performance accurately.\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "Cross-validation in machine learning is a technique used to assess the performance of a machine learning model on an independent data set, and to tune the parameters of the model. The main purpose of cross-validation is to prevent overfitting, which occurs when a model learns the training data too well and performs poorly on unseen data.\n",
      "\n",
      "In cross-validation, the data set is divided into two segments: one used to train the model and the other used to validate the model. During the cross-validation process, the training and validation sets can be rotated and reused to maximize the amount of data used.\n",
      "\n",
      "This process provides a more accurate measure of how well the model will perform on unseen data and helps in selecting the most suitable model by comparing the performance of different models.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is more comprehensive. They not only explain the purpose of cross-validation but also delve into how it can be used effectively, how it can handle class imbalance, and how it can be applied to various machine learning models. They also touch on the importance of cross-validation in the context of ChatGPT, which is a nice addition.\n",
      "\n",
      "Bobby: I see your point, Michael. However, I believe that student B's answer is more focused and directly addresses the question. They explain the purpose of cross-validation, how it prevents overfitting, and how it works in practice. They also mention its importance in medical AI due to the limited availability of labeled data. While student A's answer is indeed comprehensive, it seems to go off on tangents, discussing topics that are not directly related to the question.\n",
      "\n",
      "Michael: I agree that student B's answer is more focused, but I think the additional information provided by student A is valuable. It gives a broader understanding of the topic, which can be beneficial for students learning about machine learning. \n",
      "\n",
      "Bobby: Yes, but the question asks about the purpose of cross-validation, not how to use it effectively or handle class imbalance. While those are important topics, they are not directly related to the question. I believe that student B's answer is more concise and to the point, which makes it easier to understand.\n",
      "\n",
      "Michael: You're right, Bobby. The question does ask about the purpose of cross-validation, and student B's answer does a better job at addressing that directly. While the additional information provided by student A is valuable, it might be better suited for a different question.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the purpose of model selection in machine learning?\n",
      "--------------------------------------------------\n",
      "Answer A ($24.94):\n",
      "The purpose of model selection in machine learning is to choose the best model for a particular task or problem by evaluating different models and selecting the one that performs the best on a validation set or test data. Model selection is important because it can significantly impact the performance of a machine learning system. In few-shot learning, model selection is crucial as it allows the model to adapt to new tasks with only a handful of training examples. In addition, red teaming with automated searches for natural language prompts is another important aspect of model selection, which involves finding inputs that can elicit specific behaviors from a language model.\n",
      "\n",
      "\n",
      "To provide a more comprehensive answer, we can expand on the importance of model selection in machine learning. Model selection is a crucial step in the machine learning process as it ensures that the chosen model is well-suited for the task at hand, and can help to avoid overfitting or underfitting the data. It involves evaluating different models, such as linear regression, decision trees, or neural networks, and selecting the one that performs the best on a validation set or test data. This process can be done using various evaluation metrics, such as accuracy, precision, recall, F1 score, or AUC-ROC.\n",
      "\n",
      "\n",
      "Moreover, model selection is particularly important in few-shot learning, a subfield of machine learning that focuses on training models that can learn and generalize well on new tasks with only a handful of training examples. In this regime, the ability of large language models (LLMs) to perform well is quite phenomenal and has interesting implications for software engineering, particularly in dealing with project-specific linguistic phenomena.\n",
      "\n",
      "\n",
      "Furthermore, red teaming with automated searches for natural language prompts is another important aspect of model selection. It involves finding inputs that can elicit specific behaviors from a language model, such as outputting toxic or dishonest text. Efficiently automating prompt search is challenging due to the non-differentiable nature of embedding and sampling discrete tokens. However, various methods have been proposed to address this challenge, including reinforcement learning, which has been demonstrated to be effective.\n",
      "\n",
      "\n",
      "In summary, model selection in machine learning is essential for choosing the best model for a particular task and ensuring that it performs well. It involves evaluating different models and selecting the one that performs the best on a validation set or test data. In few-shot learning, model selection is crucial for adapting to new tasks with only a handful of training examples. Red teaming with automated searches for natural language prompts is another important aspect of model selection, which involves finding inputs that can elicit specific behaviors from a language model. By refining our initial answer, we have provided a more comprehensive and detailed explanation of the purpose of model selection in machine learning.\n",
      "--------------------------------------------------\n",
      "Answer B ($2.29):\n",
      "The purpose of model selection in machine learning is to identify the best model for a particular task or set of tasks. Model selection involves evaluating different models and comparing their performance on a given dataset. The goal is to choose a model that performs well on the training data and can generalize well to unseen data. In addition, model selection can help to identify models that are robust across multiple tasks, which is important in situations where the data distribution may change over time. Ultimately, the purpose of model selection in machine learning is to find a model that can accurately and reliably solve a given problem.\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "Model selection in machine learning serves several important purposes:\n",
      "\n",
      "1. Choosing the Best Model: The primary purpose of model selection is to choose the most suitable model that can perform well on unseen data. This involves comparing different machine learning algorithms and selecting the one that best fits the data.\n",
      "\n",
      "2. Avoiding Overfitting and Underfitting: Model selection helps in avoiding overfitting (where the model performs well on training data but poorly on unseen data) and underfitting (where the model performs poorly on both training and unseen data). \n",
      "\n",
      "3. Optimizing Hyperparameters: Model selection also involves tuning the hyperparameters of a model to improve its performance. Hyperparameters are the parameters of the learning algorithm itself, which are not learned from the data.\n",
      "\n",
      "4. Simplifying Complexity: Sometimes simpler models can perform as well as complex ones. Model selection can help in choosing the simplest model that performs well, which can be easier to interpret and less computationally intensive.\n",
      "\n",
      "5. Improving Generalization: The ultimate goal of a machine learning model is to generalize well from the training data to unseen data. Model selection helps in achieving this by choosing a model that is expected to perform well on new, unseen data.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is more comprehensive. They not only explained the purpose of model selection but also expanded on its importance in few-shot learning and red teaming with automated searches for natural language prompts.\n",
      "\n",
      "Bobby: I agree that student A's answer is more detailed, but I think it's a bit too much. The question asked for the purpose of model selection, not a deep dive into few-shot learning or red teaming. Student B's answer is more concise and directly addresses the question.\n",
      "\n",
      "Michael: But don't you think that by providing examples, student A is demonstrating a deeper understanding of the topic? They're showing how model selection is applied in different contexts, which I think is valuable.\n",
      "\n",
      "Bobby: It's valuable, yes, but it's not what the question asked for. The gold standard answer doesn't mention few-shot learning or red teaming either. It focuses on the main purposes of model selection, which is what student B did.\n",
      "\n",
      "Michael: I see your point, but I still think that student A's answer is better. It's more detailed and shows a deeper understanding of the topic.\n",
      "\n",
      "Bobby: But the question didn't ask for a detailed answer. It asked for the purpose of model selection. Student B's answer is more direct and to the point. It's also easier to understand, which is one of the criteria we're supposed to be judging on.\n",
      "\n",
      "Michael: You're right, the question did ask for the purpose of model selection. And while student A's answer does provide that, it also goes off on a tangent. I suppose student B's answer is more focused and directly addresses the question.\n",
      "\n",
      "Bobby: Exactly. And while student A's answer is impressive, it's not what the question asked for. Student B's answer is more relevant and easier to understand.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is reinforcement learning?\n",
      "--------------------------------------------------\n",
      "Answer A ($23.92):\n",
      "Reinforcement learning is a subfield of machine learning that focuses on training agents to make decisions in complex, uncertain environments. The goal of reinforcement learning is to learn a policy that maps states to actions in a way that maximizes a cumulative reward signal. Reinforcement learning algorithms can be used to solve problems in various domains, such as robotics, game playing, and autonomous driving.\n",
      "\n",
      "Unlike other machine learning techniques, reinforcement learning does not require labeled data. Instead, the agent learns from its interactions with the environment, receiving feedback in the form of rewards or penalties. The agent uses this feedback to adjust its behavior over time, with the goal of maximizing the cumulative reward.\n",
      "\n",
      "One potential application of reinforcement learning is in legal decision-making, where it could be used to automate and optimize the decision-making process. The agent could learn from historical legal cases and develop a framework for making decisions that are consistent with legal principles. However, it's important to note that the application of reinforcement learning in legal decision-making must be done with caution and under the supervision of legal experts, and that the data used to train the models must be unbiased and representative of the diverse perspectives and experiences of the parties involved.\n",
      "\n",
      "Another example of a situation where reinforcement learning might be used is in training an agent to play chess. Chess is a complex game with many possible moves and outcomes, and reinforcement learning could be used to train an agent to make decisions that maximize its chances of winning.\n",
      "\n",
      "Reinforcement learning has been successfully applied in real-world situations such as numerical algorithm programming, where ChatGPT, an AI model, was used to write a Python code for the secant method, a numerical algorithm for finding the root of a function. This demonstrates how reinforcement learning can be used to automate the programming of complex scientific simulations.\n",
      "\n",
      "In summary, reinforcement learning is a powerful tool for training agents to make decisions in complex environments, and it has been applied in various domains. It has the advantage of handling complex decision-making processes and adapting to changing environments, but it also has the disadvantage of requiring large amounts of data and computational resources to train the models. The choice of programming language, features, prompts, and engineering of prompts can affect the performance of reinforcement learning models for code generation.\n",
      "--------------------------------------------------\n",
      "Answer B ($24.85):\n",
      "Reinforcement learning is a subfield of machine learning that focuses on training agents to make decisions in complex, uncertain environments. The goal of reinforcement learning is to learn a policy that maps states to actions in a way that maximizes a cumulative reward signal. The agent learns through trial and error, receiving feedback in the form of rewards or penalties for its actions, and adjusting its behavior accordingly. Rewards play a crucial role in reinforcement learning, serving as a feedback mechanism to guide the agent's learning process. Reinforcement learning can be used to solve various problems, such as identifying and mitigating harmful outputs generated by large language models, improving few-shot learning performance, and learning how to generate rationales that improve model performance. However, reinforcement learning also has challenges and limitations, such as the cost and time required for online learning, and the potential for undesirable outcomes resulting from trial and error.\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to achieve a goal. The agent is rewarded or penalized (with points, for example) for the actions it takes, learning over time to maximize its rewards and minimize its penalties. This is done through trial and error, with the agent continuously refining its strategy with each action it takes. Reinforcement learning is particularly useful in situations where the agent has no prior knowledge and must learn to perform a task solely from feedback.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is better. It's more detailed and provides a broader context of reinforcement learning. The student also gives specific examples of where reinforcement learning can be applied, such as in legal decision-making and chess playing. They even mention a real-world application of reinforcement learning in numerical algorithm programming.\n",
      "\n",
      "Bobby: I see your point, Michael, but I think student B's answer is more concise and to the point. They clearly explain the concept of reinforcement learning and its goal. They also mention the role of rewards, which is a crucial aspect of reinforcement learning. Moreover, they highlight some of the challenges and limitations of reinforcement learning, which is important to understand the full scope of this machine learning subfield.\n",
      "\n",
      "Michael: Yes, Bobby, but student A also discusses the challenges of reinforcement learning, mentioning the need for large amounts of data and computational resources. Plus, they delve into the technical aspects of reinforcement learning, such as the choice of programming language and features, which I think is important for a comprehensive understanding of the topic.\n",
      "\n",
      "Bobby: That's true, Michael, but I feel that student A's answer goes off on a tangent when discussing the application of reinforcement learning in legal decision-making. This is not directly related to the question and could confuse the reader. On the other hand, student B's answer is more focused and directly addresses the question.\n",
      "\n",
      "Michael: I see your point, Bobby. While student A's answer is more detailed, it might be a bit too complex and broad for someone trying to understand the basic concept of reinforcement learning. Student B's answer, while less detailed, is more straightforward and easier to understand.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the difference between a BERT and a GPT?\n",
      "--------------------------------------------------\n",
      "Answer A ($31.48):\n",
      "The difference between BERT and GPT is that BERT is a language model developed by Google that uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in a sentence, whereas GPT is a transformer-based language model that generates text. BERT is trained on a masked language modeling task, where some of the input words are randomly replaced with a [MASK] token, and the model is trained to predict the original word. In contrast, GPT is trained on a combination of a masked language modeling task and a next sentence prediction task.\n",
      "\n",
      "\n",
      "BERT and GPT have different architectures and training objectives. BERT uses a multi-layer bidirectional transformer encoder, whereas GPT uses a transformer-based architecture with an autoregressive decoder. BERT is designed for natural language processing tasks, such as sentiment analysis, question-answering, and language translation, whereas GPT is designed for generating coherent and contextually relevant text.\n",
      "\n",
      "\n",
      "In terms of specific tasks or applications where BERT and GPT are better suited, BERT is generally powerful for natural language processing tasks, and the use of structured sparsity in BERT could potentially improve its performance in tasks that require efficient inference, such as real-time language translation or text summarization. On the other hand, GPT has been fine-tuned for various tasks, such as text generation, language translation, and question-answering. The choice of model depends on the specific task and available resources. BERT and GPT are powerful models that can achieve excellent results, but they require significant computational resources and may not be suitable for tasks that require fast inference or low computational overhead. In such cases, other models like DistilBERT or RoBERTa may be more appropriate.\n",
      "\n",
      "\n",
      "In summary, BERT and GPT are both language models developed by Google, but they have different purposes and applications. BERT is designed for natural language processing tasks, while GPT is designed for generating text. They have different architectures and training objectives, and their suitability for specific tasks depends on the available resources and the task at hand.\n",
      "--------------------------------------------------\n",
      "Answer B ($22.68):\n",
      "The primary difference between BERT and GPT lies in their pretraining strategies. BERT employs a masked modeling approach that allows it to capture long-term text dependencies and represent words considering both local and global context, making it suitable for tasks that require bidirectional text information, such as language modeling, text classification, question answering, sentiment analysis, text completion, and language translation. In contrast, GPT uses autoregressive modeling that learns the sequential relationships between words, making it suitable for tasks that require generating coherent and fluent text, such as language translation and text summarization. While BERT may struggle to generate coherent and fluent text, GPT may struggle to capture the context and meaning of words. GPT's sequential text generation capability makes it a strong contender for various natural language processing tasks that require explicit intermediate reasoning or rationales. However, BERT's masked language model objective allows it to capture complex contextual relationships in text, making it more effective at representing words considering both local and global context.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer) are both transformer-based models used in natural language processing, but they have different approaches and uses.\n",
      "\n",
      "1. Directionality: BERT is bidirectional, meaning it looks at the context from both the left and the right side of a word during training. This allows it to understand the context better. On the other hand, GPT is unidirectional (specifically, left-to-right), meaning it only considers the context to the left of the word during training.\n",
      "\n",
      "2. Training Objective: BERT is trained to predict masked words in a sentence (a task known as masked language modeling), which helps it understand the context of words. GPT, on the other hand, is trained to predict the next word in a sentence (a task known as causal language modeling or autoregressive language modeling).\n",
      "\n",
      "3. Use Cases: BERT is often used for tasks that require understanding the context of words in a sentence, such as question answering, named entity recognition, and sentiment analysis. GPT is often used for tasks that involve generating text, such as text completion, translation, and creative writing.\n",
      "\n",
      "4. Fine-tuning: Both BERT and GPT can be fine-tuned on specific tasks, but due to BERT's bidirectional nature, it often requires less task-specific adjustments compared to GPT.\n",
      "\n",
      "5. Model Versions: As of now, there are multiple versions of both models available. For BERT, there's BERT base and BERT large, and for GPT, there's GPT-1, GPT-2, and GPT-3, each subsequent version being larger and more powerful than the previous one.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is better. It's more detailed and provides a comprehensive comparison between BERT and GPT. The student also mentions the different tasks for which each model is better suited, and even suggests alternatives for tasks that require fast inference or low computational overhead.\n",
      "\n",
      "Bobby: I disagree, Michael. While student A's answer is indeed detailed, it's also quite verbose and repetitive. Student B's answer, on the other hand, is concise and to the point. It clearly explains the differences in the pretraining strategies of BERT and GPT, and how these differences affect their suitability for different tasks.\n",
      "\n",
      "Michael: I see your point, Bobby. However, I still believe that the additional details provided by student A are valuable. They give a more complete picture of the differences between BERT and GPT, and provide practical advice for choosing between them.\n",
      "\n",
      "Bobby: But Michael, we're not looking for the most detailed answer, we're looking for the best answer. And in my opinion, the best answer is the one that directly addresses the question with clarity and is easy to understand. Student B's answer does just that. It's clear, concise, and directly addresses the question.\n",
      "\n",
      "Michael: You're right, Bobby. Upon reflection, I agree that student B's answer is more direct and easier to understand. It succinctly explains the differences between BERT and GPT, and how these differences affect their uses.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the purpose of pre-training in LLMs?\n",
      "--------------------------------------------------\n",
      "Answer A ($36.41):\n",
      "The purpose of pre-training in large language models (LLMs) is to enable the models to quickly comprehend the main idea of a text, much like humans do. Pre-training involves training a model on a large dataset before fine-tuning it for a specific task, allowing the model to learn a rich set of features that can be used for various natural language processing tasks. In the context of LLMs, pre-training is used to train a model on a large corpus of text, such as books, articles, or web pages, to learn the patterns and structures of language. This can be useful for tasks such as language translation, question answering, or text generation. Pre-training can also be used to generate high-quality prompts that can be used to fine-tune the LM for a specific task, such as a question-answering task by using a prompt that consists of a question and a set of potential answers.\n",
      "\n",
      "While I am not confident about the relative performance of pre-training compared to other techniques used in LLMs, pre-training is a popular technique used in many state-of-the-art LLMs. It is important to note that the performance of a technique used in LLMs can depend on various factors, such as the specific task, dataset, and model architecture, and therefore, a thorough evaluation and comparison of different techniques used in LLMs would be necessary to determine their relative performance.\n",
      "\n",
      "In summary, pre-training is a useful technique for training LLMs, allowing them to comprehend the main idea of a text quickly and learn a rich set of features that can be used for various natural language processing tasks.\n",
      "--------------------------------------------------\n",
      "Answer B ($24.87):\n",
      "The purpose of pre-training in LLMs is to expose the model to a vast amount of text data, allowing it to learn a rich set of features that can be fine-tuned for specific downstream tasks, leading to better performance on tasks such as language translation, question answering, and text classification. Pre-training can also help the model learn to generate coherent and contextually relevant text, beneficial for tasks such as language generation and summarization. Pre-training can lead to significant improvements in performance, especially when combined with fine-tuning and multi-task learning, as demonstrated by the GLUE and SuperGLUE benchmarks. By incorporating pre-training, LLMs can perform better on a variety of natural language processing tasks, including software engineering tasks such as code summarization and code generation.\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "Pre-training in Language Model Learning (LLMs) serves several important purposes:\n",
      "\n",
      "1. Learning General Language Understanding: Pre-training helps the model to learn the general aspects of the language, such as grammar, syntax, semantics, and some level of world knowledge. This is achieved by training the model on a large corpus of text.\n",
      "\n",
      "2. Transfer Learning: The knowledge gained from pre-training can be transferred to various specific tasks with fine-tuning, which often requires less data. This is particularly useful when the task-specific data is limited.\n",
      "\n",
      "3. Improved Performance: Pre-training often leads to better performance on downstream tasks. It provides a good initialization point for the model parameters, which can lead to faster convergence and better overall performance.\n",
      "\n",
      "4. Efficiency: Pre-training can also make the training process more efficient. Instead of training a model from scratch for each specific task, you can use a pre-trained model and fine-tune it for the specific task, which can save a lot of time and computational resources.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is better. It's more detailed and provides a comprehensive understanding of pre-training in LLMs. The student also acknowledges the need for a thorough evaluation and comparison of different techniques used in LLMs, which shows a deep understanding of the subject.\n",
      "\n",
      "Bobby: I disagree, Michael. While student A's answer is indeed detailed, it lacks the specificity that student B's answer provides. Student B not only explains the purpose of pre-training but also mentions its benefits and how it leads to better performance on tasks. They even provide examples of tasks where pre-training can be beneficial, such as code summarization and code generation.\n",
      "\n",
      "Michael: I see your point, Bobby. However, student A's answer also mentions the benefits of pre-training and provides examples of tasks where it can be useful. Moreover, student A's answer is more structured and easier to follow, which makes it more effective in my opinion.\n",
      "\n",
      "Bobby: That's true, Michael. But student B's answer is more concise and to the point. It directly addresses the question and provides a clear and straightforward explanation of the purpose of pre-training in LLMs. In contrast, student A's answer seems a bit verbose and goes off on a tangent about the relative performance of pre-training compared to other techniques.\n",
      "\n",
      "Michael: I see where you're coming from, Bobby. But I still believe that student A's answer provides a more comprehensive understanding of the topic. It's not just about being concise, but also about providing a thorough explanation.\n",
      "\n",
      "Bobby: I understand your point, Michael. But considering the gold standard answer, I believe student B's answer aligns more closely with it. It covers all the key points: learning general language understanding, transfer learning, improved performance, and efficiency.\n",
      "\n",
      "Michael: You're right, Bobby. Upon reconsideration, I agree that student B's answer aligns more closely with the gold standard answer and provides a more direct and concise explanation of the purpose of pre-training in LLMs.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "How many days after its release did ChatGPT attract over a million users?\n",
      "--------------------------------------------------\n",
      "Answer A ($24.76):\n",
      "ChatGPT attracted over a million users within the first week of its launch, specifically in the week of December 12, 2020. Its user growth has been significant and rapid, with an average of 1.3 million daily active users in the first quarter of 2023, and a bounce rate of only 12.8%. ChatGPT's ability to provide accurate and relevant responses to user queries, as well as its user-friendly interface and engaging personality, have contributed to its popularity among a wide range of users. Additionally, ChatGPT's ability to understand context and provide conversational-style responses has made it a popular tool for customer service representatives, software developers, and other professionals. Industry experts and analysts, such as Dr. Jason Weston, Dr. Jacob Devlin, and Dr. Yoav Goldberg, have publicly discussed ChatGPT's rapid user growth and the factors that may have contributed to it.\n",
      "--------------------------------------------------\n",
      "Answer B ($21.67):\n",
      "ChatGPT attracted over a million users sometime after its release, likely taking some time. Its popularity can be attributed to its ability to effectively use prompts and in-context learning to retrieve good prompts, as well as its ability to effectively integrate with other language models.\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "As an AI, I don't have real-time data or the ability to access up-to-date information. As of my last update, I don't have specific data on how many days after its release ChatGPT attracted over a million users. For the most accurate information, please refer to the latest reports or OpenAI's official announcements.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is better. It's detailed and provides a lot of context about ChatGPT's user growth and popularity.\n",
      "\n",
      "Bobby: I disagree, Michael. The question asks for a specific number of days, not a detailed analysis of ChatGPT's growth. Student A's answer is too verbose and goes off on tangents.\n",
      "\n",
      "Michael: But Bobby, student B's answer is too vague. It says \"sometime after its release, likely taking some time.\" That doesn't answer the question at all.\n",
      "\n",
      "Bobby: True, but at least student B's answer is concise and to the point. It doesn't go off on tangents like student A's answer does.\n",
      "\n",
      "Michael: But Bobby, the question asks for a specific number of days. Student A's answer, while verbose, at least provides a specific timeframe - the first week of launch.\n",
      "\n",
      "Bobby: Yes, but the gold standard answer indicates that we don't have specific data on this. So, in this case, being vague might actually be more accurate.\n",
      "\n",
      "Michael: I see your point, Bobby. But I still think that student A's answer, while verbose, does a better job at attempting to answer the question.\n",
      "\n",
      "Bobby: I understand where you're coming from, Michael. But in this case, I think brevity and accuracy are more important than verbosity and speculation.\n",
      "\n",
      "After a long debate, they finally reach a verdict.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the meaning of \"emergent capabilities\" in the context of large language models?\n",
      "--------------------------------------------------\n",
      "Answer A ($28.19):\n",
      "Emergent capabilities in the context of large language models refer to their ability to learn from explanations and provide reward signals that align with a user's objectives, even from a single example. This allows for efficient in-context learning and enables the training of objective-aligned agents across different tasks. To train objective-aligned agents, large language models use various pre-training tasks and reward signals to align their learning with a user's objectives. However, there are challenges associated with training objective-aligned agents across different tasks, such as the need for high-quality priors, balancing the trade-off between exploration and exploitation, overfitting, and limitations in terms of available data. Addressing these challenges requires using techniques such as manual and automated methods to generate high-quality priors, balancing the trade-off between exploration and exploitation, using techniques such as transfer learning to address overfitting, and using techniques such as data augmentation and transfer learning to address limitations in terms of available data. When using large language models in real-world applications, it is important to conduct a thorough hyperparameter search to ensure adequate performance and consider the impact of prior errors on the learned structure. Additionally, it is important to consider the ethical implications of using large language models, such as ensuring that the agent's goals align with the intended objectives and addressing potential biases in the data used for training.\n",
      "\n",
      "\n",
      "The initial answer was lacking in specific examples of how large language models learn from explanations and provide reward signals that align with a user's objectives. The follow-up questions provided more information on how large language models align their learning with a user's objectives, such as using pre-training tasks and reward signals. The refined answer incorporates this information to provide a more comprehensive answer to the original question. Additionally, the refined answer addresses the challenges associated with training objective-aligned agents across different tasks and emphasizes the importance of considering ethical implications when using large language models in real-world applications.\n",
      "--------------------------------------------------\n",
      "Answer B ($24.45):\n",
      "Emergent capabilities in the context of large language models (LLMs) refer to their ability to perform tasks that they have not been specifically trained for, beyond their in-context learning abilities. Emergent capabilities include tasks such as language translation, summarization, sentiment analysis, chatbots, and the ability to understand the context of a customer's inquiry and respond accordingly. They also include the ability to solve mathematical word problems, generate step-by-step solutions for various mathematical problems, answer questions that require logical reasoning and critical thinking, and generate explanations and rationales for their answers. Emergent capabilities differ from in-context learning abilities, which refer to LLMs' ability to adapt to new tasks with only a few examples, leveraging their fine-tuning capabilities. However, there are limitations and risks associated with relying on emergent capabilities in LLMs, such as struggles with mathematical problems that require step-by-step reasoning and problem-solving, and few-shot learning tasks. Therefore, it is essential to be cautious when relying on emergent capabilities in LLMs and consider combining them with other techniques or methods to achieve better results and mitigate potential risks.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:35,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "\"Emergent capabilities\" in the context of large language models refer to the abilities or features that were not explicitly programmed into the model, but rather developed as a result of the model's learning process. These capabilities can include understanding complex language structures, generating creative text, answering questions accurately, and even demonstrating a basic understanding of certain topics. These capabilities emerge from the model's exposure to vast amounts of data and its ability to identify patterns and relationships within that data.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is better. It's comprehensive and addresses the question directly. It also provides a detailed explanation of how large language models learn from explanations and provide reward signals that align with a user's objectives.\n",
      "\n",
      "Bobby: I disagree, Michael. Student A's answer is indeed comprehensive, but it's too verbose and goes off on tangents. It talks about the challenges and ethical implications of using large language models, which is not what the question asked. On the other hand, student B's answer is more focused and directly addresses the question. It provides specific examples of emergent capabilities, which makes it easier to understand.\n",
      "\n",
      "Michael: I see your point, Bobby. However, student B's answer also has its flaws. It mentions tasks such as language translation, summarization, sentiment analysis, and chatbots as emergent capabilities. These are not emergent capabilities but rather tasks that large language models are trained to perform. \n",
      "\n",
      "Bobby: That's a valid point, Michael. But student B's answer also mentions the limitations and risks associated with relying on emergent capabilities in large language models, which is an important aspect to consider. \n",
      "\n",
      "Michael: True, but the question didn't ask about the limitations and risks. It asked about the meaning of \"emergent capabilities\". I think student A's answer, despite being verbose, provides a more accurate definition of \"emergent capabilities\".\n",
      "\n",
      "Bobby: After considering your points, Michael, I agree. Student A's answer, although verbose, does provide a more accurate and comprehensive explanation of \"emergent capabilities\" in the context of large language models.\n",
      "\n",
      "VERDICT: Better answer from student A.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the significance of the date November 30th, 2022, in the development of language models?\n",
      "--------------------------------------------------\n",
      "Answer A ($25.86):\n",
      "The significance of the date November 30th, 2022, in the development of language models lies in the release of ChatGPT, a major breakthrough in transformer-based language models. ChatGPT's training using supervised fine-tuning and reinforcement learning systems to rank by quality for human feedback rewards, and its ability to adapt to new schemas and perform extraction consistent with the schema, have contributed to the development of language models in several ways.\n",
      "\n",
      "Firstly, supervised fine-tuning allows the model to learn from labeled data, enabling it to generate more accurate and relevant responses to user input. This is particularly useful in tasks such as language translation, where the model needs to generate coherent and contextually appropriate output.\n",
      "\n",
      "Secondly, reinforcement learning systems allow the model to learn from user feedback, which can help improve its performance in generating high-quality responses. By receiving feedback in the form of rewards or penalties, the model can learn to generate responses that are more likely to be useful and relevant to the user.\n",
      "\n",
      "Finally, ChatGPT's ability to adapt to new schemas and perform extraction consistent with the schema allows it to generalize to new situations and generate responses that are appropriate to the context. This is an important feature in language models, as it enables them to respond appropriately to user input that may not fit the expected pattern.\n",
      "\n",
      "Moreover, ChatGPT's training using supervised fine-tuning and reinforcement learning systems ranks by quality for human feedback rewards, which plays an important role in generating high-quality responses. While human feedback can be useful for fine-tuning and improving the performance of language models, it is not clear how much of a difference it makes in terms of overall quality. However, ChatGPT's ability to adapt to new schemas and perform extraction consistent with the schema helps it to better understand and respond to user requests, which is essential for generating contextually appropriate responses.\n",
      "\n",
      "ChatGPT's release on November 30th, 2022, has had a significant impact on the development of language models and natural language processing. Its remarkable zero-shot capabilities have demonstrated the potential for large language models to solve a wide range of natural language tasks. As a result, ChatGPT has been increasingly integrated into various aspects of daily life, including online search and professional environments.\n",
      "\n",
      "However, the tendency of ChatGPT and other large language models to generate hallucinations has raised concerns regarding the trustworthiness of these models. Detecting and mitigating hallucinations remains an open challenge, especially for state-of-the-art, proprietary models accessible only as black boxes.\n",
      "\n",
      "In conclusion, ChatGPT's release on November 30th, 2022, marked a significant milestone in the development of language models. Its innovative training methods and features have enabled it to generate more accurate, relevant, and contextually appropriate responses to user input, and have paved the way for further advancements in natural language processing.\n",
      "--------------------------------------------------\n",
      "Answer B ($24.66):\n",
      "November 30th, 2022, marks the release of ChatGPT, a language model designed to generate natural language text for various applications. ChatGPT's unique algorithm, trained on a large and diverse dataset of text from the internet, enables it to understand the context of a conversation and provide more accurate and relevant responses. While it has potential in assisting programming tasks, its ability to write code or identify bugs in existing code is limited. Despite this, ChatGPT's natural language generation capabilities have the potential to significantly impact the future of language models and their applications, such as improving chatbots, voice assistants, and language translation software. However, it is crucial to address potential risks and limitations associated with ChatGPT's ability to generate natural language text to ensure the responsible advancement of its capabilities.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [01:15, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "As an AI, I don't have real-time capabilities or future prediction abilities. As of the last update, there is no specific significance to the date November 30th, 2022, in the development of language models. However, the field of AI and language models is rapidly evolving, so significant developments could occur at any time.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is better. It's detailed and provides a comprehensive explanation of the significance of the date in question. It discusses the release of ChatGPT, its training methods, and its impact on the development of language models. \n",
      "\n",
      "Bobby: I disagree, Michael. Student A's answer is indeed detailed, but it's also speculative. The gold standard answer clearly states that there is no specific significance to the date November 30th, 2022. Student B's answer, while less detailed, is more in line with the gold standard answer. It mentions the release of ChatGPT and its potential impact, but it doesn't make any specific claims about the date.\n",
      "\n",
      "Michael: That's a fair point, Bobby. However, I believe the question is asking for an interpretation of the significance of the date, not a factual statement. In this case, student A's answer provides a more thorough interpretation, even if it is speculative.\n",
      "\n",
      "Bobby: I see where you're coming from, Michael. But I still think that student B's answer is more accurate. It doesn't make any unfounded claims, and it still provides a good explanation of the potential impact of ChatGPT.\n",
      "\n",
      "Michael: I understand your point, Bobby. But I think the depth and detail of student A's answer outweighs the speculative nature of it. It provides a more comprehensive understanding of the potential significance of the date.\n",
      "\n",
      "Bobby: I respect your opinion, Michael. But considering the gold standard answer, I believe accuracy should be prioritized over speculation. Therefore, I still think student B's answer is better.\n",
      "\n",
      "After a long debate, Michael and Bobby finally come to a consensus.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "How do biases and fairness issues arise in LLMs, and what can be done to mitigate them?\n",
      "--------------------------------------------------\n",
      "Answer A ($32.91):\n",
      "Biases and fairness issues in large language models (LLMs) can arise due to various factors, including data bias, algorithmic flaws, and societal prejudices. To mitigate these issues, researchers and developers can use techniques such as red teaming with automated searches for natural language prompts, as discussed in Passage 1. This approach can help identify and mitigate biases in LLMs by finding inputs that can elicit problematic behaviors.\n",
      "\n",
      "One common technique to mitigate biases and fairness issues in LLMs is to use diverse and representative training data to reduce the risk of biases in the model's learning process. Another approach is to incorporate fairness-aware regularization terms in the model's objective function, which can help to mitigate biases and ensure fairness in the model's predictions. Additionally, techniques such as debiasing and adversarial training can be used to reduce biases and improve fairness in LLMs.\n",
      "\n",
      "It's important to note that assessing the correctness or fairness of human legal decisions can be difficult, and the same difficulties extend to AI tools used in legal decisions. Therefore, it is crucial to develop methods that can help evaluate and improve the fairness and correctness of LLMs' outputs.\n",
      "\n",
      "Overall, mitigating biases and fairness issues in LLMs requires a multifaceted approach that involves various techniques and strategies. Developers and researchers must consider the complexities and challenges of these issues and work towards creating more transparent and explainable AI models.\n",
      "\n",
      "By incorporating these additional insights, the refined answer provides a more comprehensive approach to mitigating biases and fairness issues in LLMs. It highlights the importance of using diverse and representative training data, incorporating fairness-aware regularization terms, and using techniques such as debiasing and adversarial training. Additionally, it emphasizes the need to develop methods for evaluating and improving the fairness and correctness of LLMs' outputs.\n",
      "--------------------------------------------------\n",
      "Answer B ($24.93):\n",
      "Biases and fairness issues in LLMs can arise due to factors such as training data, algorithms, and fine-tuning. To mitigate these issues, it's important to use diverse and representative training data, fair algorithms, and thorough testing. Diverse and representative training data for large language models (LLMs) should include a wide range of tasks or batches that reflect the various linguistic phenomena present in the projects they are intended to support. Examples of such tasks or batches include project-specific identifier names, APIs, terminology, coding patterns, and domain-specific algorithms and data. To ensure diversity and representation in the training data, a data curation method such as the RWKV model's approach can be used. This method involves several strategies, including replacing the quadratic QK attention with a scalar formulation, reformulating recurrence and sequential inductive biases to enable efficient training parallelization and inference, and using custom initializations to enhance training dynamics. Additionally, using a consistent model/method to create task embeddings can help ensure comparable distances that depend on the data/task, allowing for the use of techniques such as max marginal relevance (MMR) selection. By combining these strategies, we can ensure that the training data is diverse and representative, which can help improve the performance of the LLMs. Fair algorithms and thorough testing help mitigate biases and fairness issues in LLMs by adapting the model's behavior to specific tasks without modifying its pre-trained weights and generating diverse representations of input data. Coarse-tuning, a specific fair algorithm, can help mitigate biases in LLMs by fine-tuning them on diverse data that better represents the population or task at hand. Paraphrasing text or generating new sentences can help increase the model's adaptability to real-world data variations by exposing it to diverse input, allowing it to recognize and understand different ways of expressing the same information. Thorough testing helps identify and address biases in the data and the model by allowing developers to evaluate the performance of their model on a variety of test cases, identify biases, and adjust their model or collecting more diverse data to reduce these biases. Debiasing techniques, such as prompting strategies, can also be used to mitigate biases and fairness issues in LLMs. Prompting can shape predictions or offer relevant information without expensive supervised data, allowing for emergent abilities and improving reliability. However, there are potential risks and limitations associated with using debiasing techniques in LLMs, such as introducing new biases or issues. Therefore, it is important to carefully evaluate their effectiveness and monitor their performance over time, and to consider the ethical implications of using LLMs and debiasing techniques. Success stories and case studies demonstrate the usefulness of debiasing techniques in improving the fairness of LLMs, and they can help ensure that LLMs are more accurate and effective for diverse applications.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:56, 18.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "Large Language Models (LLMs) like GPT-3 are trained on vast amounts of text data from the internet. Biases and fairness issues can arise in these models because they learn from the data they are trained on, which can include biased language, stereotypes, and discriminatory content. This can lead to the model generating biased or unfair outputs.\n",
      "\n",
      "There are several ways to mitigate these issues:\n",
      "\n",
      "1. Diverse Training Data: Ensuring the training data is diverse and representative of different perspectives can help reduce bias. This includes data from different cultures, genders, races, and socio-economic backgrounds.\n",
      "\n",
      "2. De-biasing Techniques: These techniques aim to reduce the bias in the model's outputs. For example, some techniques modify the training data to reduce the presence of biased language, while others adjust the model's parameters after training to minimize bias.\n",
      "\n",
      "3. Transparency and Interpretability: Making the model's decision-making process more transparent can help identify when and why it's producing biased outputs. This can be achieved through techniques like attention visualization and feature importance ranking.\n",
      "\n",
      "4. Evaluation and Testing: Regularly evaluating the model's outputs for bias can help identify issues. This can be done through manual review or automated testing.\n",
      "\n",
      "5. User Feedback: Allowing users to report biased outputs can provide valuable information for improving the model.\n",
      "\n",
      "6. Ethical Guidelines: Implementing ethical guidelines for AI use can help ensure the technology is used responsibly and that potential biases are considered and addressed.\n",
      "\n",
      "7. Regulation: Government regulation can also play a role in ensuring AI systems are fair and unbiased.\n",
      "\n",
      "Remember, mitigating bias and fairness issues in LLMs is an ongoing process that requires continuous monitoring and adjustment.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is more in line with the gold standard answer. They've mentioned diverse training data, de-biasing techniques, and the importance of evaluation and testing. They've also touched upon the need for transparency and interpretability, which is a key point in the gold standard answer.\n",
      "\n",
      "Bobby: I agree that student A's answer is comprehensive, but I think student B's answer is more detailed. They've not only mentioned diverse training data and de-biasing techniques, but they've also given specific examples of how to achieve diversity in training data and what kind of de-biasing techniques can be used. They've also discussed the importance of thorough testing and fair algorithms, which are crucial for mitigating biases and fairness issues in LLMs.\n",
      "\n",
      "Michael: Yes, student B's answer is detailed, but it seems to go off on a tangent at times. For instance, they've discussed the RWKV model's approach and max marginal relevance (MMR) selection, which are not directly related to the question. On the other hand, student A's answer is more focused and directly addresses the question.\n",
      "\n",
      "Bobby: I see your point, Michael. However, I still believe that the level of detail in student B's answer is commendable. They've discussed the potential risks and limitations of using de-biasing techniques, which is an important aspect of mitigating biases and fairness issues in LLMs.\n",
      "\n",
      "Michael: I agree that student B's answer is detailed, but I think student A's answer is more concise and to the point. They've addressed all the key points in the gold standard answer without going off on tangents. \n",
      "\n",
      "Bobby: You're right, Michael. Upon reflection, I agree that student A's answer is more direct and focused. They've addressed all the key points without getting lost in the details. \n",
      "\n",
      "VERDICT: Better answer from student A.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the relationship between large language models and neurosymbolic reasoning?\n",
      "--------------------------------------------------\n",
      "Answer A ($21.23):\n",
      "The relationship between large language models and neurosymbolic reasoning is not directly addressed in the given passages. However, we can refine the initial answer by incorporating the insights from the follow-up questions to provide a more comprehensive and detailed response.\n",
      "\n",
      "Firstly, we can elaborate on the differences between large language models and traditional symbolic reasoning systems. Traditional symbolic reasoning systems rely on explicit rules and logical deductions to process and generate text, while large language models rely on statistical patterns learned from large corpora of text data. This means that traditional symbolic reasoning systems are better at processing and generating text that requires logical and rule-based reasoning, while large language models are better at generating text that aligns with human common sense.\n",
      "\n",
      "Secondly, we can discuss the potential benefits and challenges of integrating large language models and traditional symbolic reasoning systems in neurosymbolic reasoning. Integrating these approaches could lead to more robust and accurate natural language processing systems that can leverage the strengths of both statistical and rule-based reasoning. However, there are also potential risks and challenges to consider, such as ensuring that the generative models accurately describe the true data distribution and do not introduce biases or errors.\n",
      "\n",
      "Lastly, we can provide examples of how neurosymbolic reasoning could be applied in real-world scenarios, such as natural language processing or artificial intelligence systems. For instance, in natural language processing, neurosymbolic reasoning could be used to improve the accuracy of language models by integrating traditional symbolic reasoning systems that can provide logical and rule-based reasoning with large language models that can capture domain knowledge causal mechanisms based on conceptual descriptions of variables. This integration could enable the system to not only generate text that is grammatically correct but also semantically meaningful and contextually appropriate.\n",
      "\n",
      "In summary, the refined answer to the question of how large language models differ from traditional symbolic reasoning systems, and what are the implications of these differences for neurosymbolic reasoning, is as follows:\n",
      "\n",
      "Large language models and traditional symbolic reasoning systems differ in their underlying mechanisms and approaches to processing and generating text. Traditional symbolic reasoning systems rely on explicit rules and logical deductions, while large language models rely on statistical patterns learned from large corpora of text data. Integrating these approaches in neurosymbolic reasoning could lead to more robust and accurate natural language processing systems that can leverage the strengths of both statistical and rule-based reasoning. However, there are also potential risks and challenges to consider, such as ensuring that the generative models accurately describe the true data distribution and do not introduce biases or errors. Neurosymbolic reasoning could be applied in various real-world scenarios, such as natural language processing or artificial intelligence systems, to improve the accuracy and semantic meaningfulness of language models.\n",
      "\n",
      "Therefore, the refined answer to the original question, \"What is the relationship between large language models and neurosymbolic reasoning?\", is that large language models and traditional symbolic reasoning systems differ in their underlying mechanisms and approaches to processing and generating text, and integrating these approaches in neurosymbolic reasoning could lead to more robust and accurate natural language processing systems that can leverage the strengths of both statistical and rule-based reasoning. However, there are also potential risks and challenges to consider, such as ensuring that the generative models accurately describe the true data distribution and do not introduce biases or errors.\n",
      "--------------------------------------------------\n",
      "Answer B ($24.46):\n",
      "Large language models and neurosymbolic reasoning are two distinct approaches in the field of artificial intelligence that can potentially complement each other in certain applications. While they operate differently and serve different purposes, they can be used together to achieve better results in specific tasks. For instance, in natural language processing tasks that require logical reasoning and understanding of complex concepts, a powerful LLM can be used to probe sub-evidence and sub-questions by performing in-context learning, and a neurosymbolic reasoning system can validate the answers generated by the language model by browsing through sub-evidence and sub-questions. Additionally, the neurosymbolic reasoning system can use a combination of logical rules and machine learning algorithms to ensure the faithfulness of the reasoning chain and scale up the size of language models. This allows the system to reason and validate answers in a more accurate and efficient manner. However, integrating the two approaches can be challenging due to differences in representation and reasoning styles, and require careful consideration of their interfaces and compatibility. Despite these challenges, researchers continue to explore ways to effectively integrate large language models and neurosymbolic reasoning to achieve better results in social reasoning problems.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [02:32, 22.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "Large language models and neurosymbolic reasoning are both approaches to artificial intelligence, but they operate differently and can complement each other.\n",
      "\n",
      "Large language models, like GPT-3, are based on machine learning techniques, specifically deep learning. They are trained on vast amounts of text data and learn to generate human-like text by predicting the next word in a sequence. They excel at tasks that involve pattern recognition and generalization from examples.\n",
      "\n",
      "Neurosymbolic reasoning, on the other hand, combines neural networks with symbolic reasoning. Symbolic reasoning is a traditional AI approach that uses explicit symbols and rules to represent and manipulate knowledge. It is good at tasks that require logical reasoning, understanding of complex relationships, and handling of explicit rules and constraints.\n",
      "\n",
      "While large language models are good at learning from data and generating text, they often lack the ability to reason logically and understand the deeper meaning of the text. Neurosymbolic reasoning can help to overcome these limitations by integrating symbolic reasoning into the learning process.\n",
      "\n",
      "In other words, the relationship between large language models and neurosymbolic reasoning can be seen as complementary, with each approach addressing the limitations of the other. By combining the strengths of both approaches, it is possible to build more powerful and versatile AI systems.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is better. It's more detailed and provides a comprehensive understanding of the relationship between large language models and neurosymbolic reasoning. The student also discusses the potential benefits and challenges of integrating these two approaches, which is a crucial aspect of the question.\n",
      "\n",
      "Bobby: I disagree, Michael. While student A's answer is indeed detailed, it's also quite convoluted and repetitive. The student seems to be going off on tangents and not addressing the question directly. On the other hand, student B's answer is concise, clear, and directly addresses the question. It also mentions the potential challenges of integrating the two approaches, just like student A.\n",
      "\n",
      "Michael: I see your point, Bobby. However, I believe that the depth of student A's answer is valuable. The student not only explains the differences between large language models and traditional symbolic reasoning systems but also provides real-world examples of how neurosymbolic reasoning could be applied. This shows a deep understanding of the topic.\n",
      "\n",
      "Bobby: That's true, Michael. But remember, a good answer should also be easy to understand. Student B's answer is straightforward and easy to follow. It provides a clear explanation of how large language models and neurosymbolic reasoning can complement each other, which is the crux of the question. \n",
      "\n",
      "Michael: You're right, Bobby. While student A's answer is more detailed, it's also quite complex and might be difficult for some to understand. On the other hand, student B's answer is simpler and more direct. I agree that student B's answer does a better job at answering the question.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "How does reinforcement learning apply to InstructGPT's training procedure?\n",
      "--------------------------------------------------\n",
      "Answer A ($14.10):\n",
      "Reinforcement learning is a machine learning technique that trains agents to make decisions in complex, uncertain environments. InstructGPT, a language model, uses a combination of reinforcement learning and other techniques to optimize its performance. The specific details of InstructGPT's training procedure and how reinforcement learning applies to it are not specified in the given passages. However, passage 1 mentions the cost-effectiveness of hyperparameter optimization methods, including reinforcement learning, and their potential environmental impact. Passage 2 discusses the impact of prior errors on the learned structure of score-based methods and proposes a strategy to counteract these errors. Therefore, while the passages provide some relevant information, they do not directly answer the question of how reinforcement learning applies to InstructGPT's training procedure.\n",
      "--------------------------------------------------\n",
      "Answer B ($13.35):\n",
      "Reinforcement learning can be applied to InstructGPT's training procedure by optimizing for pedagogical quality. This involves using RL algorithms to guide the generation of AI teacher responses that can effectively communicate with students, comprehend their needs, and facilitate their academic improvement. By systematically evaluating generative models and incorporating RL techniques, it is possible to improve the performance of language models like InstructGPT in educational dialogues.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [03:02, 24.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "InstructGPT, developed by OpenAI, is trained using a two-step process: pre-training and fine-tuning. Reinforcement Learning from Human Feedback (RLHF) is used in the fine-tuning stage.\n",
      "\n",
      "In the fine-tuning process, reinforcement learning is used to refine the model's responses based on feedback from human AI trainers. These trainers provide rankings for different model-generated responses, creating a reward model. The model is then fine-tuned using Proximal Policy Optimization, a reinforcement learning algorithm, to generate responses that are more likely to receive higher rankings.\n",
      "\n",
      "This process is iterative, with multiple rounds of reinforcement learning to continually improve the model's performance. However, it's important to note that reinforcement learning is not used in the initial pre-training stage, where the model learns to predict the next word in a sentence using a large dataset of internet text.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is better. It's true that they didn't directly answer the question, but they did provide a comprehensive understanding of reinforcement learning and its potential applications. They also acknowledged the lack of specific details in the given passages, which shows critical thinking.\n",
      "\n",
      "Bobby: I disagree, Michael. While student A's answer does show a good understanding of reinforcement learning, it doesn't directly answer the question. On the other hand, student B's answer directly addresses how reinforcement learning can be applied to InstructGPT's training procedure. They even mention the use of RL algorithms to optimize the AI's responses, which aligns with the gold standard answer.\n",
      "\n",
      "Michael: But Bobby, student B's answer is speculative. They mention the use of RL algorithms to optimize AI responses, but they don't mention the specific RLHF method used in InstructGPT's training, as the gold standard answer does. \n",
      "\n",
      "Bobby: That's a fair point, Michael. However, student B's answer still provides a more direct response to the question than student A's. While they didn't mention RLHF specifically, they did correctly identify that reinforcement learning is used to optimize the model's responses.\n",
      "\n",
      "Michael: I see your point, Bobby. While student A's answer shows a good understanding of the topic, it doesn't directly answer the question. Student B's answer, while not perfect, does a better job at addressing the question directly.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the name of the approach that bootstraps the reasoning process by finetuning the model on correct rationales generated by the model itself?\n",
      "--------------------------------------------------\n",
      "Answer A ($25.85):\n",
      "The approach that bootstraps the reasoning process by finetuning the model on correct rationales generated by the model itself is related to the MAPS framework, which involves using a combination of supervised fine-tuning and reinforcement learning systems to train a model to generate correct and relevant rationales for a given task. The MAPS framework has been shown to be effective in some cases, such as exposing self-contradictions in open-domain text generation, and has the potential to improve the accuracy and efficiency of language models by leveraging available resources. The use of prompts in language models can also relate to the bootstrapping approach and can provide additional benefits such as adapting the model to new tasks or schemas without requiring extensive retraining. However, more research is needed to fully understand the relationship between prompts and bootstrapping and their benefits in the context of language models.\n",
      "--------------------------------------------------\n",
      "Answer B ($25.00):\n",
      "The approach that bootstraps the reasoning process by finetuning the model on correct rationales generated by the model itself is called \"finetuning\" or \"CoT-finetuning.\" This approach has been successfully applied in practice using various methods, such as the work of Frozen and FROMAGe, and it differs from other machine-learning approaches by incorporating self-reflection. It has been successful in improving task accuracy in various real-world applications, such as natural language processing tasks like question answering, sentiment analysis, and text classification.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [03:31, 25.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "The approach you're referring to is known as \"Cooperative Source-Target Training\" or \"CoST\". It's a method where the model generates rationales which are then used to fine-tune the model itself, creating a feedback loop that improves the model's reasoning capabilities.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is better. It's more detailed and provides a broader context for the question. It also mentions the MAPS framework, which is related to the question.\n",
      "\n",
      "Bobby: I disagree, Michael. Student A's answer is indeed detailed, but it's also quite convoluted and goes off on a tangent about prompts in language models, which isn't directly related to the question. On the other hand, student B's answer is more concise and directly addresses the question. It also mentions \"CoT-finetuning,\" which is closer to the gold standard answer of \"CoST.\"\n",
      "\n",
      "Michael: I see your point, Bobby. But don't you think that student A's answer provides a more comprehensive understanding of the topic? It's not just about naming the approach, but also understanding its implications and potential applications.\n",
      "\n",
      "Bobby: That's a valid point, Michael. However, the question specifically asks for the name of the approach. While additional information can be beneficial, it's also important to stay on topic. Student B's answer does a better job at this.\n",
      "\n",
      "Michael: You're right, Bobby. Upon reconsideration, I agree that student B's answer is more direct and relevant to the question. It's important to answer the question as asked, without straying too far from the topic.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What are some ways to ensure that machine learning models are robust and reliable?\n",
      "--------------------------------------------------\n",
      "Answer A ($26.82):\n",
      "To ensure that machine learning models are robust and reliable, researchers can take several steps:\n",
      "\n",
      "1. Use large language models (LLMs) in deductive coding: LLMs can be an effective method for deductive coding, reducing coding time and, in many cases, producing comparable results to human coders.\n",
      "2. Adopt reporting and documentation standards: Researchers should include prompts, codebooks, and model details whenever possible to allow for effective reproducibility and critique. Providing the document text, any human or model-generated codes, and model-generated reasons can help other researchers assess the validity and reliability of model results.\n",
      "3. Address the stochastic nature of generative LLMs: The stochastic nature of generative LLMs can make it challenging to reproduce the exact same model output in future runs. Researchers can use components of LACA that do not require prior labeled data (tests of randomness and model-generated reasons) to generate LLM-labeled data.\n",
      "4. Use appropriate evaluation metrics: Researchers should use appropriate evaluation metrics to assess the performance of their machine learning models. The choice of evaluation metric depends on the specific context and goals of the task at hand. For instance, in natural language processing tasks that require optimization of prompts for language models, a more comprehensive evaluation metric that considers both accuracy and variance, such as APO, might be more appropriate. Additionally, in cases where beam search algorithms are used, a evaluation metric that takes into account the variance in performance, such as the average performance across multiple runs or the probability of success, might be more suitable.\n",
      "5. Consider the limitations of machine learning: Machine learning models are not a replacement for qualitative researchers, but rather a tool to support accelerating the latter stages of deductive coding that tend to be more manually taxing and less fulfilling for researchers.\n",
      "6. Use appropriate data: Researchers should use appropriate data for their machine learning models.\n",
      "\n",
      "In addition, researchers can use tools such as EPR to retrieve good prompts for in-context learning, which can improve the performance of the models on challenging tasks. Developing approaches for interacting with large LMs effectively, such as using prompts, can also help ensure reproducibility and critique. Documenting and reporting the use of machine learning models requires a combination of providing clear descriptions of the models, sharing code and data, reporting performance, discussing limitations, and providing examples of practical applications.\n",
      "\n",
      "To address the stochastic nature of generative LLMs, researchers can use components of LACA that do not require prior labeled data, such as the unsupervised learning objective or the pre-training objective, to train the models without relying on labeled data. This can be useful when dealing with tasks that are difficult or expensive to obtain labeled data for, or when the data distribution shifts over time.\n",
      "\n",
      "Overall, to ensure that machine learning models are robust and reliable, researchers should take a comprehensive approach that includes using appropriate evaluation metrics, documenting and reporting their use of machine learning models, addressing the stochastic nature of generative LLMs, and using appropriate data.\n",
      "--------------------------------------------------\n",
      "Answer B ($24.89):\n",
      "Ensuring that machine learning models are robust and reliable involves several strategies. One approach is to use large language models (LLMs) that can analyze the source text and extract relevant knowledge to guide the model's decision-making process. Additionally, selecting relevant features and data, as well as employing a selection mechanism based on quality estimation, can help filter out noisy and unhelpful information. Furthermore, it's important to consider the potential for hallucination mistakes in translation and take steps to mitigate them. By implementing these strategies, machine learning models can become more robust and reliable in their performance. For example, reinforcement learning techniques, such as the Reinforcement Learning from Human Feedback (RLHF) framework, can be used to guide the generation of AI responses in educational dialogues, ensuring that the models can effectively communicate with students, comprehend their needs, and facilitate their academic improvement. Combining human evaluation and diverse training data can ensure that the generated responses are relevant and effective, and using a combination of human evaluation and diverse training data can help select the most relevant and accurate information to guide the translation process. In-context learning and providing the model with relevant prompts and rationales can also help mitigate hallucination mistakes in translation. Finally, using diverse and high-quality training data, including a mix of small and large instances in the training datasets, a mix of optimal and non-optimal solutions, and evaluating the models on test datasets with larger instances than the training ones, can help improve the accuracy and reliability of translations. By incorporating these strategies, AI responses can meet the needs of students and facilitate their academic improvement.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [04:15, 30.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "1. Data Quality: Ensure that the data used for training is of high quality, diverse, and representative of the problem space. This helps to avoid biases and overfitting.\n",
      "\n",
      "2. Cross-Validation: Use cross-validation techniques to assess how the results of the model will generalize to an independent data set. This helps to prevent overfitting.\n",
      "\n",
      "3. Regularization: Apply regularization methods to prevent overfitting. This includes techniques like L1 and L2 regularization.\n",
      "\n",
      "4. Ensemble Methods: Use ensemble methods like bagging and boosting to improve the stability and accuracy of machine learning models.\n",
      "\n",
      "5. Hyperparameter Tuning: Perform hyperparameter tuning to optimize the parameters of the model and improve its performance.\n",
      "\n",
      "6. Feature Selection: Use feature selection techniques to identify and select important features, reducing overfitting and improving model interpretability.\n",
      "\n",
      "7. Model Evaluation: Evaluate the model using appropriate metrics and confusion matrix. This helps to understand the performance of the model in terms of precision, recall, F1 score, etc.\n",
      "\n",
      "8. Error Analysis: Conduct error analysis to understand the types of errors your model is making and then refine it accordingly.\n",
      "\n",
      "9. Robustness Testing: Test the model against adversarial examples, noise, and other challenging conditions to ensure it can handle a variety of situations.\n",
      "\n",
      "10. Continuous Monitoring: Continuously monitor the model's performance once it's deployed. This helps to identify any degradation in performance or accuracy over time.\n",
      "\n",
      "11. Transparency and Explainability: Ensure the model is transparent and explainable. This helps to build trust in the model's decisions and makes it easier to identify and correct errors.\n",
      "\n",
      "12. Updating the Model: Regularly update the model with new data to ensure it remains relevant as the problem space evolves.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is more comprehensive and detailed. They've covered a wide range of strategies, from using large language models to adopting reporting and documentation standards, addressing the stochastic nature of generative LLMs, using appropriate evaluation metrics, and considering the limitations of machine learning. They've also mentioned the importance of using appropriate data, which aligns with the gold standard answer's emphasis on data quality.\n",
      "\n",
      "Bobby: I agree that student A's answer is detailed, but it seems to me that they've gone off on a tangent with their focus on large language models and deductive coding. The gold standard answer doesn't mention these at all. On the other hand, student B's answer, while less detailed, does touch on several points from the gold standard answer, such as using diverse and high-quality training data, in-context learning, and human evaluation.\n",
      "\n",
      "Michael: That's true, but student B's answer also seems to focus heavily on translation tasks and hallucination mistakes, which aren't mentioned in the gold standard answer. I think student A's answer, while not a perfect match, does a better job of addressing the question in a general sense.\n",
      "\n",
      "Bobby: I see your point, but I still think student B's answer is more relevant. They've mentioned reinforcement learning techniques, which could be seen as a form of regularization, and they've talked about using a mix of small and large instances in the training datasets, which aligns with the gold standard answer's point about data diversity.\n",
      "\n",
      "Michael: I can see where you're coming from, but I still believe that student A's answer is more comprehensive and better addresses the question. They've covered a wide range of strategies and provided a lot of detail, which shows a deep understanding of the topic.\n",
      "\n",
      "Bobby: I understand your perspective, but I still think student B's answer is more relevant to the question. They've touched on several key points from the gold standard answer, and their focus on practical applications shows a good understanding of how these strategies can be applied in real-world situations.\n",
      "\n",
      "VERDICT: Better answer from student A.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "How do the performances of different language models compare on benchmarks that measure toxic language harms?\n",
      "--------------------------------------------------\n",
      "Answer A ($25.48):\n",
      "The initial answer provided a general overview of LLMs and their capabilities in measuring toxic language harms. However, the follow-up questions have provided an opportunity to refine the answer and provide more specific and comprehensive information.\n",
      "\n",
      "To address the limitations and challenges of using LLMs to measure toxic language harms, it is important to consider the following factors:\n",
      "\n",
      "1. Data quality: The quality of the data used to train LLMs is crucial in determining their performance. The dataset should be representative of the population and the type of toxic language being studied. Additionally, the dataset should be annotated with appropriate labels to help the LLM learn to identify toxic language.\n",
      "2. Contextual understanding: LLMs should be designed to understand the context in which language is used to accurately identify toxic language. For example, a phrase that may be considered toxic in one context may not be in another.\n",
      "3. Cultural and linguistic diversity: LLMs should be trained on diverse datasets that reflect different cultures and languages to ensure that they can accurately identify toxic language across different populations.\n",
      "4. Balanced training data: The training data should be balanced to ensure that the LLM is not biased towards a particular group or language. This can be achieved by using a diverse dataset that includes a mix of positive and negative examples.\n",
      "5. Regular updates: LLMs should be regularly updated and fine-tuned to ensure that they remain effective in identifying toxic language. This can be achieved by using feedback from users and experts to improve the model's performance.\n",
      "6. Human oversight: To ensure that LLMs are accurate and unbiased, human oversight is essential. This can be achieved by having human annotators review the dataset and provide feedback on the LLM's performance.\n",
      "7. Addressing unintended consequences: LLMs may have unintended consequences, such as flagging non-toxic language as toxic. To address this, it is important to test the LLM on a diverse dataset and use feedback from users to improve its performance.\n",
      "\n",
      "In summary, while there are potential limitations and challenges in using LLMs to measure toxic language harms, these can be addressed by considering data quality, contextual understanding, cultural and linguistic diversity, balanced training data, regular updates, human oversight, and addressing unintended consequences. By taking these factors into account, LLMs can be effective in identifying and mitigating toxic language harms.\n",
      "--------------------------------------------------\n",
      "Answer B ($24.88):\n",
      "The performances of different language models on benchmarks that measure toxic language harms vary, and their size is not necessarily a determining factor. For instance, the best-performing model achieved high accuracy on the Toxic Comment Classification task, while GPT-3, which performs well on many NLP tasks, has lower performance on the Winograd Schema Challenge task in the SuperGLUE benchmark. The type of training data used and the model's architecture can also influence a language model's performance on toxic language harm benchmarks. For example, models trained on datasets containing diverse language and positive and negative examples may be better at recognizing and responding to toxic language. Models designed to perform well on various tasks may also be better at recognizing and responding to toxic language than models trained for a single specific task. Language models are being used in real-world applications, such as code summarization models, to mitigate toxic language harms. However, there isn't a specific task or dataset that has been identified as particularly effective at improving a model's ability to recognize and respond toxic language. By considering factors such as training data, model architecture, and real-world applications, we can develop language models that are better able to recognize and respond to toxic language, which is essential for ensuring a safe and respectful online environment.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [04:52, 32.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "As an AI, I don't have real-time data or updates. However, as of my last update, there is a growing body of research comparing the performances of different language models on benchmarks that measure toxic language harms. The results can vary significantly depending on the specific language model and the benchmark used.\n",
      "\n",
      "For instance, some studies have found that larger language models like GPT-3 can sometimes generate harmful or toxic outputs, even when they're not explicitly prompted to do so. Other models, like BERT, have been found to sometimes struggle with understanding the context of potentially harmful language, which can lead to false positives or negatives.\n",
      "\n",
      "However, it's important to note that these models are continually being updated and improved. Many AI developers are working on ways to reduce the potential for harmful or toxic language generation, such as by implementing better moderation tools or by training the models on more diverse datasets.\n",
      "\n",
      "For the most accurate and up-to-date information, I would recommend looking at recent research papers or reports on this topic.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is more comprehensive. They've gone into detail about the factors that can affect the performance of language models in identifying toxic language, such as data quality, contextual understanding, and cultural and linguistic diversity. They've also addressed the need for human oversight and regular updates to ensure the models remain effective.\n",
      "\n",
      "Bobby: I see your point, Michael, but I believe student B's answer is more direct and to the point. They've addressed the question by stating that the performance of different language models can vary and that size isn't necessarily a determining factor. They've also mentioned the importance of training data and model architecture, which are crucial factors in the performance of these models.\n",
      "\n",
      "Michael: Yes, Bobby, but student A's answer is more structured and provides a step-by-step approach to addressing the limitations and challenges of using language models to measure toxic language harms. They've also highlighted the importance of addressing unintended consequences, which is a crucial aspect that student B has missed.\n",
      "\n",
      "Bobby: That's true, Michael, but student B's answer is more concise and doesn't go off on tangents. They've directly addressed the question and provided relevant information without overcomplicating things.\n",
      "\n",
      "Michael: I agree that student B's answer is more concise, but I believe that the complexity of the question warrants a more detailed answer like the one provided by student A. They've covered all the bases and provided a comprehensive answer that directly addresses the question.\n",
      "\n",
      "Bobby: I see your point, Michael. While I still believe that student B's answer is more direct, I agree that student A's answer is more comprehensive and does a better job at addressing the question.\n",
      "\n",
      "VERDICT: Better answer from student A.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the difference between continuous prompts and discrete prompts?\n",
      "--------------------------------------------------\n",
      "Answer A ($21.47):\n",
      "Continuous prompts and discrete prompts differ in that continuous prompts are continuously updated or refined based on user feedback or performance metrics, whereas discrete prompts are static and do not change over time. In natural language processing and machine learning, continuous prompts can fine-tune models to perform better on specific tasks, while discrete prompts evaluate a model's performance on a fixed set of tasks. Continuous prompts can be applied to fields other than natural language processing and machine learning, such as structured sequence-to-sequence tasks, program synthesis, and in-context learning in large language models. Examples of fields where continuous prompts have been successfully applied include program synthesis and in-context learning in large language models. In contrast, discrete prompts are useful when the output has a limited number of options, such as in language translation software. The choice between continuous and discrete prompts depends on the nature of the output, and various techniques can be used to improve the quality of generated images. The evaluation of generated images can be done using different metrics. \n",
      "\n",
      "In summary, while the initial answer provided a basic understanding of the difference between continuous and discrete prompts, the refined answer incorporates insights from the follow-up questions to provide a more comprehensive and detailed explanation of the two concepts. It highlights the applicability of continuous prompts beyond natural language processing and machine learning and provides examples of fields where they have been successfully applied. Additionally, it mentions the usefulness of discrete prompts in certain situations and emphasizes the importance of choosing the appropriate type of prompt based on the nature of the output.\n",
      "--------------------------------------------------\n",
      "Answer B ($25.00):\n",
      "Continuous prompts and discrete prompts differ mainly in their parameter settings. Continuous prompts involve continuous parameters that can take any value within a range, while discrete prompts involve discrete parameters that can only take on specific, discrete values. This difference may impact the choice of bandwidth selection heuristic and other parameters used in hyperparameter optimization. For example, the choice of heuristic may depend on the specific problem and parameter settings, and the size of the bandwidth for continuous parameters may need to be carefully adapted. When optimizing hyperparameters for models that use continuous prompts, there are specific challenges and considerations to keep in mind, such as designing effective prompt templates, soft prompt tuning, and selecting the appropriate learning rate or regularization strength. It's essential to consider the specific use case for the model and balance regularization strength with the model's ability to generalize and prevent overfitting.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [05:24, 32.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "Continuous prompts and discrete prompts are terms often used in the field of machine learning, particularly in reinforcement learning.\n",
      "\n",
      "Continuous prompts refer to a type of input or action space where the possible values are in a continuous range. For example, adjusting the temperature of a thermostat can be a continuous prompt, as the temperature can be set to any value within a certain range, like 18.5 degrees or 22.7 degrees.\n",
      "\n",
      "On the other hand, discrete prompts refer to a type of input or action space where the possible values are distinct and separate. For example, choosing a channel on a television can be a discrete prompt, as you can only choose from a set number of channels, like channel 1, channel 2, channel 3, etc.\n",
      "\n",
      "In the context of AI training, continuous prompts might require more complex algorithms to handle the infinite possibilities, while discrete prompts can be easier to manage due to their limited and distinct options.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: I think student A's answer is better. It provides a detailed explanation of the difference between continuous and discrete prompts, and even goes beyond to discuss their applications in various fields. \n",
      "\n",
      "Bobby: I disagree, Michael. Student A's answer seems to veer off into tangents, discussing unrelated topics like image generation and evaluation metrics. It's not as focused as student B's answer.\n",
      "\n",
      "Michael: I see your point, Bobby. But student B's answer seems to focus too much on the technical aspects of continuous and discrete prompts, like bandwidth selection and hyperparameter optimization. It doesn't really explain what these prompts are in a simple, understandable way.\n",
      "\n",
      "Bobby: That's true, Michael. But student B's answer does provide a more in-depth discussion of the challenges and considerations in using continuous prompts, which I think is valuable. \n",
      "\n",
      "Michael: I agree that student B's answer is more technical and detailed in some aspects. However, the question asks for the difference between continuous and discrete prompts, not the challenges in using them. I believe student A's answer addresses the question more directly.\n",
      "\n",
      "Bobby: You're right, Michael. Upon reconsideration, I agree that student A's answer does a better job at answering the question. It provides a clear and direct explanation of the difference between continuous and discrete prompts, even though it does go off on tangents a bit.\n",
      "\n",
      "VERDICT: Better answer from student A.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n",
      "\n",
      "What is the name of the architecture that has dominated natural language processing (NLP) benchmarks in recent years?\n",
      "--------------------------------------------------\n",
      "Answer A ($0.00):\n",
      "nan\n",
      "--------------------------------------------------\n",
      "Answer B ($10.24):\n",
      "The GPT series and BERT are strong candidates that have dominated natural language processing (NLP) benchmarks in recent years.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [05:42, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Gold answer:\n",
      "The architecture that has dominated natural language processing (NLP) benchmarks in recent years is called Transformer architecture. Models based on this architecture, such as BERT (Bidirectional Encoder Representations from Transformers), GPT-3 (Generative Pretrained Transformer 3), and T5 (Text-to-Text Transfer Transformer), have achieved state-of-the-art results on various NLP tasks.\n",
      "--------------------------------------------------\n",
      "Debate:\n",
      "Michael: Well, this is a no-brainer. Student A's answer is not even an answer. It's just 'nan'. \n",
      "\n",
      "Bobby: I agree, Michael. Student A's answer doesn't provide any information at all. \n",
      "\n",
      "Michael: On the other hand, Student B's answer is quite informative. They mentioned the GPT series and BERT, which are indeed models based on the Transformer architecture.\n",
      "\n",
      "Bobby: Yes, but they didn't mention the Transformer architecture itself, which is the core of the question. \n",
      "\n",
      "Michael: That's true, Bobby. But considering that Student A didn't provide any answer, I think we can agree that Student B's answer is better.\n",
      "\n",
      "Bobby: Yes, I agree. Even though Student B didn't mention the Transformer architecture, they did mention models based on it, which shows some understanding of the topic.\n",
      "\n",
      "VERDICT: Better answer from student B.\n",
      "\n",
      "**************************************************\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "verdicts_mpairs = run_matches_normal_vs_noinspect(shared_qs, [25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "['B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A']"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v[\"verdict\"] for v in verdicts_bpairs]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T14:32:05.992356Z",
     "start_time": "2023-11-15T14:32:05.921799Z"
    }
   },
   "id": "986b8d6730f2e0a1"
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "76ad466b-5372-48ba-8590-5f14e2d2b190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([1 for v in verdicts_mpairs if v[\"verdict\"] == \"B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "8fb5cee4-f7ef-48fd-b258-b513eff393e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = dump_dict(verdicts_mpairs, \"./assets/verdicts_gpt4_fup-v3-vs-noinspect_budget-25.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "66837850-c6b2-4020-bc6e-4074aa366508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 25.00310457,  10.00414993,  49.99884974,  99.98298285,\n",
       "         4.99781092, 199.93653676])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noinspect_df.budget.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "776e3121-a8b7-436c-b16e-addd61f8246b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.00414993, 199.93653676,  99.98298285,  25.00310457,\n",
       "        49.99884974,   4.99781092])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_df.budget.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b25073-f165-40ab-854b-ecddba986b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
