[{"tags": ["transformer", "large-language-models"], "owner": {"account_id": 29065817, "reputation": 88, "user_id": 74581, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/Nt9VK.jpg?s=256&g=1", "display_name": "Cesar Ruiz", "link": "https://ai.stackexchange.com/users/74581/cesar-ruiz"}, "is_answered": false, "view_count": 12, "answer_count": 0, "score": 0, "last_activity_date": 1693016820, "creation_date": 1693016820, "question_id": 41887, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41887/does-embeddings-and-vector-databases-solve-the-need-of-having-longer-context-win", "title": "Does Embeddings and Vector Databases solve the need of having longer context windows?"}, {"tags": ["open-ai", "gpt", "large-language-models", "gpt-3", "prompt"], "owner": {"account_id": 29297011, "reputation": 9, "user_id": 75523, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AAcHTtfxCg9L8xm--WFm6QhyUGkLLNL8iwHZbvjxQRFMCHIA=k-s256", "display_name": "tech-buddy", "link": "https://ai.stackexchange.com/users/75523/tech-buddy"}, "is_answered": false, "view_count": 15, "answer_count": 0, "score": 0, "last_activity_date": 1692961291, "creation_date": 1692961291, "question_id": 41879, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41879/improving-contextual-consistency-and-quality-in-openai-api-responses", "title": "Improving Contextual Consistency and Quality in OpenAI API Responses"}, {"tags": ["chatgpt", "large-language-models"], "owner": {"account_id": 19324558, "reputation": 1, "user_id": 75511, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gg17PG4hMoQph8bnSTjf-9nPxDY5m_QFpT5hmscSw=k-s256", "display_name": "Ganesh Saravanan", "link": "https://ai.stackexchange.com/users/75511/ganesh-saravanan"}, "is_answered": false, "view_count": 8, "answer_count": 0, "score": 0, "last_activity_date": 1692942126, "creation_date": 1692942126, "question_id": 41877, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41877/how-to-handle-multiple-instances-when-hosting-llm", "title": "How to handle multiple instances when hosting LLM"}, {"tags": ["large-language-models", "text-generation", "inference", "production-systems"], "owner": {"account_id": 6984622, "reputation": 101, "user_id": 75440, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/sKX9g.png?s=256&g=1", "display_name": "Jack Avante", "link": "https://ai.stackexchange.com/users/75440/jack-avante"}, "is_answered": false, "view_count": 13, "answer_count": 0, "score": 0, "last_activity_date": 1692717372, "creation_date": 1692717372, "question_id": 41842, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41842/why-does-a-small-model-become-huge-when-sharded-and-loaded-into-gpu-on-text-gene", "title": "Why does a small model become huge when sharded and loaded into GPU on text-generation-inference?"}, {"tags": ["neural-networks", "machine-learning", "natural-language-processing", "large-language-models", "language-model"], "owner": {"account_id": 14530464, "reputation": 1, "user_id": 74107, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/25bb659fff37fb483b9fb142e63792b6?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Dimits", "link": "https://ai.stackexchange.com/users/74107/dimits"}, "is_answered": false, "view_count": 11, "answer_count": 0, "score": 0, "last_activity_date": 1692203697, "creation_date": 1692203697, "question_id": 41793, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41793/create-samples-out-of-documents-for-causal-language-modelling", "title": "Create samples out of documents for Causal Language Modelling"}, {"tags": ["deep-learning", "training", "data-preprocessing", "large-language-models", "training-datasets"], "owner": {"account_id": 24323462, "reputation": 1, "user_id": 75277, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/196a8f5977c5126b863435379f5216d9?s=256&d=identicon&r=PG", "display_name": "bmatzelle", "link": "https://ai.stackexchange.com/users/75277/bmatzelle"}, "is_answered": false, "view_count": 16, "answer_count": 0, "score": 0, "last_activity_date": 1692200888, "creation_date": 1692200888, "question_id": 41792, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41792/what-causes-my-loss-curve-to-consistently-oscillate-when-training-an-llm", "title": "What causes my loss curve to consistently oscillate when training an LLM?"}, {"tags": ["natural-language-processing", "supervised-learning", "large-language-models", "binary-classification", "fine-tuning"], "owner": {"account_id": 326925, "reputation": 101, "user_id": 75245, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/DUiuD.jpg?s=256&g=1", "display_name": "leventov", "link": "https://ai.stackexchange.com/users/75245/leventov"}, "is_answered": false, "view_count": 11, "answer_count": 0, "score": 0, "last_activity_date": 1692134194, "creation_date": 1692134194, "question_id": 41779, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41779/any-research-in-probe-tuning-of-llms", "title": "Any research in &quot;probe-tuning&quot; of LLMs?"}, {"tags": ["machine-learning", "natural-language-processing", "transformer", "large-language-models"], "owner": {"account_id": 29209130, "reputation": 1, "user_id": 75243, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AAcHTtfK9hjBPgecHYiHDcEmXr2Y5ei-TJwxINEfFZbRGAv3=k-s256", "display_name": "naren", "link": "https://ai.stackexchange.com/users/75243/naren"}, "is_answered": false, "view_count": 11, "answer_count": 0, "score": 0, "last_activity_date": 1692130980, "creation_date": 1692130980, "question_id": 41778, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41778/what-if-in-dpr-dense-passage-retrieval-the-answer-belongs-to-more-than-one-pa", "title": "What if in DPR (dense passage retrieval), the answer belongs to more than one passage?"}, {"tags": ["deep-learning", "natural-language-processing", "large-language-models"], "owner": {"account_id": 14530464, "reputation": 1, "user_id": 74107, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/25bb659fff37fb483b9fb142e63792b6?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Dimits", "link": "https://ai.stackexchange.com/users/74107/dimits"}, "is_answered": false, "view_count": 54, "answer_count": 1, "score": 0, "last_activity_date": 1692079625, "creation_date": 1689449363, "question_id": 41295, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41295/fine-tune-llama-on-main-and-auxiliary-task", "title": "Fine-Tune Llama on main and auxiliary task"}, {"tags": ["large-language-models", "fine-tuning"], "owner": {"account_id": 29155133, "reputation": 1, "user_id": 75222, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0b0b6a6ca36b702c0d4fd11101afdf84?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "trailer_swift", "link": "https://ai.stackexchange.com/users/75222/trailer-swift"}, "is_answered": false, "view_count": 10, "answer_count": 0, "score": 0, "last_activity_date": 1692068006, "creation_date": 1692068006, "question_id": 41770, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41770/can-i-finetune-a-model-on-azure-for-information-extraction-based-on-question", "title": "can I finetune a model on Azure for information extraction based on &quot;question&quot;, &quot;context&quot;, and &quot;answer&quot; training data?"}, {"tags": ["large-language-models"], "owner": {"account_id": 49543, "reputation": 214, "user_id": 72562, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fb9f657e0b37fa483135635211e7d167?s=256&d=identicon&r=PG", "display_name": "morpheus", "link": "https://ai.stackexchange.com/users/72562/morpheus"}, "is_answered": false, "view_count": 49, "answer_count": 1, "score": 0, "last_activity_date": 1691953360, "creation_date": 1688682733, "last_edit_date": 1688683085, "question_id": 41179, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41179/what-is-better-train-a-model-from-scratch-on-your-own-data-vs-fine-tune-pretra", "title": "What is better: train a model from scratch on your own data vs. fine-tune pretrained model?"}, {"tags": ["math", "large-language-models"], "owner": {"account_id": 407237, "reputation": 301, "user_id": 65757, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fe87113ced5f370d038b90a43fd1fe1f?s=256&d=identicon&r=PG", "display_name": "Anixx", "link": "https://ai.stackexchange.com/users/65757/anixx"}, "is_answered": true, "view_count": 87, "answer_count": 3, "score": 1, "last_activity_date": 1691785918, "creation_date": 1691685809, "question_id": 41728, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41728/what-will-happen-if-to-train-an-llm-on-mathematical-exersises", "title": "What will happen if to train an LLM on mathematical exersises?"}, {"tags": ["recurrent-neural-networks", "meta-learning", "inference", "large-language-models"], "owner": {"account_id": 1021583, "reputation": 355, "user_id": 61871, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fb710534fda446d2286074bb7692e65a?s=256&d=identicon&r=PG", "display_name": "MaiaVictor", "link": "https://ai.stackexchange.com/users/61871/maiavictor"}, "is_answered": true, "view_count": 3918, "accepted_answer_id": 39898, "answer_count": 4, "score": 12, "last_activity_date": 1691249345, "creation_date": 1680265193, "last_edit_date": 1681058379, "question_id": 39863, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39863/why-llms-and-rnns-learn-so-fast-during-inference-but-ironically-are-so-slow-du", "title": "Why LLMs and RNNs learn so fast during inference but, ironically, are so slow during training?"}, {"tags": ["natural-language-processing", "gpt", "large-language-models", "language-model", "fine-tuning"], "owner": {"account_id": 9914032, "reputation": 534, "user_id": 23811, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-6jIAe62Q10s/AAAAAAAAAAI/AAAAAAAAABw/kRBVixTmLmk/photo.jpg?sz=256", "display_name": "Peyman", "link": "https://ai.stackexchange.com/users/23811/peyman"}, "is_answered": false, "view_count": 46, "answer_count": 0, "score": 0, "last_activity_date": 1691134980, "creation_date": 1691134980, "question_id": 41643, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41643/optimal-quantity-of-training-data-for-fine-tuning-an-llm-is-bigger-always-bette", "title": "Optimal Quantity of Training Data for Fine-Tuning an LLM: Is Bigger Always Better?"}, {"tags": ["search", "generative-model", "large-language-models"], "migrated_to": {"other_site": {"styling": {"tag_background_color": "#E0EAF1", "tag_foreground_color": "#000", "link_color": "#0077CC"}, "related_sites": [{"relation": "meta", "api_site_parameter": "genai.meta", "site_url": "https://genai.meta.stackexchange.com", "name": "GenAI Meta Stack Exchange"}, {"relation": "chat", "site_url": "https://chat.stackexchange.com?tab=site&host=genai.stackexchange.com", "name": "Chat Stack Exchange"}], "open_beta_date": 1690297236, "closed_beta_date": 1689617537, "site_state": "open_beta", "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/genai/Img/apple-touch-icon@2.png", "favicon_url": "https://cdn.sstatic.net/Sites/genai/Img/favicon.ico", "icon_url": "https://cdn.sstatic.net/Sites/genai/Img/apple-touch-icon.png", "audience": "GenAI enthusiasts and practitioners and those interested in learning more about using GenAI tools", "site_url": "https://genai.stackexchange.com", "api_site_parameter": "genai", "logo_url": "https://cdn.sstatic.net/Sites/genai/Img/apple-touch-icon.png", "name": "GenAI", "site_type": "main_site"}, "on_date": 1691048580, "question_id": 252}, "owner": {"account_id": 6702487, "reputation": 1, "user_id": 74756, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6538e613d1c576214074905c690ca34e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "anshuk_pal", "link": "https://ai.stackexchange.com/users/74756/anshuk-pal"}, "is_answered": false, "view_count": 27, "closed_date": 1691048580, "answer_count": 0, "score": 0, "locked_date": 1691048580, "last_activity_date": 1690966150, "creation_date": 1690891163, "question_id": 41588, "link": "https://ai.stackexchange.com/questions/41588/generative-ai-use-case-for-search-domain", "closed_reason": "Not suitable for this site", "title": "Generative AI Use Case for Search Domain"}, {"tags": ["explainable-ai", "large-language-models"], "owner": {"account_id": 233741, "reputation": 111, "user_id": 72064, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9b5a6fb80ac613af1d2f3a78c985cee1?s=256&d=identicon&r=PG", "display_name": "lithuak", "link": "https://ai.stackexchange.com/users/72064/lithuak"}, "is_answered": false, "view_count": 377, "answer_count": 1, "score": 1, "last_activity_date": 1690766264, "creation_date": 1684599561, "question_id": 40519, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40519/surveys-important-papers-in-explainability-for-llm", "title": "Surveys/Important papers in Explainability for LLM?"}, {"tags": ["large-language-models", "fine-tuning", "gpt-4", "few-shot-learning", "rlhf"], "owner": {"account_id": 2526078, "reputation": 223, "user_id": 41187, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3bab6dd8e8cd71a3e564d68189721571?s=256&d=identicon&r=PG", "display_name": "Exploring", "link": "https://ai.stackexchange.com/users/41187/exploring"}, "is_answered": true, "view_count": 29, "answer_count": 1, "score": 0, "last_activity_date": 1690587576, "creation_date": 1690504965, "question_id": 41509, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41509/what-is-the-difference-betwen-fine-runing-and-rlhf-for-llm", "title": "What is the difference betwen fine runing and rlhf for llm?"}, {"tags": ["philosophy", "large-language-models", "logic"], "owner": {"account_id": 1532620, "reputation": 163, "user_id": 48038, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/16df5085d4b8372b5dcadabb87545ad3?s=256&d=identicon&r=PG", "display_name": "Geremia", "link": "https://ai.stackexchange.com/users/48038/geremia"}, "is_answered": true, "view_count": 56, "answer_count": 1, "score": 1, "last_activity_date": 1690183728, "creation_date": 1690171555, "question_id": 41444, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41444/can-llms-be-used-to-discover-new-laws-of-logic", "title": "Can LLMs be used to discover new laws of logic?"}, {"tags": ["natural-language-processing", "recurrent-neural-networks", "transformer", "long-short-term-memory", "large-language-models"], "owner": {"account_id": 7857371, "reputation": 146, "user_id": 68775, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6f9aa372691e4af70e3420cb944d6360?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "llllvvuu", "link": "https://ai.stackexchange.com/users/68775/llllvvuu"}, "is_answered": false, "view_count": 44, "answer_count": 0, "score": 0, "last_activity_date": 1689944286, "creation_date": 1689937360, "last_edit_date": 1689937806, "question_id": 41393, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41393/what-are-the-non-cost-related-reasons-rnnattention-underperform-transformers", "title": "What are the *non-cost-related* reasons RNN+Attention underperform Transformers?"}, {"tags": ["generative-model", "chatgpt", "large-language-models"], "owner": {"account_id": 2883964, "reputation": 123, "user_id": 74298, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/BDsoE.jpg?s=256&g=1", "display_name": "machine_1", "link": "https://ai.stackexchange.com/users/74298/machine-1"}, "is_answered": true, "view_count": 84, "closed_date": 1690236135, "accepted_answer_id": 41380, "answer_count": 1, "score": 2, "last_activity_date": 1689937411, "creation_date": 1689865286, "question_id": 41379, "link": "https://ai.stackexchange.com/questions/41379/could-hallucinations-be-the-demise-of-the-ai-hype", "closed_reason": "Needs more focus", "title": "Could hallucinations be the demise of the AI hype?"}, {"tags": ["transformer", "open-ai", "embeddings", "large-language-models", "inference"], "owner": {"account_id": 7857371, "reputation": 146, "user_id": 68775, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6f9aa372691e4af70e3420cb944d6360?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "llllvvuu", "link": "https://ai.stackexchange.com/users/68775/llllvvuu"}, "is_answered": true, "view_count": 159, "accepted_answer_id": 41388, "answer_count": 1, "score": 0, "last_activity_date": 1689907385, "creation_date": 1689735302, "last_edit_date": 1689907181, "question_id": 41352, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41352/why-does-llm-inference-cost-scale-in-both-input-tokens-and-output-tokens", "title": "Why does LLM inference cost scale in both input tokens and output tokens?"}, {"tags": ["reference-request", "large-language-models"], "owner": {"account_id": 12955697, "reputation": 1, "user_id": 26468, "user_type": "registered", "profile_image": "https://graph.facebook.com/1168112533324031/picture?type=large", "display_name": "Sudhanshu Mishra", "link": "https://ai.stackexchange.com/users/26468/sudhanshu-mishra"}, "is_answered": false, "view_count": 12, "answer_count": 0, "score": 0, "last_activity_date": 1689752245, "creation_date": 1689752245, "question_id": 41355, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41355/does-anyone-know-about-a-reference-where-someone-has-aggregated-the-cost-optimiz", "title": "Does anyone know about a reference where someone has aggregated the cost optimization strategies for deploying LLMs?"}, {"tags": ["machine-learning", "deep-learning", "natural-language-processing", "transformer", "large-language-models"], "owner": {"account_id": 25792554, "reputation": 11, "user_id": 73994, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AFdZucpsmLwU4IWcVcC07X72WJ1RqyoubofvikkSMmgf=k-s256", "display_name": "jgeddes", "link": "https://ai.stackexchange.com/users/73994/jgeddes"}, "is_answered": false, "view_count": 183, "answer_count": 0, "score": 1, "last_activity_date": 1689675822, "creation_date": 1689161718, "last_edit_date": 1689675822, "question_id": 41249, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41249/what-is-considered-the-pre-fill-and-what-is-considered-the-decoding-phase-in-th", "title": "What is considered the pre-fill, and what is considered the decoding phase in this process?"}, {"tags": ["neural-networks", "natural-language-processing", "transformer", "natural-language-generation", "large-language-models"], "owner": {"account_id": 28150295, "reputation": 21, "user_id": 69933, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/293bc9ca2e3f1b9e0fd6083054588c69?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "MLBeginner", "link": "https://ai.stackexchange.com/users/69933/mlbeginner"}, "is_answered": false, "view_count": 639, "answer_count": 2, "score": 2, "last_activity_date": 1689516395, "creation_date": 1680027833, "last_edit_date": 1684298671, "question_id": 39824, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39824/how-does-a-llm-transformer-pick-words-from-its-vocabulary", "title": "How does a LLM (transformer) pick words from its vocabulary?"}, {"tags": ["large-language-models", "training-datasets"], "owner": {"account_id": 7857371, "reputation": 146, "user_id": 68775, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6f9aa372691e4af70e3420cb944d6360?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "llllvvuu", "link": "https://ai.stackexchange.com/users/68775/llllvvuu"}, "is_answered": true, "view_count": 29, "closed_date": 1690305169, "accepted_answer_id": 41307, "answer_count": 1, "score": -1, "last_activity_date": 1689486325, "creation_date": 1689385775, "question_id": 41293, "link": "https://ai.stackexchange.com/questions/41293/open-access-adept-like-dataset-llm-to-computer-input", "closed_reason": "Not suitable for this site", "title": "Open access Adept-like dataset? (LLM-to-computer-input)"}, {"tags": ["large-language-models"], "owner": {"account_id": 49543, "reputation": 214, "user_id": 72562, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fb9f657e0b37fa483135635211e7d167?s=256&d=identicon&r=PG", "display_name": "morpheus", "link": "https://ai.stackexchange.com/users/72562/morpheus"}, "is_answered": true, "view_count": 546, "answer_count": 3, "score": 3, "last_activity_date": 1689480774, "creation_date": 1689282875, "question_id": 41277, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41277/why-cant-lucene-search-be-used-to-power-llm-applications", "title": "Why can&#39;t Lucene search be used to power LLM applications?"}, {"tags": ["chatgpt", "large-language-models", "prompt", "prompt-design"], "owner": {"account_id": 8711855, "reputation": 131, "user_id": 41531, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/5Jsv3.jpg?s=256&g=1", "display_name": "cnmesr", "link": "https://ai.stackexchange.com/users/41531/cnmesr"}, "is_answered": true, "view_count": 1042, "answer_count": 2, "score": 3, "last_activity_date": 1689277108, "creation_date": 1689215342, "question_id": 41262, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41262/how-to-formulate-a-realiable-chatgpt-prompt-for-sentiment-analysis-of-a-text-an", "title": "How to Formulate a realiable ChatGPT Prompt for Sentiment Analysis of a Text, and show that it is reliable?"}, {"tags": ["datasets", "large-language-models"], "owner": {"account_id": 18613454, "reputation": 123, "user_id": 73068, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/327839fd17c445cd219177c746b10097?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Blue Nebula", "link": "https://ai.stackexchange.com/users/73068/blue-nebula"}, "is_answered": false, "view_count": 133, "answer_count": 0, "score": 0, "last_activity_date": 1689150350, "creation_date": 1689150350, "question_id": 41247, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41247/what-role-does-data-quality-plays-in-the-llm-scaling-laws", "title": "What role does data quality plays in the LLM scaling laws?"}, {"tags": ["transformer", "open-ai", "large-language-models", "gpt-4", "open-source"], "owner": {"account_id": 20621839, "reputation": 103, "user_id": 65278, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/OVkG6.jpg?s=256&g=1", "display_name": "hmltn", "link": "https://ai.stackexchange.com/users/65278/hmltn"}, "is_answered": true, "view_count": 386, "answer_count": 2, "score": 0, "last_activity_date": 1688977409, "creation_date": 1688892865, "question_id": 41214, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41214/how-do-open-source-llms-compare-to-gpt-4", "title": "How do open source LLMs compare to GPT-4?"}, {"tags": ["large-language-models", "prompt", "prompt-design"], "owner": {"account_id": 20621839, "reputation": 103, "user_id": 65278, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/OVkG6.jpg?s=256&g=1", "display_name": "hmltn", "link": "https://ai.stackexchange.com/users/65278/hmltn"}, "is_answered": true, "view_count": 93, "answer_count": 1, "score": 4, "last_activity_date": 1688961486, "creation_date": 1688815472, "question_id": 41207, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41207/who-invented-dan", "title": "Who invented DAN?"}, {"tags": ["large-language-models"], "owner": {"account_id": 1630025, "reputation": 1, "user_id": 72661, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/abafd268f00a23dfe78355b4195208d5?s=256&d=identicon&r=PG", "display_name": "amh", "link": "https://ai.stackexchange.com/users/72661/amh"}, "is_answered": false, "view_count": 75, "answer_count": 1, "score": 0, "last_activity_date": 1688821425, "creation_date": 1685928566, "question_id": 40704, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40704/why-do-llama-and-its-variants-have-non-round-numbers-of-parameters", "title": "Why do LLaMa and its variants have non-\u201cround\u201d numbers of parameters?"}, {"tags": ["natural-language-processing", "language-model", "large-language-models", "information-theory", "benchmarks"], "owner": {"account_id": 20621839, "reputation": 103, "user_id": 65278, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/OVkG6.jpg?s=256&g=1", "display_name": "hmltn", "link": "https://ai.stackexchange.com/users/65278/hmltn"}, "is_answered": false, "view_count": 31, "answer_count": 0, "score": 0, "last_activity_date": 1688712054, "creation_date": 1688712054, "question_id": 41184, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41184/a-technique-to-show-what-tokens-are-relatively-predicted-by-an-llm", "title": "A technique to show what tokens are relatively predicted by an LLM"}, {"tags": ["large-language-models", "kl-divergence"], "owner": {"account_id": 18889400, "reputation": 101, "user_id": 38765, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/ZrXEK.png?s=256&g=1", "display_name": "Safdar Faisal", "link": "https://ai.stackexchange.com/users/38765/safdar-faisal"}, "is_answered": false, "view_count": 64, "answer_count": 0, "score": 0, "last_activity_date": 1688644599, "creation_date": 1688644599, "question_id": 41171, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41171/how-does-one-decide-the-probability-distribution-for-an-llm-during-rlhf", "title": "How does one decide the probability distribution for an LLM during RLHF?"}, {"tags": ["large-language-models"], "owner": {"account_id": 304165, "reputation": 103, "user_id": 73765, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/1691d582633281b0c236ec3eca94a034?s=256&d=identicon&r=PG", "display_name": "adjfac", "link": "https://ai.stackexchange.com/users/73765/adjfac"}, "is_answered": true, "view_count": 308, "closed_date": 1689639238, "accepted_answer_id": 41158, "answer_count": 1, "score": 0, "last_activity_date": 1688578518, "creation_date": 1688567837, "question_id": 41154, "link": "https://ai.stackexchange.com/questions/41154/using-llm-to-query-specific-databases-where-can-i-find-implementation-examples", "closed_reason": "Needs more focus", "title": "Using LLM to query specific databases - where can I find implementation examples"}, {"tags": ["embeddings", "large-language-models"], "owner": {"account_id": 9620435, "reputation": 1, "user_id": 73736, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/22efec6999336f34074e7a3b891b4cc6?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "curl-up", "link": "https://ai.stackexchange.com/users/73736/curl-up"}, "is_answered": true, "view_count": 25, "answer_count": 1, "score": 0, "last_activity_date": 1688518420, "creation_date": 1688513876, "last_edit_date": 1688513908, "question_id": 41149, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41149/ignoring-aspects-of-text-embeddings-e-g-making-the-embedding-topic-agnostic", "title": "Ignoring aspects of text embeddings, e.g. making the embedding topic-agnostic"}, {"tags": ["large-language-models", "gpu", "hardware", "hardware-evaluation"], "owner": {"account_id": 5490866, "reputation": 131, "user_id": 63672, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/839478ae95586357c0ef4d395eb29d8f?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "ahron", "link": "https://ai.stackexchange.com/users/63672/ahron"}, "is_answered": true, "view_count": 986, "answer_count": 1, "score": 1, "last_activity_date": 1688255522, "creation_date": 1686744097, "question_id": 40839, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40839/for-an-llm-model-how-can-i-estimate-its-memory-requirements-based-on-storage-us", "title": "For an LLM model, how can I estimate its memory requirements based on storage usage?"}, {"tags": ["deep-learning", "training", "pytorch", "large-language-models", "gpu"], "owner": {"account_id": 5609802, "reputation": 101, "user_id": 53497, "user_type": "registered", "profile_image": "https://graph.facebook.com/100001646853218/picture?type=large", "display_name": "Rituraj Singh", "link": "https://ai.stackexchange.com/users/53497/rituraj-singh"}, "is_answered": false, "view_count": 169, "answer_count": 0, "score": 0, "last_activity_date": 1688235812, "creation_date": 1688235812, "question_id": 41087, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41087/relation-between-batch-size-and-micro-batch-size", "title": "Relation between Batch Size and Micro Batch Size"}, {"tags": ["large-language-models"], "owner": {"account_id": 1938654, "reputation": 121, "user_id": 73594, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/2997871e12024728f5c1a92a77560dbf?s=256&d=identicon&r=PG", "display_name": "Balaji Kartheeswaran", "link": "https://ai.stackexchange.com/users/73594/balaji-kartheeswaran"}, "is_answered": false, "view_count": 693, "answer_count": 0, "score": 2, "last_activity_date": 1688158812, "creation_date": 1688158812, "question_id": 41066, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41066/ram-capacity-of-mac-studio-with-m2-ultra-for-inference-of-65b-llm", "title": "RAM Capacity of Mac Studio with M2 Ultra for inference of 65B LLM"}, {"tags": ["large-language-models"], "owner": {"account_id": 1711161, "reputation": 203, "user_id": 19686, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/bd0db7459f9940095b1518fa7ec32a72?s=256&d=identicon&r=PG", "display_name": "Bruce Adams", "link": "https://ai.stackexchange.com/users/19686/bruce-adams"}, "is_answered": true, "view_count": 234, "answer_count": 1, "score": 1, "last_activity_date": 1687525128, "creation_date": 1687455831, "last_edit_date": 1687474854, "question_id": 40939, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40939/what-is-function-calling-in-openais-chatgpt-models", "title": "What is Function calling in openAI&#39;s chatGPT models?"}, {"tags": ["embeddings", "large-language-models"], "owner": {"account_id": 303500, "reputation": 101, "user_id": 73236, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6fe42a41d445efbe0b87ed7b36e79428?s=256&d=identicon&r=PG", "display_name": "Charles", "link": "https://ai.stackexchange.com/users/73236/charles"}, "is_answered": false, "view_count": 42, "answer_count": 0, "score": 0, "last_activity_date": 1687280340, "creation_date": 1687280340, "question_id": 40906, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40906/is-it-possible-to-generate-new-text-matching-a-vector-embedding-with-an-llm", "title": "Is it possible to generate new text matching a vector embedding with an LLM"}, {"tags": ["chatgpt", "knowledge-representation", "explainable-ai", "large-language-models", "uncertainty-quantification"], "owner": {"account_id": 150314, "reputation": 811, "user_id": 25362, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/tVWLM.png?s=256&g=1", "display_name": "Hans-Peter Stricker", "link": "https://ai.stackexchange.com/users/25362/hans-peter-stricker"}, "is_answered": true, "view_count": 150, "answer_count": 2, "score": 1, "last_activity_date": 1686956076, "creation_date": 1686906638, "last_edit_date": 1686928297, "question_id": 40874, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40874/how-do-language-models-know-what-they-dont-know-and-report-it", "title": "How do language models know what they don&#39;t know - and report it?"}, {"tags": ["large-language-models"], "owner": {"account_id": 4033122, "reputation": 131, "user_id": 73105, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fa52f0ed961993dce0a5c271dca0b4b7?s=256&d=identicon&r=PG", "display_name": "Daniel Darabos", "link": "https://ai.stackexchange.com/users/73105/daniel-darabos"}, "is_answered": false, "view_count": 146, "answer_count": 0, "score": 0, "last_activity_date": 1686936757, "creation_date": 1686936757, "question_id": 40883, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40883/how-does-vocabulary-size-affect-quality", "title": "How does vocabulary size affect quality?"}, {"tags": ["chatgpt", "natural-language-generation", "large-language-models"], "owner": {"account_id": 24980580, "reputation": 121, "user_id": 57463, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c6c5d9858e0b80805a52c8442693e9ca?s=256&d=identicon&r=PG", "display_name": "userrandrand", "link": "https://ai.stackexchange.com/users/57463/userrandrand"}, "is_answered": true, "view_count": 2179, "answer_count": 2, "score": 2, "last_activity_date": 1685646656, "creation_date": 1679932925, "last_edit_date": 1680079925, "question_id": 39813, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39813/what-temperature-would-you-recommend-for-the-chatgpt-api", "title": "What temperature would you recommend for the chatgpt api?"}, {"tags": ["deep-learning", "transformer", "attention", "gpt", "large-language-models"], "owner": {"account_id": 11887562, "reputation": 1800, "user_id": 34383, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c3938672a162d7f04ee40a146836de6d?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Robin van Hoorn", "link": "https://ai.stackexchange.com/users/34383/robin-van-hoorn"}, "is_answered": true, "view_count": 7061, "answer_count": 1, "score": 10, "last_activity_date": 1685456475, "creation_date": 1682278110, "last_edit_date": 1685456475, "question_id": 40179, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40179/how-does-the-decoder-only-transformer-architecture-work", "title": "How does the (decoder-only) transformer architecture work?"}, {"tags": ["attention", "large-language-models"], "owner": {"account_id": 3251786, "reputation": 103, "user_id": 72310, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c638b05bf29d8b4305f592fd7c570e10?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "user2741831", "link": "https://ai.stackexchange.com/users/72310/user2741831"}, "is_answered": true, "view_count": 167, "accepted_answer_id": 40602, "answer_count": 2, "score": 0, "last_activity_date": 1685105335, "creation_date": 1685082144, "last_edit_date": 1685089874, "question_id": 40592, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40592/how-is-a-parameter-explosion-prevented-when-connecting-a-mutlihead-attention-la", "title": "How is a parameter explosion prevented, when connecting a mutlihead attention layer with the dense layers in LLMs (speciafially, LLama)?"}, {"tags": ["open-ai", "embeddings", "large-language-models"], "owner": {"account_id": 8885044, "reputation": 1, "user_id": 72322, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6c4d4676036162ad3253aa48e425bbe7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Stefan", "link": "https://ai.stackexchange.com/users/72322/stefan"}, "is_answered": false, "view_count": 94, "answer_count": 0, "score": 0, "last_activity_date": 1685100342, "creation_date": 1685100342, "question_id": 40600, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40600/improving-embedding-similarity-search-of-aggregated-embeddings", "title": "Improving embedding similarity search of aggregated embeddings"}, {"tags": ["neural-networks", "machine-learning", "regression", "large-language-models"], "owner": {"account_id": 19366364, "reputation": 23, "user_id": 71961, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9e3fb46ef8e4f6dd5c81ed739a1da8a1?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "sharkeater123", "link": "https://ai.stackexchange.com/users/71961/sharkeater123"}, "is_answered": true, "view_count": 519, "accepted_answer_id": 40488, "answer_count": 1, "score": 2, "last_activity_date": 1684693863, "creation_date": 1684354716, "last_edit_date": 1684693863, "question_id": 40486, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40486/is-it-possible-to-use-llms-for-regression-tasks", "title": "Is it possible to use LLMs for regression tasks?"}, {"tags": ["fine-tuning", "large-language-models"], "owner": {"account_id": 1066163, "reputation": 101, "user_id": 71401, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/8fe68d99410e3519f970008f80826ffe?s=256&d=identicon&r=PG", "display_name": "CWcx", "link": "https://ai.stackexchange.com/users/71401/cwcx"}, "is_answered": false, "view_count": 254, "answer_count": 0, "score": 0, "last_activity_date": 1684329797, "creation_date": 1684329797, "question_id": 40478, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40478/fine-tune-nanogpt-for-instructions", "title": "fine-tune nanoGPT for instructions"}, {"tags": ["large-language-models"], "owner": {"account_id": 7711330, "reputation": 141, "user_id": 69166, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/93d53e3977612b13cd43f768c3bac40e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "zzzgoo", "link": "https://ai.stackexchange.com/users/69166/zzzgoo"}, "is_answered": true, "view_count": 26, "answer_count": 1, "score": 0, "last_activity_date": 1683962608, "creation_date": 1679805743, "question_id": 39795, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39795/in-which-process-does-filter-and-watermark-take-place", "title": "In which process does filter and watermark take place?"}, {"tags": ["transformer", "autoencoders", "large-language-models", "encoder-decoder"], "owner": {"account_id": 9546969, "reputation": 183, "user_id": 62288, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/feded8d3ed8dca160f793ff782cb5226?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "wrek", "link": "https://ai.stackexchange.com/users/62288/wrek"}, "is_answered": true, "view_count": 63, "accepted_answer_id": 40377, "answer_count": 1, "score": 0, "last_activity_date": 1683673855, "creation_date": 1683590746, "last_edit_date": 1683673855, "question_id": 40370, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40370/for-a-transformer-decoder-how-exactly-are-k-q-and-v-for-each-decoding-step", "title": "For a transformer decoder, how exactly are K, Q, and V for each decoding step?"}, {"tags": ["training", "recurrent-neural-networks", "large-language-models"], "owner": {"account_id": 1021583, "reputation": 355, "user_id": 61871, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fb710534fda446d2286074bb7692e65a?s=256&d=identicon&r=PG", "display_name": "MaiaVictor", "link": "https://ai.stackexchange.com/users/61871/maiavictor"}, "is_answered": false, "view_count": 599, "answer_count": 0, "score": 4, "last_activity_date": 1683471895, "creation_date": 1680143303, "question_id": 39840, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39840/llm-like-architecture-capable-of-dynamically-learning-from-its-own-output", "title": "LLM-like architecture capable of dynamically learning from its own output"}, {"tags": ["fine-tuning", "large-language-models", "gpt-4", "prompt", "prompt-design"], "owner": {"account_id": 20621839, "reputation": 103, "user_id": 65278, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/OVkG6.jpg?s=256&g=1", "display_name": "hmltn", "link": "https://ai.stackexchange.com/users/65278/hmltn"}, "is_answered": true, "view_count": 200, "answer_count": 1, "score": 1, "last_activity_date": 1683216328, "creation_date": 1683203150, "question_id": 40302, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40302/what-researched-backed-findings-is-there-for-prompting-llm-s-gpt-4-to-give-spe", "title": "What researched-backed findings is there for prompting LLM\u2019s / GPT-4 to give specific information or actionable plans?"}, {"tags": ["generative-model", "chat-bots", "large-language-models"], "owner": {"account_id": 3450916, "reputation": 101, "user_id": 71363, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/MiNJV.jpg?s=256&g=1", "display_name": "lesssugar", "link": "https://ai.stackexchange.com/users/71363/lesssugar"}, "is_answered": false, "view_count": 13, "answer_count": 0, "score": 0, "last_activity_date": 1683100609, "creation_date": 1683099881, "last_edit_date": 1683100609, "question_id": 40281, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40281/whats-the-right-approach-to-teach-a-chatbot-about-a-big-specific-data-set-tr", "title": "What&#39;s the right approach to &quot;teach&quot; a chatbot about a big specific data set: training it or feeding it the input once?"}, {"tags": ["neural-networks", "cross-entropy", "large-language-models"], "owner": {"account_id": 12528967, "reputation": 121, "user_id": 42832, "user_type": "registered", "profile_image": "https://graph.facebook.com/10210927549521147/picture?type=large", "display_name": "Tommaso Bendinelli", "link": "https://ai.stackexchange.com/users/42832/tommaso-bendinelli"}, "is_answered": false, "view_count": 113, "answer_count": 0, "score": 0, "last_activity_date": 1682675990, "creation_date": 1682675990, "question_id": 40224, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40224/do-llm-or-machine-learning-models-with-a-large-number-of-classes-employ-standard", "title": "Do LLM or Machine Learning models with a large number of classes employ standard cross entropy?"}, {"tags": ["math", "large-language-models", "tensor"], "owner": {"account_id": 2246028, "reputation": 111, "user_id": 70772, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/626641784f75a42ad8a6a86e3dc8bc62?s=256&d=identicon&r=PG", "display_name": "Wolfgang", "link": "https://ai.stackexchange.com/users/70772/wolfgang"}, "is_answered": true, "view_count": 206, "answer_count": 1, "score": 1, "last_activity_date": 1682611779, "creation_date": 1681799086, "question_id": 40105, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40105/what-operation-is-ggml-mul-mat-performing-k%c3%97q-in-llama", "title": "What operation is ggml_mul_mat performing? (K&#215;Q in LLaMA)"}, {"tags": ["reinforcement-learning", "chatgpt", "large-language-models", "rlhf"], "owner": {"account_id": 1893897, "reputation": 184, "user_id": 20874, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/593c2d4f4372384ae06efe0d991fa184?s=256&d=identicon&r=PG", "display_name": "DeltaIV", "link": "https://ai.stackexchange.com/users/20874/deltaiv"}, "is_answered": true, "view_count": 365, "closed_date": 1682548335, "answer_count": 1, "score": 2, "last_activity_date": 1682553853, "creation_date": 1677795547, "last_edit_date": 1682365836, "question_id": 39392, "link": "https://ai.stackexchange.com/questions/39392/why-do-we-need-rl-in-rlhf", "closed_reason": "Needs details or clarity", "title": "Why do we need RL in RLHF?"}, {"tags": ["natural-language-processing", "chatgpt", "language-model", "large-language-models", "prompt"], "owner": {"account_id": 24670361, "reputation": 11, "user_id": 54746, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/Llcof.jpg?s=256&g=1", "display_name": "Iris88", "link": "https://ai.stackexchange.com/users/54746/iris88"}, "is_answered": true, "view_count": 107, "answer_count": 1, "score": 1, "last_activity_date": 1682424481, "creation_date": 1679321664, "last_edit_date": 1679829494, "question_id": 39689, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39689/if-we-prompt-a-large-language-model-on-a-task-will-its-ability-for-other-tasks", "title": "If we prompt a large language model on a task, will its ability for other tasks be affected? How to recover?"}, {"tags": ["open-ai", "chatgpt", "large-language-models"], "owner": {"account_id": 69659, "reputation": 143, "user_id": 70786, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/Clpiv.jpg?s=256&g=1", "display_name": "knb", "link": "https://ai.stackexchange.com/users/70786/knb"}, "is_answered": true, "view_count": 5387, "accepted_answer_id": 40165, "answer_count": 1, "score": 2, "last_activity_date": 1682174171, "creation_date": 1681825916, "question_id": 40111, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40111/openai-what-is-the-difference-between-model-gpt-3-5-turbo-and-gpt-3-5-turbo", "title": "OpenAI: What is the difference between model &quot;gpt-3.5-turbo&quot; and &quot;gpt-3.5-turbo-0301&quot;?"}, {"tags": ["large-language-models", "bias", "gpt-4", "culture"], "owner": {"account_id": 4153260, "reputation": 589, "user_id": 2317, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3492e6631a5b35e255a348d92e47b2c7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Volker Siegel", "link": "https://ai.stackexchange.com/users/2317/volker-siegel"}, "is_answered": false, "view_count": 71, "answer_count": 0, "score": 0, "last_activity_date": 1682004094, "creation_date": 1682004094, "question_id": 40132, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40132/how-can-a-bias-in-a-large-language-model-like-gpt-4-depend-on-the-languages-used", "title": "How can a bias in a large language model like GPT-4 depend on the languages used for interaction?"}, {"tags": ["training", "language-model", "large-language-models"], "owner": {"account_id": 11887562, "reputation": 1800, "user_id": 34383, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c3938672a162d7f04ee40a146836de6d?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Robin van Hoorn", "link": "https://ai.stackexchange.com/users/34383/robin-van-hoorn"}, "is_answered": true, "view_count": 451, "accepted_answer_id": 38972, "answer_count": 2, "score": 4, "last_activity_date": 1681854735, "creation_date": 1674631986, "last_edit_date": 1681808168, "question_id": 38879, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/38879/what-makes-reproducing-a-model-like-gpt3-gpt3-5-chatgpt-difficult", "title": "What makes reproducing a model like GPT3/GPT3.5/ChatGPT difficult?"}, {"tags": ["transformer", "large-language-models"], "owner": {"account_id": 13098694, "reputation": 31, "user_id": 68445, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/4b3c8044ba8986a1a01bfa20a6c5bc92?s=256&d=identicon&r=PG", "display_name": "hokage555", "link": "https://ai.stackexchange.com/users/68445/hokage555"}, "is_answered": false, "view_count": 114, "answer_count": 0, "score": 3, "last_activity_date": 1681814513, "creation_date": 1677003271, "last_edit_date": 1681814513, "question_id": 39249, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39249/why-do-llms-like-gpt-3-or-bloom-use-vanilla-transformer-instead-of-long-sequence", "title": "Why do LLMs like GPT-3 or Bloom use Vanilla Transformer instead of long sequence variants like Transformer-XL?"}, {"tags": ["open-ai", "gpt", "large-language-models", "intelligence-quotient"], "owner": {"account_id": 2054280, "reputation": 137, "user_id": 25452, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/NgCbm.png?s=256&g=1", "display_name": "R1-", "link": "https://ai.stackexchange.com/users/25452/r1"}, "is_answered": true, "view_count": 141, "closed_date": 1680532870, "accepted_answer_id": 39882, "answer_count": 1, "score": -1, "last_activity_date": 1681758079, "creation_date": 1680306997, "last_edit_date": 1681758079, "question_id": 39873, "link": "https://ai.stackexchange.com/questions/39873/could-it-be-probable-to-quantify-or-measure-the-iq-of-a-super-intelligent-machin", "closed_reason": "Opinion-based", "title": "Could it be probable to quantify or measure the IQ of a super-intelligent machine like GPT?"}, {"tags": ["image-recognition", "image-generation", "language-model", "large-language-models", "gpt-4"], "owner": {"account_id": 4153260, "reputation": 589, "user_id": 2317, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3492e6631a5b35e255a348d92e47b2c7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Volker Siegel", "link": "https://ai.stackexchange.com/users/2317/volker-siegel"}, "is_answered": true, "view_count": 645, "answer_count": 2, "score": 0, "last_activity_date": 1681741400, "creation_date": 1681415042, "question_id": 40049, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40049/is-the-gpt-4-for-text-the-same-model-that-can-input-and-output-images", "title": "Is the GPT-4 for text the same model that can input and output images?"}, {"tags": ["natural-language-processing", "classification", "gpt", "large-language-models"], "owner": {"account_id": 22245473, "reputation": 109, "user_id": 69430, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/65c431665dd627d97f61397062e6ad73?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Glue", "link": "https://ai.stackexchange.com/users/69430/glue"}, "is_answered": true, "view_count": 170, "answer_count": 1, "score": 0, "last_activity_date": 1681734291, "creation_date": 1679146149, "last_edit_date": 1681734291, "question_id": 39652, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39652/large-language-models-vs-tabular-data", "title": "Large Language Models vs Tabular Data"}, {"tags": ["deep-learning", "transformer", "pytorch", "large-language-models"], "owner": {"account_id": 15779382, "reputation": 11, "user_id": 70696, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c611394566c1f6858cf4d0f766789b31?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "JB1", "link": "https://ai.stackexchange.com/users/70696/jb1"}, "is_answered": false, "view_count": 107, "closed_date": 1682225293, "answer_count": 1, "score": 1, "last_activity_date": 1681683400, "creation_date": 1681634953, "last_edit_date": 1681641010, "question_id": 40080, "link": "https://ai.stackexchange.com/questions/40080/playing-around-with-transformer-accuracy-not-improving", "closed_reason": "Not suitable for this site", "title": "Playing around with Transformer - accuracy not improving"}, {"tags": ["backpropagation", "gpt", "knowledge-representation", "language-model", "large-language-models"], "owner": {"account_id": 236479, "reputation": 235, "user_id": 17302, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/O3XKH.png?s=256&g=1", "display_name": "Yan King Yin", "link": "https://ai.stackexchange.com/users/17302/yan-king-yin"}, "is_answered": false, "view_count": 117, "answer_count": 0, "score": 0, "last_activity_date": 1680819513, "creation_date": 1680620021, "last_edit_date": 1680819513, "question_id": 39913, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39913/can-an-llm-such-as-gpt-learn-from-a-sentence-prediction-that-is-close-in-meaning", "title": "Can an LLM such as GPT learn from a sentence prediction that is close in meaning but drastically different in wording?"}, {"tags": ["transformer", "attention", "chatgpt", "large-language-models"], "owner": {"account_id": 1068872, "reputation": 109, "user_id": 70053, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/70db3c99ed56a69773468bc97c12f5bf?s=256&d=identicon&r=PG", "display_name": "Bin Wang", "link": "https://ai.stackexchange.com/users/70053/bin-wang"}, "is_answered": false, "view_count": 43, "closed_date": 1680614991, "answer_count": 0, "score": 0, "last_activity_date": 1680302781, "creation_date": 1680302781, "question_id": 39872, "link": "https://ai.stackexchange.com/questions/39872/why-chatgpt-output-one-token-at-a-time", "closed_reason": "Duplicate", "title": "Why ChatGPT output one token at a time?"}, {"tags": ["natural-language-processing", "large-language-models"], "owner": {"account_id": 27766009, "reputation": 33, "user_id": 68072, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AEdFTp4DfxPrILFwRdreOXKPThTGXUC2W9JVnlOG027e=k-s256", "display_name": "user68072", "link": "https://ai.stackexchange.com/users/68072/user68072"}, "is_answered": true, "view_count": 87, "accepted_answer_id": 39767, "answer_count": 1, "score": 1, "last_activity_date": 1679629948, "creation_date": 1678803213, "question_id": 39579, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39579/sparsegpt-code-reproduction", "title": "SparseGPT code reproduction"}, {"tags": ["chat-bots", "large-language-models"], "owner": {"account_id": 7711330, "reputation": 141, "user_id": 69166, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/93d53e3977612b13cd43f768c3bac40e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "zzzgoo", "link": "https://ai.stackexchange.com/users/69166/zzzgoo"}, "is_answered": true, "view_count": 163, "accepted_answer_id": 39733, "answer_count": 1, "score": 3, "last_activity_date": 1679507897, "creation_date": 1679501847, "last_edit_date": 1679507897, "question_id": 39732, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39732/how-could-chatgpt-avoid-consuming-what-it-produces", "title": "How could chatGPT avoid consuming what it produces"}, {"tags": ["gpt", "large-language-models"], "owner": {"account_id": 48200, "reputation": 101, "user_id": 48529, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c096715708403c3711af16fb490319f1?s=256&d=identicon&r=PG", "display_name": "jdm", "link": "https://ai.stackexchange.com/users/48529/jdm"}, "is_answered": false, "view_count": 49, "answer_count": 0, "score": 0, "last_activity_date": 1679065627, "creation_date": 1679065627, "question_id": 39644, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39644/smaller-competitive-llms-that-are-not-autoregressive-transformers", "title": "Smaller, competitive LLMs that are not autoregressive transformers?"}, {"tags": ["machine-learning", "deep-learning", "training", "distributed-computing", "large-language-models"], "owner": {"account_id": 1589784, "reputation": 161, "user_id": 9289, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/2B6rV.png?s=256&g=1", "display_name": "Charlie Parker", "link": "https://ai.stackexchange.com/users/9289/charlie-parker"}, "is_answered": true, "view_count": 477, "answer_count": 2, "score": 2, "last_activity_date": 1677981279, "creation_date": 1676569537, "last_edit_date": 1677772428, "question_id": 39186, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39186/why-do-llms-need-massive-distributed-training-across-nodes-if-the-models-fit", "title": "Why do LLMs need massive distributed training across nodes -- if the models fit in one GPU while batch decreases the variance of gradients?"}, {"tags": ["chatgpt", "knowledge-base", "knowledge-graph", "large-language-models"], "owner": {"account_id": 4353517, "reputation": 877, "user_id": 5351, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/Lr5jY.jpg?s=256&g=1", "display_name": "Lerner Zhang", "link": "https://ai.stackexchange.com/users/5351/lerner-zhang"}, "is_answered": true, "view_count": 1760, "answer_count": 2, "score": 2, "last_activity_date": 1676123230, "creation_date": 1676024087, "last_edit_date": 1676025086, "question_id": 39098, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39098/does-chatgpt-imply-that-the-direction-of-knowledge-graph-is-unpromising", "title": "Does ChatGPT imply that the direction of knowledge graph is unpromising?"}]