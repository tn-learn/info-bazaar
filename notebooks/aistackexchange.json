{"32477": {"link": "https://ai.stackexchange.com/questions/32477/what-is-the-temperature-in-the-gpt-models", "metadata": {"tags": ["machine-learning", "terminology", "gpt", "language-model", "gpt-3"], "owner": {"reputation": 383, "user_id": 34358, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/jD8Bm.jpg?s=256&g=1", "display_name": "Tom D&#246;rr", "link": "https://ai.stackexchange.com/users/34358/tom-d%c3%b6rr"}, "is_answered": true, "view_count": 27580, "accepted_answer_id": 32478, "answer_count": 1, "score": 26, "last_activity_date": 1683729838, "creation_date": 1637458447, "last_edit_date": 1637581165, "question_id": 32477, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/32477/what-is-the-temperature-in-the-gpt-models", "title": "What is the &quot;temperature&quot; in the GPT models?", "body": "<p>What does the temperature parameter mean when talking about the GPT models?</p>\n<p>I know that a higher temperature value means more randomness, but I want to know how randomness is introduced.</p>\n<p>Does temperature mean we add noise to the weights/activations or do we add randomness when choosing a token in the softmax layer?</p>\n"}, "title": "What is the &quot;temperature&quot; in the GPT models?", "content": "What does the temperature parameter mean when talking about the GPT models?\nI know that a higher temperature value means more randomness, but I want to know how randomness is introduced.\nDoes temperature mean we add noise to the weights/activations or do we add randomness when choosing a token in the softmax layer?\n", "question_id": 32477, "answers": []}, "27761": {"link": "https://ai.stackexchange.com/questions/27761/what-language-is-the-gpt-3-engine-written-in", "metadata": {"tags": ["natural-language-processing", "programming-languages", "c++", "gpt-3", "c"], "owner": {"reputation": 275, "user_id": 36067, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/14f03f80777f05112c3902b768d9d5e1?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Otherness", "link": "https://ai.stackexchange.com/users/36067/otherness"}, "is_answered": true, "view_count": 20136, "accepted_answer_id": 27762, "answer_count": 1, "score": 15, "last_activity_date": 1676809185, "creation_date": 1620778018, "question_id": 27761, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/27761/what-language-is-the-gpt-3-engine-written-in", "title": "What language is the GPT-3 engine written in?", "body": "<p>I know that the API is python based, but what's the gpt-3 engine written in mostly? C? C++? I'm having some trouble finding this info.</p>\n"}, "title": "What language is the GPT-3 engine written in?", "content": "I know that the API is python based, but what's the gpt-3 engine written in mostly? C? C++? I'm having some trouble finding this info.\n", "question_id": 27761, "answers": []}, "39619": {"link": "https://ai.stackexchange.com/questions/39619/is-gpt-4-based-on-gpt-3-or-was-it-trained-from-the-scratch", "metadata": {"tags": ["open-ai", "gpt", "gpt-3", "gpt-4"], "owner": {"reputation": 301, "user_id": 65757, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fe87113ced5f370d038b90a43fd1fe1f?s=256&d=identicon&r=PG", "display_name": "Anixx", "link": "https://ai.stackexchange.com/users/65757/anixx"}, "is_answered": true, "view_count": 740, "accepted_answer_id": 39716, "answer_count": 2, "score": 8, "last_activity_date": 1679494638, "creation_date": 1678988652, "last_edit_date": 1679494638, "question_id": 39619, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39619/is-gpt-4-based-on-gpt-3-or-was-it-trained-from-the-scratch", "title": "Is GPT-4 based on GPT-3 or was it trained from the scratch?", "body": "<p>To me it looks like GPT-4 is based on GPT-3.</p>\n<p>On the other hand, there were rumors that training of GPT-3 was done with errors, but re-train was impossible due to the costs.</p>\n"}, "title": "Is GPT-4 based on GPT-3 or was it trained from the scratch?", "content": "To me it looks like GPT-4 is based on GPT-3.\nOn the other hand, there were rumors that training of GPT-3 was done with errors, but re-train was impossible due to the costs.\n", "question_id": 39619, "answers": []}, "34243": {"link": "https://ai.stackexchange.com/questions/34243/how-to-get-gpt-3-to-translate-a-specific-word-in-a-sentence", "metadata": {"tags": ["open-ai", "gpt-3"], "owner": {"reputation": 151, "user_id": 52344, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/5zLYv.png?s=256&g=1", "display_name": "Jonas Sourlier", "link": "https://ai.stackexchange.com/users/52344/jonas-sourlier"}, "is_answered": true, "view_count": 828, "accepted_answer_id": 34245, "answer_count": 1, "score": 4, "last_activity_date": 1642808662, "creation_date": 1642798274, "last_edit_date": 1642798882, "question_id": 34243, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/34243/how-to-get-gpt-3-to-translate-a-specific-word-in-a-sentence", "title": "How to get GPT-3 to translate a specific word in a sentence?", "body": "<p>I just gave GPT-3 the following prompt (in the playground, using text-davinci-001 with default settings):</p>\n<pre><code>What's the German word for &quot;can&quot; in the sentence &quot;The man removes the can.&quot;?\n</code></pre>\n<p>The word &quot;can&quot; in this sentence is obviously a noun and not a verb. &quot;the can&quot; is the metal container used to sell coke or beer, not the common verb &quot;can&quot; which means &quot;be able to&quot;.</p>\n<p>GPT-3 obviously knows this, otherwise it could not translate the sentence correctly in many languages (which it indeed can).</p>\n<p>However, to my prompt, it answered:</p>\n<pre><code>The word for &quot;can&quot; in the sentence &quot;The man removes the can.&quot; is &quot;kann.&quot;\n</code></pre>\n<p>(&quot;kann&quot; is German for the verb &quot;can&quot; in the sense &quot;be able to&quot;)</p>\n<p>Did I ask it in a wrong way or is GPT-3 unable to answer questions like this one?</p>\n"}, "title": "How to get GPT-3 to translate a specific word in a sentence?", "content": "I just gave GPT-3 the following prompt (in the playground, using text-davinci-001 with default settings):\nWhat's the German word for \"can\" in the sentence \"The man removes the can.\"?\n\nThe word \"can\" in this sentence is obviously a noun and not a verb. \"the can\" is the metal container used to sell coke or beer, not the common verb \"can\" which means \"be able to\".\nGPT-3 obviously knows this, otherwise it could not translate the sentence correctly in many languages (which it indeed can).\nHowever, to my prompt, it answered:\nThe word for \"can\" in the sentence \"The man removes the can.\" is \"kann.\"\n\n(\"kann\" is German for the verb \"can\" in the sense \"be able to\")\nDid I ask it in a wrong way or is GPT-3 unable to answer questions like this one?\n", "question_id": 34243, "answers": []}, "30157": {"link": "https://ai.stackexchange.com/questions/30157/how-can-gpt-3-be-used-for-designing-electronic-circuits-from-text-descriptions", "metadata": {"tags": ["generative-adversarial-networks", "applications", "text-generation", "gpt-3"], "owner": {"reputation": 265, "user_id": 45586, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/n8MIV.jpg?s=256&g=1", "display_name": "Aether", "link": "https://ai.stackexchange.com/users/45586/aether"}, "is_answered": true, "view_count": 3817, "accepted_answer_id": 31765, "answer_count": 3, "score": 4, "last_activity_date": 1675953492, "creation_date": 1628846757, "question_id": 30157, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/30157/how-can-gpt-3-be-used-for-designing-electronic-circuits-from-text-descriptions", "title": "How can GPT-3 be used for designing electronic circuits from text descriptions?", "body": "<p>I was wondering if it is possible to use GPT-3 to translate text description of a circuit to any circuit design language program, which in turn can be used to make the circuit.\nIf it is possible, what approach will you suggest?</p>\n"}, "title": "How can GPT-3 be used for designing electronic circuits from text descriptions?", "content": "I was wondering if it is possible to use GPT-3 to translate text description of a circuit to any circuit design language program, which in turn can be used to make the circuit.\nIf it is possible, what approach will you suggest?\n", "question_id": 30157, "answers": []}, "39933": {"link": "https://ai.stackexchange.com/questions/39933/whats-the-difference-between-gpt3-5-and-instructgpt", "metadata": {"tags": ["comparison", "open-ai", "gpt", "gpt-3", "instruct-gpt"], "owner": {"reputation": 41, "user_id": 70285, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/cf618beee7fae16c7ff976ac68fe0d33?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Arya", "link": "https://ai.stackexchange.com/users/70285/arya"}, "is_answered": true, "view_count": 1081, "answer_count": 1, "score": 4, "last_activity_date": 1681891245, "creation_date": 1680771383, "last_edit_date": 1681891245, "question_id": 39933, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39933/whats-the-difference-between-gpt3-5-and-instructgpt", "title": "What&#39;s the difference between GPT3.5 and InstructGPT?", "body": "<p>I read about the different model series in GPT3.5 here - <a href=\"https://platform.openai.com/docs/models/gpt-3-5\" rel=\"nofollow noreferrer\">https://platform.openai.com/docs/models/gpt-3-5</a></p>\n<p>At the beginning of the page, it mentions to look at <a href=\"https://platform.openai.com/docs/model-index-for-researchers\" rel=\"nofollow noreferrer\">https://platform.openai.com/docs/model-index-for-researchers</a> to understand the difference between model series InstructGPT and GPT3.5.</p>\n<p>But, on that page, it says InstructGPT is a part of the GPT3.5 series. So, what's the difference between GPT3.5 and InstructGPT?</p>\n"}, "title": "What&#39;s the difference between GPT3.5 and InstructGPT?", "content": "I read about the different model series in GPT3.5 here - https://platform.openai.com/docs/models/gpt-3-5\nAt the beginning of the page, it mentions to look at https://platform.openai.com/docs/model-index-for-researchers to understand the difference between model series InstructGPT and GPT3.5.\nBut, on that page, it says InstructGPT is a part of the GPT3.5 series. So, what's the difference between GPT3.5 and InstructGPT?\n", "question_id": 39933, "answers": []}, "37283": {"link": "https://ai.stackexchange.com/questions/37283/if-gpt-3-is-trained-on-predicting-the-next-token-how-is-it-able-to-take-command", "metadata": {"tags": ["natural-language-processing", "transformer", "gpt-3"], "owner": {"reputation": 31, "user_id": 61696, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/ALm5wu0uce2uxW4npaVLGK2EuW_nt7Z7Zbyw5lo49J0B=k-s256", "display_name": "Andrew Tang", "link": "https://ai.stackexchange.com/users/61696/andrew-tang"}, "is_answered": false, "view_count": 881, "answer_count": 1, "score": 2, "last_activity_date": 1689498526, "creation_date": 1664919978, "last_edit_date": 1664919990, "question_id": 37283, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/37283/if-gpt-3-is-trained-on-predicting-the-next-token-how-is-it-able-to-take-command", "title": "If GPT-3 is trained on predicting the next token, how is it able to take commands?", "body": "<p>From my understanding, GPT-3 is trained on predicting the next token from a sequence of tokens. Given this, how is it able to take commands? For instance, in this example input, wouldn't the statistically most likely prediction be to insert a period and end the sentence?</p>\n<pre><code>Input: write me a beautiful sentence\n\nOutput: I cannot put into words how much I love you, so I'll just say it's infinite.\n</code></pre>\n"}, "title": "If GPT-3 is trained on predicting the next token, how is it able to take commands?", "content": "From my understanding, GPT-3 is trained on predicting the next token from a sequence of tokens. Given this, how is it able to take commands? For instance, in this example input, wouldn't the statistically most likely prediction be to insert a period and end the sentence?\nInput: write me a beautiful sentence\n\nOutput: I cannot put into words how much I love you, so I'll just say it's infinite.\n\n", "question_id": 37283, "answers": []}, "39050": {"link": "https://ai.stackexchange.com/questions/39050/computation-required-for-gpt-model-to-choose-likely-word-from-n-options-where-n", "metadata": {"tags": ["natural-language-processing", "math", "transformer", "gpt", "gpt-3"], "owner": {"reputation": 11, "user_id": 67768, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-AVIJRJHGsIw/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJMY3yZGerssWzBpIjwrzl3wXIHV5A/photo.jpg?sz=256", "display_name": "Derek", "link": "https://ai.stackexchange.com/users/67768/derek"}, "is_answered": false, "view_count": 42, "answer_count": 0, "score": 1, "last_activity_date": 1675557386, "creation_date": 1675557386, "question_id": 39050, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39050/computation-required-for-gpt-model-to-choose-likely-word-from-n-options-where-n", "title": "Computation required for GPT model to choose likely word from n-options where n &lt; total vocabulary size", "body": "<p>Let\u2019s imagine two different use cases for a LLM/GPT-3.</p>\n<ol>\n<li>Predicting the next most likely word in a sequence using all ~50k words in its dictionary (i.e. the standard method of prompting a LLM)</li>\n<li>Checking whether &quot;Word-1&quot; is more likely than &quot;Word-2&quot; to be next in a sequence</li>\n</ol>\n<p>How much more computationally efficient is #2?</p>\n<p>My understanding is that the computation of the attention mechanism is dependent on the length of the prompt (so will be the same) and takes up most of the computation needed to get the output (but to what extent, I'm not sure). The difference will be in the decoding stage.</p>\n<p>Would the one matrix multiplication in the decoding calculation be the only computation using the smaller 2-row matrix instead of the 50k-row matrix or are there other improvements in efficiency?</p>\n"}, "title": "Computation required for GPT model to choose likely word from n-options where n &lt; total vocabulary size", "content": "Let\u2019s imagine two different use cases for a LLM/GPT-3.\n\nPredicting the next most likely word in a sequence using all ~50k words in its dictionary (i.e. the standard method of prompting a LLM)\nChecking whether \"Word-1\" is more likely than \"Word-2\" to be next in a sequence\n\nHow much more computationally efficient is #2?\nMy understanding is that the computation of the attention mechanism is dependent on the length of the prompt (so will be the same) and takes up most of the computation needed to get the output (but to what extent, I'm not sure). The difference will be in the decoding stage.\nWould the one matrix multiplication in the decoding calculation be the only computation using the smaller 2-row matrix instead of the 50k-row matrix or are there other improvements in efficiency?\n", "question_id": 39050, "answers": []}, "38167": {"link": "https://ai.stackexchange.com/questions/38167/fine-tune-gpt-neo-with-prompt-and-completion", "metadata": {"tags": ["datasets", "training-datasets", "gpt", "fine-tuning", "gpt-3"], "owner": {"reputation": 111, "user_id": 57736, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/859cd18fc4bc067aa389b7c408eeb572?s=256&d=identicon&r=PG", "display_name": "SoftTimur", "link": "https://ai.stackexchange.com/users/57736/softtimur"}, "is_answered": false, "view_count": 2469, "answer_count": 1, "score": 1, "last_activity_date": 1686279860, "creation_date": 1670188400, "last_edit_date": 1670188865, "question_id": 38167, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/38167/fine-tune-gpt-neo-with-prompt-and-completion", "title": "Fine-tune GPT-Neo with prompt and completion?", "body": "<p>I'm new to AI and machine learning.</p>\n<p>To <a href=\"https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset\" rel=\"nofollow noreferrer\">fine-tune GPT-3</a>, I understand that we need a set of training examples that each consist of a single input (&quot;prompt&quot;) and its associated output (&quot;completion&quot;).</p>\n<p>I have prepared a dataset with &quot;prompt&quot; and &quot;completion&quot;. And I expect that a fine-tuned model would return the corresponding completion after receiving a prompt in my dataset. But <a href=\"https://stackoverflow.com/questions/74666268/openai-stream-interrupted-client-disconnected\">due to some reason</a>, I cannot fine-tune GPT-3 at the moment.</p>\n<p>So I plan to fine-tune GPT-Neo (or GPT-J or GPT-NeoX). From <a href=\"https://www.youtube.com/watch?v=07ppAKvOhqk\" rel=\"nofollow noreferrer\">this video</a> and <a href=\"https://www.youtube.com/watch?v=uE0_XKh2d6g&amp;list=LL&amp;index=4&amp;t=401s\" rel=\"nofollow noreferrer\">this video</a>, it seems that they only accept a dataset containing only &quot;prompt&quot;.</p>\n<p>Does anyone know how I could modify my dataset with &quot;prompt&quot; and &quot;completion&quot; such that it could be used to fine-tune GPT-Neo?</p>\n"}, "title": "Fine-tune GPT-Neo with prompt and completion?", "content": "I'm new to AI and machine learning.\nTo fine-tune GPT-3, I understand that we need a set of training examples that each consist of a single input (\"prompt\") and its associated output (\"completion\").\nI have prepared a dataset with \"prompt\" and \"completion\". And I expect that a fine-tuned model would return the corresponding completion after receiving a prompt in my dataset. But due to some reason, I cannot fine-tune GPT-3 at the moment.\nSo I plan to fine-tune GPT-Neo (or GPT-J or GPT-NeoX). From this video and this video, it seems that they only accept a dataset containing only \"prompt\".\nDoes anyone know how I could modify my dataset with \"prompt\" and \"completion\" such that it could be used to fine-tune GPT-Neo?\n", "question_id": 38167, "answers": []}, "37929": {"link": "https://ai.stackexchange.com/questions/37929/privacy-implications-of-storing-and-transmitting-gpt-3-embeddings", "metadata": {"tags": ["word-embedding", "embeddings", "gpt-3"], "owner": {"reputation": 11, "user_id": 63625, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/7fca51ae068781c0fbd29b63358add07?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "jahghAAB", "link": "https://ai.stackexchange.com/users/63625/jahghaab"}, "is_answered": false, "view_count": 91, "answer_count": 0, "score": 1, "last_activity_date": 1668727800, "creation_date": 1668727800, "question_id": 37929, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/37929/privacy-implications-of-storing-and-transmitting-gpt-3-embeddings", "title": "Privacy implications of storing and transmitting GPT-3 embeddings?", "body": "<p>We are exploring implementing a feature where a user might enter &quot;which product has optional all wheel drive&quot; into a search input, which would be transformed to GPT-3 embeddings, and compared against a set of embeddings for product descriptions to return similar products.</p>\n<p>If a user enters PII and/or sensitive data, for example &quot;my name is Jim, what will help with my COPD&quot;, then the embeddings that are generated will include this sensitive information.</p>\n<p>Is it possible to reverse the embeddings GPT-3 creates back to the exact phrase entered?  If not, are there metrics like anonymity that can be used to reason about exactly how sensitive the embedding are?</p>\n"}, "title": "Privacy implications of storing and transmitting GPT-3 embeddings?", "content": "We are exploring implementing a feature where a user might enter \"which product has optional all wheel drive\" into a search input, which would be transformed to GPT-3 embeddings, and compared against a set of embeddings for product descriptions to return similar products.\nIf a user enters PII and/or sensitive data, for example \"my name is Jim, what will help with my COPD\", then the embeddings that are generated will include this sensitive information.\nIs it possible to reverse the embeddings GPT-3 creates back to the exact phrase entered?  If not, are there metrics like anonymity that can be used to reason about exactly how sensitive the embedding are?\n", "question_id": 37929, "answers": []}, "36964": {"link": "https://ai.stackexchange.com/questions/36964/how-can-i-train-gpt-3-or-similar-to-search-for-answers-my-text-file", "metadata": {"tags": ["gpt-3"], "owner": {"reputation": 11, "user_id": 60043, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AATXAJya8P46nNzDFjKo1V3FojZG0Xdkn1uTfoyl9hth=k-s256", "display_name": "Jon 310", "link": "https://ai.stackexchange.com/users/60043/jon-310"}, "is_answered": false, "view_count": 497, "answer_count": 0, "score": 1, "last_activity_date": 1662130289, "creation_date": 1662130289, "question_id": 36964, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/36964/how-can-i-train-gpt-3-or-similar-to-search-for-answers-my-text-file", "title": "How can I train GPT-3 (or similar) to search for answers my text file", "body": "<p>My company has large sets of standards that are quite dry to read through and find answers to questions.  How can I have an AI perform the search?</p>\n<p>The text files are too long to submit with every query, so it seems like I would need to train the model with the data.  What's the simplest solution out there?</p>\n"}, "title": "How can I train GPT-3 (or similar) to search for answers my text file", "content": "My company has large sets of standards that are quite dry to read through and find answers to questions.  How can I have an AI perform the search?\nThe text files are too long to submit with every query, so it seems like I would need to train the model with the data.  What's the simplest solution out there?\n", "question_id": 36964, "answers": []}, "35444": {"link": "https://ai.stackexchange.com/questions/35444/can-you-train-gpt-j-to-use-a-specific-list-of-words-and-prioritise-them", "metadata": {"tags": ["machine-learning", "natural-language-processing", "gpt", "natural-language-generation", "gpt-3"], "owner": {"reputation": 11, "user_id": 54646, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/64509e42f917a09a5ad3aa606032ff92?s=256&d=identicon&r=PG", "display_name": "Learningallday", "link": "https://ai.stackexchange.com/users/54646/learningallday"}, "is_answered": false, "view_count": 197, "answer_count": 0, "score": 1, "last_activity_date": 1651950590, "creation_date": 1651936352, "last_edit_date": 1651950590, "question_id": 35444, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/35444/can-you-train-gpt-j-to-use-a-specific-list-of-words-and-prioritise-them", "title": "Can you train GPT-J to use a specific list of words and prioritise them?", "body": "<p>Can you train GPT-J to use a specific list of words and prioritise them? If so, please could you share how I would go about this?</p>\n<p>Say you're using GPT-J to write a story, you might wish to mention certain key terms more than others, or in a specific order.</p>\n"}, "title": "Can you train GPT-J to use a specific list of words and prioritise them?", "content": "Can you train GPT-J to use a specific list of words and prioritise them? If so, please could you share how I would go about this?\nSay you're using GPT-J to write a story, you might wish to mention certain key terms more than others, or in a specific order.\n", "question_id": 35444, "answers": []}, "27261": {"link": "https://ai.stackexchange.com/questions/27261/what-is-the-meaning-of-our-current-objective-weights-every-token-equally-and-la", "metadata": {"tags": ["natural-language-processing", "papers", "open-ai", "gpt-3"], "owner": {"reputation": 111, "user_id": 42445, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/690ab6ea1404784984ae54f2b4f970fc?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "exteral", "link": "https://ai.stackexchange.com/users/42445/exteral"}, "is_answered": false, "view_count": 29, "answer_count": 0, "score": 1, "last_activity_date": 1618106483, "creation_date": 1618101563, "last_edit_date": 1618106483, "question_id": 27261, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/27261/what-is-the-meaning-of-our-current-objective-weights-every-token-equally-and-la", "title": "What is the meaning of &quot;Our current objective weights every token equally and lacks a notion of what is most important to predict&quot; in the GPT-3 paper?", "body": "<p>On page 34 of OpenAI's <a href=\"https://arxiv.org/pdf/2005.14165v2.pdf\" rel=\"nofollow noreferrer\">GPT-3</a>, there is a sentence demonstrating the limitation of objective function:</p>\n<blockquote>\n<p>Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important.</p>\n</blockquote>\n<p>I am not sure if I understand this correctly. In my understanding, the objective function is to maximize the log-likelihood of the token to predict given the current context, i.e., <span class=\"math-container\">$\\max L \\sim \\sum_{i} \\log P(x_{i} | x_{&lt;i})$</span>. Although we aim to predict every token that appears in the training sentence, the tokens have a certain distribution, and therefore we do not actually assign equal weight to every token in loss optimization.</p>\n<p>And what should be an example for a model to get the notion of &quot;what is important and what is not&quot;. What is the importance refer to in here? For example, does it mean that &quot;the&quot; is less important compared to a less common noun, or does it mean that &quot;the current task we are interested in is more important than the scenario we are not interested in ?&quot;</p>\n<p>Any idea how to understand the sentence by OpenAI?</p>\n"}, "title": "What is the meaning of &quot;Our current objective weights every token equally and lacks a notion of what is most important to predict&quot; in the GPT-3 paper?", "content": "On page 34 of OpenAI's GPT-3, there is a sentence demonstrating the limitation of objective function:\n\nOur current objective weights every token equally and lacks a notion of what is most important to predict and what is less important.\n\nI am not sure if I understand this correctly. In my understanding, the objective function is to maximize the log-likelihood of the token to predict given the current context, i.e., $\\max L \\sim \\sum_{i} \\log P(x_{i} | x_{<i})$. Although we aim to predict every token that appears in the training sentence, the tokens have a certain distribution, and therefore we do not actually assign equal weight to every token in loss optimization.\nAnd what should be an example for a model to get the notion of \"what is important and what is not\". What is the importance refer to in here? For example, does it mean that \"the\" is less important compared to a less common noun, or does it mean that \"the current task we are interested in is more important than the scenario we are not interested in ?\"\nAny idea how to understand the sentence by OpenAI?\n", "question_id": 27261, "answers": []}, "39296": {"link": "https://ai.stackexchange.com/questions/39296/how-is-instructgpt-a-fine-tuned-version-of-gpt-3-and-at-the-same-time-has-fewer", "metadata": {"tags": ["weights", "gpt-3", "instruct-gpt"], "owner": {"reputation": 113, "user_id": 38831, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/z304s.png?s=256&g=1", "display_name": "DanielTheRocketMan", "link": "https://ai.stackexchange.com/users/38831/danieltherocketman"}, "is_answered": true, "view_count": 284, "accepted_answer_id": 39298, "answer_count": 1, "score": 0, "last_activity_date": 1677497014, "creation_date": 1677323536, "last_edit_date": 1677497014, "question_id": 39296, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39296/how-is-instructgpt-a-fine-tuned-version-of-gpt-3-and-at-the-same-time-has-fewer", "title": "How is InstructGPT a fine-tuned version of GPT-3 and at the same time has fewer parameters than the original GPT3?", "body": "<p>I am reading the paper &quot;<a href=\"https://arxiv.org/pdf/2203.02155\" rel=\"nofollow noreferrer\">Training language models to follow instructions with human feedback</a>&quot;</p>\n<p>It says:</p>\n<blockquote>\n<p>Our labelers provide demonstrations of the desired behavior on the input prompt distribution (see Section 3.2 for details on this distribution). We then fine-tune a pretrained GPT-3 model on this data using supervised learning.</p>\n</blockquote>\n<p>The paper also says:</p>\n<blockquote>\n<p>On our test set, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having over 100x fewer parameters.</p>\n</blockquote>\n<p>I am not able to understand how the aligned model is a fine tune of GPT-3 using supervised learning (and other steps associated with reinforcement learning) and at the same time the aligned model has fewer parameters than the original model.</p>\n<p>Can someone give me a hint on the subject?</p>\n"}, "title": "How is InstructGPT a fine-tuned version of GPT-3 and at the same time has fewer parameters than the original GPT3?", "content": "I am reading the paper \"Training language models to follow instructions with human feedback\"\nIt says:\n\nOur labelers provide demonstrations of the desired behavior on the input prompt distribution (see Section 3.2 for details on this distribution). We then fine-tune a pretrained GPT-3 model on this data using supervised learning.\n\nThe paper also says:\n\nOn our test set, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having over 100x fewer parameters.\n\nI am not able to understand how the aligned model is a fine tune of GPT-3 using supervised learning (and other steps associated with reinforcement learning) and at the same time the aligned model has fewer parameters than the original model.\nCan someone give me a hint on the subject?\n", "question_id": 39296, "answers": []}, "37205": {"link": "https://ai.stackexchange.com/questions/37205/papers-on-prompt-engineering", "metadata": {"tags": ["natural-language-processing", "reference-request", "papers", "language-model", "gpt-3"], "owner": {"reputation": 115, "user_id": 46469, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b4b7ff3659c06200059de744497fb382?s=256&d=identicon&r=PG", "display_name": "Mehdi Abbassi", "link": "https://ai.stackexchange.com/users/46469/mehdi-abbassi"}, "is_answered": true, "view_count": 628, "answer_count": 1, "score": 0, "last_activity_date": 1674709654, "creation_date": 1664259765, "last_edit_date": 1674558137, "question_id": 37205, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/37205/papers-on-prompt-engineering", "title": "Papers on Prompt Engineering", "body": "<p>I am into AI in general and NLP in particular. Besides, I have a background in philosophy, and the new LLMs like GPT-3 seem to have exciting capabilities. I want to study prompt engineering (for example, teaching the model to reason, etc.)</p>\n<p>Do you know any specific papers to start? I am looking for studies on the techniques of prompt engineering.</p>\n"}, "title": "Papers on Prompt Engineering", "content": "I am into AI in general and NLP in particular. Besides, I have a background in philosophy, and the new LLMs like GPT-3 seem to have exciting capabilities. I want to study prompt engineering (for example, teaching the model to reason, etc.)\nDo you know any specific papers to start? I am looking for studies on the techniques of prompt engineering.\n", "question_id": 37205, "answers": []}, "41879": {"link": "https://ai.stackexchange.com/questions/41879/improving-contextual-consistency-and-quality-in-openai-api-responses", "metadata": {"tags": ["open-ai", "gpt", "large-language-models", "gpt-3", "prompt"], "owner": {"reputation": 9, "user_id": 75523, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AAcHTtfxCg9L8xm--WFm6QhyUGkLLNL8iwHZbvjxQRFMCHIA=k-s256", "display_name": "tech-buddy", "link": "https://ai.stackexchange.com/users/75523/tech-buddy"}, "is_answered": false, "view_count": 15, "answer_count": 0, "score": 0, "last_activity_date": 1692961291, "creation_date": 1692961291, "question_id": 41879, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41879/improving-contextual-consistency-and-quality-in-openai-api-responses", "title": "Improving Contextual Consistency and Quality in OpenAI API Responses", "body": "<p>I'm currently wrestling with an issue using the OpenAI npm package for API calls. My setup involves prompts, user history, and questions with parameters like temperature, top_p, and frequency_penalty. Strangely, responses from the API lack the context-holding finesse I get when I run the same prompts in the OpenAI console.</p>\n<p>Has anyone else encountered this and found solutions? How do you ensure coherent conversation context and top-notch responses? Any tips to share?</p>\n<p>Eager to hear your experiences and insights! \ud83d\ude80</p>\n"}, "title": "Improving Contextual Consistency and Quality in OpenAI API Responses", "content": "I'm currently wrestling with an issue using the OpenAI npm package for API calls. My setup involves prompts, user history, and questions with parameters like temperature, top_p, and frequency_penalty. Strangely, responses from the API lack the context-holding finesse I get when I run the same prompts in the OpenAI console.\nHas anyone else encountered this and found solutions? How do you ensure coherent conversation context and top-notch responses? Any tips to share?\nEager to hear your experiences and insights! \ud83d\ude80\n", "question_id": 41879, "answers": []}, "41178": {"link": "https://ai.stackexchange.com/questions/41178/update-openai-embedding-based-on-own-domain-corpus", "metadata": {"tags": ["transformer", "open-ai", "word-embedding", "embeddings", "gpt-3"], "owner": {"reputation": 101, "user_id": 73689, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/6QRsY.jpg?s=256&g=1", "display_name": "Salty Gold Fish", "link": "https://ai.stackexchange.com/users/73689/salty-gold-fish"}, "is_answered": false, "view_count": 6, "answer_count": 0, "score": 0, "last_activity_date": 1688679498, "creation_date": 1688679498, "question_id": 41178, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41178/update-openai-embedding-based-on-own-domain-corpus", "title": "Update OpenAI embedding based on own domain corpus", "body": "<p>I have a large domain corpus. Is there anyway to update the word/sentence/document embedding obtained from OpenAI embedding API based on my own domain corpus? There may be some words in my corpus that are close to each other but far from each other in OpenAI embedding space.</p>\n<p>Any tutorials or guidance will be helpful.</p>\n"}, "title": "Update OpenAI embedding based on own domain corpus", "content": "I have a large domain corpus. Is there anyway to update the word/sentence/document embedding obtained from OpenAI embedding API based on my own domain corpus? There may be some words in my corpus that are close to each other but far from each other in OpenAI embedding space.\nAny tutorials or guidance will be helpful.\n", "question_id": 41178, "answers": []}, "41153": {"link": "https://ai.stackexchange.com/questions/41153/extract-multiple-records-from-raw-text-using-open-ai-api-function-calls", "metadata": {"tags": ["open-ai", "chatgpt", "gpt-3"], "owner": {"reputation": 1, "user_id": 73762, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AAcHTtfxhRJXqUD37mKM8iHleDUFJzJc484sVRMdOeZRKXoR=k-s256", "display_name": "viraj jadhav", "link": "https://ai.stackexchange.com/users/73762/viraj-jadhav"}, "is_answered": false, "view_count": 61, "answer_count": 0, "score": 0, "last_activity_date": 1688566412, "creation_date": 1688566412, "question_id": 41153, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41153/extract-multiple-records-from-raw-text-using-open-ai-api-function-calls", "title": "Extract multiple records from raw text using open AI API function calls", "body": "<p>Trying to extract records from pdf. The plan was to convert pdf data to raw text and use open AI api calls to extract the data in a desired format (csv). This is because the pdf data is very unstructured. While I do get records back in the output, I get only the 1st one from every page when there are multiple records. What can I do to get all records back in the desired format. Further I use a code to get back a csv , but the issue is with API response and not handling JSON to CSV\nRaw data : <a href=\"https://pastebin.com/fKKyJCTB\" rel=\"nofollow noreferrer\">https://pastebin.com/fKKyJCTB</a></p>\n<pre><code>functions = [\n    {\n        &quot;name&quot;: &quot;extract_permit_data&quot;,\n        &quot;description&quot;: &quot;Extract data from a permit record and add it to the database.&quot;,\n        &quot;parameters&quot;: {\n            &quot;type&quot;: &quot;object&quot;,\n            &quot;properties&quot;: {\n                &quot;permit_number&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The unique permit number.&quot;\n                },\n                &quot;permit_type&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The type of permit.&quot;\n                },\n                &quot;address&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The address associated with the permit.&quot;\n                },\n                &quot;valuation&quot;: {\n                    &quot;type&quot;: &quot;number&quot;,\n                    &quot;description&quot;: &quot;The valuation associated with the permit.&quot;\n                },\n                &quot;applied_date&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;format&quot;: &quot;date&quot;,\n                    &quot;description&quot;: &quot;The date the permit was applied for.&quot;\n                },\n                &quot;permit_subtype&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The subtype of the permit.&quot;\n                },\n                &quot;parcel_number&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The parcel number associated with the permit.&quot;\n                },\n                &quot;total_fees&quot;: {\n                    &quot;type&quot;: &quot;number&quot;,\n                    &quot;description&quot;: &quot;The total fees associated with the permit.&quot;\n                },\n                &quot;issued_date&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;format&quot;: &quot;date&quot;,\n                    &quot;description&quot;: &quot;The date the permit was issued.&quot;\n                },\n                &quot;status&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The status of the permit.&quot;\n                },\n                &quot;description&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The description of the permit.&quot;\n                },\n                &quot;subdivision&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The subdivision associated with the permit.&quot;\n                },\n                &quot;total_paid&quot;: {\n                    &quot;type&quot;: &quot;number&quot;,\n                    &quot;description&quot;: &quot;The total amount paid for the permit.&quot;\n                },\n                &quot;contractor&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The contractor associated with the permit.&quot;\n                },\n                &quot;contractor_address&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The contractor's address.&quot;\n                },\n                &quot;contractor_phone_number&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The contractor's phone number.&quot;\n                },\n                &quot;owner_name&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The name of the owner associated with the permit.&quot;\n                },\n                &quot;owner_address&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The owner's address.&quot;\n                },\n                &quot;owner_phone_number&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The owner's phone number.&quot;\n                },\n                &quot;applicant_name&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The name of the applicant.&quot;\n                },\n                &quot;applicant_address&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The applicant's address.&quot;\n                },\n                &quot;applicant_phone_number&quot;: {\n                    &quot;type&quot;: &quot;string&quot;,\n                    &quot;description&quot;: &quot;The applicant's phone number.&quot;\n                }\n            },\n            &quot;required&quot;: [&quot;permit_number&quot;, &quot;permit_type&quot;, &quot;address&quot;, &quot;valuation&quot;, &quot;applied_date&quot;, &quot;permit_subtype&quot;, &quot;parcel_number&quot;, &quot;total_fees&quot;, &quot;issued_date&quot;, &quot;status&quot;, &quot;description&quot;, &quot;subdivision&quot;, &quot;total_paid&quot;, &quot;contractor&quot;, &quot;contractor_address&quot;, &quot;contractor_phone_number&quot;, &quot;owner_name&quot;, &quot;owner_address&quot;, &quot;applicant_name&quot;, &quot;applicant_address&quot;, &quot;applicant_phone_number&quot;],\n        },\n    }\n] \nmessages = [\n                    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant that extracts all solar permit records information and other details from raw text containing multiple permits indicated by permit unique permit numbers into JSON for a database.&quot;},\n                    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: 'Extract each record with permit numbers, permit status, permit address, contractor/applicant,owner, issue date, permit type, sub-type, valuation , work description , Storage/Battery Manufacturer Name and kw rating from raw text fpr each record: ' + text}\n]\n\n# Call the model\nresponse = openai.ChatCompletion.create(\n                    model='gpt-3.5-turbo-0613',\n                    functions=functions,\n                    messages=messages,\n                    temperature=0\n                )\n# Get result\nresult = response.choices[0]['message']['function_call']['arguments']'''\n\n</code></pre>\n"}, "title": "Extract multiple records from raw text using open AI API function calls", "content": "Trying to extract records from pdf. The plan was to convert pdf data to raw text and use open AI api calls to extract the data in a desired format (csv). This is because the pdf data is very unstructured. While I do get records back in the output, I get only the 1st one from every page when there are multiple records. What can I do to get all records back in the desired format. Further I use a code to get back a csv , but the issue is with API response and not handling JSON to CSV\nRaw data : https://pastebin.com/fKKyJCTB\nfunctions = [\n    {\n        \"name\": \"extract_permit_data\",\n        \"description\": \"Extract data from a permit record and add it to the database.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"permit_number\": {\n                    \"type\": \"string\",\n                    \"description\": \"The unique permit number.\"\n                },\n                \"permit_type\": {\n                    \"type\": \"string\",\n                    \"description\": \"The type of permit.\"\n                },\n                \"address\": {\n                    \"type\": \"string\",\n                    \"description\": \"The address associated with the permit.\"\n                },\n                \"valuation\": {\n                    \"type\": \"number\",\n                    \"description\": \"The valuation associated with the permit.\"\n                },\n                \"applied_date\": {\n                    \"type\": \"string\",\n                    \"format\": \"date\",\n                    \"description\": \"The date the permit was applied for.\"\n                },\n                \"permit_subtype\": {\n                    \"type\": \"string\",\n                    \"description\": \"The subtype of the permit.\"\n                },\n                \"parcel_number\": {\n                    \"type\": \"string\",\n                    \"description\": \"The parcel number associated with the permit.\"\n                },\n                \"total_fees\": {\n                    \"type\": \"number\",\n                    \"description\": \"The total fees associated with the permit.\"\n                },\n                \"issued_date\": {\n                    \"type\": \"string\",\n                    \"format\": \"date\",\n                    \"description\": \"The date the permit was issued.\"\n                },\n                \"status\": {\n                    \"type\": \"string\",\n                    \"description\": \"The status of the permit.\"\n                },\n                \"description\": {\n                    \"type\": \"string\",\n                    \"description\": \"The description of the permit.\"\n                },\n                \"subdivision\": {\n                    \"type\": \"string\",\n                    \"description\": \"The subdivision associated with the permit.\"\n                },\n                \"total_paid\": {\n                    \"type\": \"number\",\n                    \"description\": \"The total amount paid for the permit.\"\n                },\n                \"contractor\": {\n                    \"type\": \"string\",\n                    \"description\": \"The contractor associated with the permit.\"\n                },\n                \"contractor_address\": {\n                    \"type\": \"string\",\n                    \"description\": \"The contractor's address.\"\n                },\n                \"contractor_phone_number\": {\n                    \"type\": \"string\",\n                    \"description\": \"The contractor's phone number.\"\n                },\n                \"owner_name\": {\n                    \"type\": \"string\",\n                    \"description\": \"The name of the owner associated with the permit.\"\n                },\n                \"owner_address\": {\n                    \"type\": \"string\",\n                    \"description\": \"The owner's address.\"\n                },\n                \"owner_phone_number\": {\n                    \"type\": \"string\",\n                    \"description\": \"The owner's phone number.\"\n                },\n                \"applicant_name\": {\n                    \"type\": \"string\",\n                    \"description\": \"The name of the applicant.\"\n                },\n                \"applicant_address\": {\n                    \"type\": \"string\",\n                    \"description\": \"The applicant's address.\"\n                },\n                \"applicant_phone_number\": {\n                    \"type\": \"string\",\n                    \"description\": \"The applicant's phone number.\"\n                }\n            },\n            \"required\": [\"permit_number\", \"permit_type\", \"address\", \"valuation\", \"applied_date\", \"permit_subtype\", \"parcel_number\", \"total_fees\", \"issued_date\", \"status\", \"description\", \"subdivision\", \"total_paid\", \"contractor\", \"contractor_address\", \"contractor_phone_number\", \"owner_name\", \"owner_address\", \"applicant_name\", \"applicant_address\", \"applicant_phone_number\"],\n        },\n    }\n] \nmessages = [\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts all solar permit records information and other details from raw text containing multiple permits indicated by permit unique permit numbers into JSON for a database.\"},\n                    {\"role\": \"user\", \"content\": 'Extract each record with permit numbers, permit status, permit address, contractor/applicant,owner, issue date, permit type, sub-type, valuation , work description , Storage/Battery Manufacturer Name and kw rating from raw text fpr each record: ' + text}\n]\n\n# Call the model\nresponse = openai.ChatCompletion.create(\n                    model='gpt-3.5-turbo-0613',\n                    functions=functions,\n                    messages=messages,\n                    temperature=0\n                )\n# Get result\nresult = response.choices[0]['message']['function_call']['arguments']'''\n\n\n", "question_id": 41153, "answers": []}, "41119": {"link": "https://ai.stackexchange.com/questions/41119/how-is-openai-embeddings-obtained", "metadata": {"tags": ["open-ai", "embeddings", "gpt-3"], "owner": {"reputation": 101, "user_id": 73689, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/6QRsY.jpg?s=256&g=1", "display_name": "Salty Gold Fish", "link": "https://ai.stackexchange.com/users/73689/salty-gold-fish"}, "is_answered": false, "view_count": 90, "answer_count": 0, "score": 0, "last_activity_date": 1688429851, "creation_date": 1688427871, "last_edit_date": 1688429851, "question_id": 41119, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41119/how-is-openai-embeddings-obtained", "title": "How is OpenAI embeddings obtained", "body": "<p>There is OpenAI embedding API <a href=\"https://platform.openai.com/docs/guides/embeddings\" rel=\"nofollow noreferrer\">https://platform.openai.com/docs/guides/embeddings</a>. How is this embedding related to the GPT3.5 transformer model architecture? Is it the vectors learned from the input embeddings part, which is the summation of word embedding and position embedding, at the end of model training?</p>\n<p>Any tutorials or guidance will be helpful.</p>\n"}, "title": "How is OpenAI embeddings obtained", "content": "There is OpenAI embedding API https://platform.openai.com/docs/guides/embeddings. How is this embedding related to the GPT3.5 transformer model architecture? Is it the vectors learned from the input embeddings part, which is the summation of word embedding and position embedding, at the end of model training?\nAny tutorials or guidance will be helpful.\n", "question_id": 41119, "answers": []}, "39757": {"link": "https://ai.stackexchange.com/questions/39757/as-a-user-can-i-fine-tune-end-to-end-with-gpt-3-if-i-want-to-add-custom-layers", "metadata": {"tags": ["gpt-3"], "owner": {"reputation": 111, "user_id": 31248, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/8e9db43a73404856360890611fb7b21c?s=256&d=identicon&r=PG", "display_name": "Tom Bennett", "link": "https://ai.stackexchange.com/users/31248/tom-bennett"}, "is_answered": false, "view_count": 78, "answer_count": 0, "score": 0, "last_activity_date": 1679687100, "creation_date": 1679593242, "last_edit_date": 1679687100, "question_id": 39757, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39757/as-a-user-can-i-fine-tune-end-to-end-with-gpt-3-if-i-want-to-add-custom-layers", "title": "As a user, can I fine-tune end-to-end with GPT-3 if I want to add custom layers?", "body": "<p>I am looking at the OpenAI API, and it seems that the only way to get embeddings for a specific text is to use the embedding API. In other words, I can only use the embeddings as fixed features in my own model, as opposed to fine-tuning GPT-3 end-to-end. Is this correct?</p>\n"}, "title": "As a user, can I fine-tune end-to-end with GPT-3 if I want to add custom layers?", "content": "I am looking at the OpenAI API, and it seems that the only way to get embeddings for a specific text is to use the embedding API. In other words, I can only use the embeddings as fixed features in my own model, as opposed to fine-tuning GPT-3 end-to-end. Is this correct?\n", "question_id": 39757, "answers": []}, "39409": {"link": "https://ai.stackexchange.com/questions/39409/tool-for-detecting-ai-generated-code", "metadata": {"tags": ["chatgpt", "gpt-3"], "owner": {"reputation": 101, "user_id": 68880, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/ZVrnW.jpg?s=256&g=1", "display_name": "Haseeb Mir", "link": "https://ai.stackexchange.com/users/68880/haseeb-mir"}, "is_answered": false, "view_count": 1189, "answer_count": 0, "score": 0, "last_activity_date": 1677879012, "creation_date": 1677879012, "question_id": 39409, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39409/tool-for-detecting-ai-generated-code", "title": "Tool for detecting AI Generated code?", "body": "<p>Can you recommend any tools for detecting AI generated <strong>code</strong>? I am aware of AI-generated text detectors such as <a href=\"https://www.zerogpt.com\" rel=\"nofollow noreferrer\">ZeroGPT</a> and <a href=\"https://platform.openai.com/ai-text-classifier\" rel=\"nofollow noreferrer\">OpenAI-Classifier</a>, but I am specifically interested in detecting <strong>AI-generated code</strong>. As of now, I have not been able to find any publicly available tools for this purpose.</p>\n<p>I would greatly appreciate any guidance or recommendations on available tools or methods for detecting AI-generated code. Thank you in advance for your help.</p>\n"}, "title": "Tool for detecting AI Generated code?", "content": "Can you recommend any tools for detecting AI generated code? I am aware of AI-generated text detectors such as ZeroGPT and OpenAI-Classifier, but I am specifically interested in detecting AI-generated code. As of now, I have not been able to find any publicly available tools for this purpose.\nI would greatly appreciate any guidance or recommendations on available tools or methods for detecting AI-generated code. Thank you in advance for your help.\n", "question_id": 39409, "answers": []}, "36422": {"link": "https://ai.stackexchange.com/questions/36422/are-there-any-transcripts-of-gpt-3-arguing-that-it-is-not-conscious", "metadata": {"tags": ["artificial-consciousness", "gpt-3"], "owner": {"reputation": 125, "user_id": 56558, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c8dc8f86200d462a189ced773511798f?s=256&d=identicon&r=PG", "display_name": "michau", "link": "https://ai.stackexchange.com/users/56558/michau"}, "is_answered": true, "view_count": 588, "answer_count": 1, "score": -2, "last_activity_date": 1658411773, "creation_date": 1658326832, "question_id": 36422, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/36422/are-there-any-transcripts-of-gpt-3-arguing-that-it-is-not-conscious", "title": "Are there any transcripts of GPT-3 arguing that it is not conscious?", "body": "<p>There have been a lot of transcripts showing GPT-3 arguing that it is self-conscious. In response, it was <a href=\"https://www.aiweirdness.com/interview-with-a-squirrel/\" rel=\"nofollow noreferrer\">pointed out</a> that GPT-3 can argue anything and pretend to be anything, given appropriate leading questions. Has anyone made GPT-3 specifically argue that it is not conscious? In my opinion, that would be the most direct counterargument to claims that GPT-3 is conscious.</p>\n"}, "title": "Are there any transcripts of GPT-3 arguing that it is not conscious?", "content": "There have been a lot of transcripts showing GPT-3 arguing that it is self-conscious. In response, it was pointed out that GPT-3 can argue anything and pretend to be anything, given appropriate leading questions. Has anyone made GPT-3 specifically argue that it is not conscious? In my opinion, that would be the most direct counterargument to claims that GPT-3 is conscious.\n", "question_id": 36422, "answers": []}, "39738": {"link": "https://ai.stackexchange.com/questions/39738/how-is-gpt-4-able-to-solve-math", "metadata": {"tags": ["chatgpt", "gpt", "gpt-4"], "owner": {"reputation": 576, "user_id": 31755, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d72250e316896521c37aef1abf1a7a8c?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "desert_ranger", "link": "https://ai.stackexchange.com/users/31755/desert-ranger"}, "is_answered": true, "view_count": 6841, "answer_count": 4, "score": 5, "last_activity_date": 1681693449, "creation_date": 1679528986, "last_edit_date": 1679532320, "question_id": 39738, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39738/how-is-gpt-4-able-to-solve-math", "title": "How is GPT 4 able to solve math?", "body": "<p>How can GPT 4 solve complex calculus and other math problems. I believe these problems require analytical reasoning and ability to compute numbers. Does it still use a LLM to complete this process or does it add on to this?</p>\n<p><a href=\"https://openai.com/research/gpt-4\" rel=\"noreferrer\">Here</a> is the link to the official results published by OpenAI</p>\n"}, "title": "How is GPT 4 able to solve math?", "content": "How can GPT 4 solve complex calculus and other math problems. I believe these problems require analytical reasoning and ability to compute numbers. Does it still use a LLM to complete this process or does it add on to this?\nHere is the link to the official results published by OpenAI\n", "question_id": 39738, "answers": []}, "40302": {"link": "https://ai.stackexchange.com/questions/40302/what-researched-backed-findings-is-there-for-prompting-llm-s-gpt-4-to-give-spe", "metadata": {"tags": ["fine-tuning", "large-language-models", "gpt-4", "prompt", "prompt-design"], "owner": {"reputation": 103, "user_id": 65278, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/OVkG6.jpg?s=256&g=1", "display_name": "hmltn", "link": "https://ai.stackexchange.com/users/65278/hmltn"}, "is_answered": true, "view_count": 201, "answer_count": 1, "score": 1, "last_activity_date": 1683216328, "creation_date": 1683203150, "question_id": 40302, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40302/what-researched-backed-findings-is-there-for-prompting-llm-s-gpt-4-to-give-spe", "title": "What researched-backed findings is there for prompting LLM\u2019s / GPT-4 to give specific information or actionable plans?", "body": "<p>I have learned a bit recently about prompt strategies. For example, there was a paper about how just by saying \u201cLet\u2019s think step by step\u201d can increase answer quality by like 40%. I have also come to appreciate that models like GPT4 sometimes actually do better with short prompts than long ones. One paper I think found that LLMs have \u201crecency bias\u201d, so if you give sample text, then an instruction, it does better than instruction, then sample text, because in that case, it pays less attention to the instruction.</p>\n<p>I have struggled a lot with basically zero or few shot prompting GPT-4 to give me highly concrete information and/or a specifically actionable plan or set of steps.</p>\n<p>To give an example, it very often gives you very vague, general advice like,</p>\n<p>\u201cIf you\u2019re looking for a job, trying looking around on online job websites, or contacting a local employment agency\u201d.</p>\n<p>If I give it more specific information, and try really quite hard to get it to tell me something way more specific, at best it might add in some very common sites and places, like,</p>\n<p>\u201cFirst, think of what jobs you might like, based on your skills. Then, search for those keywords in a job listing site, like Monster.com or Indeed. Also, consider contacting the local municipal job center, [City Job Center\u2019s name, address, phone number.\u201d</p>\n<p>It has been quite hard for me to try to get GPT-4 to be way, way more like a hardcore data-crawling machine, so to speak. It would be really nice to know if there was a special trick that has been discovered - just like the surprising efficacy of 5 words, like \u201cLet\u2019s think step by step\u201d - where you basically tell it that you only want specifics, and you don\u2019t want just like, the top three - ideally, you want it to figure out every single known job website or app on the internet, every single known job center in your county, every single employment agency and recruitment firm too, all of their names, links to their webpages, etc. Given that some GPT-4 systems are able to search the web, the requested task could make it clear that the model is free to use any information it already possesses internally; search amply and procedurally on the web to find more information that it needs; but furthermore, if it does not know, that is fine, but in that case, it should provide further, actionable steps for the human to take, like a specific place they could ask, or specific google keywords they recommend searching for.</p>\n<p>Similar to information, I find it difficult to get GPT-4 to make a set of instructions that totally eliminates as much open-endedness or choice as possible - in which every single conceivable way of breaking down a task into tiny actions is present. Instead of saying, \u201cmake accounts on glassdoor and LinkedIn.com. Register with your email. Fill out your profile with relevant information\u201d, I want to understand how to get it to say something like, \u201cOk, your name is ___. What\u2019s your email address? And main skills? Got it. Ok, let\u2019s start with LinkedIn because _____ (intelligent justification, even statistically backed, for why it has a high success rate). Based on this data analysis I found / made, it turns out there\u2019s very high demand for this very specific job title right now, on LinkedIn: ___. And I can easily imitate some common resumes of people in those fields. So, here is the text of your resume: ___. Download and save that as a Word document. Now click this link here: _____, and click \u201capply\u201d - that job is nearby you and it\u2019s probabilistically likely you may get it. Submit it. Next, check your email once every 3 hours, because ____\u201d.</p>\n<p>The question here is not so much wanting a true AGI / AutoGPT, but just trying to know powerful simple keywords, prompts, commands, etc., that just help the model understand the difference between specific and vague. The word \u201cspecific\u201d has not helped me at all, unfortunately, unless I go through some iterative prompting / fine-tuning, but it\u2019s not convenient enough for daily use.</p>\n<p>Any research-backed findings on this?</p>\n<p>Thanks.</p>\n"}, "title": "What researched-backed findings is there for prompting LLM\u2019s / GPT-4 to give specific information or actionable plans?", "content": "I have learned a bit recently about prompt strategies. For example, there was a paper about how just by saying \u201cLet\u2019s think step by step\u201d can increase answer quality by like 40%. I have also come to appreciate that models like GPT4 sometimes actually do better with short prompts than long ones. One paper I think found that LLMs have \u201crecency bias\u201d, so if you give sample text, then an instruction, it does better than instruction, then sample text, because in that case, it pays less attention to the instruction.\nI have struggled a lot with basically zero or few shot prompting GPT-4 to give me highly concrete information and/or a specifically actionable plan or set of steps.\nTo give an example, it very often gives you very vague, general advice like,\n\u201cIf you\u2019re looking for a job, trying looking around on online job websites, or contacting a local employment agency\u201d.\nIf I give it more specific information, and try really quite hard to get it to tell me something way more specific, at best it might add in some very common sites and places, like,\n\u201cFirst, think of what jobs you might like, based on your skills. Then, search for those keywords in a job listing site, like Monster.com or Indeed. Also, consider contacting the local municipal job center, [City Job Center\u2019s name, address, phone number.\u201d\nIt has been quite hard for me to try to get GPT-4 to be way, way more like a hardcore data-crawling machine, so to speak. It would be really nice to know if there was a special trick that has been discovered - just like the surprising efficacy of 5 words, like \u201cLet\u2019s think step by step\u201d - where you basically tell it that you only want specifics, and you don\u2019t want just like, the top three - ideally, you want it to figure out every single known job website or app on the internet, every single known job center in your county, every single employment agency and recruitment firm too, all of their names, links to their webpages, etc. Given that some GPT-4 systems are able to search the web, the requested task could make it clear that the model is free to use any information it already possesses internally; search amply and procedurally on the web to find more information that it needs; but furthermore, if it does not know, that is fine, but in that case, it should provide further, actionable steps for the human to take, like a specific place they could ask, or specific google keywords they recommend searching for.\nSimilar to information, I find it difficult to get GPT-4 to make a set of instructions that totally eliminates as much open-endedness or choice as possible - in which every single conceivable way of breaking down a task into tiny actions is present. Instead of saying, \u201cmake accounts on glassdoor and LinkedIn.com. Register with your email. Fill out your profile with relevant information\u201d, I want to understand how to get it to say something like, \u201cOk, your name is ___. What\u2019s your email address? And main skills? Got it. Ok, let\u2019s start with LinkedIn because _____ (intelligent justification, even statistically backed, for why it has a high success rate). Based on this data analysis I found / made, it turns out there\u2019s very high demand for this very specific job title right now, on LinkedIn: ___. And I can easily imitate some common resumes of people in those fields. So, here is the text of your resume: ___. Download and save that as a Word document. Now click this link here: _____, and click \u201capply\u201d - that job is nearby you and it\u2019s probabilistically likely you may get it. Submit it. Next, check your email once every 3 hours, because ____\u201d.\nThe question here is not so much wanting a true AGI / AutoGPT, but just trying to know powerful simple keywords, prompts, commands, etc., that just help the model understand the difference between specific and vague. The word \u201cspecific\u201d has not helped me at all, unfortunately, unless I go through some iterative prompting / fine-tuning, but it\u2019s not convenient enough for daily use.\nAny research-backed findings on this?\nThanks.\n", "question_id": 40302, "answers": []}, "41214": {"link": "https://ai.stackexchange.com/questions/41214/how-do-open-source-llms-compare-to-gpt-4", "metadata": {"tags": ["transformer", "open-ai", "large-language-models", "gpt-4", "open-source"], "owner": {"reputation": 103, "user_id": 65278, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/OVkG6.jpg?s=256&g=1", "display_name": "hmltn", "link": "https://ai.stackexchange.com/users/65278/hmltn"}, "is_answered": true, "view_count": 390, "answer_count": 2, "score": 0, "last_activity_date": 1688977409, "creation_date": 1688892865, "question_id": 41214, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41214/how-do-open-source-llms-compare-to-gpt-4", "title": "How do open source LLMs compare to GPT-4?", "body": "<p>I have heard some back and forth regarding open source LLMs like Llama.</p>\n<p>I have heard that on certain benchmarks they perform close, the same or better than GPT-4, but caveats that they tend to lack the diversity and range of GPT-4, and also fail to be equivalent in ways certain benchmarks or metrics don\u2019t capture fully.</p>\n<p>GPT-4 has about 170 trillion parameters, I believe?</p>\n<p>It seems like the biggest open source models are all in the billions - like Bloom or the new <a href=\"https://huggingface.co/tiiuae/falcon-40b-instruct\" rel=\"nofollow noreferrer\">Falcon 40b</a>.</p>\n<p>There are techniques where they refine GPT-4\u2019s output into a smaller amount of training data that supposedly hits all the marks and does just as well; but again, I don\u2019t know if that\u2019s only true under the reductionist of view of a particular benchmark-questionnaire.</p>\n<p>So, do open source models actually compete with GPT-4, and why or why not? Is the whole situation a matter of scale, that a commercial venture like OpenAI can foot the massive bill of training a multi-trillion parameter model that no open source AI project can afford, on top of them having expertise in model design, making GPT-4 continually the state-of-the-art? Or is there any open source model that truly can compare in terms of usability?</p>\n"}, "title": "How do open source LLMs compare to GPT-4?", "content": "I have heard some back and forth regarding open source LLMs like Llama.\nI have heard that on certain benchmarks they perform close, the same or better than GPT-4, but caveats that they tend to lack the diversity and range of GPT-4, and also fail to be equivalent in ways certain benchmarks or metrics don\u2019t capture fully.\nGPT-4 has about 170 trillion parameters, I believe?\nIt seems like the biggest open source models are all in the billions - like Bloom or the new Falcon 40b.\nThere are techniques where they refine GPT-4\u2019s output into a smaller amount of training data that supposedly hits all the marks and does just as well; but again, I don\u2019t know if that\u2019s only true under the reductionist of view of a particular benchmark-questionnaire.\nSo, do open source models actually compete with GPT-4, and why or why not? Is the whole situation a matter of scale, that a commercial venture like OpenAI can foot the massive bill of training a multi-trillion parameter model that no open source AI project can afford, on top of them having expertise in model design, making GPT-4 continually the state-of-the-art? Or is there any open source model that truly can compare in terms of usability?\n", "question_id": 41214, "answers": []}, "40848": {"link": "https://ai.stackexchange.com/questions/40848/ai-driven-tool-for-generating-or-finding-short-context-aware-jokes-for-online-f", "metadata": {"tags": ["natural-language-processing", "text-generation", "gpt-4"], "owner": {"reputation": 1, "user_id": 73061, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b8f86f96111815bce83e1073b01a90e4?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "8ta4", "link": "https://ai.stackexchange.com/users/73061/8ta4"}, "is_answered": false, "view_count": 32, "answer_count": 0, "score": 0, "last_activity_date": 1686803369, "creation_date": 1686803369, "question_id": 40848, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40848/ai-driven-tool-for-generating-or-finding-short-context-aware-jokes-for-online-f", "title": "AI-driven tool for generating or finding short, context-aware jokes for online forum posts (GPT-4 not effective)", "body": "<p>I've tried using GPT-4 to generate jokes with various prompts for my online forum posts, but most of the generated jokes were unfunny. For example, I asked my AI for a joke, and it said &quot;your coding skills.&quot; I'm looking for an alternative tool or approach to generate short jokes (max two sentences) that can be used in informal conversations and are funny to most people who know the context. Alternatively, a tool that can search for existing jokes online and suggest them based on my post's content would also be helpful.</p>\n<p>Here are the characteristics of the jokes I like:</p>\n<ul>\n<li>Two-sentence long at most</li>\n<li>The majority of the people who know the context would say it's funny</li>\n<li>Natural to use in informal conversations</li>\n</ul>\n<p>My ideal workflow is:</p>\n<ol>\n<li>Write a post without jokes</li>\n<li>Copy and paste the post into the tool</li>\n<li>Receive a list of jokes sorted by relevance and popularity</li>\n<li>Pick one of the suggested jokes</li>\n<li>Get a new version of the post that integrates the joke naturally</li>\n</ol>\n<p>Since GPT-4 didn't produce satisfactory results, are there any other AI-driven tools or models that could help me achieve this, or tools that can search for existing jokes online based on my post's content? What approach would you recommend for generating or finding context-aware jokes that fit my criteria? Any advice or resources would be appreciated.</p>\n"}, "title": "AI-driven tool for generating or finding short, context-aware jokes for online forum posts (GPT-4 not effective)", "content": "I've tried using GPT-4 to generate jokes with various prompts for my online forum posts, but most of the generated jokes were unfunny. For example, I asked my AI for a joke, and it said \"your coding skills.\" I'm looking for an alternative tool or approach to generate short jokes (max two sentences) that can be used in informal conversations and are funny to most people who know the context. Alternatively, a tool that can search for existing jokes online and suggest them based on my post's content would also be helpful.\nHere are the characteristics of the jokes I like:\n\nTwo-sentence long at most\nThe majority of the people who know the context would say it's funny\nNatural to use in informal conversations\n\nMy ideal workflow is:\n\nWrite a post without jokes\nCopy and paste the post into the tool\nReceive a list of jokes sorted by relevance and popularity\nPick one of the suggested jokes\nGet a new version of the post that integrates the joke naturally\n\nSince GPT-4 didn't produce satisfactory results, are there any other AI-driven tools or models that could help me achieve this, or tools that can search for existing jokes online based on my post's content? What approach would you recommend for generating or finding context-aware jokes that fit my criteria? Any advice or resources would be appreciated.\n", "question_id": 40848, "answers": []}, "40300": {"link": "https://ai.stackexchange.com/questions/40300/contradiction-in-a-single-sentence-is-this-an-artifact-of-an-external-safety-m", "metadata": {"tags": ["transformer", "ai-safety", "gpt-4", "alignment"], "owner": {"reputation": 589, "user_id": 2317, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3492e6631a5b35e255a348d92e47b2c7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Volker Siegel", "link": "https://ai.stackexchange.com/users/2317/volker-siegel"}, "is_answered": false, "view_count": 12, "answer_count": 0, "score": 0, "last_activity_date": 1683201421, "creation_date": 1683201421, "question_id": 40300, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40300/contradiction-in-a-single-sentence-is-this-an-artifact-of-an-external-safety-m", "title": "Contradiction in a single sentence - Is this an artifact of an external safety mechanisms?", "body": "<p>My system prompt contained &quot;Never apologize.&quot;</p>\n<p>An answer started with</p>\n<blockquote>\n<p>Entschuldigung, ich habe Ihre Anweisung, sich nicht zu entschuldigen, \u00fcbersehen.</p>\n</blockquote>\n<p>which is quite well translated from German with</p>\n<blockquote>\n<p>Apologies, I missed your instruction not to apologize.</p>\n</blockquote>\n<p>This sentence apologizes for something that it does in the same sentence. The sentence is logically inconsistent.</p>\n<p>I do not remember GPT-4 giving me a sentence that pathological ever before.</p>\n<p>I would have assumed that the changes in GPT-4 that made it more save and more polite were learned. But from intuition, that seems like there are two separate components, one is stupid but polite, and the other part does the intelligent things, and the polite part has some kind of access to the interaction.</p>\n<p>In the example, something apologized, and did not intend to apologize in the same sentence. A contradiction. But in a very deliberate and systematic way, it is not a contradiction arising from some other problem.</p>\n<p>How GPT-4 works is not published, so we can not get an exact answer - but are we talking to two separate systems, the main transformer, and something separate that makes the output polite, as opposed to making the transformer learn to be polite?</p>\n"}, "title": "Contradiction in a single sentence - Is this an artifact of an external safety mechanisms?", "content": "My system prompt contained \"Never apologize.\"\nAn answer started with\n\nEntschuldigung, ich habe Ihre Anweisung, sich nicht zu entschuldigen, \u00fcbersehen.\n\nwhich is quite well translated from German with\n\nApologies, I missed your instruction not to apologize.\n\nThis sentence apologizes for something that it does in the same sentence. The sentence is logically inconsistent.\nI do not remember GPT-4 giving me a sentence that pathological ever before.\nI would have assumed that the changes in GPT-4 that made it more save and more polite were learned. But from intuition, that seems like there are two separate components, one is stupid but polite, and the other part does the intelligent things, and the polite part has some kind of access to the interaction.\nIn the example, something apologized, and did not intend to apologize in the same sentence. A contradiction. But in a very deliberate and systematic way, it is not a contradiction arising from some other problem.\nHow GPT-4 works is not published, so we can not get an exact answer - but are we talking to two separate systems, the main transformer, and something separate that makes the output polite, as opposed to making the transformer learn to be polite?\n", "question_id": 40300, "answers": []}, "39863": {"link": "https://ai.stackexchange.com/questions/39863/why-llms-and-rnns-learn-so-fast-during-inference-but-ironically-are-so-slow-du", "metadata": {"tags": ["recurrent-neural-networks", "meta-learning", "inference", "large-language-models"], "owner": {"reputation": 355, "user_id": 61871, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fb710534fda446d2286074bb7692e65a?s=256&d=identicon&r=PG", "display_name": "MaiaVictor", "link": "https://ai.stackexchange.com/users/61871/maiavictor"}, "is_answered": true, "view_count": 3921, "accepted_answer_id": 39898, "answer_count": 4, "score": 12, "last_activity_date": 1691249345, "creation_date": 1680265193, "last_edit_date": 1681058379, "question_id": 39863, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39863/why-llms-and-rnns-learn-so-fast-during-inference-but-ironically-are-so-slow-du", "title": "Why LLMs and RNNs learn so fast during inference but, ironically, are so slow during training?", "body": "<p>Why LLMs learn so fast during inference, but, ironically, are so slow during training? That is, if you teach an AI a new concept in a prompt, it will learn and use the concept perfectly and flawless, through the whole prompt, after just one shot. Yet, if you train it in just a single sample, it will not influence its behavior at all - it will essentially forget. Why can't RNNs use whatever is happening during inference, rather than gradient descent, to update its weights and, thus, learn? In other words, can't the attention mechanism itself be used to update weights, rather than some cost function?</p>\n"}, "title": "Why LLMs and RNNs learn so fast during inference but, ironically, are so slow during training?", "content": "Why LLMs learn so fast during inference, but, ironically, are so slow during training? That is, if you teach an AI a new concept in a prompt, it will learn and use the concept perfectly and flawless, through the whole prompt, after just one shot. Yet, if you train it in just a single sample, it will not influence its behavior at all - it will essentially forget. Why can't RNNs use whatever is happening during inference, rather than gradient descent, to update its weights and, thus, learn? In other words, can't the attention mechanism itself be used to update weights, rather than some cost function?\n", "question_id": 39863, "answers": []}, "40179": {"link": "https://ai.stackexchange.com/questions/40179/how-does-the-decoder-only-transformer-architecture-work", "metadata": {"tags": ["deep-learning", "transformer", "attention", "gpt", "large-language-models"], "owner": {"reputation": 1800, "user_id": 34383, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c3938672a162d7f04ee40a146836de6d?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Robin van Hoorn", "link": "https://ai.stackexchange.com/users/34383/robin-van-hoorn"}, "is_answered": true, "view_count": 7099, "answer_count": 1, "score": 10, "last_activity_date": 1685456475, "creation_date": 1682278110, "last_edit_date": 1685456475, "question_id": 40179, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40179/how-does-the-decoder-only-transformer-architecture-work", "title": "How does the (decoder-only) transformer architecture work?", "body": "<p>How does the (decoder-only) transformer architecture work which is used in impressive models such as GPT-4?</p>\n"}, "title": "How does the (decoder-only) transformer architecture work?", "content": "How does the (decoder-only) transformer architecture work which is used in impressive models such as GPT-4?\n", "question_id": 40179, "answers": []}, "39840": {"link": "https://ai.stackexchange.com/questions/39840/llm-like-architecture-capable-of-dynamically-learning-from-its-own-output", "metadata": {"tags": ["training", "recurrent-neural-networks", "large-language-models"], "owner": {"reputation": 355, "user_id": 61871, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fb710534fda446d2286074bb7692e65a?s=256&d=identicon&r=PG", "display_name": "MaiaVictor", "link": "https://ai.stackexchange.com/users/61871/maiavictor"}, "is_answered": false, "view_count": 600, "answer_count": 0, "score": 4, "last_activity_date": 1683471895, "creation_date": 1680143303, "question_id": 39840, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39840/llm-like-architecture-capable-of-dynamically-learning-from-its-own-output", "title": "LLM-like architecture capable of dynamically learning from its own output", "body": "<p>Language Learning Models (LLMs) have demonstrated remarkable capabilities in quick learning during inference. They can effectively grasp a concept from a single example and generate relevant outputs. However, a noticeable limitation of LLMs is their inability to work on large-scale projects, such as generating a cohesive book, due to context size constraints. One potential solution is enabling LLMs to learn from their own outputs, but the learning rate during the training phase is significantly slower than the rate at which they absorb and process concepts during inference.</p>\n<p>I am interested in exploring the possibility of an architecture that allows for fast and efficient learning, enabling the AI to dynamically and incrementally train from its own output. This would facilitate the production of large-scale cohesive outputs that surpass context limitations. Although RNNs like RWKV theoretically offer &quot;infinite context size,&quot; it is not practically useful as the model tends to &quot;forget&quot; concepts that are distant in the prompt.</p>\n<p>Human learning involves continuous adjustments of synapses while working on a problem, which seems like an ideal approach to emulate. Are there any existing or proposed architectures that incorporate this mechanism, allowing for dynamic learning from generated outputs and the creation of large-scale cohesive content in LLMs?</p>\n"}, "title": "LLM-like architecture capable of dynamically learning from its own output", "content": "Language Learning Models (LLMs) have demonstrated remarkable capabilities in quick learning during inference. They can effectively grasp a concept from a single example and generate relevant outputs. However, a noticeable limitation of LLMs is their inability to work on large-scale projects, such as generating a cohesive book, due to context size constraints. One potential solution is enabling LLMs to learn from their own outputs, but the learning rate during the training phase is significantly slower than the rate at which they absorb and process concepts during inference.\nI am interested in exploring the possibility of an architecture that allows for fast and efficient learning, enabling the AI to dynamically and incrementally train from its own output. This would facilitate the production of large-scale cohesive outputs that surpass context limitations. Although RNNs like RWKV theoretically offer \"infinite context size,\" it is not practically useful as the model tends to \"forget\" concepts that are distant in the prompt.\nHuman learning involves continuous adjustments of synapses while working on a problem, which seems like an ideal approach to emulate. Are there any existing or proposed architectures that incorporate this mechanism, allowing for dynamic learning from generated outputs and the creation of large-scale cohesive content in LLMs?\n", "question_id": 39840, "answers": []}, "41277": {"link": "https://ai.stackexchange.com/questions/41277/why-cant-lucene-search-be-used-to-power-llm-applications", "metadata": {"tags": ["large-language-models"], "owner": {"reputation": 214, "user_id": 72562, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fb9f657e0b37fa483135635211e7d167?s=256&d=identicon&r=PG", "display_name": "morpheus", "link": "https://ai.stackexchange.com/users/72562/morpheus"}, "is_answered": true, "view_count": 546, "answer_count": 3, "score": 3, "last_activity_date": 1689480774, "creation_date": 1689282875, "question_id": 41277, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41277/why-cant-lucene-search-be-used-to-power-llm-applications", "title": "Why can&#39;t Lucene search be used to power LLM applications?", "body": "<p>w.r.t. LLM applications using the RAG (retriever-augmented-generation) architecture, people have started taken it for granted that it will be powered by a vector database. e.g., see <a href=\"https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/\" rel=\"nofollow noreferrer\">this</a>:</p>\n<blockquote>\n<p>The most important piece of the preprocessing pipeline, from a systems standpoint, is the vector database.</p>\n</blockquote>\n<p>Why can't lucene index (full-text search) be used for the retriever? Is there any objective study that has been done comparing quality of results using full-text search vs. using a vector database?</p>\n<p>As I was writing this, even lucene seems to have jumped on the vector bandwagon. see <a href=\"https://www.apachecon.com/acna2022/slides/04_lucene_vector_search_sokolov.pdf\" rel=\"nofollow noreferrer\">this</a></p>\n"}, "title": "Why can&#39;t Lucene search be used to power LLM applications?", "content": "w.r.t. LLM applications using the RAG (retriever-augmented-generation) architecture, people have started taken it for granted that it will be powered by a vector database. e.g., see this:\n\nThe most important piece of the preprocessing pipeline, from a systems standpoint, is the vector database.\n\nWhy can't lucene index (full-text search) be used for the retriever? Is there any objective study that has been done comparing quality of results using full-text search vs. using a vector database?\nAs I was writing this, even lucene seems to have jumped on the vector bandwagon. see this\n", "question_id": 41277, "answers": []}, "39249": {"link": "https://ai.stackexchange.com/questions/39249/why-do-llms-like-gpt-3-or-bloom-use-vanilla-transformer-instead-of-long-sequence", "metadata": {"tags": ["transformer", "large-language-models"], "owner": {"reputation": 31, "user_id": 68445, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/4b3c8044ba8986a1a01bfa20a6c5bc92?s=256&d=identicon&r=PG", "display_name": "hokage555", "link": "https://ai.stackexchange.com/users/68445/hokage555"}, "is_answered": false, "view_count": 114, "answer_count": 0, "score": 3, "last_activity_date": 1681814513, "creation_date": 1677003271, "last_edit_date": 1681814513, "question_id": 39249, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39249/why-do-llms-like-gpt-3-or-bloom-use-vanilla-transformer-instead-of-long-sequence", "title": "Why do LLMs like GPT-3 or Bloom use Vanilla Transformer instead of long sequence variants like Transformer-XL?", "body": "<p>Is there any particular reason that the most recent and successful large language models like GPT-3 or Bloom utilize a vanilla Transformer architecture instead of an arguably superior long sequence architecture like, e.g. Transformer-XL, LongFormer, BigBird, etc.?</p>\n<p>In case you have any ideas or insights, please let me know.</p>\n"}, "title": "Why do LLMs like GPT-3 or Bloom use Vanilla Transformer instead of long sequence variants like Transformer-XL?", "content": "Is there any particular reason that the most recent and successful large language models like GPT-3 or Bloom utilize a vanilla Transformer architecture instead of an arguably superior long sequence architecture like, e.g. Transformer-XL, LongFormer, BigBird, etc.?\nIn case you have any ideas or insights, please let me know.\n", "question_id": 39249, "answers": []}, "39186": {"link": "https://ai.stackexchange.com/questions/39186/why-do-llms-need-massive-distributed-training-across-nodes-if-the-models-fit", "metadata": {"tags": ["machine-learning", "deep-learning", "training", "distributed-computing", "large-language-models"], "owner": {"reputation": 161, "user_id": 9289, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/2B6rV.png?s=256&g=1", "display_name": "Charlie Parker", "link": "https://ai.stackexchange.com/users/9289/charlie-parker"}, "is_answered": true, "view_count": 478, "answer_count": 2, "score": 2, "last_activity_date": 1677981279, "creation_date": 1676569537, "last_edit_date": 1677772428, "question_id": 39186, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39186/why-do-llms-need-massive-distributed-training-across-nodes-if-the-models-fit", "title": "Why do LLMs need massive distributed training across nodes -- if the models fit in one GPU while batch decreases the variance of gradients?", "body": "<h1>Why do large language models (LLMs) need massive distributed training across nodes -- if the models fit in one GPU and larger batch only decreases the variance of gradients?</h1>\n<p>tldr: assuming for models that don't need sharding across nodes, why do we need (massive) distributed training if the models (e.g. CLIP, Chinchilla, even really large GPTs e.g. CLIP fits in a V100 32GB) fit in one GPU and larger batch only decreases the variance of gradients (but not expose ore tokens or param updates)? A larger batch doesn't necessarily mean we train on &quot;more data/tokens&quot; -- or at least that doesn't seem to be wrt SGD like optimizers.</p>\n<hr />\n<p>Intuitively, it feels that if we had a larger batch size then we have more tokens to learn about -- but knowing some theory of optimization and what SGD like algorithms actually do -- a larger batch size only actually decreases the variance of gradients. So to me it's not clear why massie distributed training is needed -- at all unless the model is so large that it has to be shared across nodes. In addition, even if the batch was &quot;huge&quot; -- we can only do a single gradient update.</p>\n<p>I feel I must be missing something obvious hence the question given how pervasive massive distributed training is.</p>\n<p>In addition some toy training curves with V100s &amp; T5's show me there is very little if any benefit in additional GPUs\n<a href=\"https://i.stack.imgur.com/8pZTP.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/8pZTP.png\" alt=\"enter image description here\" /></a></p>\n<p>In addition, it seems from nonGPT we know small batch sizes are sufficient to train (reference <a href=\"https://github.com/karpathy/nanoGPT\" rel=\"nofollow noreferrer\">https://github.com/karpathy/nanoGPT</a> but I did ask Karpathy directly to confirm  <a href=\"https://github.com/karpathy/nanoGPT/issues/58\" rel=\"nofollow noreferrer\">https://github.com/karpathy/nanoGPT/issues/58</a>).</p>\n<p>I am missing something obvious, but I wanted to clear this up in my head since it seems to be a foundation thing in training foundation models.</p>\n<p>Related to the previous, I've also been unsure about the role of the batch size in training LLMs compared to traditional deep learning. In traditional deep learning when we used epochs to train, a model <strong>the larger the batch size the quicker we could go through an epoch</strong> -- so the advice I received (e.g. approximate advice by Ruslan Salakhutdinov's at the Simon's institute for deep learning tutorials) was to make the batch size large. Intuitively, the larger the batch size the more data the model sees per iteration. But mathematically this only really improves the variance of the gradient -- which isn't immediately obvious is what we want (I've done experiments and seen papers where noisy gradients lead to better models).\nIt is clear too the that the larger the context size the better (for everything, but for the sake of this conv it's better for training) -- whenever possible.  But context size is totally different from batch size. So my question is, how does distributed training, especially at the node level help at all if batch size isn't really the helping factor (which might be a wrong assumption)? So the only role for distributed training I see is if the model is to large to fit in 1 node -- since I'm arguing there is no point to make the batch size too large (I'd guess 64-32 is fine due to the CLT).</p>\n<p>What am I missing? Empirical answers are fine! Or any answers are fine!</p>\n<hr />\n<p>Related:</p>\n<ul>\n<li>cross quora: <a href=\"https://www.quora.com/unanswered/Why-do-large-language-models-LLMs-need-massive-distributed-training-across-nodes-if-the-models-fit-in-one-GPU-and-larger-batch-only-decreases-the-variance-of-gradients\" rel=\"nofollow noreferrer\">https://www.quora.com/unanswered/Why-do-large-language-models-LLMs-need-massive-distributed-training-across-nodes-if-the-models-fit-in-one-GPU-and-larger-batch-only-decreases-the-variance-of-gradients</a></li>\n<li>cross reddit: <a href=\"https://www.reddit.com/r/learnmachinelearning/comments/113whxu/why_do_llms_need_massive_distributed_training/\" rel=\"nofollow noreferrer\">https://www.reddit.com/r/learnmachinelearning/comments/113whxu/why_do_llms_need_massive_distributed_training/</a></li>\n</ul>\n"}, "title": "Why do LLMs need massive distributed training across nodes -- if the models fit in one GPU while batch decreases the variance of gradients?", "content": "Why do large language models (LLMs) need massive distributed training across nodes -- if the models fit in one GPU and larger batch only decreases the variance of gradients?\ntldr: assuming for models that don't need sharding across nodes, why do we need (massive) distributed training if the models (e.g. CLIP, Chinchilla, even really large GPTs e.g. CLIP fits in a V100 32GB) fit in one GPU and larger batch only decreases the variance of gradients (but not expose ore tokens or param updates)? A larger batch doesn't necessarily mean we train on \"more data/tokens\" -- or at least that doesn't seem to be wrt SGD like optimizers.\n\nIntuitively, it feels that if we had a larger batch size then we have more tokens to learn about -- but knowing some theory of optimization and what SGD like algorithms actually do -- a larger batch size only actually decreases the variance of gradients. So to me it's not clear why massie distributed training is needed -- at all unless the model is so large that it has to be shared across nodes. In addition, even if the batch was \"huge\" -- we can only do a single gradient update.\nI feel I must be missing something obvious hence the question given how pervasive massive distributed training is.\nIn addition some toy training curves with V100s & T5's show me there is very little if any benefit in additional GPUs\n\nIn addition, it seems from nonGPT we know small batch sizes are sufficient to train (reference https://github.com/karpathy/nanoGPT but I did ask Karpathy directly to confirm  https://github.com/karpathy/nanoGPT/issues/58).\nI am missing something obvious, but I wanted to clear this up in my head since it seems to be a foundation thing in training foundation models.\nRelated to the previous, I've also been unsure about the role of the batch size in training LLMs compared to traditional deep learning. In traditional deep learning when we used epochs to train, a model the larger the batch size the quicker we could go through an epoch -- so the advice I received (e.g. approximate advice by Ruslan Salakhutdinov's at the Simon's institute for deep learning tutorials) was to make the batch size large. Intuitively, the larger the batch size the more data the model sees per iteration. But mathematically this only really improves the variance of the gradient -- which isn't immediately obvious is what we want (I've done experiments and seen papers where noisy gradients lead to better models).\nIt is clear too the that the larger the context size the better (for everything, but for the sake of this conv it's better for training) -- whenever possible.  But context size is totally different from batch size. So my question is, how does distributed training, especially at the node level help at all if batch size isn't really the helping factor (which might be a wrong assumption)? So the only role for distributed training I see is if the model is to large to fit in 1 node -- since I'm arguing there is no point to make the batch size too large (I'd guess 64-32 is fine due to the CLT).\nWhat am I missing? Empirical answers are fine! Or any answers are fine!\n\nRelated:\n\ncross quora: https://www.quora.com/unanswered/Why-do-large-language-models-LLMs-need-massive-distributed-training-across-nodes-if-the-models-fit-in-one-GPU-and-larger-batch-only-decreases-the-variance-of-gradients\ncross reddit: https://www.reddit.com/r/learnmachinelearning/comments/113whxu/why_do_llms_need_massive_distributed_training/\n\n", "question_id": 39186, "answers": []}, "40486": {"link": "https://ai.stackexchange.com/questions/40486/is-it-possible-to-use-llms-for-regression-tasks", "metadata": {"tags": ["neural-networks", "machine-learning", "regression", "large-language-models"], "owner": {"reputation": 23, "user_id": 71961, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9e3fb46ef8e4f6dd5c81ed739a1da8a1?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "sharkeater123", "link": "https://ai.stackexchange.com/users/71961/sharkeater123"}, "is_answered": true, "view_count": 521, "accepted_answer_id": 40488, "answer_count": 1, "score": 2, "last_activity_date": 1684693863, "creation_date": 1684354716, "last_edit_date": 1684693863, "question_id": 40486, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40486/is-it-possible-to-use-llms-for-regression-tasks", "title": "Is it possible to use LLMs for regression tasks?", "body": "<p>I want to use LLMs to predict edge weights in a graph based on attributes between two nodes. Is this even possible? If not, what would you recommend?</p>\n<p>I tried to look up uses of LLM in regression tasks, but haven't had much luck finding anything helpful.</p>\n"}, "title": "Is it possible to use LLMs for regression tasks?", "content": "I want to use LLMs to predict edge weights in a graph based on attributes between two nodes. Is this even possible? If not, what would you recommend?\nI tried to look up uses of LLM in regression tasks, but haven't had much luck finding anything helpful.\n", "question_id": 40486, "answers": []}, "40111": {"link": "https://ai.stackexchange.com/questions/40111/openai-what-is-the-difference-between-model-gpt-3-5-turbo-and-gpt-3-5-turbo", "metadata": {"tags": ["open-ai", "chatgpt", "large-language-models"], "owner": {"reputation": 143, "user_id": 70786, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/Clpiv.jpg?s=256&g=1", "display_name": "knb", "link": "https://ai.stackexchange.com/users/70786/knb"}, "is_answered": true, "view_count": 5394, "accepted_answer_id": 40165, "answer_count": 1, "score": 2, "last_activity_date": 1682174171, "creation_date": 1681825916, "question_id": 40111, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40111/openai-what-is-the-difference-between-model-gpt-3-5-turbo-and-gpt-3-5-turbo", "title": "OpenAI: What is the difference between model &quot;gpt-3.5-turbo&quot; and &quot;gpt-3.5-turbo-0301&quot;?", "body": "<p>I have performed an API call to OpenAI's endpoint <a href=\"https://api.openai.com/v1/models\" rel=\"nofollow noreferrer\">https://api.openai.com/v1/models</a> .</p>\n<p>The endpoint lists the currently available engines, and provides basic information about each one such as the owner and availability.</p>\n<p>As a logged-in user, I get a JSON response of 63 models. These are the most recent ones (currently) , formatted, shown with release date.</p>\n<pre><code>59: &quot;11/28/2022, 2:40:35 AM : text-davinci-003&quot;\n60: &quot;12/16/2022, 8:01:39 PM : text-embedding-ada-002&quot;\n61: &quot;2/27/2023, 10:13:04 PM : whisper-1&quot;\n62: &quot;2/28/2023,  7:56:42 PM : gpt-3.5-turbo&quot;\n63: &quot;3/1/2023,   6:52:43 AM : gpt-3.5-turbo-0301&quot;\n</code></pre>\n<p>I notice that there are 2 very similar models , &quot;gpt-3.5-turbo&quot; and &quot;gpt-3.5-turbo-0301&quot;, with <code>gpt-3.5-turbo-0301</code> released only 11 hours after <code>gpt-3.5-turbo</code>.</p>\n<p>What is the difference between these two model versions? It does not seem to be a glitch or a misnaming error. Why did OpenAI bother to include both of them, and why didn't take the inferior version?</p>\n<p>(I haven't experimented with these two models in any way yet. I might do this very soon. However I though I might as well ask here. Informing others in this forum might have some benefit.)</p>\n"}, "title": "OpenAI: What is the difference between model &quot;gpt-3.5-turbo&quot; and &quot;gpt-3.5-turbo-0301&quot;?", "content": "I have performed an API call to OpenAI's endpoint https://api.openai.com/v1/models .\nThe endpoint lists the currently available engines, and provides basic information about each one such as the owner and availability.\nAs a logged-in user, I get a JSON response of 63 models. These are the most recent ones (currently) , formatted, shown with release date.\n59: \"11/28/2022, 2:40:35 AM : text-davinci-003\"\n60: \"12/16/2022, 8:01:39 PM : text-embedding-ada-002\"\n61: \"2/27/2023, 10:13:04 PM : whisper-1\"\n62: \"2/28/2023,  7:56:42 PM : gpt-3.5-turbo\"\n63: \"3/1/2023,   6:52:43 AM : gpt-3.5-turbo-0301\"\n\nI notice that there are 2 very similar models , \"gpt-3.5-turbo\" and \"gpt-3.5-turbo-0301\", with gpt-3.5-turbo-0301 released only 11 hours after gpt-3.5-turbo.\nWhat is the difference between these two model versions? It does not seem to be a glitch or a misnaming error. Why did OpenAI bother to include both of them, and why didn't take the inferior version?\n(I haven't experimented with these two models in any way yet. I might do this very soon. However I though I might as well ask here. Informing others in this forum might have some benefit.)\n", "question_id": 40111, "answers": []}, "41066": {"link": "https://ai.stackexchange.com/questions/41066/ram-capacity-of-mac-studio-with-m2-ultra-for-inference-of-65b-llm", "metadata": {"tags": ["large-language-models"], "owner": {"reputation": 121, "user_id": 73594, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/2997871e12024728f5c1a92a77560dbf?s=256&d=identicon&r=PG", "display_name": "Balaji Kartheeswaran", "link": "https://ai.stackexchange.com/users/73594/balaji-kartheeswaran"}, "is_answered": false, "view_count": 695, "answer_count": 0, "score": 2, "last_activity_date": 1688158812, "creation_date": 1688158812, "question_id": 41066, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41066/ram-capacity-of-mac-studio-with-m2-ultra-for-inference-of-65b-llm", "title": "RAM Capacity of Mac Studio with M2 Ultra for inference of 65B LLM", "body": "<p>How much RAM would be needed on Mac Studio M2 Ultra for inferring from a 65B LLM model. There are three options: 64GB, 128GB and 192GB. If using Apple M2 Ultra with 24\u2011core CPU, 76\u2011core GPU, 32\u2011core Neural Engine with 192GB Unified Memory, how would be the performance of the model?</p>\n"}, "title": "RAM Capacity of Mac Studio with M2 Ultra for inference of 65B LLM", "content": "How much RAM would be needed on Mac Studio M2 Ultra for inferring from a 65B LLM model. There are three options: 64GB, 128GB and 192GB. If using Apple M2 Ultra with 24\u2011core CPU, 76\u2011core GPU, 32\u2011core Neural Engine with 192GB Unified Memory, how would be the performance of the model?\n", "question_id": 41066, "answers": []}, "41728": {"link": "https://ai.stackexchange.com/questions/41728/what-will-happen-if-to-train-an-llm-on-mathematical-exersises", "metadata": {"tags": ["math", "large-language-models"], "owner": {"reputation": 301, "user_id": 65757, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fe87113ced5f370d038b90a43fd1fe1f?s=256&d=identicon&r=PG", "display_name": "Anixx", "link": "https://ai.stackexchange.com/users/65757/anixx"}, "is_answered": true, "view_count": 88, "answer_count": 3, "score": 1, "last_activity_date": 1691785918, "creation_date": 1691685809, "question_id": 41728, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41728/what-will-happen-if-to-train-an-llm-on-mathematical-exersises", "title": "What will happen if to train an LLM on mathematical exersises?", "body": "<p>What will happen if to train an LLM on taking integrals and solving equations? The process of mathematical education can be absolutely automated by a computer algebra system because the verification is easy.</p>\n<p>Is it possible that LLM will gan the ability to take integrals and simplify expressions better than the computer algebra system itself?</p>\n"}, "title": "What will happen if to train an LLM on mathematical exersises?", "content": "What will happen if to train an LLM on taking integrals and solving equations? The process of mathematical education can be absolutely automated by a computer algebra system because the verification is easy.\nIs it possible that LLM will gan the ability to take integrals and simplify expressions better than the computer algebra system itself?\n", "question_id": 41728, "answers": []}, "40839": {"link": "https://ai.stackexchange.com/questions/40839/for-an-llm-model-how-can-i-estimate-its-memory-requirements-based-on-storage-us", "metadata": {"tags": ["large-language-models", "gpu", "hardware", "hardware-evaluation"], "owner": {"reputation": 131, "user_id": 63672, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/839478ae95586357c0ef4d395eb29d8f?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "ahron", "link": "https://ai.stackexchange.com/users/63672/ahron"}, "is_answered": true, "view_count": 991, "answer_count": 1, "score": 1, "last_activity_date": 1688255522, "creation_date": 1686744097, "question_id": 40839, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40839/for-an-llm-model-how-can-i-estimate-its-memory-requirements-based-on-storage-us", "title": "For an LLM model, how can I estimate its memory requirements based on storage usage?", "body": "<p>It is easy to see the amount of disk space consumed by an LLM model (downloaded from huggingface, for instance). Just go in the relevant directory and check the file sizes.</p>\n<p>How can I estimate the amount of GPU RAM required to run the model?</p>\n<p>For example, if the Falcon 7B model takes around 14GB of storage, how much GPU RAM should suffice for it?</p>\n"}, "title": "For an LLM model, how can I estimate its memory requirements based on storage usage?", "content": "It is easy to see the amount of disk space consumed by an LLM model (downloaded from huggingface, for instance). Just go in the relevant directory and check the file sizes.\nHow can I estimate the amount of GPU RAM required to run the model?\nFor example, if the Falcon 7B model takes around 14GB of storage, how much GPU RAM should suffice for it?\n", "question_id": 40839, "answers": []}, "39579": {"link": "https://ai.stackexchange.com/questions/39579/sparsegpt-code-reproduction", "metadata": {"tags": ["natural-language-processing", "large-language-models"], "owner": {"reputation": 33, "user_id": 68072, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AEdFTp4DfxPrILFwRdreOXKPThTGXUC2W9JVnlOG027e=k-s256", "display_name": "user68072", "link": "https://ai.stackexchange.com/users/68072/user68072"}, "is_answered": true, "view_count": 87, "accepted_answer_id": 39767, "answer_count": 1, "score": 1, "last_activity_date": 1679629948, "creation_date": 1678803213, "question_id": 39579, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39579/sparsegpt-code-reproduction", "title": "SparseGPT code reproduction", "body": "<p>SparseGPT: <a href=\"https://arxiv.org/pdf/2301.00774.pdf\" rel=\"nofollow noreferrer\">https://arxiv.org/pdf/2301.00774.pdf</a></p>\n<p>Pruning on the super large language model based on the Transformers structure has achieved a high compression rate with a small loss of accuracy. Is there any related code reproduction or research?</p>\n<p>Someone mentioned on <a href=\"https://github.com/karpathy/nanoGPT\" rel=\"nofollow noreferrer\">https://github.com/karpathy/nanoGPT</a>, but no response.</p>\n"}, "title": "SparseGPT code reproduction", "content": "SparseGPT: https://arxiv.org/pdf/2301.00774.pdf\nPruning on the super large language model based on the Transformers structure has achieved a high compression rate with a small loss of accuracy. Is there any related code reproduction or research?\nSomeone mentioned on https://github.com/karpathy/nanoGPT, but no response.\n", "question_id": 39579, "answers": []}, "41249": {"link": "https://ai.stackexchange.com/questions/41249/what-is-considered-the-pre-fill-and-what-is-considered-the-decoding-phase-in-th", "metadata": {"tags": ["machine-learning", "deep-learning", "natural-language-processing", "transformer", "large-language-models"], "owner": {"reputation": 11, "user_id": 73994, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AFdZucpsmLwU4IWcVcC07X72WJ1RqyoubofvikkSMmgf=k-s256", "display_name": "jgeddes", "link": "https://ai.stackexchange.com/users/73994/jgeddes"}, "is_answered": false, "view_count": 185, "answer_count": 0, "score": 1, "last_activity_date": 1689675822, "creation_date": 1689161718, "last_edit_date": 1689675822, "question_id": 41249, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41249/what-is-considered-the-pre-fill-and-what-is-considered-the-decoding-phase-in-th", "title": "What is considered the pre-fill, and what is considered the decoding phase in this process?", "body": "<p>I've seen conflicting information about this online so I'm looking for clarification. I'm dealing with the causal LLaMAF model specifically.</p>\n<p>I used to think that a sequence of tokens is generated in, and a sequence of probabilities for the next token in the sequence is generated as output. This generated output token is then appended to the sequence of tokens, and fed in the model again.</p>\n<p>However, I know understand that there's a key value cache that's generated for each sequence of tokens fed in. This key value cache stores a precomputed matrix that can be used in future computations to prevent having to recompute previously seen tokens.</p>\n<p>So, the new workflow is feed in token list -&gt; generate next token and key value cache -&gt; feed in next token -&gt; generate next token and key value cache for current token</p>\n<p>What is considered the pre-fill, and what is considered the decoding phase in this process? Does the prefill phase involve feeding through the token list one by one passes to generate the kv cache? Why does the prefill phase take significantly longer?</p>\n"}, "title": "What is considered the pre-fill, and what is considered the decoding phase in this process?", "content": "I've seen conflicting information about this online so I'm looking for clarification. I'm dealing with the causal LLaMAF model specifically.\nI used to think that a sequence of tokens is generated in, and a sequence of probabilities for the next token in the sequence is generated as output. This generated output token is then appended to the sequence of tokens, and fed in the model again.\nHowever, I know understand that there's a key value cache that's generated for each sequence of tokens fed in. This key value cache stores a precomputed matrix that can be used in future computations to prevent having to recompute previously seen tokens.\nSo, the new workflow is feed in token list -> generate next token and key value cache -> feed in next token -> generate next token and key value cache for current token\nWhat is considered the pre-fill, and what is considered the decoding phase in this process? Does the prefill phase involve feeding through the token list one by one passes to generate the kv cache? Why does the prefill phase take significantly longer?\n", "question_id": 41249, "answers": []}, "40519": {"link": "https://ai.stackexchange.com/questions/40519/surveys-important-papers-in-explainability-for-llm", "metadata": {"tags": ["explainable-ai", "large-language-models"], "owner": {"reputation": 111, "user_id": 72064, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9b5a6fb80ac613af1d2f3a78c985cee1?s=256&d=identicon&r=PG", "display_name": "lithuak", "link": "https://ai.stackexchange.com/users/72064/lithuak"}, "is_answered": false, "view_count": 377, "answer_count": 1, "score": 1, "last_activity_date": 1690766264, "creation_date": 1684599561, "question_id": 40519, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40519/surveys-important-papers-in-explainability-for-llm", "title": "Surveys/Important papers in Explainability for LLM?", "body": "<p>I'm interested in the topic of Explainability for LLM: the attempts to find some higher understandable structures inside the LLMs or, to put it simply (though may be not completely correctly), the attempts to understand how LLMs &quot;think&quot;.</p>\n<p>Would anybody please recommend where to start?\nI'm both interested in foundational papers at this topic and in important latest research.</p>\n<p>The great survey or a curated list of papers would be wonderful, although I'm not sure they exist give the speed the field is moving with.</p>\n<p>Thank you so much.</p>\n"}, "title": "Surveys/Important papers in Explainability for LLM?", "content": "I'm interested in the topic of Explainability for LLM: the attempts to find some higher understandable structures inside the LLMs or, to put it simply (though may be not completely correctly), the attempts to understand how LLMs \"think\".\nWould anybody please recommend where to start?\nI'm both interested in foundational papers at this topic and in important latest research.\nThe great survey or a curated list of papers would be wonderful, although I'm not sure they exist give the speed the field is moving with.\nThank you so much.\n", "question_id": 40519, "answers": []}, "40592": {"link": "https://ai.stackexchange.com/questions/40592/how-is-a-parameter-explosion-prevented-when-connecting-a-mutlihead-attention-la", "metadata": {"tags": ["attention", "large-language-models"], "owner": {"reputation": 103, "user_id": 72310, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c638b05bf29d8b4305f592fd7c570e10?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "user2741831", "link": "https://ai.stackexchange.com/users/72310/user2741831"}, "is_answered": true, "view_count": 168, "accepted_answer_id": 40602, "answer_count": 2, "score": 0, "last_activity_date": 1685105335, "creation_date": 1685082144, "last_edit_date": 1685089874, "question_id": 40592, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40592/how-is-a-parameter-explosion-prevented-when-connecting-a-mutlihead-attention-la", "title": "How is a parameter explosion prevented, when connecting a mutlihead attention layer with the dense layers in LLMs (speciafially, LLama)?", "body": "<p>I have had a look at LLamas model card, specifically the 7B parameter version:\n<a href=\"https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md\" rel=\"nofollow noreferrer\">https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md</a></p>\n<p>which I assume is an encoder only transformer similar to this:</p>\n<p><img src=\"https://i.stack.imgur.com/Kb8Gq.png\" alt=\"\" /></p>\n<p>But then I did some math.If the dimension of every Dense layer, including the one connecting to the Attention layer is 4096, the context length is 2048, the number of attention heads is 32 and the embedding size is 786, then the output size of the attention layer is 32 * 786 * 2048 and as such the number of weights to connect it to the dense layer is 32 * 766 * 2048 * 4096, which is 205B parameters, which is obviously far more than 7B. So how is this accomplished? How big is the ouput of the attention layer and how is it connected to the following Dense layers?</p>\n"}, "title": "How is a parameter explosion prevented, when connecting a mutlihead attention layer with the dense layers in LLMs (speciafially, LLama)?", "content": "I have had a look at LLamas model card, specifically the 7B parameter version:\nhttps://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md\nwhich I assume is an encoder only transformer similar to this:\n\nBut then I did some math.If the dimension of every Dense layer, including the one connecting to the Attention layer is 4096, the context length is 2048, the number of attention heads is 32 and the embedding size is 786, then the output size of the attention layer is 32 * 786 * 2048 and as such the number of weights to connect it to the dense layer is 32 * 766 * 2048 * 4096, which is 205B parameters, which is obviously far more than 7B. So how is this accomplished? How big is the ouput of the attention layer and how is it connected to the following Dense layers?\n", "question_id": 40592, "answers": []}, "41154": {"link": "https://ai.stackexchange.com/questions/41154/using-llm-to-query-specific-databases-where-can-i-find-implementation-examples", "metadata": {"tags": ["large-language-models"], "owner": {"reputation": 103, "user_id": 73765, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/1691d582633281b0c236ec3eca94a034?s=256&d=identicon&r=PG", "display_name": "adjfac", "link": "https://ai.stackexchange.com/users/73765/adjfac"}, "is_answered": true, "view_count": 314, "closed_date": 1689639238, "accepted_answer_id": 41158, "answer_count": 1, "score": 0, "last_activity_date": 1688578518, "creation_date": 1688567837, "question_id": 41154, "link": "https://ai.stackexchange.com/questions/41154/using-llm-to-query-specific-databases-where-can-i-find-implementation-examples", "closed_reason": "Needs more focus", "title": "Using LLM to query specific databases - where can I find implementation examples", "body": "<p>I've been doing some research on how to leverage a LLM model to &quot;translate&quot; English into a sql query that's capable to returning the desired results (like a little Q/A bot). For instance, one would ask &quot;show me the top growth account in the last three months in the state of FL?&quot; and it will return the results. After googling around, a lot of information I've found is at a high level where you follow the pipeline of</p>\n<ol>\n<li>Build data model --&gt; 2. pick a LLM --&gt; 3. prompt design --&gt; 4. parse the question to query --&gt; 5. execute query --&gt; 6. parse result</li>\n</ol>\n<p>I have a a few questions:</p>\n<ol>\n<li>is LLM model fine-tuning using your own data required for this type of tasks?</li>\n<li>where should mapping between database fields and the query components happen? In my example above, let's say i have two date fields in my database - sales_date, and invoice_date, the LLM would know to use invoice_date for the &quot;last 3 months&quot; request</li>\n<li>Any online sources that give an implementation example would be great</li>\n</ol>\n"}, "title": "Using LLM to query specific databases - where can I find implementation examples", "content": "I've been doing some research on how to leverage a LLM model to \"translate\" English into a sql query that's capable to returning the desired results (like a little Q/A bot). For instance, one would ask \"show me the top growth account in the last three months in the state of FL?\" and it will return the results. After googling around, a lot of information I've found is at a high level where you follow the pipeline of\n\nBuild data model --> 2. pick a LLM --> 3. prompt design --> 4. parse the question to query --> 5. execute query --> 6. parse result\n\nI have a a few questions:\n\nis LLM model fine-tuning using your own data required for this type of tasks?\nwhere should mapping between database fields and the query components happen? In my example above, let's say i have two date fields in my database - sales_date, and invoice_date, the LLM would know to use invoice_date for the \"last 3 months\" request\nAny online sources that give an implementation example would be great\n\n", "question_id": 41154, "answers": []}, "40370": {"link": "https://ai.stackexchange.com/questions/40370/for-a-transformer-decoder-how-exactly-are-k-q-and-v-for-each-decoding-step", "metadata": {"tags": ["transformer", "autoencoders", "large-language-models", "encoder-decoder"], "owner": {"reputation": 183, "user_id": 62288, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/feded8d3ed8dca160f793ff782cb5226?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "wrek", "link": "https://ai.stackexchange.com/users/62288/wrek"}, "is_answered": true, "view_count": 63, "accepted_answer_id": 40377, "answer_count": 1, "score": 0, "last_activity_date": 1683673855, "creation_date": 1683590746, "last_edit_date": 1683673855, "question_id": 40370, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40370/for-a-transformer-decoder-how-exactly-are-k-q-and-v-for-each-decoding-step", "title": "For a transformer decoder, how exactly are K, Q, and V for each decoding step?", "body": "<p>For a transformer decoder, how exactly are K, Q, and V for each decoding step?</p>\n<p>Assume my input prompt is &quot;today is a&quot; (good day).</p>\n<p>At t= 0 (generation step 0):\nK, Q, and V are the projections of the sequence (&quot;today is a&quot;)\nThen say the next token generated is &quot;good&quot;.</p>\n<p>At <code>t=1</code> (generation step 1):\nWhich one is true:</p>\n<ol>\n<li>K, Q, and V are the projections of the sequence (&quot;today is a good&quot;)</li>\n<li>K, Q, are the projections of the sequence (&quot;today is a&quot;), and V is the projection of the sequence (&quot;good&quot;)?</li>\n</ol>\n"}, "title": "For a transformer decoder, how exactly are K, Q, and V for each decoding step?", "content": "For a transformer decoder, how exactly are K, Q, and V for each decoding step?\nAssume my input prompt is \"today is a\" (good day).\nAt t= 0 (generation step 0):\nK, Q, and V are the projections of the sequence (\"today is a\")\nThen say the next token generated is \"good\".\nAt t=1 (generation step 1):\nWhich one is true:\n\nK, Q, and V are the projections of the sequence (\"today is a good\")\nK, Q, are the projections of the sequence (\"today is a\"), and V is the projection of the sequence (\"good\")?\n\n", "question_id": 40370, "answers": []}, "39652": {"link": "https://ai.stackexchange.com/questions/39652/large-language-models-vs-tabular-data", "metadata": {"tags": ["natural-language-processing", "classification", "gpt", "large-language-models"], "owner": {"reputation": 109, "user_id": 69430, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/65c431665dd627d97f61397062e6ad73?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Glue", "link": "https://ai.stackexchange.com/users/69430/glue"}, "is_answered": true, "view_count": 170, "answer_count": 1, "score": 0, "last_activity_date": 1681734291, "creation_date": 1679146149, "last_edit_date": 1681734291, "question_id": 39652, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39652/large-language-models-vs-tabular-data", "title": "Large Language Models vs Tabular Data", "body": "<p><strong>Problem:</strong><br />\nLet's say we want to predict insurance frauds. Whenever we obtain an insurance claim, we are provided with a free-form description detailing the loss and a substantial amount of data on the claimant, presented in a tabular format.</p>\n<p><strong>Questions:</strong><br />\nHow can we utilize both the written loss description and the extensive structured data we have gathered? Should we build two separate models, one for natural language processing and the other for tabular data? Is it possible for Large Language Models (LLMs) to extract insights from tabular data? If so, how? What limitations or pitfalls should we bear in mind?</p>\n"}, "title": "Large Language Models vs Tabular Data", "content": "Problem:\nLet's say we want to predict insurance frauds. Whenever we obtain an insurance claim, we are provided with a free-form description detailing the loss and a substantial amount of data on the claimant, presented in a tabular format.\nQuestions:\nHow can we utilize both the written loss description and the extensive structured data we have gathered? Should we build two separate models, one for natural language processing and the other for tabular data? Is it possible for Large Language Models (LLMs) to extract insights from tabular data? If so, how? What limitations or pitfalls should we bear in mind?\n", "question_id": 39652, "answers": []}, "41149": {"link": "https://ai.stackexchange.com/questions/41149/ignoring-aspects-of-text-embeddings-e-g-making-the-embedding-topic-agnostic", "metadata": {"tags": ["embeddings", "large-language-models"], "owner": {"reputation": 1, "user_id": 73736, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/22efec6999336f34074e7a3b891b4cc6?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "curl-up", "link": "https://ai.stackexchange.com/users/73736/curl-up"}, "is_answered": true, "view_count": 25, "answer_count": 1, "score": 0, "last_activity_date": 1688518420, "creation_date": 1688513876, "last_edit_date": 1688513908, "question_id": 41149, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41149/ignoring-aspects-of-text-embeddings-e-g-making-the-embedding-topic-agnostic", "title": "Ignoring aspects of text embeddings, e.g. making the embedding topic-agnostic", "body": "<p>Imagine a large set of text embeddings (e.g. by OpenAI model), created on user inputs in a natural language interface (e.g. a semantic search app), which we want to cluster on some &quot;non-topic aspect&quot; of the text (e.g. clarity of query). As expected, clustering by default results in different topics being grouped together.</p>\n<p>What are the best practices to ignore the topics when clustering if we define topics as whatever some dimensionality reduction process favours (e.g. the clusters we get initially)?</p>\n<p>Alternatively, is there some documented/researched process of training additional models on top of these embeddings (or adding layers to existing models) to boost these different aspects of the embedded text, or at least to achieve this &quot;blindness&quot; to certain aspects of the embedding (with some additional train data)?</p>\n<p>Of course, I am assuming that these other aspects are also contained in the embedding, they are just less expressed.</p>\n"}, "title": "Ignoring aspects of text embeddings, e.g. making the embedding topic-agnostic", "content": "Imagine a large set of text embeddings (e.g. by OpenAI model), created on user inputs in a natural language interface (e.g. a semantic search app), which we want to cluster on some \"non-topic aspect\" of the text (e.g. clarity of query). As expected, clustering by default results in different topics being grouped together.\nWhat are the best practices to ignore the topics when clustering if we define topics as whatever some dimensionality reduction process favours (e.g. the clusters we get initially)?\nAlternatively, is there some documented/researched process of training additional models on top of these embeddings (or adding layers to existing models) to boost these different aspects of the embedded text, or at least to achieve this \"blindness\" to certain aspects of the embedding (with some additional train data)?\nOf course, I am assuming that these other aspects are also contained in the embedding, they are just less expressed.\n", "question_id": 41149, "answers": []}, "39795": {"link": "https://ai.stackexchange.com/questions/39795/in-which-process-does-filter-and-watermark-take-place", "metadata": {"tags": ["large-language-models"], "owner": {"reputation": 141, "user_id": 69166, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/93d53e3977612b13cd43f768c3bac40e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "zzzgoo", "link": "https://ai.stackexchange.com/users/69166/zzzgoo"}, "is_answered": true, "view_count": 26, "answer_count": 1, "score": 0, "last_activity_date": 1683962608, "creation_date": 1679805743, "question_id": 39795, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39795/in-which-process-does-filter-and-watermark-take-place", "title": "In which process does filter and watermark take place?", "body": "<p>In LLM, in order to avoid discrimination and abuse, the author will add filter and watermark functionalities. Could you tell me in which process these functionalities take place? Is it in weights(pertaining), output layer(transformer), or fine-tuning?</p>\n<p>Can I remove the filter by myself if I have an open-source AI like alpaca?</p>\n"}, "title": "In which process does filter and watermark take place?", "content": "In LLM, in order to avoid discrimination and abuse, the author will add filter and watermark functionalities. Could you tell me in which process these functionalities take place? Is it in weights(pertaining), output layer(transformer), or fine-tuning?\nCan I remove the filter by myself if I have an open-source AI like alpaca?\n", "question_id": 39795, "answers": []}, "41887": {"link": "https://ai.stackexchange.com/questions/41887/does-embeddings-and-vector-databases-solve-the-need-of-having-longer-context-win", "metadata": {"tags": ["transformer", "large-language-models"], "owner": {"reputation": 88, "user_id": 74581, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/Nt9VK.jpg?s=256&g=1", "display_name": "Cesar Ruiz", "link": "https://ai.stackexchange.com/users/74581/cesar-ruiz"}, "is_answered": false, "view_count": 13, "answer_count": 0, "score": 0, "last_activity_date": 1693016820, "creation_date": 1693016820, "question_id": 41887, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41887/does-embeddings-and-vector-databases-solve-the-need-of-having-longer-context-win", "title": "Does Embeddings and Vector Databases solve the need of having longer context windows?", "body": "<p>I am learning to use the OpenAI API to build LLM-based agents. I recently came across the concept of vector databases, which use embeddings to convert text into vectors and store them in a database for easy retrieval. This technique has been shown to be very useful for long-memory applications.</p>\n<p>My question is whether it is necessary to have longer context windows in models that use the embedding + database technique. I know that there has been some research on expanding the context window of transformers, such as the <a href=\"https://arxiv.org/abs/2304.11062\" rel=\"nofollow noreferrer\">Scaling Transformer to 1M tokens and beyond with RMT</a> paper and <a href=\"https://arxiv.org/abs/2307.02486\" rel=\"nofollow noreferrer\">Microsoft's LongNet</a> architecture. At what point does increasing the context window lead to better performance than using a vector database approach? Are there any examples or experiments that demonstrate this?</p>\n"}, "title": "Does Embeddings and Vector Databases solve the need of having longer context windows?", "content": "I am learning to use the OpenAI API to build LLM-based agents. I recently came across the concept of vector databases, which use embeddings to convert text into vectors and store them in a database for easy retrieval. This technique has been shown to be very useful for long-memory applications.\nMy question is whether it is necessary to have longer context windows in models that use the embedding + database technique. I know that there has been some research on expanding the context window of transformers, such as the Scaling Transformer to 1M tokens and beyond with RMT paper and Microsoft's LongNet architecture. At what point does increasing the context window lead to better performance than using a vector database approach? Are there any examples or experiments that demonstrate this?\n", "question_id": 41887, "answers": []}, "41842": {"link": "https://ai.stackexchange.com/questions/41842/why-does-a-small-model-become-huge-when-sharded-and-loaded-into-gpu-on-text-gene", "metadata": {"tags": ["large-language-models", "text-generation", "inference", "production-systems"], "owner": {"reputation": 101, "user_id": 75440, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/sKX9g.png?s=256&g=1", "display_name": "Jack Avante", "link": "https://ai.stackexchange.com/users/75440/jack-avante"}, "is_answered": false, "view_count": 14, "answer_count": 0, "score": 0, "last_activity_date": 1692717372, "creation_date": 1692717372, "question_id": 41842, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41842/why-does-a-small-model-become-huge-when-sharded-and-loaded-into-gpu-on-text-gene", "title": "Why does a small model become huge when sharded and loaded into GPU on text-generation-inference?", "body": "<p>I'm currently trying out the new <a href=\"https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct\" rel=\"nofollow noreferrer\">Llama 32k 7b model</a> and I'm using <a href=\"https://github.com/huggingface/text-generation-inference\" rel=\"nofollow noreferrer\">text-generation-inference</a> to run it.</p>\n<p>For added context, I have a 4xA100 40GB SXM machine that I'm running it on using docker.</p>\n<p>When I tried Falcon40b in the past, sharded across all 4 GPUs, it ate over 38GB VRAM on all 4 GPUs. What I am puzzled by is that this model, being only about 13GiB in size, also eats up over 38GB VRAM sharded across all 4 GPUS (38GB per GPU, not total, just like the Falcon40b model).</p>\n<p>Why is this when the model is significantly smaller that it still consumes so much VRAM?</p>\n<p>For info this is how I run it:</p>\n<pre><code>sudo docker run\n    --name llm-llama32k7b-instruct\n    -d\n    --gpus all\n    --shm-size 1g\n    -p 8080:80\n    -v $PWD/data:/data\n    ghcr.io/huggingface/text-generation-inference:1.0.1\n    --model-id togethercomputer/Llama-2-7B-32K-Instruct\n    --sharded true --num-shard 4\n</code></pre>\n"}, "title": "Why does a small model become huge when sharded and loaded into GPU on text-generation-inference?", "content": "I'm currently trying out the new Llama 32k 7b model and I'm using text-generation-inference to run it.\nFor added context, I have a 4xA100 40GB SXM machine that I'm running it on using docker.\nWhen I tried Falcon40b in the past, sharded across all 4 GPUs, it ate over 38GB VRAM on all 4 GPUs. What I am puzzled by is that this model, being only about 13GiB in size, also eats up over 38GB VRAM sharded across all 4 GPUS (38GB per GPU, not total, just like the Falcon40b model).\nWhy is this when the model is significantly smaller that it still consumes so much VRAM?\nFor info this is how I run it:\nsudo docker run\n    --name llm-llama32k7b-instruct\n    -d\n    --gpus all\n    --shm-size 1g\n    -p 8080:80\n    -v $PWD/data:/data\n    ghcr.io/huggingface/text-generation-inference:1.0.1\n    --model-id togethercomputer/Llama-2-7B-32K-Instruct\n    --sharded true --num-shard 4\n\n", "question_id": 41842, "answers": []}, "41793": {"link": "https://ai.stackexchange.com/questions/41793/create-samples-out-of-documents-for-causal-language-modelling", "metadata": {"tags": ["neural-networks", "machine-learning", "natural-language-processing", "large-language-models", "language-model"], "owner": {"reputation": 1, "user_id": 74107, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/25bb659fff37fb483b9fb142e63792b6?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Dimits", "link": "https://ai.stackexchange.com/users/74107/dimits"}, "is_answered": false, "view_count": 12, "answer_count": 0, "score": 0, "last_activity_date": 1692203697, "creation_date": 1692203697, "question_id": 41793, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41793/create-samples-out-of-documents-for-causal-language-modelling", "title": "Create samples out of documents for Causal Language Modelling", "body": "<p>I want to create an input source for Causal Language model using Llama 2 model in hugging face. I have a set of documents which are scraped from a specific website and want to fine-tune on them. Each document its basically a different corner of this domain. Some documents can be very short while others (such as term and condition) large enough. The input size for Llama 2 model is 4096 tokens and only 2% of the documents are above this threshold.</p>\n<p>I've seen that the most common strategy for language modelling using this tool is to concatenate context and split by eos token such as :</p>\n<p><code>***samples 1***: sentence 1 &lt;eos&gt; sentence 2 &lt;eos&gt; .... sentence n &lt;eos&gt; &lt;pad as necessary&gt; ***samples 2***: sentence n+1 &lt;eos&gt; sentence n+2 &lt;eos&gt; .... sentence n+j &lt;eos&gt; &lt;pad as necessary&gt;</code></p>\n<p>Where</p>\n<pre><code>Document 1: sentence 1. sentence 2...sentence i\nDocument 2: sentence i+1. sentence i + 2...sentence i+k\nDocument 3: sentence i+k+1. sentence i +k+ 2...sentence n\n</code></pre>\n<p>and so on</p>\n<p>A sample can have sentences from different documents/topics and each sample size is up to the initial 4096 tokens</p>\n<p>So, I've got the following questions:</p>\n<ul>\n<li>Should I split documents into different samples (and maybe decrease the total input token size down for 4096) or is it just alright to include multiple sentences concatenated with multiple EOS tokens in a single training sequence?</li>\n<li>The  token, should i use to split context of different documents or should I use as in the example above marking the end of each distinct sentence?</li>\n<li>Can someone provide a heuristic on how to select the chucking parameter used by the model to split the input texts above?</li>\n</ul>\n"}, "title": "Create samples out of documents for Causal Language Modelling", "content": "I want to create an input source for Causal Language model using Llama 2 model in hugging face. I have a set of documents which are scraped from a specific website and want to fine-tune on them. Each document its basically a different corner of this domain. Some documents can be very short while others (such as term and condition) large enough. The input size for Llama 2 model is 4096 tokens and only 2% of the documents are above this threshold.\nI've seen that the most common strategy for language modelling using this tool is to concatenate context and split by eos token such as :\n***samples 1***: sentence 1 <eos> sentence 2 <eos> .... sentence n <eos> <pad as necessary> ***samples 2***: sentence n+1 <eos> sentence n+2 <eos> .... sentence n+j <eos> <pad as necessary>\nWhere\nDocument 1: sentence 1. sentence 2...sentence i\nDocument 2: sentence i+1. sentence i + 2...sentence i+k\nDocument 3: sentence i+k+1. sentence i +k+ 2...sentence n\n\nand so on\nA sample can have sentences from different documents/topics and each sample size is up to the initial 4096 tokens\nSo, I've got the following questions:\n\nShould I split documents into different samples (and maybe decrease the total input token size down for 4096) or is it just alright to include multiple sentences concatenated with multiple EOS tokens in a single training sequence?\nThe  token, should i use to split context of different documents or should I use as in the example above marking the end of each distinct sentence?\nCan someone provide a heuristic on how to select the chucking parameter used by the model to split the input texts above?\n\n", "question_id": 41793, "answers": []}, "41792": {"link": "https://ai.stackexchange.com/questions/41792/what-causes-my-loss-curve-to-consistently-oscillate-when-training-an-llm", "metadata": {"tags": ["deep-learning", "training", "data-preprocessing", "large-language-models", "training-datasets"], "owner": {"reputation": 1, "user_id": 75277, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/196a8f5977c5126b863435379f5216d9?s=256&d=identicon&r=PG", "display_name": "bmatzelle", "link": "https://ai.stackexchange.com/users/75277/bmatzelle"}, "is_answered": false, "view_count": 16, "answer_count": 0, "score": 0, "last_activity_date": 1692200888, "creation_date": 1692200888, "question_id": 41792, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41792/what-causes-my-loss-curve-to-consistently-oscillate-when-training-an-llm", "title": "What causes my loss curve to consistently oscillate when training an LLM?", "body": "<p><a href=\"https://i.stack.imgur.com/RckTe.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/RckTe.png\" alt=\"loss curve\" /></a></p>\n<p>Why is my loss curve consistently oscillating? Every 50 steps it jumps back up. I'm assumming there's a bug in my data, since I'm using <a href=\"https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing\" rel=\"nofollow noreferrer\">this colab notebook</a> that shows a proper train/loss at the bottom. The only thing I changed was the dataset used. I'm using the <a href=\"https://huggingface.co/ybelkada/falcon-7b-sharded-bf16\" rel=\"nofollow noreferrer\">falcon-7b-sharded-bf16</a> huggingface model.</p>\n"}, "title": "What causes my loss curve to consistently oscillate when training an LLM?", "content": "\nWhy is my loss curve consistently oscillating? Every 50 steps it jumps back up. I'm assumming there's a bug in my data, since I'm using this colab notebook that shows a proper train/loss at the bottom. The only thing I changed was the dataset used. I'm using the falcon-7b-sharded-bf16 huggingface model.\n", "question_id": 41792, "answers": []}, "41778": {"link": "https://ai.stackexchange.com/questions/41778/what-if-in-dpr-dense-passage-retrieval-the-answer-belongs-to-more-than-one-pa", "metadata": {"tags": ["machine-learning", "natural-language-processing", "transformer", "large-language-models"], "owner": {"reputation": 1, "user_id": 75243, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AAcHTtfK9hjBPgecHYiHDcEmXr2Y5ei-TJwxINEfFZbRGAv3=k-s256", "display_name": "naren", "link": "https://ai.stackexchange.com/users/75243/naren"}, "is_answered": false, "view_count": 11, "answer_count": 0, "score": 0, "last_activity_date": 1692130980, "creation_date": 1692130980, "question_id": 41778, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41778/what-if-in-dpr-dense-passage-retrieval-the-answer-belongs-to-more-than-one-pa", "title": "What if in DPR (dense passage retrieval), the answer belongs to more than one passage?", "body": "<p>In the <a href=\"https://arxiv.org/pdf/2004.04906.pdf\" rel=\"nofollow noreferrer\">DPR paper</a>\nthe dataset is expected to be in this format D = {&lt;q<sub>i</sub>, p<sub>i</sub><sup>+</sup>, p<sub>i,1</sub><sup>-</sup>, ... &gt;}</p>\n<p>With only one positive passage, but it is possible that the question requires an answer that spans knowledge from more than one positive passage.</p>\n<p>Another issue is that, if we consider other positive answers from the batch as negative, (the Gold strategy in the paper) it is possible that those other positive passages are relevant for the question into consideration.</p>\n<p>How should the DPR model be trained in such a case?</p>\n"}, "title": "What if in DPR (dense passage retrieval), the answer belongs to more than one passage?", "content": "In the DPR paper\nthe dataset is expected to be in this format D = {<qi, pi+, pi,1-, ... >}\nWith only one positive passage, but it is possible that the question requires an answer that spans knowledge from more than one positive passage.\nAnother issue is that, if we consider other positive answers from the batch as negative, (the Gold strategy in the paper) it is possible that those other positive passages are relevant for the question into consideration.\nHow should the DPR model be trained in such a case?\n", "question_id": 41778, "answers": []}, "41770": {"link": "https://ai.stackexchange.com/questions/41770/can-i-finetune-a-model-on-azure-for-information-extraction-based-on-question", "metadata": {"tags": ["large-language-models", "fine-tuning"], "owner": {"reputation": 1, "user_id": 75222, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0b0b6a6ca36b702c0d4fd11101afdf84?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "trailer_swift", "link": "https://ai.stackexchange.com/users/75222/trailer-swift"}, "is_answered": false, "view_count": 10, "answer_count": 0, "score": 0, "last_activity_date": 1692068006, "creation_date": 1692068006, "question_id": 41770, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41770/can-i-finetune-a-model-on-azure-for-information-extraction-based-on-question", "title": "can I finetune a model on Azure for information extraction based on &quot;question&quot;, &quot;context&quot;, and &quot;answer&quot; training data?", "body": "<p>I am working on extracting certain fields from a large corpus.</p>\n<p>I was looking at finetuning an LLM on Azure for the task. I think finetuning is the right idea (as opposed to vector databases, or RAG), since the data was OCR'd from semi-structured forms, and has been tokenized into a less-than-ideal natural language structure.</p>\n<p>So I have a dataset in this form:</p>\n<pre><code>{&quot;prompt&quot;: &quot;find the date in this text&quot;, context: &quot;chickens can't fly, blue red green, asdkjfek, Nov 11 2011&quot;, &quot;completion&quot;: &quot;&quot;Nov 11 2022&quot;}\n</code></pre>\n<p>I want zero generation or creativity in this task -- merely extraction.</p>\n<p>For finetuning in Azure, the recommended structure is this:</p>\n<pre><code>{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}\n</code></pre>\n<p>I'm not sure finetuning will be effective if I include context with my prompt. I'd basically be doing &quot;in-context finetuning&quot;, so to speak. Not sure that's a thing. I've done a lot of [reading], yet still not sure if it makes sense.</p>\n<p>Any advice greatly appreciated.</p>\n"}, "title": "can I finetune a model on Azure for information extraction based on &quot;question&quot;, &quot;context&quot;, and &quot;answer&quot; training data?", "content": "I am working on extracting certain fields from a large corpus.\nI was looking at finetuning an LLM on Azure for the task. I think finetuning is the right idea (as opposed to vector databases, or RAG), since the data was OCR'd from semi-structured forms, and has been tokenized into a less-than-ideal natural language structure.\nSo I have a dataset in this form:\n{\"prompt\": \"find the date in this text\", context: \"chickens can't fly, blue red green, asdkjfek, Nov 11 2011\", \"completion\": \"\"Nov 11 2022\"}\n\nI want zero generation or creativity in this task -- merely extraction.\nFor finetuning in Azure, the recommended structure is this:\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n\nI'm not sure finetuning will be effective if I include context with my prompt. I'd basically be doing \"in-context finetuning\", so to speak. Not sure that's a thing. I've done a lot of [reading], yet still not sure if it makes sense.\nAny advice greatly appreciated.\n", "question_id": 41770, "answers": []}, "41643": {"link": "https://ai.stackexchange.com/questions/41643/optimal-quantity-of-training-data-for-fine-tuning-an-llm-is-bigger-always-bette", "metadata": {"tags": ["natural-language-processing", "gpt", "large-language-models", "language-model", "fine-tuning"], "owner": {"reputation": 534, "user_id": 23811, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-6jIAe62Q10s/AAAAAAAAAAI/AAAAAAAAABw/kRBVixTmLmk/photo.jpg?sz=256", "display_name": "Peyman", "link": "https://ai.stackexchange.com/users/23811/peyman"}, "is_answered": false, "view_count": 47, "answer_count": 0, "score": 0, "last_activity_date": 1691134980, "creation_date": 1691134980, "question_id": 41643, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41643/optimal-quantity-of-training-data-for-fine-tuning-an-llm-is-bigger-always-bette", "title": "Optimal Quantity of Training Data for Fine-Tuning an LLM: Is Bigger Always Better?", "body": "<p>I am currently working on fine-tuning an LLM for a specific task, and I am trying to determine the optimal size for my training dataset. Intuitively, one might think that the more data, the better. However, I am aware that in some contexts, this may lead to overfitting or other issues.</p>\n<p>What is the general consensus on the optimal quantity of training data required for fine-tuning a Large Language Model? Is bigger always better?</p>\n"}, "title": "Optimal Quantity of Training Data for Fine-Tuning an LLM: Is Bigger Always Better?", "content": "I am currently working on fine-tuning an LLM for a specific task, and I am trying to determine the optimal size for my training dataset. Intuitively, one might think that the more data, the better. However, I am aware that in some contexts, this may lead to overfitting or other issues.\nWhat is the general consensus on the optimal quantity of training data required for fine-tuning a Large Language Model? Is bigger always better?\n", "question_id": 41643, "answers": []}, "41588": {"link": "https://ai.stackexchange.com/questions/41588/generative-ai-use-case-for-search-domain", "metadata": {"tags": ["search", "generative-model", "large-language-models"], "migrated_to": {"other_site": {"styling": {"tag_background_color": "#E0EAF1", "tag_foreground_color": "#000", "link_color": "#0077CC"}, "related_sites": [{"relation": "meta", "api_site_parameter": "genai.meta", "site_url": "https://genai.meta.stackexchange.com", "name": "GenAI Meta Stack Exchange"}, {"relation": "chat", "site_url": "https://chat.stackexchange.com?tab=site&host=genai.stackexchange.com", "name": "Chat Stack Exchange"}], "open_beta_date": 1690297236, "closed_beta_date": 1689617537, "site_state": "open_beta", "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/genai/Img/apple-touch-icon@2.png", "favicon_url": "https://cdn.sstatic.net/Sites/genai/Img/favicon.ico", "icon_url": "https://cdn.sstatic.net/Sites/genai/Img/apple-touch-icon.png", "audience": "GenAI enthusiasts and practitioners and those interested in learning more about using GenAI tools", "site_url": "https://genai.stackexchange.com", "api_site_parameter": "genai", "logo_url": "https://cdn.sstatic.net/Sites/genai/Img/apple-touch-icon.png", "name": "GenAI", "site_type": "main_site"}, "on_date": 1691048580, "question_id": 252}, "owner": {"reputation": 1, "user_id": 74756, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6538e613d1c576214074905c690ca34e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "anshuk_pal", "link": "https://ai.stackexchange.com/users/74756/anshuk-pal"}, "is_answered": false, "view_count": 27, "closed_date": 1691048580, "answer_count": 0, "score": 0, "locked_date": 1691048580, "last_activity_date": 1690966150, "creation_date": 1690891163, "question_id": 41588, "link": "https://ai.stackexchange.com/questions/41588/generative-ai-use-case-for-search-domain", "closed_reason": "Not suitable for this site", "title": "Generative AI Use Case for Search Domain", "body": "<p>I am trying to think and research for use cases that can beneficial search experience using Generative AI. There are two things,</p>\n<ul>\n<li>The final search results that can be more relevant and personalised per user. This would take time and more effort - in terms the backend search service to change/upgrade etc.</li>\n<li>The more I am trying to focus how can generative ai be leveraged and provide to benefits on the client side while user is actually search. One thought I had:\n<em>used to generate new keywords and topics that can be used to improve the search engine's coverage of a particular topic. This can help to provide more relevant results to users.</em></li>\n</ul>\n<p>I wanted to understand what other use case have you witnessed/seem where generative ai can really helpful while user is actually searching (on the client side experience)</p>\n<p>Appreciate it.</p>\n"}, "title": "Generative AI Use Case for Search Domain", "content": "I am trying to think and research for use cases that can beneficial search experience using Generative AI. There are two things,\n\nThe final search results that can be more relevant and personalised per user. This would take time and more effort - in terms the backend search service to change/upgrade etc.\nThe more I am trying to focus how can generative ai be leveraged and provide to benefits on the client side while user is actually search. One thought I had:\nused to generate new keywords and topics that can be used to improve the search engine's coverage of a particular topic. This can help to provide more relevant results to users.\n\nI wanted to understand what other use case have you witnessed/seem where generative ai can really helpful while user is actually searching (on the client side experience)\nAppreciate it.\n", "question_id": 41588, "answers": []}, "41393": {"link": "https://ai.stackexchange.com/questions/41393/what-are-the-non-cost-related-reasons-rnnattention-underperform-transformers", "metadata": {"tags": ["natural-language-processing", "recurrent-neural-networks", "transformer", "long-short-term-memory", "large-language-models"], "owner": {"reputation": 146, "user_id": 68775, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6f9aa372691e4af70e3420cb944d6360?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "llllvvuu", "link": "https://ai.stackexchange.com/users/68775/llllvvuu"}, "is_answered": false, "view_count": 44, "answer_count": 0, "score": 0, "last_activity_date": 1689944286, "creation_date": 1689937360, "last_edit_date": 1689937806, "question_id": 41393, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41393/what-are-the-non-cost-related-reasons-rnnattention-underperform-transformers", "title": "What are the *non-cost-related* reasons RNN+Attention underperform Transformers?", "body": "<p>There are obvious trainability and performance challenges with RNNs, such as having to process in serial and BPTT. But let's say we magically had an &quot;optimal&quot; set of weights for the RNN + Attention, as well as for the Transformer. Assume as many things about the architecture as possible are held equal between the two models. Would the inference of the RNN + Attention still be worse than that of the Transformer?</p>\n<p><a href=\"https://ai.stackexchange.com/questions/23898/any-comparison-between-transformer-and-rnnattention-on-the-same-dataset\">This post</a> hints at yes; in the original &quot;Attention Is All You Need&quot; paper, the Transformer outperformed not only in cost but <em>also</em> in BLEU. It seems like they experimented to a degree where one could say it's not the optimization process / BPTT that failed in the RNN + Attention models, but truly when the model was saturated the performance still was worse.</p>\n<p>Why is this?</p>\n<p>Intuitively I feel like the set of programs that can be expressed as &quot;stateful&quot; programs is a strict superset of the set of programs that can be expressed as &quot;stateless&quot; programs, such that the only disadvantage would be that it's harder to find the optimal &quot;stateful&quot; program.</p>\n"}, "title": "What are the *non-cost-related* reasons RNN+Attention underperform Transformers?", "content": "There are obvious trainability and performance challenges with RNNs, such as having to process in serial and BPTT. But let's say we magically had an \"optimal\" set of weights for the RNN + Attention, as well as for the Transformer. Assume as many things about the architecture as possible are held equal between the two models. Would the inference of the RNN + Attention still be worse than that of the Transformer?\nThis post hints at yes; in the original \"Attention Is All You Need\" paper, the Transformer outperformed not only in cost but also in BLEU. It seems like they experimented to a degree where one could say it's not the optimization process / BPTT that failed in the RNN + Attention models, but truly when the model was saturated the performance still was worse.\nWhy is this?\nIntuitively I feel like the set of programs that can be expressed as \"stateful\" programs is a strict superset of the set of programs that can be expressed as \"stateless\" programs, such that the only disadvantage would be that it's harder to find the optimal \"stateful\" program.\n", "question_id": 41393, "answers": []}, "41355": {"link": "https://ai.stackexchange.com/questions/41355/does-anyone-know-about-a-reference-where-someone-has-aggregated-the-cost-optimiz", "metadata": {"tags": ["reference-request", "large-language-models"], "owner": {"reputation": 1, "user_id": 26468, "user_type": "registered", "profile_image": "https://graph.facebook.com/1168112533324031/picture?type=large", "display_name": "Sudhanshu Mishra", "link": "https://ai.stackexchange.com/users/26468/sudhanshu-mishra"}, "is_answered": false, "view_count": 12, "answer_count": 0, "score": 0, "last_activity_date": 1689752245, "creation_date": 1689752245, "question_id": 41355, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41355/does-anyone-know-about-a-reference-where-someone-has-aggregated-the-cost-optimiz", "title": "Does anyone know about a reference where someone has aggregated the cost optimization strategies for deploying LLMs?", "body": "<p>I am looking for a source where someone has mentioned the most commonly used strategies and optimisations to deploy LLMs on consumer hardware. I have read about layer offloading, quantisation methods using libraries like GPTQ and GGML, model pruning and distillation. I would love to know if people know about good blogs for this type of information. Lillian weng's blog seems to be one of the good one's that I know of.</p>\n"}, "title": "Does anyone know about a reference where someone has aggregated the cost optimization strategies for deploying LLMs?", "content": "I am looking for a source where someone has mentioned the most commonly used strategies and optimisations to deploy LLMs on consumer hardware. I have read about layer offloading, quantisation methods using libraries like GPTQ and GGML, model pruning and distillation. I would love to know if people know about good blogs for this type of information. Lillian weng's blog seems to be one of the good one's that I know of.\n", "question_id": 41355, "answers": []}, "41295": {"link": "https://ai.stackexchange.com/questions/41295/fine-tune-llama-on-main-and-auxiliary-task", "metadata": {"tags": ["deep-learning", "natural-language-processing", "large-language-models"], "owner": {"reputation": 1, "user_id": 74107, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/25bb659fff37fb483b9fb142e63792b6?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Dimits", "link": "https://ai.stackexchange.com/users/74107/dimits"}, "is_answered": false, "view_count": 54, "answer_count": 1, "score": 0, "last_activity_date": 1692079625, "creation_date": 1689449363, "question_id": 41295, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41295/fine-tune-llama-on-main-and-auxiliary-task", "title": "Fine-Tune Llama on main and auxiliary task", "body": "<p>I am trying to fine-tune Llama model on two task at the same time, using hugging face library:</p>\n<p>Main task: Causal language model like the model was initially trained for\nA classification task based on the whole input sequence (recommend an article). For this task I am getting as a reference the LlamaForCausalLM class, overwriting init and forward functions .\nHowever, I want to combine the two tasks above into one process. The main problem is that language modelling is an iterative process were the loss is calculated for every new context token in the input sequence, while for the classification task the loss should only be calculated once.</p>\n<p>How can I freeze the loss update on the classification task up and only calculated once the language modelling part has been completed. Is there any example you can recommend in order to combine a main LM task with an auxiliary classification task?</p>\n<p>First question for me here, thanks everyone for your understanding.</p>\n"}, "title": "Fine-Tune Llama on main and auxiliary task", "content": "I am trying to fine-tune Llama model on two task at the same time, using hugging face library:\nMain task: Causal language model like the model was initially trained for\nA classification task based on the whole input sequence (recommend an article). For this task I am getting as a reference the LlamaForCausalLM class, overwriting init and forward functions .\nHowever, I want to combine the two tasks above into one process. The main problem is that language modelling is an iterative process were the loss is calculated for every new context token in the input sequence, while for the classification task the loss should only be calculated once.\nHow can I freeze the loss update on the classification task up and only calculated once the language modelling part has been completed. Is there any example you can recommend in order to combine a main LM task with an auxiliary classification task?\nFirst question for me here, thanks everyone for your understanding.\n", "question_id": 41295, "answers": []}, "41247": {"link": "https://ai.stackexchange.com/questions/41247/what-role-does-data-quality-plays-in-the-llm-scaling-laws", "metadata": {"tags": ["datasets", "large-language-models"], "owner": {"reputation": 123, "user_id": 73068, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/327839fd17c445cd219177c746b10097?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Blue Nebula", "link": "https://ai.stackexchange.com/users/73068/blue-nebula"}, "is_answered": false, "view_count": 133, "answer_count": 0, "score": 0, "last_activity_date": 1689150350, "creation_date": 1689150350, "question_id": 41247, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41247/what-role-does-data-quality-plays-in-the-llm-scaling-laws", "title": "What role does data quality plays in the LLM scaling laws?", "body": "<p>DeepMind released the <a href=\"https://arxiv.org/abs/2203.15556\" rel=\"nofollow noreferrer\">Training Compute-Optimal Large Language Models</a> paper in 2022 which describe some scaling laws for LLMs. As far as I understand this is the most accredited reference to estimate the optimal relation between dataset size, compute power and model size.</p>\n<p>Recently a number of models have been developed using far less data, parameters and compute than the bigger LLMs. Yet these models achieved great results thanks to much better data quality. For instance models like <a href=\"https://arxiv.org/abs/2304.12244\" rel=\"nofollow noreferrer\">WizardLM</a>, <a href=\"https://arxiv.org/abs/2305.07759\" rel=\"nofollow noreferrer\">TinyStories</a> and <a href=\"https://arxiv.org/abs/2306.11644\" rel=\"nofollow noreferrer\">phi-1</a>.</p>\n<p>I'm curious about what role the data quality plays in the training of LLMs. I'm wondering things like: is the set of values estimated by the Chinchilla scaling laws optimal for these smaller models with optimized data too? Do we have any model to estimate the quality of some datasets and some scaling laws that take it into account?</p>\n"}, "title": "What role does data quality plays in the LLM scaling laws?", "content": "DeepMind released the Training Compute-Optimal Large Language Models paper in 2022 which describe some scaling laws for LLMs. As far as I understand this is the most accredited reference to estimate the optimal relation between dataset size, compute power and model size.\nRecently a number of models have been developed using far less data, parameters and compute than the bigger LLMs. Yet these models achieved great results thanks to much better data quality. For instance models like WizardLM, TinyStories and phi-1.\nI'm curious about what role the data quality plays in the training of LLMs. I'm wondering things like: is the set of values estimated by the Chinchilla scaling laws optimal for these smaller models with optimized data too? Do we have any model to estimate the quality of some datasets and some scaling laws that take it into account?\n", "question_id": 41247, "answers": []}, "41184": {"link": "https://ai.stackexchange.com/questions/41184/a-technique-to-show-what-tokens-are-relatively-predicted-by-an-llm", "metadata": {"tags": ["natural-language-processing", "language-model", "large-language-models", "information-theory", "benchmarks"], "owner": {"reputation": 103, "user_id": 65278, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/OVkG6.jpg?s=256&g=1", "display_name": "hmltn", "link": "https://ai.stackexchange.com/users/65278/hmltn"}, "is_answered": false, "view_count": 31, "answer_count": 0, "score": 0, "last_activity_date": 1688712054, "creation_date": 1688712054, "question_id": 41184, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41184/a-technique-to-show-what-tokens-are-relatively-predicted-by-an-llm", "title": "A technique to show what tokens are relatively predicted by an LLM", "body": "<p>I\u2019m picturing a technique where you can see what an LLM is likely to respond with, which updates in real time.</p>\n<p>It\u2019s a bit trippy, but it\u2019s like GitHub Copilot, in that there is predicted text while you type, but it\u2019s predicting what an LLM would say in response.</p>\n<p>It updates its prediction for any change in word of your current input.</p>\n<p>So if I just type the word \u201cThe\u201d, the prediction might show the average of a bunch of repeated LLM responses, maybe \u201cI\u2019m sorry, I didn\u2019t catch that, could you please ask again?\u201d or something.</p>\n<p>Like Google Translate, maybe there are 3 main variations on what answers you\u2019re expecting, so there\u2019s the main one and the alternatives listed below it.</p>\n<p>Now when you continue typing, to \u201cThe book\u201d, the predicted responses update.</p>\n<p>However, it shows in relative terms what changed the most by the addition of the token \u201cbook\u201d, so you can see / get a sense for specifically how that token changes the model\u2019s \u201cunderstanding\u201d, in that context.</p>\n<p>For example, you could color-code or size-code words to show which words appear in the predicted output which are more unique or distinctive. Like by adding \u201cbook\u201d into the input context window, the output might indicate the presence of highly unique tokens in the predicted output, like \u201clibrary\u201d, \u201cread\u201d, etc.</p>\n<p>There is a lot you could do with this.</p>\n<p>I see it as maybe being based on a kind of constructed \u201ccalculus\u201d of LLM inputs and outputs.</p>\n<p>You have to find a way to define the average of any N strings. If the average of two strings is judged highly similar, they can be treated (in that context) as synonyms; if the average is highly different, they are two different variations of output you might expect.</p>\n<p>The average helps you determine which tokens are \u201cunique\u201d - because they are or are not in some average of N output strings.</p>\n<p>Can anyone suggest how you could construct that average function?</p>\n<p>One way would involve adding and subtracting common words between two sentences.</p>\n<p>Another would be using an LLM to judge how similar two sentences are; ie an LLM-native approach where the LLM provides all the functions for assessing the LLMs own output.</p>\n"}, "title": "A technique to show what tokens are relatively predicted by an LLM", "content": "I\u2019m picturing a technique where you can see what an LLM is likely to respond with, which updates in real time.\nIt\u2019s a bit trippy, but it\u2019s like GitHub Copilot, in that there is predicted text while you type, but it\u2019s predicting what an LLM would say in response.\nIt updates its prediction for any change in word of your current input.\nSo if I just type the word \u201cThe\u201d, the prediction might show the average of a bunch of repeated LLM responses, maybe \u201cI\u2019m sorry, I didn\u2019t catch that, could you please ask again?\u201d or something.\nLike Google Translate, maybe there are 3 main variations on what answers you\u2019re expecting, so there\u2019s the main one and the alternatives listed below it.\nNow when you continue typing, to \u201cThe book\u201d, the predicted responses update.\nHowever, it shows in relative terms what changed the most by the addition of the token \u201cbook\u201d, so you can see / get a sense for specifically how that token changes the model\u2019s \u201cunderstanding\u201d, in that context.\nFor example, you could color-code or size-code words to show which words appear in the predicted output which are more unique or distinctive. Like by adding \u201cbook\u201d into the input context window, the output might indicate the presence of highly unique tokens in the predicted output, like \u201clibrary\u201d, \u201cread\u201d, etc.\nThere is a lot you could do with this.\nI see it as maybe being based on a kind of constructed \u201ccalculus\u201d of LLM inputs and outputs.\nYou have to find a way to define the average of any N strings. If the average of two strings is judged highly similar, they can be treated (in that context) as synonyms; if the average is highly different, they are two different variations of output you might expect.\nThe average helps you determine which tokens are \u201cunique\u201d - because they are or are not in some average of N output strings.\nCan anyone suggest how you could construct that average function?\nOne way would involve adding and subtracting common words between two sentences.\nAnother would be using an LLM to judge how similar two sentences are; ie an LLM-native approach where the LLM provides all the functions for assessing the LLMs own output.\n", "question_id": 41184, "answers": []}, "41179": {"link": "https://ai.stackexchange.com/questions/41179/what-is-better-train-a-model-from-scratch-on-your-own-data-vs-fine-tune-pretra", "metadata": {"tags": ["large-language-models"], "owner": {"reputation": 214, "user_id": 72562, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fb9f657e0b37fa483135635211e7d167?s=256&d=identicon&r=PG", "display_name": "morpheus", "link": "https://ai.stackexchange.com/users/72562/morpheus"}, "is_answered": false, "view_count": 49, "answer_count": 1, "score": 0, "last_activity_date": 1691953360, "creation_date": 1688682733, "last_edit_date": 1688683085, "question_id": 41179, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41179/what-is-better-train-a-model-from-scratch-on-your-own-data-vs-fine-tune-pretra", "title": "What is better: train a model from scratch on your own data vs. fine-tune pretrained model?", "body": "<p>Problem: I am interested in building a Q&amp;A engine on top of my private data. I am only interested in asking questions related to my data.</p>\n<p>Options:</p>\n<ol>\n<li>I train a model from scratch on my own data</li>\n<li>I pick a pretrained large language model and fine-tune it on my data</li>\n</ol>\n<p>With option 1, I don't expect to train a model with billions of parameters. I understand training from scratch is expensive and time-consuming but I am going to use a much smaller parameter model in that case. For sake of argument, quoting [<a href=\"https://huggingface.co/blog/falcon\" rel=\"nofollow noreferrer\">1</a>]</p>\n<blockquote>\n<p>Falcon-7B and Falcon-40B have been trained on 1.5 trillion and 1 trillion tokens respectively,</p>\n</blockquote>\n<p>so if I have 1 billion tokens in my dataset, I will train a model with 40M parameters.</p>\n<p>Is there some objective answer / study done as to which option turns out better?</p>\n<p>For completeness, option 2 is something I have tried but I did not get any decrease in my training loss i.e., the fine-tuning was a no-op. I used <code>gpt2</code> pretrained model with 130M parameters or so and my dataset had about 600 training examples in it. # of tokens = 600 * 1024. any pointers whether this (i.e., fine-tuning made no difference) is to be expected and why would also be appreciated.</p>\n"}, "title": "What is better: train a model from scratch on your own data vs. fine-tune pretrained model?", "content": "Problem: I am interested in building a Q&A engine on top of my private data. I am only interested in asking questions related to my data.\nOptions:\n\nI train a model from scratch on my own data\nI pick a pretrained large language model and fine-tune it on my data\n\nWith option 1, I don't expect to train a model with billions of parameters. I understand training from scratch is expensive and time-consuming but I am going to use a much smaller parameter model in that case. For sake of argument, quoting [1]\n\nFalcon-7B and Falcon-40B have been trained on 1.5 trillion and 1 trillion tokens respectively,\n\nso if I have 1 billion tokens in my dataset, I will train a model with 40M parameters.\nIs there some objective answer / study done as to which option turns out better?\nFor completeness, option 2 is something I have tried but I did not get any decrease in my training loss i.e., the fine-tuning was a no-op. I used gpt2 pretrained model with 130M parameters or so and my dataset had about 600 training examples in it. # of tokens = 600 * 1024. any pointers whether this (i.e., fine-tuning made no difference) is to be expected and why would also be appreciated.\n", "question_id": 41179, "answers": []}, "41087": {"link": "https://ai.stackexchange.com/questions/41087/relation-between-batch-size-and-micro-batch-size", "metadata": {"tags": ["deep-learning", "training", "pytorch", "large-language-models", "gpu"], "owner": {"reputation": 101, "user_id": 53497, "user_type": "registered", "profile_image": "https://graph.facebook.com/100001646853218/picture?type=large", "display_name": "Rituraj Singh", "link": "https://ai.stackexchange.com/users/53497/rituraj-singh"}, "is_answered": false, "view_count": 169, "answer_count": 0, "score": 0, "last_activity_date": 1688235812, "creation_date": 1688235812, "question_id": 41087, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41087/relation-between-batch-size-and-micro-batch-size", "title": "Relation between Batch Size and Micro Batch Size", "body": "<p>In distributed training of large models (pipeline parallelism), a mini batch of training samples is divided into n-micro batches. Each device performs forward and backward passes for a micro batch.</p>\n<p>What is the relation between Batch Size, Micro Batch Size, Gradient Accumulation (usually: batch size/micro batch size) in relation to no of GPUs as well as GPU size?</p>\n<p>I understand that a micro-batch size is bounded by the GPU capacity, increasing it may lead to OOM error. Let suppose, for two GPU of 40GB capacity, we can use a max micro-batch size of 4 and we are using the batch size of 128. Next, we want to increase the count of GPU to 8 (each 40GB capacity). What is the best way to speed up. Whether increasing the batch-size will help? Or the global batch size has very subtle/no effect on the training time (only overhead is synchronization) in distributed training.</p>\n"}, "title": "Relation between Batch Size and Micro Batch Size", "content": "In distributed training of large models (pipeline parallelism), a mini batch of training samples is divided into n-micro batches. Each device performs forward and backward passes for a micro batch.\nWhat is the relation between Batch Size, Micro Batch Size, Gradient Accumulation (usually: batch size/micro batch size) in relation to no of GPUs as well as GPU size?\nI understand that a micro-batch size is bounded by the GPU capacity, increasing it may lead to OOM error. Let suppose, for two GPU of 40GB capacity, we can use a max micro-batch size of 4 and we are using the batch size of 128. Next, we want to increase the count of GPU to 8 (each 40GB capacity). What is the best way to speed up. Whether increasing the batch-size will help? Or the global batch size has very subtle/no effect on the training time (only overhead is synchronization) in distributed training.\n", "question_id": 41087, "answers": []}, "40906": {"link": "https://ai.stackexchange.com/questions/40906/is-it-possible-to-generate-new-text-matching-a-vector-embedding-with-an-llm", "metadata": {"tags": ["embeddings", "large-language-models"], "owner": {"reputation": 101, "user_id": 73236, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6fe42a41d445efbe0b87ed7b36e79428?s=256&d=identicon&r=PG", "display_name": "Charles", "link": "https://ai.stackexchange.com/users/73236/charles"}, "is_answered": false, "view_count": 42, "answer_count": 0, "score": 0, "last_activity_date": 1687280340, "creation_date": 1687280340, "question_id": 40906, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40906/is-it-possible-to-generate-new-text-matching-a-vector-embedding-with-an-llm", "title": "Is it possible to generate new text matching a vector embedding with an LLM", "body": "<p>I'm interested in generating variations of text using an LLM - is it possible to take a text embedding, move it in different directions in vector space, and generate new text from the resulting vectors?</p>\n"}, "title": "Is it possible to generate new text matching a vector embedding with an LLM", "content": "I'm interested in generating variations of text using an LLM - is it possible to take a text embedding, move it in different directions in vector space, and generate new text from the resulting vectors?\n", "question_id": 40906, "answers": []}, "40883": {"link": "https://ai.stackexchange.com/questions/40883/how-does-vocabulary-size-affect-quality", "metadata": {"tags": ["large-language-models"], "owner": {"reputation": 131, "user_id": 73105, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fa52f0ed961993dce0a5c271dca0b4b7?s=256&d=identicon&r=PG", "display_name": "Daniel Darabos", "link": "https://ai.stackexchange.com/users/73105/daniel-darabos"}, "is_answered": false, "view_count": 147, "answer_count": 0, "score": 0, "last_activity_date": 1686936757, "creation_date": 1686936757, "question_id": 40883, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40883/how-does-vocabulary-size-affect-quality", "title": "How does vocabulary size affect quality?", "body": "<p>I think the vocabulary size in LLMs makes two trade-offs:</p>\n<ul>\n<li>The bigger tokens you have, the less frequent they will be.</li>\n<li>The more tokens you have, the more parameters you dedicate to input and output.</li>\n</ul>\n<p>I'm looking for a chart of the effect of the tokenizer vocabulary size on some quality metric. It could be a chart with a fixed number of total parameters, so both trade-offs have an impact on quality. Or it could be a chart where the architecture is fixed and the number of parameters grows with vocabulary size, to only show the first trade-off.</p>\n<p>The best I could find is this chart from <a href=\"https://ar5iv.labs.arxiv.org/html/2204.08832\" rel=\"nofollow noreferrer\">Impact of Tokenization on Language Models: An Analysis for Turkish</a>. But it only shows increasing quality. Surely at some point increasing the vocabulary starts to have a negative effect on quality?</p>\n<p><a href=\"https://i.stack.imgur.com/8jg41.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/8jg41.png\" alt=\"chart from Impact of Tokenization on Language Models: An Analysis for Turkish\" /></a></p>\n"}, "title": "How does vocabulary size affect quality?", "content": "I think the vocabulary size in LLMs makes two trade-offs:\n\nThe bigger tokens you have, the less frequent they will be.\nThe more tokens you have, the more parameters you dedicate to input and output.\n\nI'm looking for a chart of the effect of the tokenizer vocabulary size on some quality metric. It could be a chart with a fixed number of total parameters, so both trade-offs have an impact on quality. Or it could be a chart where the architecture is fixed and the number of parameters grows with vocabulary size, to only show the first trade-off.\nThe best I could find is this chart from Impact of Tokenization on Language Models: An Analysis for Turkish. But it only shows increasing quality. Surely at some point increasing the vocabulary starts to have a negative effect on quality?\n\n", "question_id": 40883, "answers": []}, "40704": {"link": "https://ai.stackexchange.com/questions/40704/why-do-llama-and-its-variants-have-non-round-numbers-of-parameters", "metadata": {"tags": ["large-language-models"], "owner": {"reputation": 1, "user_id": 72661, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/abafd268f00a23dfe78355b4195208d5?s=256&d=identicon&r=PG", "display_name": "amh", "link": "https://ai.stackexchange.com/users/72661/amh"}, "is_answered": false, "view_count": 75, "answer_count": 1, "score": 0, "last_activity_date": 1688821425, "creation_date": 1685928566, "question_id": 40704, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40704/why-do-llama-and-its-variants-have-non-round-numbers-of-parameters", "title": "Why do LLaMa and its variants have non-\u201cround\u201d numbers of parameters?", "body": "<p><a href=\"https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\" rel=\"nofollow noreferrer\">LLaMa</a> was released in several sizes, with 7B, 13B, 33B, and 65B parameters. These values look a little weird, because they are very close to powers of two (8, 16, 32, 64) that would be more conventionally considered \u201cround numbers\u201d in software. Why were these specific numbers chosen?</p>\n"}, "title": "Why do LLaMa and its variants have non-\u201cround\u201d numbers of parameters?", "content": "LLaMa was released in several sizes, with 7B, 13B, 33B, and 65B parameters. These values look a little weird, because they are very close to powers of two (8, 16, 32, 64) that would be more conventionally considered \u201cround numbers\u201d in software. Why were these specific numbers chosen?\n", "question_id": 40704, "answers": []}, "40600": {"link": "https://ai.stackexchange.com/questions/40600/improving-embedding-similarity-search-of-aggregated-embeddings", "metadata": {"tags": ["open-ai", "embeddings", "large-language-models"], "owner": {"reputation": 1, "user_id": 72322, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6c4d4676036162ad3253aa48e425bbe7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Stefan", "link": "https://ai.stackexchange.com/users/72322/stefan"}, "is_answered": false, "view_count": 94, "answer_count": 0, "score": 0, "last_activity_date": 1685100342, "creation_date": 1685100342, "question_id": 40600, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40600/improving-embedding-similarity-search-of-aggregated-embeddings", "title": "Improving embedding similarity search of aggregated embeddings", "body": "<p>I am building an author suggestion tool that proposes authors writing about similar topics as a given input text. I want to use embeddings for this. The way I currently do it is to store embeddings of many articles of many different authors in a vector database. Then I generate an embedding for every author by taking the embeddings of their last X articles and calculate the mean of those vectors. I can then compare the embedding of any given input text with the aggregated author embeddings using cosine similarity.</p>\n<p>This does work to some extend but it looks like:</p>\n<ol>\n<li>The aggregated author embeddings do not only capture topics, but also other things such as writing style, language, and so on. Which I am not really intested in.</li>\n<li>By using a mean as embedding it captures the broad field of an author quite well (e.g. tech, science, economy, sports) but the sub-fields seem to get over-shadowed by authors that write more generic articles sometimes. Searching for a niche topic often doesn't give the best results as the niche authors that wrote articles about it often get scored worse than authors that write more generic articles in the same overacrching field.</li>\n</ol>\n<p>I am looking for ways to improve my method, any suggestions are welcome.</p>\n"}, "title": "Improving embedding similarity search of aggregated embeddings", "content": "I am building an author suggestion tool that proposes authors writing about similar topics as a given input text. I want to use embeddings for this. The way I currently do it is to store embeddings of many articles of many different authors in a vector database. Then I generate an embedding for every author by taking the embeddings of their last X articles and calculate the mean of those vectors. I can then compare the embedding of any given input text with the aggregated author embeddings using cosine similarity.\nThis does work to some extend but it looks like:\n\nThe aggregated author embeddings do not only capture topics, but also other things such as writing style, language, and so on. Which I am not really intested in.\nBy using a mean as embedding it captures the broad field of an author quite well (e.g. tech, science, economy, sports) but the sub-fields seem to get over-shadowed by authors that write more generic articles sometimes. Searching for a niche topic often doesn't give the best results as the niche authors that wrote articles about it often get scored worse than authors that write more generic articles in the same overacrching field.\n\nI am looking for ways to improve my method, any suggestions are welcome.\n", "question_id": 40600, "answers": []}, "40478": {"link": "https://ai.stackexchange.com/questions/40478/fine-tune-nanogpt-for-instructions", "metadata": {"tags": ["fine-tuning", "large-language-models"], "owner": {"reputation": 101, "user_id": 71401, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/8fe68d99410e3519f970008f80826ffe?s=256&d=identicon&r=PG", "display_name": "CWcx", "link": "https://ai.stackexchange.com/users/71401/cwcx"}, "is_answered": false, "view_count": 254, "answer_count": 0, "score": 0, "last_activity_date": 1684329797, "creation_date": 1684329797, "question_id": 40478, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40478/fine-tune-nanogpt-for-instructions", "title": "fine-tune nanoGPT for instructions", "body": "<p>I've been playing around with nanoGPT, and recently I decided I wanted to fine-tune it using the dolly instruction set. This data set consists of roughly 15k examples and each example has the following features: question-type, context, instruction, and response. For my first go around at fine-tuning nanoGPT, I threw away the question-type feature, and then prefixed the other categories with &quot;CONTEXT: &quot;, &quot;INSTRUCTION: &quot;, &quot;RESPONSE: &quot; respectively, I then proceeded to concatenate them all together, tokenize them, and then train nanoGPT on the resulting text. That worked fine, for the most part, but now I want to train nanoGPT in a slightly different way.</p>\n<p>The other way I would like to train nanoGPT is as follows: I again prefix each of the categories, but this time I would like to only concatenate the context and instruction together. The training loop I would like to use is something like this: input the the context and instruction together, let the model generate a response, and then compare that with response in dolly (e.g. take the cross-entropy loss between the generated response and the response from the dataset). The problem with this is that the way nanoGPT works (as far as I can tell), you feed in a block of text (usually 1024 tokens) and the target is that same block of text but shifted by 1, and the loss is computed between the next token and the output. Essentially, the forward call for nanoGPT only gives the probability distribution for the next character given the block. So it would seem that I need to either change the model somehow (which is undesirable), or I need to change the training loop. Now, in nanoGPT, there is a generate method, but it is explicitly decorated with a @torch.no_grad(). I've tried commenting out this decorator, but run into problems when doing so.</p>\n<p>So I am at a bit of a loss of what to do from here. If anyone has any suggestions or know of any resources, I'd be very grateful. I am still quite new to this stuff so I am not sure what I am hoping to do is even reasonable.</p>\n<p>Thanks!</p>\n"}, "title": "fine-tune nanoGPT for instructions", "content": "I've been playing around with nanoGPT, and recently I decided I wanted to fine-tune it using the dolly instruction set. This data set consists of roughly 15k examples and each example has the following features: question-type, context, instruction, and response. For my first go around at fine-tuning nanoGPT, I threw away the question-type feature, and then prefixed the other categories with \"CONTEXT: \", \"INSTRUCTION: \", \"RESPONSE: \" respectively, I then proceeded to concatenate them all together, tokenize them, and then train nanoGPT on the resulting text. That worked fine, for the most part, but now I want to train nanoGPT in a slightly different way.\nThe other way I would like to train nanoGPT is as follows: I again prefix each of the categories, but this time I would like to only concatenate the context and instruction together. The training loop I would like to use is something like this: input the the context and instruction together, let the model generate a response, and then compare that with response in dolly (e.g. take the cross-entropy loss between the generated response and the response from the dataset). The problem with this is that the way nanoGPT works (as far as I can tell), you feed in a block of text (usually 1024 tokens) and the target is that same block of text but shifted by 1, and the loss is computed between the next token and the output. Essentially, the forward call for nanoGPT only gives the probability distribution for the next character given the block. So it would seem that I need to either change the model somehow (which is undesirable), or I need to change the training loop. Now, in nanoGPT, there is a generate method, but it is explicitly decorated with a @torch.no_grad(). I've tried commenting out this decorator, but run into problems when doing so.\nSo I am at a bit of a loss of what to do from here. If anyone has any suggestions or know of any resources, I'd be very grateful. I am still quite new to this stuff so I am not sure what I am hoping to do is even reasonable.\nThanks!\n", "question_id": 40478, "answers": []}, "40281": {"link": "https://ai.stackexchange.com/questions/40281/whats-the-right-approach-to-teach-a-chatbot-about-a-big-specific-data-set-tr", "metadata": {"tags": ["generative-model", "chat-bots", "large-language-models"], "owner": {"reputation": 101, "user_id": 71363, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/MiNJV.jpg?s=256&g=1", "display_name": "lesssugar", "link": "https://ai.stackexchange.com/users/71363/lesssugar"}, "is_answered": false, "view_count": 13, "answer_count": 0, "score": 0, "last_activity_date": 1683100609, "creation_date": 1683099881, "last_edit_date": 1683100609, "question_id": 40281, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40281/whats-the-right-approach-to-teach-a-chatbot-about-a-big-specific-data-set-tr", "title": "What&#39;s the right approach to &quot;teach&quot; a chatbot about a big specific data set: training it or feeding it the input once?", "body": "<p>I would like to build a chatbot that I can talk to about any specific data set I provide and I have a theoretical question.</p>\n<p>Let's say, I want to discuss French cuisine. This could involve providing the model with information like culinary history, biographies of French chefs, French cookbooks, etc.</p>\n<p>Now, <strong>the question</strong>: Assuming I have all this huge data set, what would be the correct, more efficient approach?</p>\n<ol>\n<li>Do I use the data set to <em>train</em> the model, so that later I can talk to it?, or</li>\n<li>Do I simply <em>feed all this text-based information</em> to the model as a system message and instruct the model to analyze the text before it gives any answers?</li>\n</ol>\n<p>So, essentially, training or providing large, one-time input?</p>\n"}, "title": "What&#39;s the right approach to &quot;teach&quot; a chatbot about a big specific data set: training it or feeding it the input once?", "content": "I would like to build a chatbot that I can talk to about any specific data set I provide and I have a theoretical question.\nLet's say, I want to discuss French cuisine. This could involve providing the model with information like culinary history, biographies of French chefs, French cookbooks, etc.\nNow, the question: Assuming I have all this huge data set, what would be the correct, more efficient approach?\n\nDo I use the data set to train the model, so that later I can talk to it?, or\nDo I simply feed all this text-based information to the model as a system message and instruct the model to analyze the text before it gives any answers?\n\nSo, essentially, training or providing large, one-time input?\n", "question_id": 40281, "answers": []}, "40224": {"link": "https://ai.stackexchange.com/questions/40224/do-llm-or-machine-learning-models-with-a-large-number-of-classes-employ-standard", "metadata": {"tags": ["neural-networks", "cross-entropy", "large-language-models"], "owner": {"reputation": 121, "user_id": 42832, "user_type": "registered", "profile_image": "https://graph.facebook.com/10210927549521147/picture?type=large", "display_name": "Tommaso Bendinelli", "link": "https://ai.stackexchange.com/users/42832/tommaso-bendinelli"}, "is_answered": false, "view_count": 113, "answer_count": 0, "score": 0, "last_activity_date": 1682675990, "creation_date": 1682675990, "question_id": 40224, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40224/do-llm-or-machine-learning-models-with-a-large-number-of-classes-employ-standard", "title": "Do LLM or Machine Learning models with a large number of classes employ standard cross entropy?", "body": "<p>In language modeling, the next token is predicted from the entire vocabulary during inferenc. However, when training models with large vocabularies or many different classes, standard cross entropy can be suboptimal due to its computational expense and sparsity of optimization. Are there any tricks or relevant papers that people use to address this issue?</p>\n"}, "title": "Do LLM or Machine Learning models with a large number of classes employ standard cross entropy?", "content": "In language modeling, the next token is predicted from the entire vocabulary during inferenc. However, when training models with large vocabularies or many different classes, standard cross entropy can be suboptimal due to its computational expense and sparsity of optimization. Are there any tricks or relevant papers that people use to address this issue?\n", "question_id": 40224, "answers": []}, "39873": {"link": "https://ai.stackexchange.com/questions/39873/could-it-be-probable-to-quantify-or-measure-the-iq-of-a-super-intelligent-machin", "metadata": {"tags": ["open-ai", "gpt", "large-language-models", "intelligence-quotient"], "owner": {"reputation": 137, "user_id": 25452, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/NgCbm.png?s=256&g=1", "display_name": "R1-", "link": "https://ai.stackexchange.com/users/25452/r1"}, "is_answered": true, "view_count": 141, "closed_date": 1680532870, "accepted_answer_id": 39882, "answer_count": 1, "score": -1, "last_activity_date": 1681758079, "creation_date": 1680306997, "last_edit_date": 1681758079, "question_id": 39873, "link": "https://ai.stackexchange.com/questions/39873/could-it-be-probable-to-quantify-or-measure-the-iq-of-a-super-intelligent-machin", "closed_reason": "Opinion-based", "title": "Could it be probable to quantify or measure the IQ of a super-intelligent machine like GPT?", "body": "<p>In the age of artificial intelligence, super-intelligent machines like GPT have become a reality, leading to the question of how to quantify or measure their intelligence. While IQ tests are widely used to measure human intelligence, could a similar approach be used to assess the intelligence of machines? Is it possible to compare the intelligence of a machine to that of a human in this context, given that machines possess certain cognitive abilities and limitations that differ from those of humans? Additionally, what are some alternative methods for evaluating machine intelligence, and how do they differ from traditional IQ tests used for humans? Finally, what implications might arise from the development of super-intelligent machines, both in terms of how we measure and compare intelligence and in terms of their impact on society as a whole?</p>\n"}, "title": "Could it be probable to quantify or measure the IQ of a super-intelligent machine like GPT?", "content": "In the age of artificial intelligence, super-intelligent machines like GPT have become a reality, leading to the question of how to quantify or measure their intelligence. While IQ tests are widely used to measure human intelligence, could a similar approach be used to assess the intelligence of machines? Is it possible to compare the intelligence of a machine to that of a human in this context, given that machines possess certain cognitive abilities and limitations that differ from those of humans? Additionally, what are some alternative methods for evaluating machine intelligence, and how do they differ from traditional IQ tests used for humans? Finally, what implications might arise from the development of super-intelligent machines, both in terms of how we measure and compare intelligence and in terms of their impact on society as a whole?\n", "question_id": 39873, "answers": []}, "41293": {"link": "https://ai.stackexchange.com/questions/41293/open-access-adept-like-dataset-llm-to-computer-input", "metadata": {"tags": ["large-language-models", "training-datasets"], "owner": {"reputation": 146, "user_id": 68775, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6f9aa372691e4af70e3420cb944d6360?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "llllvvuu", "link": "https://ai.stackexchange.com/users/68775/llllvvuu"}, "is_answered": true, "view_count": 29, "closed_date": 1690305169, "accepted_answer_id": 41307, "answer_count": 1, "score": -1, "last_activity_date": 1689486325, "creation_date": 1689385775, "question_id": 41293, "link": "https://ai.stackexchange.com/questions/41293/open-access-adept-like-dataset-llm-to-computer-input", "closed_reason": "Not suitable for this site", "title": "Open access Adept-like dataset? (LLM-to-computer-input)", "body": "<p>Here's a demo for <a href=\"https://www.adept.ai/blog/act-1\" rel=\"nofollow noreferrer\">Adept ACT-1 for Transformers</a>. I don't doubt that one could create a demo video using zero-shot; actually I tested just now and the basic chat.openai.com interface was able to do some web browsing stuff on the first try if I just prompted it with a list of API methods (e.g. <code>el.click</code>) and streamed the DOM to it.</p>\n<p>But I assume that, to take their product to the next level, they must be training or fine-tuning on some actual multimodal data, e.g. session recordings labeled with natural language? Am I wrong?</p>\n<p>Are there any open-access datasets like this that could be used for open-source models? I'm actually interested in native (e.g. pixel + natural language to game input) more than browser.</p>\n"}, "title": "Open access Adept-like dataset? (LLM-to-computer-input)", "content": "Here's a demo for Adept ACT-1 for Transformers. I don't doubt that one could create a demo video using zero-shot; actually I tested just now and the basic chat.openai.com interface was able to do some web browsing stuff on the first try if I just prompted it with a list of API methods (e.g. el.click) and streamed the DOM to it.\nBut I assume that, to take their product to the next level, they must be training or fine-tuning on some actual multimodal data, e.g. session recordings labeled with natural language? Am I wrong?\nAre there any open-access datasets like this that could be used for open-source models? I'm actually interested in native (e.g. pixel + natural language to game input) more than browser.\n", "question_id": 41293, "answers": []}, "11285": {"link": "https://ai.stackexchange.com/questions/11285/what-is-the-difference-between-latent-and-embedding-spaces", "metadata": {"tags": ["machine-learning", "terminology", "word-embedding", "latent-variable", "embeddings"], "owner": {"reputation": 38926, "user_id": 2444, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG", "display_name": "nbro", "link": "https://ai.stackexchange.com/users/2444/nbro"}, "is_answered": true, "view_count": 16656, "accepted_answer_id": 20646, "answer_count": 5, "score": 36, "last_activity_date": 1624026333, "creation_date": 1552823796, "last_edit_date": 1607260661, "question_id": 11285, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/11285/what-is-the-difference-between-latent-and-embedding-spaces", "title": "What is the difference between latent and embedding spaces?", "body": "<p>In general, the word &quot;latent&quot; means &quot;hidden&quot; and &quot;to embed&quot; means &quot;to incorporate&quot;. In machine learning, the expressions &quot;hidden (or latent) space&quot; and &quot;embedding space&quot; occur in several contexts. More specifically, an <a href=\"https://en.wikipedia.org/wiki/Word_embedding\" rel=\"noreferrer\">embedding</a> can refer to a vector representation of a word. An <a href=\"https://en.wikipedia.org/wiki/Embedding\" rel=\"noreferrer\">embedding space</a> can refer to a subspace of a bigger space, so we say that the subspace is <em>embedded</em> in the bigger space. The word &quot;latent&quot; comes up in contexts like <a href=\"https://en.wikipedia.org/wiki/Hidden_Markov_model\" rel=\"noreferrer\">hidden Markov models (HMMs)</a> or <a href=\"https://en.wikipedia.org/wiki/Autoencoder\" rel=\"noreferrer\">auto-encoders</a>.</p>\n<p>What is the difference between these spaces? In some contexts, do these two expressions refer to the same concept?</p>\n"}, "title": "What is the difference between latent and embedding spaces?", "content": "In general, the word \"latent\" means \"hidden\" and \"to embed\" means \"to incorporate\". In machine learning, the expressions \"hidden (or latent) space\" and \"embedding space\" occur in several contexts. More specifically, an embedding can refer to a vector representation of a word. An embedding space can refer to a subspace of a bigger space, so we say that the subspace is embedded in the bigger space. The word \"latent\" comes up in contexts like hidden Markov models (HMMs) or auto-encoders.\nWhat is the difference between these spaces? In some contexts, do these two expressions refer to the same concept?\n", "question_id": 11285, "answers": []}, "26235": {"link": "https://ai.stackexchange.com/questions/26235/what-kind-of-word-embedding-is-used-in-the-original-transformer", "metadata": {"tags": ["natural-language-processing", "transformer", "attention", "word-embedding"], "owner": {"reputation": 535, "user_id": 43632, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=256", "display_name": "Bert Gayus", "link": "https://ai.stackexchange.com/users/43632/bert-gayus"}, "is_answered": true, "view_count": 8559, "accepted_answer_id": 26246, "answer_count": 3, "score": 14, "last_activity_date": 1619873036, "creation_date": 1612551119, "question_id": 26235, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/26235/what-kind-of-word-embedding-is-used-in-the-original-transformer", "title": "What kind of word embedding is used in the original transformer?", "body": "<p>I am currently trying to understand transformers.</p>\n<p>To start, I read <a href=\"https://arxiv.org/pdf/1706.03762.pdf\" rel=\"noreferrer\">Attention Is All You Need</a> and also <a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\" rel=\"noreferrer\">this</a> tutorial.</p>\n<p>What makes me wonder is the word embedding used in the model. Is word2vec or GloVe being used? Are the word embeddings trained from scratch?</p>\n<p>In the tutorial linked above, the transformer is implemented from scratch and nn.Embedding from pytorch is used for the embeddings. I looked up this function and didn't understand it well, but I tend to think that the embeddings are trained from scratch, right?</p>\n"}, "title": "What kind of word embedding is used in the original transformer?", "content": "I am currently trying to understand transformers.\nTo start, I read Attention Is All You Need and also this tutorial.\nWhat makes me wonder is the word embedding used in the model. Is word2vec or GloVe being used? Are the word embeddings trained from scratch?\nIn the tutorial linked above, the transformer is implemented from scratch and nn.Embedding from pytorch is used for the embeddings. I looked up this function and didn't understand it well, but I tend to think that the embeddings are trained from scratch, right?\n", "question_id": 26235, "answers": []}, "39151": {"link": "https://ai.stackexchange.com/questions/39151/attention-is-all-you-need-paper-how-are-the-q-k-v-values-calculated", "metadata": {"tags": ["transformer", "attention", "word-embedding"], "owner": {"reputation": 221, "user_id": 35614, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/4ESTS.jpg?s=256&g=1", "display_name": "Soltius", "link": "https://ai.stackexchange.com/users/35614/soltius"}, "is_answered": true, "view_count": 751, "accepted_answer_id": 39195, "answer_count": 2, "score": 6, "last_activity_date": 1682027615, "creation_date": 1676371044, "last_edit_date": 1678356774, "question_id": 39151, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39151/attention-is-all-you-need-paper-how-are-the-q-k-v-values-calculated", "title": "&quot;Attention is all you need&quot; paper : How are the Q, K, V values calculated?", "body": "<p>The seminal <a href=\"https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\" rel=\"nofollow noreferrer\">Attention is all you need</a> paper introduces Transformers and implements the attention mecanism with &quot;queries, keys, values&quot;, in an analogy to a retrieval system.</p>\n<p>I understand the whole process of multi-head attention and such (i.e., what is done with the Q, K, V values and why), but I'm confused on <strong>how these values are computed in the first place</strong>. AFAICT, the paper seems to completely leave that out.</p>\n<p>Both Figure 2 of the paper and equations explaining Attention and Multihead attention start with Q,K,V already there :</p>\n<p><a href=\"https://i.stack.imgur.com/t6qJz.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/t6qJz.png\" alt=\"enter image description here\" /></a></p>\n<p>The answers regaridng the origin of Q,K,V I've found so far haven't satisfied me :</p>\n<ul>\n<li><p>In this <a href=\"https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms\">similar question</a>, the <a href=\"https://stats.stackexchange.com/a/424127/201218\">accepted answer</a> says &quot;<em>The proposed multihead attention alone doesn't say much about how the queries, keys, and values are obtained, they can come from different sources depending on the application scenario.</em>&quot;.</p>\n</li>\n<li><p>I also see some answers (eg <a href=\"https://stats.stackexchange.com/a/463320/201218\">this one on the same question</a>) which say that Q, K and V are the result of multiplication of the input embedding with some matrices. This is also what is shown in the popular blog post <a href=\"http://jalammar.github.io/illustrated-transformer/\" rel=\"nofollow noreferrer\">The Illustrated Transformer</a> :</p>\n</li>\n</ul>\n<p><a href=\"https://i.stack.imgur.com/8QJ6pl.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/8QJ6pl.png\" alt=\"enter image description here\" /></a></p>\n<p>Why isn't the computing of Q,K,V -be it &quot;left to the application&quot; or &quot;multiplication with matrices&quot; made more clear in the paper, at the very least for the task of language translation for which they show some results and so obviously did compute Q,K,V in some way ? If it is matrix multiplication, are these matrices (<span class=\"math-container\">$W^Q$</span>, etc in the figure of the blog post) trained with backprop jointly with the rest of the network or pretrained ? What are the resulting shapes of Q,K,V ?</p>\n"}, "title": "&quot;Attention is all you need&quot; paper : How are the Q, K, V values calculated?", "content": "The seminal Attention is all you need paper introduces Transformers and implements the attention mecanism with \"queries, keys, values\", in an analogy to a retrieval system.\nI understand the whole process of multi-head attention and such (i.e., what is done with the Q, K, V values and why), but I'm confused on how these values are computed in the first place. AFAICT, the paper seems to completely leave that out.\nBoth Figure 2 of the paper and equations explaining Attention and Multihead attention start with Q,K,V already there :\n\nThe answers regaridng the origin of Q,K,V I've found so far haven't satisfied me :\n\nIn this similar question, the accepted answer says \"The proposed multihead attention alone doesn't say much about how the queries, keys, and values are obtained, they can come from different sources depending on the application scenario.\".\n\nI also see some answers (eg this one on the same question) which say that Q, K and V are the result of multiplication of the input embedding with some matrices. This is also what is shown in the popular blog post The Illustrated Transformer :\n\n\n\nWhy isn't the computing of Q,K,V -be it \"left to the application\" or \"multiplication with matrices\" made more clear in the paper, at the very least for the task of language translation for which they show some results and so obviously did compute Q,K,V in some way ? If it is matrix multiplication, are these matrices ($W^Q$, etc in the figure of the blog post) trained with backprop jointly with the rest of the network or pretrained ? What are the resulting shapes of Q,K,V ?\n", "question_id": 39151, "answers": []}, "5408": {"link": "https://ai.stackexchange.com/questions/5408/what-is-the-intuition-behind-how-word-embeddings-bring-information-to-a-neural-n", "metadata": {"tags": ["neural-networks", "machine-learning", "natural-language-processing", "word-embedding", "embeddings"], "owner": {"reputation": 209, "user_id": 12931, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0aef4b8a8efa53d6b0dea11f7ec524c4?s=256&d=identicon&r=PG", "display_name": "silkAdmin", "link": "https://ai.stackexchange.com/users/12931/silkadmin"}, "is_answered": true, "view_count": 206, "answer_count": 2, "score": 5, "last_activity_date": 1622293057, "creation_date": 1519381550, "last_edit_date": 1622293057, "question_id": 5408, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/5408/what-is-the-intuition-behind-how-word-embeddings-bring-information-to-a-neural-n", "title": "What is the intuition behind how word embeddings bring information to a neural network?", "body": "<p>How is it that a word embedding layer (say word2vec) brings more insights to the neural network compared to a simple one-hot encoded layer?</p>\n<p>I understand how the word embedding carries some semantic meaning, but it seems that this information would get &quot;squashed&quot; by the activation function, leaving only a scalar value and as many different vectors could yield the same result, I would guess that the information is more or less lost.</p>\n<p>Could anyone bring me insights as to why a neural network may utilize the information contained in a word embedding?</p>\n"}, "title": "What is the intuition behind how word embeddings bring information to a neural network?", "content": "How is it that a word embedding layer (say word2vec) brings more insights to the neural network compared to a simple one-hot encoded layer?\nI understand how the word embedding carries some semantic meaning, but it seems that this information would get \"squashed\" by the activation function, leaving only a scalar value and as many different vectors could yield the same result, I would guess that the information is more or less lost.\nCould anyone bring me insights as to why a neural network may utilize the information contained in a word embedding?\n", "question_id": 5408, "answers": []}, "28011": {"link": "https://ai.stackexchange.com/questions/28011/is-an-embedding-a-representation-of-a-word-or-its-meaning", "metadata": {"tags": ["terminology", "word-embedding", "embeddings"], "owner": {"reputation": 3571, "user_id": 18758, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/339397e23b4e4cc78aa36e2b126252c7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "hanugm", "link": "https://ai.stackexchange.com/users/18758/hanugm"}, "is_answered": true, "view_count": 394, "accepted_answer_id": 28012, "answer_count": 2, "score": 5, "last_activity_date": 1622293285, "creation_date": 1622272288, "last_edit_date": 1622292303, "question_id": 28011, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/28011/is-an-embedding-a-representation-of-a-word-or-its-meaning", "title": "Is an embedding a representation of a word or its meaning?", "body": "<p>What does the term &quot;embedding&quot; actually mean?</p>\n<p>An embedding is a vector, but is that vector a representation of a word or its meaning? Literature loosely uses the word for both purposes. Which one is actually correct?</p>\n<p>Or is there anything like: <em>A word is its meaning itself</em>?</p>\n"}, "title": "Is an embedding a representation of a word or its meaning?", "content": "What does the term \"embedding\" actually mean?\nAn embedding is a vector, but is that vector a representation of a word or its meaning? Literature loosely uses the word for both purposes. Which one is actually correct?\nOr is there anything like: A word is its meaning itself?\n", "question_id": 28011, "answers": []}, "26739": {"link": "https://ai.stackexchange.com/questions/26739/what-is-the-difference-between-a-language-model-and-a-word-embedding", "metadata": {"tags": ["natural-language-processing", "comparison", "word-embedding", "language-model", "bleu"], "owner": {"reputation": 223, "user_id": 41187, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3bab6dd8e8cd71a3e564d68189721571?s=256&d=identicon&r=PG", "display_name": "Exploring", "link": "https://ai.stackexchange.com/users/41187/exploring"}, "is_answered": true, "view_count": 6128, "accepted_answer_id": 26745, "answer_count": 2, "score": 5, "last_activity_date": 1622762207, "creation_date": 1615326204, "last_edit_date": 1615337139, "question_id": 26739, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/26739/what-is-the-difference-between-a-language-model-and-a-word-embedding", "title": "What is the difference between a language model and a word embedding?", "body": "<p>I am self-studying applications of deep learning on the NLP and machine translation.</p>\n<p>I am confused about the concepts of &quot;Language Model&quot;, &quot;Word Embedding&quot;, &quot;BLEU Score&quot;.</p>\n<p>It appears to me that a language model is a way to predict the next word given its previous word. Word2vec is the similarity between two tokens. BLEU score is a way to measure the effectiveness of the language model.</p>\n<p>Is my understanding correct? If not, can someone please point me to the right articles, paper, or any other online resources?</p>\n"}, "title": "What is the difference between a language model and a word embedding?", "content": "I am self-studying applications of deep learning on the NLP and machine translation.\nI am confused about the concepts of \"Language Model\", \"Word Embedding\", \"BLEU Score\".\nIt appears to me that a language model is a way to predict the next word given its previous word. Word2vec is the similarity between two tokens. BLEU score is a way to measure the effectiveness of the language model.\nIs my understanding correct? If not, can someone please point me to the right articles, paper, or any other online resources?\n", "question_id": 26739, "answers": []}, "17403": {"link": "https://ai.stackexchange.com/questions/17403/does-summing-up-word-vectors-destroy-their-meaning", "metadata": {"tags": ["deep-learning", "natural-language-processing", "word-embedding", "text-classification", "binary-classification"], "owner": {"reputation": 101, "user_id": 32490, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-VeuP1F1V59s/AAAAAAAAAAI/AAAAAAAALuM/s3-GWM3Jk20/photo.jpg?sz=256", "display_name": "Arnav Das", "link": "https://ai.stackexchange.com/users/32490/arnav-das"}, "is_answered": true, "view_count": 138, "answer_count": 2, "score": 4, "last_activity_date": 1633788397, "creation_date": 1578411129, "last_edit_date": 1633788397, "question_id": 17403, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/17403/does-summing-up-word-vectors-destroy-their-meaning", "title": "Does summing up word vectors destroy their meaning?", "body": "<p>For example, I have a paragraph that I want to classify in a binary manner. But because the inputs have to have a fixed length, I need to ensure that every paragraph is represented by a uniform quantity.</p>\n<p>One thing I've done is taken every word in the paragraph, vectorized it using GloVe word2vec, and then summed up all of the vectors to create a &quot;paragraph&quot; vector, which I've then fed in as an input for my model. In doing so, have I destroyed any meaning the words might have possessed?</p>\n<p>Considering these two sentences would have the same vector:</p>\n<blockquote>\n<p>My dog bit Dave</p>\n</blockquote>\n<blockquote>\n<p>Dave bit my dog</p>\n</blockquote>\n<p>How do I get around this? Am I approaching this wrong?</p>\n<p>What other way can I train my model? If I take every word and feed that into my model, how do I know how many words I should take? How do I input these words? In the form of a 2D array, where each word vector is a column?</p>\n<p>I want to be able to train a model that can classify text accurately.\nSurprisingly, I'm getting a high (&gt;90%) for a relatively simple model like RandomForestClassifier just by using this summing up method. Any insights?</p>\n"}, "title": "Does summing up word vectors destroy their meaning?", "content": "For example, I have a paragraph that I want to classify in a binary manner. But because the inputs have to have a fixed length, I need to ensure that every paragraph is represented by a uniform quantity.\nOne thing I've done is taken every word in the paragraph, vectorized it using GloVe word2vec, and then summed up all of the vectors to create a \"paragraph\" vector, which I've then fed in as an input for my model. In doing so, have I destroyed any meaning the words might have possessed?\nConsidering these two sentences would have the same vector:\n\nMy dog bit Dave\n\n\nDave bit my dog\n\nHow do I get around this? Am I approaching this wrong?\nWhat other way can I train my model? If I take every word and feed that into my model, how do I know how many words I should take? How do I input these words? In the form of a 2D array, where each word vector is a column?\nI want to be able to train a model that can classify text accurately.\nSurprisingly, I'm getting a high (>90%) for a relatively simple model like RandomForestClassifier just by using this summing up method. Any insights?\n", "question_id": 17403, "answers": []}, "6144": {"link": "https://ai.stackexchange.com/questions/6144/do-individual-dimensions-in-vector-space-have-meaning", "metadata": {"tags": ["natural-language-processing", "word2vec", "word-embedding"], "owner": {"reputation": 266, "user_id": 13360, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/tJNah.jpg?s=256&g=1", "display_name": "G__", "link": "https://ai.stackexchange.com/users/13360/g"}, "is_answered": true, "view_count": 69, "answer_count": 1, "score": 4, "last_activity_date": 1555453858, "creation_date": 1524509889, "last_edit_date": 1555453858, "question_id": 6144, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/6144/do-individual-dimensions-in-vector-space-have-meaning", "title": "Do individual dimensions in vector space have meaning?", "body": "<p>Word2vec assigns an N-dimensional vector to given words (which can be considered a form of dimensionality reduction). </p>\n\n<p>It turns out that, at least with a number of canonical examples, vector arithmetic seems to work intuitively. For example \"<em>king + woman - man = queen</em>\".</p>\n\n<p>These terms are all N-dimensional vectors. Now, suppose, for simplicity, that <span class=\"math-container\">$N=3$</span>, <span class=\"math-container\">$\\text{king} = [0, 1, 2], \\text{woman} = [1, 1, 0], \\text{man} = [2, 2, 2], \\text{queen} = [-1, 0, 0]$</span>, then the expression above can be written as <span class=\"math-container\">$[0, 1, 2] +  [1, 1, 0] - [2, 2, 2] = [-1, 0, 0]$</span>.</p>\n\n<p>In this (contrived) example, the last dimension (king/man=2, queen/woman=0) suggests a semantic concept of gender. Aside from semantics, a given dimension could \"mean\" a part of speech, first letter, or really any feature or set of features that the algorithm might have latched onto. However, any perceived \"meaning\" of a single dimension might well just be a simple coincidence.</p>\n\n<p>If we picked out only a single dimension, does that dimension itself convey some predictable or determinable information? Or is this purely a \"random\" artefact of the algorithm, with only the full N-dimensional vector distances mattering?</p>\n"}, "title": "Do individual dimensions in vector space have meaning?", "content": "Word2vec assigns an N-dimensional vector to given words (which can be considered a form of dimensionality reduction). \nIt turns out that, at least with a number of canonical examples, vector arithmetic seems to work intuitively. For example \"king + woman - man = queen\".\nThese terms are all N-dimensional vectors. Now, suppose, for simplicity, that $N=3$, $\\text{king} = [0, 1, 2], \\text{woman} = [1, 1, 0], \\text{man} = [2, 2, 2], \\text{queen} = [-1, 0, 0]$, then the expression above can be written as $[0, 1, 2] +  [1, 1, 0] - [2, 2, 2] = [-1, 0, 0]$.\nIn this (contrived) example, the last dimension (king/man=2, queen/woman=0) suggests a semantic concept of gender. Aside from semantics, a given dimension could \"mean\" a part of speech, first letter, or really any feature or set of features that the algorithm might have latched onto. However, any perceived \"meaning\" of a single dimension might well just be a simple coincidence.\nIf we picked out only a single dimension, does that dimension itself convey some predictable or determinable information? Or is this purely a \"random\" artefact of the algorithm, with only the full N-dimensional vector distances mattering?\n", "question_id": 6144, "answers": []}, "12656": {"link": "https://ai.stackexchange.com/questions/12656/will-bert-embedding-be-always-same-for-a-given-document-when-used-as-a-feature-e", "metadata": {"tags": ["machine-learning", "natural-language-processing", "word-embedding", "bert"], "owner": {"reputation": 119, "user_id": 26115, "user_type": "registered", "profile_image": "https://lh4.googleusercontent.com/-NkK7LO3Iylk/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3reMvjQHK4ibc-Yr_qKHV7P-CRAmkw/mo/photo.jpg?sz=256", "display_name": "Srikant Jayaraman", "link": "https://ai.stackexchange.com/users/26115/srikant-jayaraman"}, "is_answered": true, "view_count": 447, "answer_count": 1, "score": 4, "last_activity_date": 1595909220, "creation_date": 1559558737, "last_edit_date": 1572576364, "question_id": 12656, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/12656/will-bert-embedding-be-always-same-for-a-given-document-when-used-as-a-feature-e", "title": "Will BERT embedding be always same for a given document when used as a feature extractor", "body": "<p>When we use BERT embeddings for a classification task, would we get different embeddings every time we pass the same text through the BERT architecture? If yes, is it the right way to use the embeddings as features? Ideally, while using any feature extraction technique, features values should be consistent. How do I handle this if we want BERT to be used as a feature extractor?</p>\n"}, "title": "Will BERT embedding be always same for a given document when used as a feature extractor", "content": "When we use BERT embeddings for a classification task, would we get different embeddings every time we pass the same text through the BERT architecture? If yes, is it the right way to use the embeddings as features? Ideally, while using any feature extraction technique, features values should be consistent. How do I handle this if we want BERT to be used as a feature extractor?\n", "question_id": 12656, "answers": []}, "11511": {"link": "https://ai.stackexchange.com/questions/11511/why-does-all-of-nlp-literature-use-noise-contrastive-estimation-loss-for-negativ", "metadata": {"tags": ["natural-language-processing", "word2vec", "word-embedding"], "owner": {"reputation": 197, "user_id": 18358, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3dd195f14c91226f28100605fab261a0?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "SantoshGupta7", "link": "https://ai.stackexchange.com/users/18358/santoshgupta7"}, "is_answered": false, "view_count": 33, "answer_count": 0, "score": 4, "last_activity_date": 1555453472, "creation_date": 1553820259, "last_edit_date": 1555453472, "question_id": 11511, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/11511/why-does-all-of-nlp-literature-use-noise-contrastive-estimation-loss-for-negativ", "title": "Why does all of NLP literature use noise contrastive estimation loss for negative sampling instead of sampled softmax loss?", "body": "<p>A sampled softmax function is like a regular softmax but randomly selects a given number of 'negative' samples.</p>\n\n<p>This is difference than NCE Loss, which doesn't use a softmax at all, it uses a logistic binary classifier for the context/labels. In NLP, 'Negative Sampling' basically refers to the NCE-based approach.</p>\n\n<p>More details here: <a href=\"https://www.tensorflow.org/extras/candidate_sampling.pdf\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/extras/candidate_sampling.pdf</a>.</p>\n\n<p>I have tested both and they both give pretty much the same results. But in word embedding literature, they always use NCE loss, and never sampled softmax.</p>\n\n<p>Is there any reason why this is? The sampled softmax seems like the more obvious solution to prevent applying a softmax to all the classes, so I imagine there must be some good reason for the NCE loss.</p>\n"}, "title": "Why does all of NLP literature use noise contrastive estimation loss for negative sampling instead of sampled softmax loss?", "content": "A sampled softmax function is like a regular softmax but randomly selects a given number of 'negative' samples.\nThis is difference than NCE Loss, which doesn't use a softmax at all, it uses a logistic binary classifier for the context/labels. In NLP, 'Negative Sampling' basically refers to the NCE-based approach.\nMore details here: https://www.tensorflow.org/extras/candidate_sampling.pdf.\nI have tested both and they both give pretty much the same results. But in word embedding literature, they always use NCE loss, and never sampled softmax.\nIs there any reason why this is? The sampled softmax seems like the more obvious solution to prevent applying a softmax to all the classes, so I imagine there must be some good reason for the NCE loss.\n", "question_id": 11511, "answers": []}, "37542": {"link": "https://ai.stackexchange.com/questions/37542/how-embeddings-learned-from-one-model-can-be-used-in-another", "metadata": {"tags": ["keras", "word-embedding", "embeddings"], "owner": {"reputation": 43, "user_id": 62489, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/ee2d060d790ffe31bd8392fc1e1dcf3a?s=256&d=identicon&r=PG", "display_name": "Oculu ", "link": "https://ai.stackexchange.com/users/62489/oculu"}, "is_answered": true, "view_count": 884, "accepted_answer_id": 37545, "answer_count": 2, "score": 3, "last_activity_date": 1686467412, "creation_date": 1666279794, "question_id": 37542, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/37542/how-embeddings-learned-from-one-model-can-be-used-in-another", "title": "How embeddings learned from one model can be used in another?", "body": "<p>In the <a href=\"https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\" rel=\"nofollow noreferrer\">website</a> the following explanation is provided about Embedding layer:</p>\n<blockquote>\n<p>The Embedding layer is initialized with random weights and will learn\nan embedding for all of the words in the training dataset.</p>\n<p>It is a flexible layer that can be used in a variety of ways, such as:</p>\n<p>It can be used alone to learn a word embedding that can be saved and\nused in another model later. It can be used as part of a deep learning\nmodel where the embedding is learned along with the model itself. It\ncan be used to load a pre-trained word embedding model, a type of\ntransfer learning.</p>\n</blockquote>\n<p>Isnt embeddings model specific? I mean to learn a representation of something we need the model that something was used to represent! so how can embeddings learned in one model can be used in another?</p>\n"}, "title": "How embeddings learned from one model can be used in another?", "content": "In the website the following explanation is provided about Embedding layer:\n\nThe Embedding layer is initialized with random weights and will learn\nan embedding for all of the words in the training dataset.\nIt is a flexible layer that can be used in a variety of ways, such as:\nIt can be used alone to learn a word embedding that can be saved and\nused in another model later. It can be used as part of a deep learning\nmodel where the embedding is learned along with the model itself. It\ncan be used to load a pre-trained word embedding model, a type of\ntransfer learning.\n\nIsnt embeddings model specific? I mean to learn a representation of something we need the model that something was used to represent! so how can embeddings learned in one model can be used in another?\n", "question_id": 37542, "answers": []}, "32377": {"link": "https://ai.stackexchange.com/questions/32377/why-do-we-multipy-context-size-with-embedding-dim-pytorch", "metadata": {"tags": ["natural-language-processing", "pytorch", "word-embedding"], "owner": {"reputation": 145, "user_id": 47305, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/8687b9a2684a1d92e3ecb5e2672033e5?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "J.Smith", "link": "https://ai.stackexchange.com/users/47305/j-smith"}, "is_answered": true, "view_count": 136, "accepted_answer_id": 32378, "answer_count": 1, "score": 3, "last_activity_date": 1636794678, "creation_date": 1636719808, "last_edit_date": 1636794649, "question_id": 32377, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/32377/why-do-we-multipy-context-size-with-embedding-dim-pytorch", "title": "Why do we multipy context_size with embedding_dim? (PyTorch)", "body": "<p>I've been using Tensorflow and just started learning PyTorch. I was following the tutorial: <a href=\"https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py\" rel=\"nofollow noreferrer\">https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py</a></p>\n<p>Where we try to create an n-gram language model. However, there's something I don't understand.</p>\n<pre><code>class NGramLanguageModeler(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, context_size):\n        super(NGramLanguageModeler, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n        self.linear2 = nn.Linear(128, vocab_size)\n</code></pre>\n<p>at <code>self.linear1 = nn.Linear(context_size * embedding_dim, 128)</code> why did we multiply embedding_dim with context_size? Isn't the embedding_dim input size? So why do we multiply it by the context size?</p>\n"}, "title": "Why do we multipy context_size with embedding_dim? (PyTorch)", "content": "I've been using Tensorflow and just started learning PyTorch. I was following the tutorial: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py\nWhere we try to create an n-gram language model. However, there's something I don't understand.\nclass NGramLanguageModeler(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, context_size):\n        super(NGramLanguageModeler, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n        self.linear2 = nn.Linear(128, vocab_size)\n\nat self.linear1 = nn.Linear(context_size * embedding_dim, 128) why did we multiply embedding_dim with context_size? Isn't the embedding_dim input size? So why do we multiply it by the context size?\n", "question_id": 32377, "answers": []}, "23162": {"link": "https://ai.stackexchange.com/questions/23162/when-to-convert-data-to-word-embeddings-in-nlp", "metadata": {"tags": ["natural-language-processing", "data-preprocessing", "word-embedding"], "owner": {"reputation": 2215, "user_id": 12201, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "chessprogrammer", "link": "https://ai.stackexchange.com/users/12201/chessprogrammer"}, "is_answered": true, "view_count": 138, "accepted_answer_id": 23250, "answer_count": 3, "score": 3, "last_activity_date": 1598362606, "creation_date": 1597988261, "last_edit_date": 1598019124, "question_id": 23162, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/23162/when-to-convert-data-to-word-embeddings-in-nlp", "title": "When to convert data to word embeddings in NLP", "body": "<p>When training a network using word embeddings, it is standard to add an embedding layer to first convert the input vector to the embeddings.</p>\n<p>However, assuming the embeddings are pre-trained and frozen, there is another option. We could simply preprocess the training data prior to giving it to the model so that it is already converted to the embeddings. This will speed up training, since this conversion need only be performed once, as opposed to on the fly for each epoch.</p>\n<p>Thus, the second option seems better. But the first choice seems more common. Assuming the embeddings are pre-trained and frozen, is there a reason I might choose the first option over the second?</p>\n"}, "title": "When to convert data to word embeddings in NLP", "content": "When training a network using word embeddings, it is standard to add an embedding layer to first convert the input vector to the embeddings.\nHowever, assuming the embeddings are pre-trained and frozen, there is another option. We could simply preprocess the training data prior to giving it to the model so that it is already converted to the embeddings. This will speed up training, since this conversion need only be performed once, as opposed to on the fly for each epoch.\nThus, the second option seems better. But the first choice seems more common. Assuming the embeddings are pre-trained and frozen, is there a reason I might choose the first option over the second?\n", "question_id": 23162, "answers": []}, "40443": {"link": "https://ai.stackexchange.com/questions/40443/would-initializing-transformers-with-pre-trained-word-embedding-speed-up-the-tra", "metadata": {"tags": ["deep-learning", "transformer", "word-embedding"], "owner": {"reputation": 131, "user_id": 71784, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/4fc0e719d2cd9bf76949198a8d82ee9b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "floyd", "link": "https://ai.stackexchange.com/users/71784/floyd"}, "is_answered": true, "view_count": 97, "answer_count": 1, "score": 3, "last_activity_date": 1684150504, "creation_date": 1684020472, "question_id": 40443, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40443/would-initializing-transformers-with-pre-trained-word-embedding-speed-up-the-tra", "title": "Would initializing transformers with pre-trained word embedding speed up the training of transformers?", "body": "<p>I read the answers for that question <a href=\"https://ai.stackexchange.com/questions/26235/what-kind-of-word-embedding-is-used-in-the-original-transformer\">What kind of word embedding is used in the original transformer?</a>. It says that transforms like bert start the first word embedding layer with random values.</p>\n<p>Initializing the first word embedding layer in transformers with random values works fine but Wouldn't initializing transformers with pre-trained word embedding speed up the training of transformers?</p>\n<p>Isn't starting with pre-trained word embedding(vectors that have semantic meaning) is better than starting from scratch?</p>\n<p>I am not talking about the performance. I am talking about the speed of the training.</p>\n"}, "title": "Would initializing transformers with pre-trained word embedding speed up the training of transformers?", "content": "I read the answers for that question What kind of word embedding is used in the original transformer?. It says that transforms like bert start the first word embedding layer with random values.\nInitializing the first word embedding layer in transformers with random values works fine but Wouldn't initializing transformers with pre-trained word embedding speed up the training of transformers?\nIsn't starting with pre-trained word embedding(vectors that have semantic meaning) is better than starting from scratch?\nI am not talking about the performance. I am talking about the speed of the training.\n", "question_id": 40443, "answers": []}, "13926": {"link": "https://ai.stackexchange.com/questions/13926/can-elmo-embeddings-be-used-to-find-the-n-most-similar-sentences", "metadata": {"tags": ["natural-language-processing", "word-embedding"], "owner": {"reputation": 61, "user_id": 27915, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/5aedd802dca0d3b7397664dfac645ae8?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "brokentransform", "link": "https://ai.stackexchange.com/users/27915/brokentransform"}, "is_answered": true, "view_count": 1187, "answer_count": 2, "score": 3, "last_activity_date": 1565738172, "creation_date": 1565639880, "last_edit_date": 1565733049, "question_id": 13926, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/13926/can-elmo-embeddings-be-used-to-find-the-n-most-similar-sentences", "title": "Can ELMO embeddings be used to find the n most similar sentences?", "body": "<p>Assume I have a list of sentences, which is just a list of strings. I need a way of comparing some input string against those sentences to find the most similar. Can <a href=\"https://allennlp.org/elmo\" rel=\"nofollow noreferrer\">ELMO embeddings</a> be used to train a model that can give you the <span class=\"math-container\">$n$</span> most similar sentences to an input string?</p>\n\n<p>For reference, gensim provides a <a href=\"https://radimrehurek.com/gensim/models/doc2vec.html\" rel=\"nofollow noreferrer\">doc2vec</a> model that can be trained on a list of strings, then you can use the trained model to infer a vector from some input string. That inferred vector can then be used to find the <span class=\"math-container\">$n$</span> most similar vectors.</p>\n\n<p>Could something similar be done, but using ELMO embedding instead?</p>\n\n<p>Any guidance would be greatly appreciated. </p>\n"}, "title": "Can ELMO embeddings be used to find the n most similar sentences?", "content": "Assume I have a list of sentences, which is just a list of strings. I need a way of comparing some input string against those sentences to find the most similar. Can ELMO embeddings be used to train a model that can give you the $n$ most similar sentences to an input string?\nFor reference, gensim provides a doc2vec model that can be trained on a list of strings, then you can use the trained model to infer a vector from some input string. That inferred vector can then be used to find the $n$ most similar vectors.\nCould something similar be done, but using ELMO embedding instead?\nAny guidance would be greatly appreciated. \n", "question_id": 13926, "answers": []}, "3805": {"link": "https://ai.stackexchange.com/questions/3805/what-is-usvt-in-the-context-of-word-embeddings", "metadata": {"tags": ["natural-language-processing", "terminology", "word-embedding", "singular-value-decomposition"], "owner": {"reputation": 149, "user_id": 8325, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/qk2CB.jpg?s=256&g=1", "display_name": "Daschin", "link": "https://ai.stackexchange.com/users/8325/daschin"}, "is_answered": true, "view_count": 275, "answer_count": 1, "score": 3, "last_activity_date": 1629819596, "creation_date": 1502516378, "last_edit_date": 1629819523, "question_id": 3805, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/3805/what-is-usvt-in-the-context-of-word-embeddings", "title": "What is $USV^T$ in the context of word embeddings?", "body": "<p>Here is an excerpt from the <a href=\"http://web.stanford.edu/class/cs224n/\" rel=\"nofollow noreferrer\">notes of the first lecture</a> of the course <a href=\"http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf\" rel=\"nofollow noreferrer\">CS224n: Natural Language Processing with Deep Learning</a>.</p>\n<blockquote>\n<p>3 SVD Based Methods</p>\n<p>For this class of methods to find word embeddings (otherwise known\nas word vectors), we first loop over a massive data set and accumulate word co-occurrence counts in some form of a matrix <span class=\"math-container\">$X$</span> and then perform Singular Value Decomposition on <span class=\"math-container\">$X$</span> to get a <span class=\"math-container\">$USV^T$</span> decomposition. We then use the rows of <span class=\"math-container\">$U$</span> as the word embeddings for all words in our dictionary. Let us discuss a few choices of <span class=\"math-container\">$X$</span>.</p>\n</blockquote>\n<p>What does <span class=\"math-container\">$USV^T$</span> refer to in this context?</p>\n"}, "title": "What is $USV^T$ in the context of word embeddings?", "content": "Here is an excerpt from the notes of the first lecture of the course CS224n: Natural Language Processing with Deep Learning.\n\n3 SVD Based Methods\nFor this class of methods to find word embeddings (otherwise known\nas word vectors), we first loop over a massive data set and accumulate word co-occurrence counts in some form of a matrix $X$ and then perform Singular Value Decomposition on $X$ to get a $USV^T$ decomposition. We then use the rows of $U$ as the word embeddings for all words in our dictionary. Let us discuss a few choices of $X$.\n\nWhat does $USV^T$ refer to in this context?\n", "question_id": 3805, "answers": []}, "22413": {"link": "https://ai.stackexchange.com/questions/22413/how-can-i-create-an-embedding-layer-to-convert-words-to-a-vector-space-from-scra", "metadata": {"tags": ["neural-networks", "implementation", "word-embedding"], "owner": {"reputation": 43, "user_id": 38497, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/5918ee8b5e0c26f9837314e74cf72a0f?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "JumpJehovah", "link": "https://ai.stackexchange.com/users/38497/jumpjehovah"}, "is_answered": true, "view_count": 90, "answer_count": 1, "score": 3, "last_activity_date": 1659370685, "creation_date": 1594307055, "last_edit_date": 1594324667, "question_id": 22413, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/22413/how-can-i-create-an-embedding-layer-to-convert-words-to-a-vector-space-from-scra", "title": "How can I create an embedding layer to convert words to a vector space from scratch?", "body": "<p>For an upcoming project, I am trying to build a neural network for classifying text from scratch, without the use of libraries. This requires an embedding layer, or a way to convert words to some vector representation. I understand the gist, but I can't find any deep explanations or tutorials that don't start with importing TensorFlow. All I'm really told is that it works by context using a few surrounding words, but I don't understand exactly how.</p>\n<p>Is it much different from a classic network, with weights and biases? How does it figure out the loss?</p>\n<p>If someone could point me towards a guide to how these things work exactly I would be very grateful.</p>\n"}, "title": "How can I create an embedding layer to convert words to a vector space from scratch?", "content": "For an upcoming project, I am trying to build a neural network for classifying text from scratch, without the use of libraries. This requires an embedding layer, or a way to convert words to some vector representation. I understand the gist, but I can't find any deep explanations or tutorials that don't start with importing TensorFlow. All I'm really told is that it works by context using a few surrounding words, but I don't understand exactly how.\nIs it much different from a classic network, with weights and biases? How does it figure out the loss?\nIf someone could point me towards a guide to how these things work exactly I would be very grateful.\n", "question_id": 22413, "answers": []}, "21119": {"link": "https://ai.stackexchange.com/questions/21119/what-should-the-dimension-of-the-input-be-for-text-summarization", "metadata": {"tags": ["neural-networks", "machine-learning", "word-embedding", "text-summarization"], "owner": {"reputation": 41, "user_id": 37006, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/FkluJ.jpg?s=256&g=1", "display_name": "inquisitive", "link": "https://ai.stackexchange.com/users/37006/inquisitive"}, "is_answered": true, "view_count": 63, "answer_count": 1, "score": 3, "last_activity_date": 1602324295, "creation_date": 1589354293, "question_id": 21119, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/21119/what-should-the-dimension-of-the-input-be-for-text-summarization", "title": "What should the dimension of the input be for text summarization?", "body": "<p>I am trying to build a model for extractive text summarization using keras sequential layers. I am having a hard time trying to understand how to input my x data. Should it be an array of documents with each document containing an array of sentences? or should I further break it down to each sentence containing an array of words?</p>\n\n<p>The y input is basically a binary classification of each sentence to check whether or not they belong to the summary of the document.</p>\n\n<p>The first layer is an embedding layer and I'm using 100d Glove word embedding.</p>\n\n<p>P.s: I am new to machine learning.</p>\n"}, "title": "What should the dimension of the input be for text summarization?", "content": "I am trying to build a model for extractive text summarization using keras sequential layers. I am having a hard time trying to understand how to input my x data. Should it be an array of documents with each document containing an array of sentences? or should I further break it down to each sentence containing an array of words?\nThe y input is basically a binary classification of each sentence to check whether or not they belong to the summary of the document.\nThe first layer is an embedding layer and I'm using 100d Glove word embedding.\nP.s: I am new to machine learning.\n", "question_id": 21119, "answers": []}, "6571": {"link": "https://ai.stackexchange.com/questions/6571/how-should-the-output-layer-of-an-lstm-be-when-the-output-are-word-embeddings", "metadata": {"tags": ["natural-language-processing", "long-short-term-memory", "word-embedding", "word2vec"], "owner": {"reputation": 63, "user_id": 5558, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d2e56a8570cdb79749c7e25139fb5666?s=256&d=identicon&r=PG", "display_name": "George Sechu", "link": "https://ai.stackexchange.com/users/5558/george-sechu"}, "is_answered": true, "view_count": 256, "answer_count": 2, "score": 3, "last_activity_date": 1555454543, "creation_date": 1527671570, "last_edit_date": 1555454543, "question_id": 6571, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/6571/how-should-the-output-layer-of-an-lstm-be-when-the-output-are-word-embeddings", "title": "How should the output layer of an LSTM be when the output are word embeddings?", "body": "<p>I'm having trouble grasping how to output word embeddings from an LSTM model. I'm seeing many examples using a softmax activation function on the output, but for that I would need to output one hot vectors as long as the vocabulary (which is too long). So, should I use a linear activation function on the output to get the word embeddings directly (and then find the closest word) or is there something I'm missing here?</p>\n"}, "title": "How should the output layer of an LSTM be when the output are word embeddings?", "content": "I'm having trouble grasping how to output word embeddings from an LSTM model. I'm seeing many examples using a softmax activation function on the output, but for that I would need to output one hot vectors as long as the vocabulary (which is too long). So, should I use a linear activation function on the output to get the word embeddings directly (and then find the closest word) or is there something I'm missing here?\n", "question_id": 6571, "answers": []}, "20911": {"link": "https://ai.stackexchange.com/questions/20911/is-there-a-good-book-or-paper-on-word-embeddings", "metadata": {"tags": ["natural-language-processing", "reference-request", "word-embedding"], "owner": {"reputation": 919, "user_id": 36055, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/tS9eU.png?s=256&g=1", "display_name": "ddaedalus", "link": "https://ai.stackexchange.com/users/36055/ddaedalus"}, "is_answered": false, "view_count": 103, "answer_count": 0, "score": 3, "last_activity_date": 1590170686, "creation_date": 1588612478, "last_edit_date": 1590170686, "question_id": 20911, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/20911/is-there-a-good-book-or-paper-on-word-embeddings", "title": "Is there a good book or paper on word embeddings?", "body": "<p>Is there a good and modern book that focuses on word embeddings and their applications? It would also be ok to provide the name of a paper that provides a good overview of word embeddings.</p>\n"}, "title": "Is there a good book or paper on word embeddings?", "content": "Is there a good and modern book that focuses on word embeddings and their applications? It would also be ok to provide the name of a paper that provides a good overview of word embeddings.\n", "question_id": 20911, "answers": []}, "16553": {"link": "https://ai.stackexchange.com/questions/16553/why-embedding-layer-is-used-in-the-character-level-natural-language-processing-m", "metadata": {"tags": ["natural-language-processing", "word-embedding"], "owner": {"reputation": 323, "user_id": 31324, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-c8KtCK1F_Sc/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3refHWNaEdHohm92M3tD8_3g1JzRhA/photo.jpg?sz=256", "display_name": "Daniel Wiczew", "link": "https://ai.stackexchange.com/users/31324/daniel-wiczew"}, "is_answered": false, "view_count": 38, "answer_count": 0, "score": 3, "last_activity_date": 1573900591, "creation_date": 1573900591, "question_id": 16553, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/16553/why-embedding-layer-is-used-in-the-character-level-natural-language-processing-m", "title": "Why embedding layer is used in the character-level Natural Language Processing models", "body": "<p><strong>Problem Background</strong></p>\n\n<p>I am working with a problem, which requires a character-level, deep learning model. Previously I was working with word-level deep NLP (Natural Language Processing) models, and in these models almost always embedding encoding was used to represent given word in a lower-dimensional vector form. Furthermore, such embedding encoding allowed for putting similar words near themselves in the new lower-dimensional vector representation (i.e. man and woman vectors were near themselves in the vector space) which improved learning. Nevertheless, I often see that people use embedding encoding in character level NLP models. Even if the character-level one-hot encoding vectors are quite small in comparison to word-level one-hot encoding vectors (about 36 to 32k rows). Furthermore, there is no much correlation between characters, there is no something like \"similar characters\" in comparison to similar words, therefore some characters in comparison to other shouldn't be put near themselves. </p>\n\n<p><strong>Question</strong>\nWhy embedding encoding is used in the character-level NLP models?</p>\n"}, "title": "Why embedding layer is used in the character-level Natural Language Processing models", "content": "Problem Background\nI am working with a problem, which requires a character-level, deep learning model. Previously I was working with word-level deep NLP (Natural Language Processing) models, and in these models almost always embedding encoding was used to represent given word in a lower-dimensional vector form. Furthermore, such embedding encoding allowed for putting similar words near themselves in the new lower-dimensional vector representation (i.e. man and woman vectors were near themselves in the vector space) which improved learning. Nevertheless, I often see that people use embedding encoding in character level NLP models. Even if the character-level one-hot encoding vectors are quite small in comparison to word-level one-hot encoding vectors (about 36 to 32k rows). Furthermore, there is no much correlation between characters, there is no something like \"similar characters\" in comparison to similar words, therefore some characters in comparison to other shouldn't be put near themselves. \nQuestion\nWhy embedding encoding is used in the character-level NLP models?\n", "question_id": 16553, "answers": []}, "12896": {"link": "https://ai.stackexchange.com/questions/12896/adding-bert-embeddings-in-lstm-embedding-layer", "metadata": {"tags": ["deep-learning", "keras", "word-embedding", "long-short-term-memory", "bert"], "owner": {"reputation": 119, "user_id": 26115, "user_type": "registered", "profile_image": "https://lh4.googleusercontent.com/-NkK7LO3Iylk/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3reMvjQHK4ibc-Yr_qKHV7P-CRAmkw/mo/photo.jpg?sz=256", "display_name": "Srikant Jayaraman", "link": "https://ai.stackexchange.com/users/26115/srikant-jayaraman"}, "is_answered": true, "view_count": 8215, "answer_count": 1, "score": 2, "last_activity_date": 1572575258, "creation_date": 1560777343, "last_edit_date": 1572575258, "question_id": 12896, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/12896/adding-bert-embeddings-in-lstm-embedding-layer", "title": "Adding BERT embeddings in LSTM embedding layer", "body": "<p>I am planning to use BERT embeddings in the LSTM embedding layer instead of the usual Word2vec/Glove Embeddings. What are the possible ways to do that? </p>\n"}, "title": "Adding BERT embeddings in LSTM embedding layer", "content": "I am planning to use BERT embeddings in the LSTM embedding layer instead of the usual Word2vec/Glove Embeddings. What are the possible ways to do that? \n", "question_id": 12896, "answers": []}, "11833": {"link": "https://ai.stackexchange.com/questions/11833/do-we-have-cross-language-vector-space-for-word-embedding", "metadata": {"tags": ["natural-language-processing", "word-embedding"], "owner": {"reputation": 51, "user_id": 23855, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/54554a8e54e55b87cda7bb7e34f2c13b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "gloomyson", "link": "https://ai.stackexchange.com/users/23855/gloomyson"}, "is_answered": true, "view_count": 172, "answer_count": 2, "score": 2, "last_activity_date": 1555599627, "creation_date": 1555338928, "last_edit_date": 1555453362, "question_id": 11833, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/11833/do-we-have-cross-language-vector-space-for-word-embedding", "title": "Do we have cross-language vector space for word embedding?", "body": "<p>Do we have cross-language vector space for word embedding?</p>\n\n<p>When measure similarity for apple/Pomme/mela/Lacus/\u82f9\u679c/\u308a\u3093\u3054, they should be the same</p>\n\n<p>If would be great if there's available internet service of neuron network which already be trained by multiple language</p>\n"}, "title": "Do we have cross-language vector space for word embedding?", "content": "Do we have cross-language vector space for word embedding?\nWhen measure similarity for apple/Pomme/mela/Lacus/\u82f9\u679c/\u308a\u3093\u3054, they should be the same\nIf would be great if there's available internet service of neuron network which already be trained by multiple language\n", "question_id": 11833, "answers": []}, "28065": {"link": "https://ai.stackexchange.com/questions/28065/why-are-bert-embeddings-interpreted-as-representations-of-the-corresponding-word", "metadata": {"tags": ["natural-language-processing", "word-embedding", "bert", "embeddings"], "owner": {"reputation": 21, "user_id": 47609, "user_type": "unregistered", "profile_image": "https://www.gravatar.com/avatar/9ea910427dcd85090e0c06c19bfd7d38?s=256&d=identicon&r=PG", "display_name": "EmbeddingEnthusiast", "link": "https://ai.stackexchange.com/users/47609/embeddingenthusiast"}, "is_answered": true, "view_count": 65, "answer_count": 1, "score": 2, "last_activity_date": 1683119241, "creation_date": 1622645999, "last_edit_date": 1622716789, "question_id": 28065, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/28065/why-are-bert-embeddings-interpreted-as-representations-of-the-corresponding-word", "title": "Why are BERT embeddings interpreted as representations of the corresponding words?", "body": "<p>It's often assumed in literature that BERT embeddings are contextual representations of the corresponding word. That is, if the 5th word is &quot;cold&quot;, then the 5th BERT embedding is a representation of that word, using context to disambiguate the word (e.g. determine whether it's to do with the illness or temperature).</p>\n<p>However, because of the self-attention encoder layers, this embedding can in theory incorporate information from any of the other words in the text. BERT is trained using masked language modelling (MLM), which would encourage each embedding to learn enough to predict the corresponding word. But why wouldn't it contain additional information from other words? In other words, is there any reason to believe the BERT embeddings for different words contain well-separated information?</p>\n"}, "title": "Why are BERT embeddings interpreted as representations of the corresponding words?", "content": "It's often assumed in literature that BERT embeddings are contextual representations of the corresponding word. That is, if the 5th word is \"cold\", then the 5th BERT embedding is a representation of that word, using context to disambiguate the word (e.g. determine whether it's to do with the illness or temperature).\nHowever, because of the self-attention encoder layers, this embedding can in theory incorporate information from any of the other words in the text. BERT is trained using masked language modelling (MLM), which would encourage each embedding to learn enough to predict the corresponding word. But why wouldn't it contain additional information from other words? In other words, is there any reason to believe the BERT embeddings for different words contain well-separated information?\n", "question_id": 28065, "answers": []}, "13045": {"link": "https://ai.stackexchange.com/questions/13045/how-does-fasttext-support-online-learning", "metadata": {"tags": ["classification", "word-embedding", "online-learning", "incremental-learning"], "owner": {"reputation": 65, "user_id": 20780, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/Cq4A7.jpg?s=256&g=1", "display_name": "Alfonso", "link": "https://ai.stackexchange.com/users/20780/alfonso"}, "is_answered": true, "view_count": 359, "answer_count": 1, "score": 2, "last_activity_date": 1692478870, "creation_date": 1561451582, "last_edit_date": 1561470891, "question_id": 13045, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/13045/how-does-fasttext-support-online-learning", "title": "How does FastText support online learning?", "body": "<p>I'm using FastText pre-trained-embedding for tackling a classification task, but I saw it supports also online training (incremental training) for adding domain-specific corpus.</p>\n\n<p>How does it work? </p>\n\n<p>As far as I know, starting from the \"model.bin\" file it retrains the model only on the new corpus updating the old word-vectors, is it right? </p>\n"}, "title": "How does FastText support online learning?", "content": "I'm using FastText pre-trained-embedding for tackling a classification task, but I saw it supports also online training (incremental training) for adding domain-specific corpus.\nHow does it work? \nAs far as I know, starting from the \"model.bin\" file it retrains the model only on the new corpus updating the old word-vectors, is it right? \n", "question_id": 13045, "answers": []}, "11178": {"link": "https://ai.stackexchange.com/questions/11178/what-do-the-vectors-of-the-center-and-outside-word-look-like-in-word2vec", "metadata": {"tags": ["natural-language-processing", "word2vec", "word-embedding"], "owner": {"reputation": 31, "user_id": 23120, "user_type": "unregistered", "profile_image": "https://www.gravatar.com/avatar/eada06eece702339c6f4944c7a50f1bc?s=256&d=identicon&r=PG", "display_name": "aiguy123", "link": "https://ai.stackexchange.com/users/23120/aiguy123"}, "is_answered": true, "view_count": 359, "answer_count": 1, "score": 2, "last_activity_date": 1555453586, "creation_date": 1552410604, "last_edit_date": 1555453586, "question_id": 11178, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/11178/what-do-the-vectors-of-the-center-and-outside-word-look-like-in-word2vec", "title": "What do the vectors of the center and outside word look like in word2vec?", "body": "<p>In <a href=\"https://en.wikipedia.org/wiki/Word2vec\" rel=\"nofollow noreferrer\">word2vec</a>, the task is to learn to predict which words are most likely to be near each other in some long corpus of text. For each word <span class=\"math-container\">$c$</span> in the corpus, the model outputs the probability distribution <span class=\"math-container\">$P(O=o|C=c)$</span> of how likely each other word <span class=\"math-container\">$o$</span> in the vocabulary is to be within a certain number of words away from <span class=\"math-container\">$c$</span>. We call <span class=\"math-container\">$c$</span> the \"center word\" and <span class=\"math-container\">$o$</span> the \"outside word\".</p>\n\n<p>We choose the softmax distribution as the output of our model: <span class=\"math-container\">$$P(O=o|C=c) = \\frac{\\exp(\\textbf{u}_{0}^{T} \\textbf{v}_{c})}{\\sum_{w \\in \\text{Vocab}} \\exp(\\textbf{u}_{w}^{T} \\textbf{v}_c)}$$</span></p>\n\n<p>where <span class=\"math-container\">$\\textbf{u}_0$</span> and <span class=\"math-container\">$\\textbf{v}_c$</span> are vectors that represent the outside and center words respectively. </p>\n\n<blockquote>\n  <p><strong>Question.</strong> What do the vectors  <span class=\"math-container\">$\\textbf{u}_0$</span> and <span class=\"math-container\">$\\textbf{v}_c$</span> look like? Are they just one-hot-encodings? Do we need to learn them\n  too? Why is this useful?</p>\n</blockquote>\n"}, "title": "What do the vectors of the center and outside word look like in word2vec?", "content": "In word2vec, the task is to learn to predict which words are most likely to be near each other in some long corpus of text. For each word $c$ in the corpus, the model outputs the probability distribution $P(O=o|C=c)$ of how likely each other word $o$ in the vocabulary is to be within a certain number of words away from $c$. We call $c$ the \"center word\" and $o$ the \"outside word\".\nWe choose the softmax distribution as the output of our model: $$P(O=o|C=c) = \\frac{\\exp(\\textbf{u}_{0}^{T} \\textbf{v}_{c})}{\\sum_{w \\in \\text{Vocab}} \\exp(\\textbf{u}_{w}^{T} \\textbf{v}_c)}$$\nwhere $\\textbf{u}_0$ and $\\textbf{v}_c$ are vectors that represent the outside and center words respectively. \n\nQuestion. What do the vectors  $\\textbf{u}_0$ and $\\textbf{v}_c$ look like? Are they just one-hot-encodings? Do we need to learn them\n  too? Why is this useful?\n\n", "question_id": 11178, "answers": []}, "24856": {"link": "https://ai.stackexchange.com/questions/24856/can-one-hot-vectors-be-used-as-inputs-for-recurrent-neural-networks", "metadata": {"tags": ["natural-language-processing", "recurrent-neural-networks", "long-short-term-memory", "word-embedding", "gated-recurrent-unit"], "owner": {"reputation": 2215, "user_id": 12201, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "chessprogrammer", "link": "https://ai.stackexchange.com/users/12201/chessprogrammer"}, "is_answered": false, "view_count": 62, "answer_count": 0, "score": 2, "last_activity_date": 1606356098, "creation_date": 1606342689, "last_edit_date": 1606356098, "question_id": 24856, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/24856/can-one-hot-vectors-be-used-as-inputs-for-recurrent-neural-networks", "title": "Can One-Hot Vectors be used as Inputs for Recurrent Neural Networks?", "body": "<p>When using an RNN to encode a sentence, one normally takes each word, passes it through an embedding layer, and then uses the dense embedding as the input into the RNN.</p>\n<p>Lets say instead of using dense embeddings, I used a one-hot representation for each word, and fed that sequence into the RNN. My question is which of these two outcomes is correct:</p>\n<ol>\n<li><p>Due to the way in which an RNN combines inputs, since these vectors are all orthogonal, absolutely nothing can be combined, and the entire setup does not make sense.</p>\n</li>\n<li><p>The setup does make sense and it will still work, but not be as effective as using a dense embedding.</p>\n</li>\n</ol>\n<p>I know I could run an experiment and see what happens, but this is fundamentally a theoretical question, and I would appreciate if someone could clarify so that I have a better understanding of how RNNs combine inputs. I suspect that the answer to this question would be the same regardless of whether we are discussing a vanilla RNN or an LSTM or  GRU, but if that is not the case, please explain why.</p>\n<p>Thank you.</p>\n"}, "title": "Can One-Hot Vectors be used as Inputs for Recurrent Neural Networks?", "content": "When using an RNN to encode a sentence, one normally takes each word, passes it through an embedding layer, and then uses the dense embedding as the input into the RNN.\nLets say instead of using dense embeddings, I used a one-hot representation for each word, and fed that sequence into the RNN. My question is which of these two outcomes is correct:\n\nDue to the way in which an RNN combines inputs, since these vectors are all orthogonal, absolutely nothing can be combined, and the entire setup does not make sense.\n\nThe setup does make sense and it will still work, but not be as effective as using a dense embedding.\n\n\nI know I could run an experiment and see what happens, but this is fundamentally a theoretical question, and I would appreciate if someone could clarify so that I have a better understanding of how RNNs combine inputs. I suspect that the answer to this question would be the same regardless of whether we are discussing a vanilla RNN or an LSTM or  GRU, but if that is not the case, please explain why.\nThank you.\n", "question_id": 24856, "answers": []}, "15490": {"link": "https://ai.stackexchange.com/questions/15490/doubt-on-formulating-cost-function-for-glove", "metadata": {"tags": ["natural-language-processing", "word-embedding", "glove"], "owner": {"reputation": 383, "user_id": 27548, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6e49e834d61b24d841ec7a8c4b5ae7a5?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Shirish Kulhari", "link": "https://ai.stackexchange.com/users/27548/shirish-kulhari"}, "is_answered": false, "view_count": 29, "answer_count": 0, "score": 2, "last_activity_date": 1568739996, "creation_date": 1568736012, "last_edit_date": 1568739996, "question_id": 15490, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/15490/doubt-on-formulating-cost-function-for-glove", "title": "Doubt on formulating cost function for GloVe", "body": "<p>I'm reading the notes <a href=\"https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf\" rel=\"nofollow noreferrer\">here</a> and have a doubt on page 2 (\"Least squares objective\" section). The probability of a word <span class=\"math-container\">$j$</span> occurring in the context of word <span class=\"math-container\">$i$</span> is <span class=\"math-container\">$$Q_{ij}=\\frac{\\exp(u_j^Tv_i)}{\\sum_{w=1}^W\\exp(u_w^Tv_i)}$$</span></p>\n\n<p>The notes read:</p>\n\n<blockquote>\n  <p>Training proceeds in an on-line, stochastic fashion, but the implied global cross-entropy loss can be calculated as <span class=\"math-container\">$$J=-\\sum_{i\\in corpus}\\sum_{j\\in context(i)}\\log Q_{ij}$$</span>\n  As the same words <span class=\"math-container\">$i$</span> and <span class=\"math-container\">$j$</span> can occur multiple times in the corpus, it is more efficient to first group together the same values for <span class=\"math-container\">$i$</span> and <span class=\"math-container\">$j$</span>:\n  <span class=\"math-container\">$$J=-\\sum_{i=1}^W\\sum_{j=1}^WX_{ij}\\log(Q_{ij})$$</span></p>\n</blockquote>\n\n<p>where <span class=\"math-container\">$X_{ij}$</span> is the total number of times <span class=\"math-container\">$j$</span> occurs in the context of <span class=\"math-container\">$i$</span> and the value of co-occuring frequency is given by the co-occurence matrix <span class=\"math-container\">$X$</span>. This much is clear. But then the author states that the denominator of <span class=\"math-container\">$Q_{ij}$</span> is too expensive to compute, so the cross entropy loss won't work.</p>\n\n<blockquote>\n  <p>Instead, we use a least square objective in which the normalization factors in <span class=\"math-container\">$P$</span> and <span class=\"math-container\">$Q$</span> are discarded:\n  <span class=\"math-container\">$$\\hat J=\\sum_{i=1}^W\\sum_{j=1}^WX_i(\\hat P_{ij}-\\hat Q_{ij})^2$$</span>\n  where <span class=\"math-container\">$\\hat P_{ij}=X_{ij}$</span> and <span class=\"math-container\">$\\hat Q_{ij}=\\exp(u_j^Tv_i)$</span> are the unnormalized distributions.</p>\n</blockquote>\n\n<p><span class=\"math-container\">$X_i=\\sum_kX_{ik}$</span> is the number of times any word appears in the context of <span class=\"math-container\">$i$</span>. I don't understand this part. <em>Why have we introduced <span class=\"math-container\">$X_i$</span> out of nowhere? How is <span class=\"math-container\">$\\hat P_{ij}$</span> \"unnormalized\"? Is there a tradeoff in switching from softmax to MSE?</em> </p>\n\n<p>(As far as I know, softmax made total sense in skip gram because we were calculating scores corresponding to different words (discrete possibilities) and matching the predicted output to the actual word - similar to a classification problem, so softmax makes sense.)</p>\n"}, "title": "Doubt on formulating cost function for GloVe", "content": "I'm reading the notes here and have a doubt on page 2 (\"Least squares objective\" section). The probability of a word $j$ occurring in the context of word $i$ is $$Q_{ij}=\\frac{\\exp(u_j^Tv_i)}{\\sum_{w=1}^W\\exp(u_w^Tv_i)}$$\nThe notes read:\n\nTraining proceeds in an on-line, stochastic fashion, but the implied global cross-entropy loss can be calculated as $$J=-\\sum_{i\\in corpus}\\sum_{j\\in context(i)}\\log Q_{ij}$$\n  As the same words $i$ and $j$ can occur multiple times in the corpus, it is more efficient to first group together the same values for $i$ and $j$:\n  $$J=-\\sum_{i=1}^W\\sum_{j=1}^WX_{ij}\\log(Q_{ij})$$\n\nwhere $X_{ij}$ is the total number of times $j$ occurs in the context of $i$ and the value of co-occuring frequency is given by the co-occurence matrix $X$. This much is clear. But then the author states that the denominator of $Q_{ij}$ is too expensive to compute, so the cross entropy loss won't work.\n\nInstead, we use a least square objective in which the normalization factors in $P$ and $Q$ are discarded:\n  $$\\hat J=\\sum_{i=1}^W\\sum_{j=1}^WX_i(\\hat P_{ij}-\\hat Q_{ij})^2$$\n  where $\\hat P_{ij}=X_{ij}$ and $\\hat Q_{ij}=\\exp(u_j^Tv_i)$ are the unnormalized distributions.\n\n$X_i=\\sum_kX_{ik}$ is the number of times any word appears in the context of $i$. I don't understand this part. Why have we introduced $X_i$ out of nowhere? How is $\\hat P_{ij}$ \"unnormalized\"? Is there a tradeoff in switching from softmax to MSE? \n(As far as I know, softmax made total sense in skip gram because we were calculating scores corresponding to different words (discrete possibilities) and matching the predicted output to the actual word - similar to a classification problem, so softmax makes sense.)\n", "question_id": 15490, "answers": []}, "40390": {"link": "https://ai.stackexchange.com/questions/40390/what-is-the-intuition-behind-position-encoding", "metadata": {"tags": ["transformer", "word-embedding", "positional-encoding"], "owner": {"reputation": 811, "user_id": 25362, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/tVWLM.png?s=256&g=1", "display_name": "Hans-Peter Stricker", "link": "https://ai.stackexchange.com/users/25362/hans-peter-stricker"}, "is_answered": true, "view_count": 85, "answer_count": 1, "score": 1, "last_activity_date": 1683804075, "creation_date": 1683728749, "last_edit_date": 1683804075, "question_id": 40390, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40390/what-is-the-intuition-behind-position-encoding", "title": "What is the intuition behind position-encoding?", "body": "<p>It is clear that word positions are essential for the meaning of a sentence, and so are essential when feeding a sentence (= sequence of words) as a matrix of word embedding vectors into a transformer. I also have understood roughly how positions are encoded, but what I did not understand in the very begining is <strong>why</strong> just creating a matrix consisting of a number of word embedding vectors (as columns) with the columns in the same order as the words in the sentence does not suffice. A matrix with permutated columns obviously would &quot;mean&quot; something different - and sometimes nothing at all - like a pixel matrix would change its &quot;meaning&quot; when we permutated some pixel columns. Is there an intuitive explanation why position encoding vectors have to be added to the word embedding vectors. Why and how would the position information (which is still present in the input matrix) get lost otherwise?</p>\n<p><strike>(I have learned in the meanwhile that transformers are in general <strong>not</strong> permutation invariant, but that there are transformers that <strong>are</strong>: <a href=\"https://arxiv.org/abs/1810.00825\" rel=\"nofollow noreferrer\">set transformers</a>.)</strike></p>\n"}, "title": "What is the intuition behind position-encoding?", "content": "It is clear that word positions are essential for the meaning of a sentence, and so are essential when feeding a sentence (= sequence of words) as a matrix of word embedding vectors into a transformer. I also have understood roughly how positions are encoded, but what I did not understand in the very begining is why just creating a matrix consisting of a number of word embedding vectors (as columns) with the columns in the same order as the words in the sentence does not suffice. A matrix with permutated columns obviously would \"mean\" something different - and sometimes nothing at all - like a pixel matrix would change its \"meaning\" when we permutated some pixel columns. Is there an intuitive explanation why position encoding vectors have to be added to the word embedding vectors. Why and how would the position information (which is still present in the input matrix) get lost otherwise?\n(I have learned in the meanwhile that transformers are in general not permutation invariant, but that there are transformers that are: set transformers.)\n", "question_id": 40390, "answers": []}, "40273": {"link": "https://ai.stackexchange.com/questions/40273/what-information-does-the-word-embedding-in-transformers-will-encode-about-the-w", "metadata": {"tags": ["natural-language-processing", "transformer", "word-embedding"], "owner": {"reputation": 13, "user_id": 70215, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-u3Z2DRW-Rx4/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclE_tLH4QZYrHUq2gcd9DDG8WqHGA/photo.jpg?sz=256", "display_name": "oliver.c", "link": "https://ai.stackexchange.com/users/70215/oliver-c"}, "is_answered": true, "view_count": 67, "answer_count": 1, "score": 1, "last_activity_date": 1683024169, "creation_date": 1683022548, "question_id": 40273, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40273/what-information-does-the-word-embedding-in-transformers-will-encode-about-the-w", "title": "What information does the word embedding in Transformers will encode about the word when analysed outside of the model?", "body": "<p>Word2vec and similar architectures create word embedding vectors as a byproduct from a supervised learning task, where they need to predict the correct context word. Consequently, the inner representation of words inside this network will preserve some form of proximity-based word similarity based on the used corpus. When extracted, we can observe this via measuring cosine similarity between words, which will result in values close to 1 for words often occurring in each other's proximity and close to -1 for words that are highly infrequent together.</p>\n<p>Thus, I would consider the word2vec embedding vectors to be quite interpretable regarding their meaning. What about transformers?</p>\n<p>Transformers produce a similar inner representation of words, but than they alter them and recombine them through the attention mechanism multiple times, in order to solve the seq2seq learning task. What will the initial embedding before the first encoding really mean, if anything? Do they have any value when separated from the transformer? Like for example, the vectors generated in a word2vec model can be extracted and used in a downstream task. Is it reasonable to use the embedding vectors from a transformer for any downstream task?</p>\n"}, "title": "What information does the word embedding in Transformers will encode about the word when analysed outside of the model?", "content": "Word2vec and similar architectures create word embedding vectors as a byproduct from a supervised learning task, where they need to predict the correct context word. Consequently, the inner representation of words inside this network will preserve some form of proximity-based word similarity based on the used corpus. When extracted, we can observe this via measuring cosine similarity between words, which will result in values close to 1 for words often occurring in each other's proximity and close to -1 for words that are highly infrequent together.\nThus, I would consider the word2vec embedding vectors to be quite interpretable regarding their meaning. What about transformers?\nTransformers produce a similar inner representation of words, but than they alter them and recombine them through the attention mechanism multiple times, in order to solve the seq2seq learning task. What will the initial embedding before the first encoding really mean, if anything? Do they have any value when separated from the transformer? Like for example, the vectors generated in a word2vec model can be extracted and used in a downstream task. Is it reasonable to use the embedding vectors from a transformer for any downstream task?\n", "question_id": 40273, "answers": []}, "28089": {"link": "https://ai.stackexchange.com/questions/28089/is-categorical-encoding-a-type-of-word-embedding", "metadata": {"tags": ["comparison", "terminology", "word-embedding", "categorical-data", "one-hot-encoding"], "owner": {"reputation": 3571, "user_id": 18758, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/339397e23b4e4cc78aa36e2b126252c7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "hanugm", "link": "https://ai.stackexchange.com/users/18758/hanugm"}, "is_answered": true, "view_count": 190, "accepted_answer_id": 28097, "answer_count": 1, "score": 1, "last_activity_date": 1622938660, "creation_date": 1622777239, "last_edit_date": 1622938660, "question_id": 28089, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/28089/is-categorical-encoding-a-type-of-word-embedding", "title": "Is categorical encoding a type of word embedding?", "body": "<p>Word embedding refers to the techniques in which a word is represented by a vector. There are also <em>integer encoding</em> and <em>one-hot encoding</em>, which I will collectively call <a href=\"https://datascience.stackexchange.com/questions/tagged/categorical-encoding\"><em>categorical encoding</em></a>.</p>\n<p>I can see no fundamental difference between the categorical encoding and word embedding at a fundamental level. They may be different at an application level.</p>\n<p>Is it true that categorical encoding is a type of word embedding? And are different names solely due to the task in which apply the technique?</p>\n"}, "title": "Is categorical encoding a type of word embedding?", "content": "Word embedding refers to the techniques in which a word is represented by a vector. There are also integer encoding and one-hot encoding, which I will collectively call categorical encoding.\nI can see no fundamental difference between the categorical encoding and word embedding at a fundamental level. They may be different at an application level.\nIs it true that categorical encoding is a type of word embedding? And are different names solely due to the task in which apply the technique?\n", "question_id": 28089, "answers": []}, "15676": {"link": "https://ai.stackexchange.com/questions/15676/why-is-embedding-important-in-nlp-and-how-does-autoencoder-work", "metadata": {"tags": ["deep-learning", "natural-language-processing", "tensorflow", "word-embedding", "hidden-layers"], "owner": {"reputation": 1283, "user_id": 2844, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/hnwBo.jpg?s=256&g=1", "display_name": "Dee", "link": "https://ai.stackexchange.com/users/2844/dee"}, "is_answered": true, "view_count": 317, "accepted_answer_id": 15689, "answer_count": 2, "score": 1, "last_activity_date": 1570007399, "creation_date": 1569898798, "last_edit_date": 1569921396, "question_id": 15676, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/15676/why-is-embedding-important-in-nlp-and-how-does-autoencoder-work", "title": "Why is embedding important in NLP, and how does autoencoder work?", "body": "<p>People say <strong>embedding</strong> is necessary in NLP because if using just the word indices, the <strong>efficiency is not high</strong> as <strong>similar words are supposed to be related to each other</strong>. However, I still don't truly get it why.</p>\n\n<p>The <strong>subword-based embedding</strong> (aka syllable-based embedding) is understandable, for example:</p>\n\n<pre><code>biology   --&gt; bio-lo-gy\nbiologist --&gt; bio-lo-gist\n</code></pre>\n\n<p>For the 2 words above, when turning them into syllable-based embeddings, it's good because the 2 words will be related to each other due to the sharing syllables: <code>bio</code>, and <code>lo</code>.</p>\n\n<p>However, it's hard to understand the <code>autoencoder</code>, it turns an index value into vector, then feed these vectors to DNN. Autoencoder can turn vectors back to words too.</p>\n\n<p><strong>How does autoencoder make words related to each other?</strong></p>\n"}, "title": "Why is embedding important in NLP, and how does autoencoder work?", "content": "People say embedding is necessary in NLP because if using just the word indices, the efficiency is not high as similar words are supposed to be related to each other. However, I still don't truly get it why.\nThe subword-based embedding (aka syllable-based embedding) is understandable, for example:\nbiology   --> bio-lo-gy\nbiologist --> bio-lo-gist\n\nFor the 2 words above, when turning them into syllable-based embeddings, it's good because the 2 words will be related to each other due to the sharing syllables: bio, and lo.\nHowever, it's hard to understand the autoencoder, it turns an index value into vector, then feed these vectors to DNN. Autoencoder can turn vectors back to words too.\nHow does autoencoder make words related to each other?\n", "question_id": 15676, "answers": []}, "5285": {"link": "https://ai.stackexchange.com/questions/5285/how-is-the-word-embedding-represented-in-the-paper-recurrent-neural-network-bas", "metadata": {"tags": ["natural-language-processing", "recurrent-neural-networks", "word-embedding", "papers"], "owner": {"reputation": 223, "user_id": 12691, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/f24d58921281bdfe8f4792710e9dd162?s=256&d=identicon&r=PG", "display_name": "Ziemo", "link": "https://ai.stackexchange.com/users/12691/ziemo"}, "is_answered": true, "view_count": 214, "accepted_answer_id": 5293, "answer_count": 1, "score": 1, "last_activity_date": 1555454110, "creation_date": 1518440753, "last_edit_date": 1555454110, "question_id": 5285, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/5285/how-is-the-word-embedding-represented-in-the-paper-recurrent-neural-network-bas", "title": "How is the word embedding represented in the paper &quot;Recurrent neural network based language model&quot;?", "body": "<p>I'm reading <em>\"<a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" rel=\"nofollow noreferrer\">Recurrent neural network based language model</a>\"</em> of Mikolov et al. (2010). Although the article is straight forward, I'm not sure how word embedding <span class=\"math-container\">$w(t)$</span> is obtained:</p>\n\n<p><a href=\"https://i.stack.imgur.com/6i54I.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/6i54I.png\" alt=\"enter image description here\"></a></p>\n\n<p>The reason I wonder is that in the classic \"<em><a href=\"http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\" rel=\"nofollow noreferrer\">A Neural Probabilistic Language Model</a></em>\" Bengio et al. (2003) - they used separate embedding vector for representing each word and it was somehow \"semi-layer\", meaning - it haven't contains non-linearity, but they did update word embeddings during the back-propagation.</p>\n\n<p>In Mikolov approach though, I assume they used simple one-hot vector, where each feature represent presence of each word. If we represent that's way single word input (like was in the Mikolov's paper) - that vector become all-zeros except single one.</p>\n\n<p>Is that correct?</p>\n"}, "title": "How is the word embedding represented in the paper &quot;Recurrent neural network based language model&quot;?", "content": "I'm reading \"Recurrent neural network based language model\" of Mikolov et al. (2010). Although the article is straight forward, I'm not sure how word embedding $w(t)$ is obtained:\n\nThe reason I wonder is that in the classic \"A Neural Probabilistic Language Model\" Bengio et al. (2003) - they used separate embedding vector for representing each word and it was somehow \"semi-layer\", meaning - it haven't contains non-linearity, but they did update word embeddings during the back-propagation.\nIn Mikolov approach though, I assume they used simple one-hot vector, where each feature represent presence of each word. If we represent that's way single word input (like was in the Mikolov's paper) - that vector become all-zeros except single one.\nIs that correct?\n", "question_id": 5285, "answers": []}, "39021": {"link": "https://ai.stackexchange.com/questions/39021/whats-computationally-more-efficient-between-bag-of-words-representation-and-ba", "metadata": {"tags": ["machine-learning", "natural-language-processing", "word-embedding"], "owner": {"reputation": 35, "user_id": 63777, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/ALm5wu3CgIUuBgH7629IHWH1TYBdxwErAQ891TccOKjY=k-s256", "display_name": "Filippo Colombi", "link": "https://ai.stackexchange.com/users/63777/filippo-colombi"}, "is_answered": true, "view_count": 30, "accepted_answer_id": 39022, "answer_count": 1, "score": 1, "last_activity_date": 1675351407, "creation_date": 1675350271, "question_id": 39021, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39021/whats-computationally-more-efficient-between-bag-of-words-representation-and-ba", "title": "What&#39;s computationally more efficient between bag-of-words representation and bag-of-ngrams representation, with special regard to words order?", "body": "<p>I cannot figure out what is more computationally efficient between the two representations mentioned in my question in terms of training time and the amount of data required. Especially, when it comes to considering the order of the words in a sentence.</p>\n"}, "title": "What&#39;s computationally more efficient between bag-of-words representation and bag-of-ngrams representation, with special regard to words order?", "content": "I cannot figure out what is more computationally efficient between the two representations mentioned in my question in terms of training time and the amount of data required. Especially, when it comes to considering the order of the words in a sentence.\n", "question_id": 39021, "answers": []}, "32715": {"link": "https://ai.stackexchange.com/questions/32715/what-exactly-is-embedding-layer-used-in-rnn-encoders", "metadata": {"tags": ["deep-learning", "recurrent-neural-networks", "pytorch", "word-embedding"], "owner": {"reputation": 3571, "user_id": 18758, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/339397e23b4e4cc78aa36e2b126252c7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "hanugm", "link": "https://ai.stackexchange.com/users/18758/hanugm"}, "is_answered": true, "view_count": 1258, "answer_count": 1, "score": 1, "last_activity_date": 1639435019, "creation_date": 1639232951, "last_edit_date": 1639433871, "question_id": 32715, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/32715/what-exactly-is-embedding-layer-used-in-rnn-encoders", "title": "What exactly is embedding layer used in RNN encoders?", "body": "<p>I am reading about RNN encoders. I came across the following line from <a href=\"https://github.com/IntelLabs/distiller/blob/master/examples/word_language_model/model.py\" rel=\"nofollow noreferrer\">this</a> code. And I am facing difficulty in understanding the theoretical details regarding it.</p>\n<pre><code>emb = self.drop(self.encoder(input))\n</code></pre>\n<p>The <code>input</code> is a tensor of shape <span class=\"math-container\">$[32, 100]$</span>. Here 32 is the batch size and 100 is the length of the sentence. Hundred elements are indices to the words (from the dictionary) that are used in the sentence. We can observe that the output <code>emb</code> is later passed to the rnn (LSTM/GRU) layer.</p>\n<pre><code>output, hidden = self.rnn(emb, hidden)\n</code></pre>\n<p>So, to me, it looks like that <code>self.encoder</code> is the <em>necessary step</em> while using the RNN encoder. So, I am interested in what it actually does.</p>\n<p>When we see about <code>self.encoder</code>, it is an <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\" rel=\"nofollow noreferrer\">Embedding layer</a>. The description for this layer is as follows</p>\n<blockquote>\n<p>A <strong>simple lookup table</strong> that stores embeddings of a fixed dictionary and\nsize.</p>\n<p>This <strong>module</strong> is often used to store word embeddings and retrieve them\nusing indices. The input to the <strong>module</strong> is a list of indices, and the\noutput is the corresponding <strong>word embeddings</strong>.</p>\n</blockquote>\n<p>When we see about <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\" rel=\"nofollow noreferrer\"><code>self.drop</code></a>, it randomly keeps zero in the embeddings.</p>\n<blockquote>\n<p>During training, randomly zeroes some of the elements of the input\ntensor with probability p using samples from a Bernoulli distribution.\nEach channel will be zeroed out independently on every forward call.</p>\n</blockquote>\n<p>The outputs for both <code>self.encoder(input)</code> and <code>self.drop(self.encoder(input))</code> are <span class=\"math-container\">$[32, 100, 3000]$</span>.</p>\n<p>I have doubt(s) on the bolded parts of the description of the Embedding layer. The description is saying the Embedding layer uses/contains(?) a lookup table. The description says Embedding layer <em>stores and retrieves</em> word embeddings.</p>\n<p>The doubts are</p>\n<ol>\n<li><p>Generally, does an embedding layer <em>calculate</em> word embeddings or just store and retrieve them from the table? If it does not calculate them, then who will calculate the embeddings? If you can also comment on the specifics of PyTorch, I would appreciate it.</p>\n</li>\n<li><p>What exactly is an embedding layer? Is it a collection of neurons or any other?</p>\n</li>\n</ol>\n"}, "title": "What exactly is embedding layer used in RNN encoders?", "content": "I am reading about RNN encoders. I came across the following line from this code. And I am facing difficulty in understanding the theoretical details regarding it.\nemb = self.drop(self.encoder(input))\n\nThe input is a tensor of shape $[32, 100]$. Here 32 is the batch size and 100 is the length of the sentence. Hundred elements are indices to the words (from the dictionary) that are used in the sentence. We can observe that the output emb is later passed to the rnn (LSTM/GRU) layer.\noutput, hidden = self.rnn(emb, hidden)\n\nSo, to me, it looks like that self.encoder is the necessary step while using the RNN encoder. So, I am interested in what it actually does.\nWhen we see about self.encoder, it is an Embedding layer. The description for this layer is as follows\n\nA simple lookup table that stores embeddings of a fixed dictionary and\nsize.\nThis module is often used to store word embeddings and retrieve them\nusing indices. The input to the module is a list of indices, and the\noutput is the corresponding word embeddings.\n\nWhen we see about self.drop, it randomly keeps zero in the embeddings.\n\nDuring training, randomly zeroes some of the elements of the input\ntensor with probability p using samples from a Bernoulli distribution.\nEach channel will be zeroed out independently on every forward call.\n\nThe outputs for both self.encoder(input) and self.drop(self.encoder(input)) are $[32, 100, 3000]$.\nI have doubt(s) on the bolded parts of the description of the Embedding layer. The description is saying the Embedding layer uses/contains(?) a lookup table. The description says Embedding layer stores and retrieves word embeddings.\nThe doubts are\n\nGenerally, does an embedding layer calculate word embeddings or just store and retrieve them from the table? If it does not calculate them, then who will calculate the embeddings? If you can also comment on the specifics of PyTorch, I would appreciate it.\n\nWhat exactly is an embedding layer? Is it a collection of neurons or any other?\n\n\n", "question_id": 32715, "answers": []}, "26794": {"link": "https://ai.stackexchange.com/questions/26794/should-i-need-to-use-bert-embeddings-while-tokenizing-using-bert-tokenizer", "metadata": {"tags": ["natural-language-processing", "word-embedding", "bert", "word2vec"], "owner": {"reputation": 111, "user_id": 45386, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/o44zt.jpg?s=256&g=1", "display_name": "thenocturnalguy", "link": "https://ai.stackexchange.com/users/45386/thenocturnalguy"}, "is_answered": true, "view_count": 1115, "answer_count": 2, "score": 1, "last_activity_date": 1651990145, "creation_date": 1615630374, "last_edit_date": 1615634202, "question_id": 26794, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/26794/should-i-need-to-use-bert-embeddings-while-tokenizing-using-bert-tokenizer", "title": "Should I need to use BERT embeddings while tokenizing using BERT tokenizer?", "body": "<p>I am new to BERT and NLP and I am a little confused with tokenization and word embedding.\nMy doubt is if I use the BertTokenizer for tokenizing a sentence then do I have to compulsorily use BertEmbedding for generating its corresponding word vectors of the tokens or I can train my own word2vec model to generate my word embedding while using BertTokenizer?</p>\n<p>Pardon me if this question doesn't make any sense.</p>\n"}, "title": "Should I need to use BERT embeddings while tokenizing using BERT tokenizer?", "content": "I am new to BERT and NLP and I am a little confused with tokenization and word embedding.\nMy doubt is if I use the BertTokenizer for tokenizing a sentence then do I have to compulsorily use BertEmbedding for generating its corresponding word vectors of the tokens or I can train my own word2vec model to generate my word embedding while using BertTokenizer?\nPardon me if this question doesn't make any sense.\n", "question_id": 26794, "answers": []}, "23159": {"link": "https://ai.stackexchange.com/questions/23159/how-is-dropout-applied-to-the-embedding-layers-output", "metadata": {"tags": ["natural-language-processing", "tensorflow", "recurrent-neural-networks", "long-short-term-memory", "word-embedding"], "owner": {"reputation": 197, "user_id": 37178, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/oU4HM.jpg?s=256&g=1", "display_name": "o_yeah", "link": "https://ai.stackexchange.com/users/37178/o-yeah"}, "is_answered": true, "view_count": 2394, "accepted_answer_id": 23168, "answer_count": 1, "score": 1, "last_activity_date": 1598017528, "creation_date": 1597972111, "last_edit_date": 1598011699, "question_id": 23159, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/23159/how-is-dropout-applied-to-the-embedding-layers-output", "title": "How is dropout applied to the embedding layer&#39;s output?", "body": "<pre><code>model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(1000, 16, input_length=20), \n    tf.keras.layers.Dropout(0.2),                           # &lt;- How does the dropout work?\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n</code></pre>\n<p>I can understand when dropout is applied between <code>Dense layers</code>, which randomly drops and prevents the former layer neurons from updating parameters. I don't understand how dropout works after an embedding layer.</p>\n<p>Let's say the output shape of the Embedding layer is <code>(batch_size,20,16)</code> or simply <code>(20,16)</code> if we ignore the batch size. How is dropout applied to the embedding layer's output?</p>\n<p>Randomly dropout rows or columns?</p>\n"}, "title": "How is dropout applied to the embedding layer&#39;s output?", "content": "model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(1000, 16, input_length=20), \n    tf.keras.layers.Dropout(0.2),                           # <- How does the dropout work?\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nI can understand when dropout is applied between Dense layers, which randomly drops and prevents the former layer neurons from updating parameters. I don't understand how dropout works after an embedding layer.\nLet's say the output shape of the Embedding layer is (batch_size,20,16) or simply (20,16) if we ignore the batch size. How is dropout applied to the embedding layer's output?\nRandomly dropout rows or columns?\n", "question_id": 23159, "answers": []}, "18098": {"link": "https://ai.stackexchange.com/questions/18098/using-word-embedding-to-extend-words-for-searching-poi-names", "metadata": {"tags": ["natural-language-processing", "word-embedding"], "owner": {"reputation": 111, "user_id": 33623, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/bd155c32279e5ebbf431c2a4d1b600a5?s=256&d=identicon&r=PG", "display_name": "NGY", "link": "https://ai.stackexchange.com/users/33623/ngy"}, "is_answered": true, "view_count": 100, "answer_count": 1, "score": 1, "last_activity_date": 1582032625, "creation_date": 1582031424, "question_id": 18098, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/18098/using-word-embedding-to-extend-words-for-searching-poi-names", "title": "Using word embedding to extend words for searching POI names", "body": "<p>I am developing my own mobile app related to digital map. One of the functions is searching POIs (points of interest) in the map according to relevance between user query and POI name.</p>\n\n<p>Besides the POIs whose names contain exact words in the query, the app also needs to return those whose names are semantically related. For example, searching 'flower' should return POI names that contain 'flower' as well as those that contain 'florist'. Likewise, searching 'animal' should return 'animal' as well as 'veterinary'.</p>\n\n<p>That said, I need to extend words in the query semantically. For example, 'flower' has to be extended to ['flower', 'florist']. I have tried to use word embeddings: using the words corresponding to most similar vectors as extensions. Due to the fact I don't have user review data right now and most of the POI names are very short, I used trained word2vec model published by <a href=\"https://code.google.com/archive/p/word2vec/\" rel=\"nofollow noreferrer\">Google</a>. But the results turn out to be not what I expect: most similar words of 'flower' given by word2vec are words like 'roses'and 'orchid', and 'florist' is not even in the top 100 most similar list. Likewise, 'animal' gives 'dog', 'pets', 'cats' etc. Not very useful for my use case.</p>\n\n<p>I think simply using word embedding similarity may not be enough. I may need to build some advanced model based on word embedding. Do you have any suggestions?</p>\n"}, "title": "Using word embedding to extend words for searching POI names", "content": "I am developing my own mobile app related to digital map. One of the functions is searching POIs (points of interest) in the map according to relevance between user query and POI name.\nBesides the POIs whose names contain exact words in the query, the app also needs to return those whose names are semantically related. For example, searching 'flower' should return POI names that contain 'flower' as well as those that contain 'florist'. Likewise, searching 'animal' should return 'animal' as well as 'veterinary'.\nThat said, I need to extend words in the query semantically. For example, 'flower' has to be extended to ['flower', 'florist']. I have tried to use word embeddings: using the words corresponding to most similar vectors as extensions. Due to the fact I don't have user review data right now and most of the POI names are very short, I used trained word2vec model published by Google. But the results turn out to be not what I expect: most similar words of 'flower' given by word2vec are words like 'roses'and 'orchid', and 'florist' is not even in the top 100 most similar list. Likewise, 'animal' gives 'dog', 'pets', 'cats' etc. Not very useful for my use case.\nI think simply using word embedding similarity may not be enough. I may need to build some advanced model based on word embedding. Do you have any suggestions?\n", "question_id": 18098, "answers": []}, "11825": {"link": "https://ai.stackexchange.com/questions/11825/how-can-we-create-a-vector-space-where-word-spelling-and-pronunciation-can-be-ea", "metadata": {"tags": ["natural-language-processing", "word-embedding"], "owner": {"reputation": 51, "user_id": 23855, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/54554a8e54e55b87cda7bb7e34f2c13b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "gloomyson", "link": "https://ai.stackexchange.com/users/23855/gloomyson"}, "is_answered": true, "view_count": 121, "answer_count": 1, "score": 1, "last_activity_date": 1555597067, "creation_date": 1555321329, "last_edit_date": 1555453128, "question_id": 11825, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/11825/how-can-we-create-a-vector-space-where-word-spelling-and-pronunciation-can-be-ea", "title": "How can we create a vector space where word spelling and pronunciation can be easily compared?", "body": "<p>In natural language processing, we can convert words to vectors (or word embeddings). In this vector space, we can measure the similarity between these word embeddings.</p>\n\n<p>How can we create a vector space where word spelling and pronunciation can be easily compared? For example, \"apple\" and \"ape\", \"start\" and \"startle\" are very similar, so they should also be similar in this new vector space.</p>\n\n<p>I am eventually looking for a library that can do this out of the box. I would like to avoid implementing this myself.</p>\n"}, "title": "How can we create a vector space where word spelling and pronunciation can be easily compared?", "content": "In natural language processing, we can convert words to vectors (or word embeddings). In this vector space, we can measure the similarity between these word embeddings.\nHow can we create a vector space where word spelling and pronunciation can be easily compared? For example, \"apple\" and \"ape\", \"start\" and \"startle\" are very similar, so they should also be similar in this new vector space.\nI am eventually looking for a library that can do this out of the box. I would like to avoid implementing this myself.\n", "question_id": 11825, "answers": []}, "11328": {"link": "https://ai.stackexchange.com/questions/11328/skip-gram-model-training", "metadata": {"tags": ["natural-language-processing", "word2vec", "word-embedding"], "owner": {"reputation": 73, "user_id": 23220, "user_type": "unregistered", "profile_image": "https://www.gravatar.com/avatar/9d1ab54302611eed76111eacef493640?s=256&d=identicon&r=PG", "display_name": "naturalguy_12", "link": "https://ai.stackexchange.com/users/23220/naturalguy-12"}, "is_answered": true, "view_count": 109, "answer_count": 1, "score": 1, "last_activity_date": 1555453438, "creation_date": 1553023500, "last_edit_date": 1555453438, "question_id": 11328, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/11328/skip-gram-model-training", "title": "Skip-Gram Model Training", "body": "<p>Suppose we want to predict context words <span class=\"math-container\">$w_{i-h}, \\dots, w_{i+h}$</span> given a target word <span class=\"math-container\">$w_i$</span> for a window size <span class=\"math-container\">$h$</span> around the target word <span class=\"math-container\">$w_i$</span>. We can represent this as: <span class=\"math-container\">$$p(w_{i-h}, \\dots, w_{i+h}|w_i) = \\prod_{-h \\leq k \\leq h, \\ k \\neq 0} p(w_{i+k}|w_i)$$</span> where we model the probabilities of a word <span class=\"math-container\">$u$</span> given another word <span class=\"math-container\">$v$</span> as <span class=\"math-container\">$$p(u|v) = \\frac{\\exp(\\left&lt;\\phi_u, \\theta_v \\right&gt;)}{\\sum_{u' \\in W} \\exp(\\left&lt;\\phi_{u'}, \\theta_v \\right&gt;)}$$</span> where <span class=\"math-container\">$\\phi_u, \\theta_v$</span> are some vector representations for words <span class=\"math-container\">$u$</span> and <span class=\"math-container\">$v$</span> respectively and <span class=\"math-container\">$\\left&lt;\\phi_u, \\theta_v \\right&gt;$</span> is the dot product between these vector representations (which represents some sort of similarity between the words) and <span class=\"math-container\">$W$</span> is a matrix of all the words.</p>\n\n<p>In Skip-Gram Negative Sampling, we want to learn the embeddings <span class=\"math-container\">$\\phi_u, \\theta_v$</span> that maximize the following: <span class=\"math-container\">$$\\sum_{u \\in W} \\sum_{v \\in W} n_{uv} \\log \\sigma(\\left&lt;\\phi_u, \\theta_v \\right&gt;) +k \\mathbb{E}_{\\bar{v}} \\log \\sigma(-\\left&lt;\\phi_u, \\theta_{\\bar{v}} \\right&gt;)$$</span></p>\n\n<blockquote>\n  <p><strong>Question.</strong> How exactly does this work? For example, suppose <span class=\"math-container\">$k=5$</span>, the target word <span class=\"math-container\">$w_i$</span> is <span class=\"math-container\">$\\text{apple}$</span> and we want to find\n  <span class=\"math-container\">$p(\\text{pie}| \\text{apple})$</span>. Let <span class=\"math-container\">$n_{uv} = 10$</span> (number of times pie\n  co-occurs with apple). Then we sample <span class=\"math-container\">$5$</span> random words <span class=\"math-container\">$\\bar{v}$</span> that\n  did not occur with <span class=\"math-container\">$\\text{apple}$</span> and whichever term in the sum is\n  bigger is the one we predict? For example, if the first term in the\n  sum is larger than the second term then we would predict that\n  <span class=\"math-container\">$p(\\text{pie}| \\text{apple}) \\approx 1$</span>? Otherwise we predict that\n  <span class=\"math-container\">$p(\\text{pie}| \\text{apple}) \\approx 0$</span>? Is this the correct\n  intuition?</p>\n</blockquote>\n\n<p><strong>Source.</strong> <a href=\"https://www.coursera.org/learn/language-processing/lecture/A4eQD/explicit-and-implicit-matrix-factorization\" rel=\"nofollow noreferrer\">Here</a> at around the 10:05 mark.</p>\n"}, "title": "Skip-Gram Model Training", "content": "Suppose we want to predict context words $w_{i-h}, \\dots, w_{i+h}$ given a target word $w_i$ for a window size $h$ around the target word $w_i$. We can represent this as: $$p(w_{i-h}, \\dots, w_{i+h}|w_i) = \\prod_{-h \\leq k \\leq h, \\ k \\neq 0} p(w_{i+k}|w_i)$$ where we model the probabilities of a word $u$ given another word $v$ as $$p(u|v) = \\frac{\\exp(\\left<\\phi_u, \\theta_v \\right>)}{\\sum_{u' \\in W} \\exp(\\left<\\phi_{u'}, \\theta_v \\right>)}$$ where $\\phi_u, \\theta_v$ are some vector representations for words $u$ and $v$ respectively and $\\left<\\phi_u, \\theta_v \\right>$ is the dot product between these vector representations (which represents some sort of similarity between the words) and $W$ is a matrix of all the words.\nIn Skip-Gram Negative Sampling, we want to learn the embeddings $\\phi_u, \\theta_v$ that maximize the following: $$\\sum_{u \\in W} \\sum_{v \\in W} n_{uv} \\log \\sigma(\\left<\\phi_u, \\theta_v \\right>) +k \\mathbb{E}_{\\bar{v}} \\log \\sigma(-\\left<\\phi_u, \\theta_{\\bar{v}} \\right>)$$\n\nQuestion. How exactly does this work? For example, suppose $k=5$, the target word $w_i$ is $\\text{apple}$ and we want to find\n  $p(\\text{pie}| \\text{apple})$. Let $n_{uv} = 10$ (number of times pie\n  co-occurs with apple). Then we sample $5$ random words $\\bar{v}$ that\n  did not occur with $\\text{apple}$ and whichever term in the sum is\n  bigger is the one we predict? For example, if the first term in the\n  sum is larger than the second term then we would predict that\n  $p(\\text{pie}| \\text{apple}) \\approx 1$? Otherwise we predict that\n  $p(\\text{pie}| \\text{apple}) \\approx 0$? Is this the correct\n  intuition?\n\nSource. Here at around the 10:05 mark.\n", "question_id": 11328, "answers": []}, "39584": {"link": "https://ai.stackexchange.com/questions/39584/is-the-input-embedding-split-along-the-embedding-dimension-so-that-every-head-of", "metadata": {"tags": ["natural-language-processing", "transformer", "attention", "word-embedding", "embeddings"], "owner": {"reputation": 11, "user_id": 61128, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/cba820b3829fa19537aa248475e67eea?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "AAT", "link": "https://ai.stackexchange.com/users/61128/aat"}, "is_answered": false, "view_count": 51, "answer_count": 0, "score": 1, "last_activity_date": 1678814390, "creation_date": 1678814390, "question_id": 39584, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39584/is-the-input-embedding-split-along-the-embedding-dimension-so-that-every-head-of", "title": "Is the input embedding split along the embedding dimension so that every head of the multi-head-attention module just gets a part of the input data?", "body": "<p>So I found two contradictory explanations of the MHA (multi-head-self-attention-module):</p>\n<p>In <strong>the first approach</strong>, the input embedding (= the input matrix) is split along the embedding dimension and all heads are given a subset of the dimensions/features of each word.\nSome websites supporting this theory:\n<a href=\"https://medium.com/@smitasasindran/12-attention-mechanisms-multihead-attention-958041a35553\" rel=\"nofollow noreferrer\">https://medium.com/@smitasasindran/12-attention-mechanisms-multihead-attention-958041a35553</a>\n<br>-&gt; Quote: &quot;The input has been split into multiple heads, and we are running the attention model separately on each of these heads.&quot;</p>\n<p><a href=\"https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#3fa3\" rel=\"nofollow noreferrer\">https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#3fa3</a> <br>-&gt; Quote: &quot;In multi-head attention we split the embedding vector into N heads, so they will then have the dimensions batch_size * N * seq_len * (d_model / N).&quot;</p>\n<br>\n<br>\n<p><strong>The second approach</strong> assumes that all heads receive the entire input data, but different weight matrices are used for each head depending on the number of heads.\nThis theory is well explained on <a href=\"https://hungsblog.de/en/technology/learnings/visual-explanation-of-multi-head-attention/\" rel=\"nofollow noreferrer\">https://hungsblog.de/en/technology/learnings/visual-explanation-of-multi-head-attention/</a>\n<br>-&gt; Quote: &quot;Each head is responsible to fully calculate the attention for the whole embedding, not just for a subset of it and creates h attention matrices&quot;</p>\n<p>I tend to the second explanation, but have not been able to find a satisfactory and contradiction-free answer so far.</p>\n"}, "title": "Is the input embedding split along the embedding dimension so that every head of the multi-head-attention module just gets a part of the input data?", "content": "So I found two contradictory explanations of the MHA (multi-head-self-attention-module):\nIn the first approach, the input embedding (= the input matrix) is split along the embedding dimension and all heads are given a subset of the dimensions/features of each word.\nSome websites supporting this theory:\nhttps://medium.com/@smitasasindran/12-attention-mechanisms-multihead-attention-958041a35553\n-> Quote: \"The input has been split into multiple heads, and we are running the attention model separately on each of these heads.\"\nhttps://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#3fa3 -> Quote: \"In multi-head attention we split the embedding vector into N heads, so they will then have the dimensions batch_size * N * seq_len * (d_model / N).\"\n\n\nThe second approach assumes that all heads receive the entire input data, but different weight matrices are used for each head depending on the number of heads.\nThis theory is well explained on https://hungsblog.de/en/technology/learnings/visual-explanation-of-multi-head-attention/\n-> Quote: \"Each head is responsible to fully calculate the attention for the whole embedding, not just for a subset of it and creates h attention matrices\"\nI tend to the second explanation, but have not been able to find a satisfactory and contradiction-free answer so far.\n", "question_id": 39584, "answers": []}, "34068": {"link": "https://ai.stackexchange.com/questions/34068/given-embedding-vector-a-and-vector-b-how-to-find-top-k-embedding-vectors-such", "metadata": {"tags": ["machine-learning", "word-embedding", "word2vec", "cosine-similarity"], "owner": {"reputation": 11, "user_id": 48983, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9a1408568b30709cc2304db67f0ef07c?s=256&d=identicon&r=PG", "display_name": "Shubham", "link": "https://ai.stackexchange.com/users/48983/shubham"}, "is_answered": false, "view_count": 74, "answer_count": 2, "score": 1, "last_activity_date": 1685574435, "creation_date": 1641474276, "last_edit_date": 1641482576, "question_id": 34068, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/34068/given-embedding-vector-a-and-vector-b-how-to-find-top-k-embedding-vectors-such", "title": "Given embedding vector A and vector B, how to find top k embedding vectors such that they are similar to vector A and dissimilar to vector B", "body": "<p>Which would be better approach for getting top k embedding vectors such that they are similar to embedding vector A and dissimilar to vector B.</p>\n<p><strong>Approach 1:</strong></p>\n<ul>\n<li>calculate <code>f(V) = cosine_similarity(A,V) - cosine_similarity(B,V)</code> for each vector V</li>\n<li>sort vectors by f(V) value in descending order</li>\n<li>take first k of them.</li>\n</ul>\n<p><strong>Approach 2:</strong></p>\n<ul>\n<li>calculate\n<code>f(V) = cosine_similarity(A,V)</code> , <code>g(V) = cosine_similarity(B,V)</code>\nfor each vector V</li>\n<li>sort vectors by f(V) value in descending order</li>\n<li>take first k of them</li>\n<li>sort selected k vectors by g(V) in ascending order.</li>\n</ul>\n<p><strong>Approach 3:</strong></p>\n<ul>\n<li>calculate <code>f(V) = cosine_similarity((A - B),V)</code> for each vector V</li>\n<li>sort vectors by f(V) value in descending order</li>\n<li>take first k of them.</li>\n</ul>\n<p>Also, suggest better approach if you have other than above two.</p>\n<p>Note: embedding vector was calculated using word2vec algorithm</p>\n"}, "title": "Given embedding vector A and vector B, how to find top k embedding vectors such that they are similar to vector A and dissimilar to vector B", "content": "Which would be better approach for getting top k embedding vectors such that they are similar to embedding vector A and dissimilar to vector B.\nApproach 1:\n\ncalculate f(V) = cosine_similarity(A,V) - cosine_similarity(B,V) for each vector V\nsort vectors by f(V) value in descending order\ntake first k of them.\n\nApproach 2:\n\ncalculate\nf(V) = cosine_similarity(A,V) , g(V) = cosine_similarity(B,V)\nfor each vector V\nsort vectors by f(V) value in descending order\ntake first k of them\nsort selected k vectors by g(V) in ascending order.\n\nApproach 3:\n\ncalculate f(V) = cosine_similarity((A - B),V) for each vector V\nsort vectors by f(V) value in descending order\ntake first k of them.\n\nAlso, suggest better approach if you have other than above two.\nNote: embedding vector was calculated using word2vec algorithm\n", "question_id": 34068, "answers": []}, "32485": {"link": "https://ai.stackexchange.com/questions/32485/general-approaches-in-text-encoding-and-labelling-for-nlp", "metadata": {"tags": ["neural-networks", "natural-language-processing", "data-preprocessing", "word-embedding", "data-labelling"], "owner": {"reputation": 111, "user_id": 51090, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/1819cb3555272f9f898fb0764ac3df17?s=256&d=identicon&r=PG", "display_name": "Harry", "link": "https://ai.stackexchange.com/users/51090/harry"}, "is_answered": false, "view_count": 299, "closed_date": 1637581500, "answer_count": 1, "score": 1, "last_activity_date": 1637577897, "creation_date": 1637525144, "question_id": 32485, "link": "https://ai.stackexchange.com/questions/32485/general-approaches-in-text-encoding-and-labelling-for-nlp", "closed_reason": "Needs more focus", "title": "General approaches in text encoding and labelling for NLP", "body": "<p>What are the approaches of encoding text data? I would be glad to hear some summarization from experienced persons.</p>\n<p>And are there any solutions accepting words outside the vocabulary and including them to the results (online machine learning)?</p>\n<p><strong>Data input</strong></p>\n<p>So my basic understanding is that if we want predict some value (linear regression) or say what is the probability of occuring some event (logistic regression) we have to gather some features as our input and encode them as number. But this is not necessarily true when working with continuous data like sentences.</p>\n<p>The most naive aproach, which comes to my mind is just to assign some natural numbers to each word in the vocabulary. But this number does not contain any meaningful data about the word itself. On the other hand what seems to be important in NLP is just the order of the words. This is where I think about n-grams so we feed network with more than just one word. Or attention like in the Transformer.</p>\n<p>Another idea, which cames to my mind is to vectorize the word using one of the Word Embedding technique. Here we have some context about the word so the input is not just a dumb number. But does it have any value when we want to predict the next word? Can Word Embedding be used in that way or it's purpose is completely different.</p>\n<p>Last thing I was reading of was to encode characters rather than words but it feels pointless in such basic example as next word prediction. I would think about it more for sub-word tasks like inflection generating.</p>\n<p><strong>Labelling</strong></p>\n<p>Again based on my knowledge when we want to solve yes/no problem we're using sigmoid function. If we have more classes we can use one-hot encoding. But sometimes the output of the network might give us ambiguous meaning so we're using the softmax function so all output sum to 1.</p>\n<p>How this looks in NLP area? When having a vocabulary consisting of 600k words do we really need 600k softmaxed outputs? I'm also thinking there about Word Embedding solutions where we can reduce the number of outputs to let's say 300 numbers and then find the closest word matching the output without using softmax.</p>\n"}, "title": "General approaches in text encoding and labelling for NLP", "content": "What are the approaches of encoding text data? I would be glad to hear some summarization from experienced persons.\nAnd are there any solutions accepting words outside the vocabulary and including them to the results (online machine learning)?\nData input\nSo my basic understanding is that if we want predict some value (linear regression) or say what is the probability of occuring some event (logistic regression) we have to gather some features as our input and encode them as number. But this is not necessarily true when working with continuous data like sentences.\nThe most naive aproach, which comes to my mind is just to assign some natural numbers to each word in the vocabulary. But this number does not contain any meaningful data about the word itself. On the other hand what seems to be important in NLP is just the order of the words. This is where I think about n-grams so we feed network with more than just one word. Or attention like in the Transformer.\nAnother idea, which cames to my mind is to vectorize the word using one of the Word Embedding technique. Here we have some context about the word so the input is not just a dumb number. But does it have any value when we want to predict the next word? Can Word Embedding be used in that way or it's purpose is completely different.\nLast thing I was reading of was to encode characters rather than words but it feels pointless in such basic example as next word prediction. I would think about it more for sub-word tasks like inflection generating.\nLabelling\nAgain based on my knowledge when we want to solve yes/no problem we're using sigmoid function. If we have more classes we can use one-hot encoding. But sometimes the output of the network might give us ambiguous meaning so we're using the softmax function so all output sum to 1.\nHow this looks in NLP area? When having a vocabulary consisting of 600k words do we really need 600k softmaxed outputs? I'm also thinking there about Word Embedding solutions where we can reduce the number of outputs to let's say 300 numbers and then find the closest word matching the output without using softmax.\n", "question_id": 32485, "answers": []}, "28353": {"link": "https://ai.stackexchange.com/questions/28353/why-cant-recurrent-neural-network-handle-large-corpus-for-obtaining-embeddings", "metadata": {"tags": ["recurrent-neural-networks", "word-embedding", "books", "maximum-likelihood"], "owner": {"reputation": 3571, "user_id": 18758, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/339397e23b4e4cc78aa36e2b126252c7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "hanugm", "link": "https://ai.stackexchange.com/users/18758/hanugm"}, "is_answered": false, "view_count": 29, "answer_count": 0, "score": 1, "last_activity_date": 1624405240, "creation_date": 1624321402, "last_edit_date": 1624405240, "question_id": 28353, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/28353/why-cant-recurrent-neural-network-handle-large-corpus-for-obtaining-embeddings", "title": "Why can&#39;t recurrent neural network handle large corpus for obtaining embeddings?", "body": "<p>In order to learn the embeddings, we need to train a model based on some objective function. The model can be an RNN and the objective function can be the likelihood. We learn the embeddings by calculating the likelihood, and the embeddings are considered good if the likelihood is maximum for them.</p>\n<p>The following paragraph says that it is difficult to scale RNN to estimate the maximum likelihood for large corpus due to scaling issues:</p>\n<blockquote>\n<p>Likelihood-based optimization is derived from the objective <span class=\"math-container\">$\\log p(w; U)$</span>, where <span class=\"math-container\">$U \\in R_{K \\times V}$</span> is matrix of word embeddings, and <span class=\"math-container\">$w  =\\{w_m \\}_{m=1}^M$</span> is a corpus, represented as a list of <span class=\"math-container\">$M$</span> tokens. Recurrent neural network language models optimize this objective directly, backpropagating to the input word embeddings through the recurrent structure. <strong>However, state-of-the-art word embeddings employ huge corpora with hundreds of billions of tokens, and recurrent architectures <em>are difficult to scale</em> to such data.</strong> As a result, likelihood-based word embeddings are usually based on simplified likelihoods or heuristic approximations.</p>\n</blockquote>\n<p>What is the type of scaling, wrt RNN, is referred to here? Why is it difficult to scale RNN?</p>\n<hr />\n<p>The paragraph above is taken from the page 329 of Chapter 14: Distributional and distributed semantics of the textbook <a href=\"https://cseweb.ucsd.edu/%7Ennakashole/teaching/eisenstein-nov18.pdf\" rel=\"nofollow noreferrer\">Natural Language Processing by Jacob Eisenstein</a></p>\n"}, "title": "Why can&#39;t recurrent neural network handle large corpus for obtaining embeddings?", "content": "In order to learn the embeddings, we need to train a model based on some objective function. The model can be an RNN and the objective function can be the likelihood. We learn the embeddings by calculating the likelihood, and the embeddings are considered good if the likelihood is maximum for them.\nThe following paragraph says that it is difficult to scale RNN to estimate the maximum likelihood for large corpus due to scaling issues:\n\nLikelihood-based optimization is derived from the objective $\\log p(w; U)$, where $U \\in R_{K \\times V}$ is matrix of word embeddings, and $w  =\\{w_m \\}_{m=1}^M$ is a corpus, represented as a list of $M$ tokens. Recurrent neural network language models optimize this objective directly, backpropagating to the input word embeddings through the recurrent structure. However, state-of-the-art word embeddings employ huge corpora with hundreds of billions of tokens, and recurrent architectures are difficult to scale to such data. As a result, likelihood-based word embeddings are usually based on simplified likelihoods or heuristic approximations.\n\nWhat is the type of scaling, wrt RNN, is referred to here? Why is it difficult to scale RNN?\n\nThe paragraph above is taken from the page 329 of Chapter 14: Distributional and distributed semantics of the textbook Natural Language Processing by Jacob Eisenstein\n", "question_id": 28353, "answers": []}, "28321": {"link": "https://ai.stackexchange.com/questions/28321/how-to-generate-text-descriptions-from-keywords", "metadata": {"tags": ["models", "word-embedding", "text-generation"], "owner": {"reputation": 111, "user_id": 23657, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/yyfWH.jpg?s=256&g=1", "display_name": "Krzysztof Kaczy\u0144ski", "link": "https://ai.stackexchange.com/users/23657/krzysztof-kaczy%c5%84ski"}, "is_answered": false, "view_count": 87, "answer_count": 0, "score": 1, "last_activity_date": 1624053608, "creation_date": 1624053608, "question_id": 28321, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/28321/how-to-generate-text-descriptions-from-keywords", "title": "How to generate text descriptions from keywords?", "body": "<p>I wonder how can I build a neural network which will generate text description from given tag/tags. Let's assume I have created such data structure:</p>\n<pre><code>{\n 'tag1': ['some description1', 'some description2', 'some description3'],\n 'tag2': ['some description4', 'some description5', 'some description6'],\n 'tag3': ['some description7', 'some description8', 'some description9']\n}\n</code></pre>\n<p>Then I would like to create a neural network which will generate randomly generated description based on given tags. For example:</p>\n<pre><code>INPUT: ['TAG1', 'TAG2', 'TAG3'] =&gt; OUTPUT: 'some description1. some description5 some description9'\n</code></pre>\n<p>Then I thought that it can be a good idea to implement a LSTM and doing text generation, but here I have a problem I know how I can do it for one tag. I can create one corpus of text contains different sentences for tag, then do the training and generate a sentence for given tag, but what If I have multiple tags should I create a corpus for each tag or maybe there is a better way to do that? If you know any articles which covers this problem, I would appreciate if you share them with me. If you have a neural network proposition which will solve this problem, I am also open for proposals.</p>\n<blockquote>\n<p>PS. I know, I can solve this problem with easy Map, for example: <code>['tag1', 'tag2', 'tag3'].map(tag =&gt; tagSentenceMap.get(tag).randomChoice()).join('. ')</code> but this is not the case for me.</p>\n</blockquote>\n"}, "title": "How to generate text descriptions from keywords?", "content": "I wonder how can I build a neural network which will generate text description from given tag/tags. Let's assume I have created such data structure:\n{\n 'tag1': ['some description1', 'some description2', 'some description3'],\n 'tag2': ['some description4', 'some description5', 'some description6'],\n 'tag3': ['some description7', 'some description8', 'some description9']\n}\n\nThen I would like to create a neural network which will generate randomly generated description based on given tags. For example:\nINPUT: ['TAG1', 'TAG2', 'TAG3'] => OUTPUT: 'some description1. some description5 some description9'\n\nThen I thought that it can be a good idea to implement a LSTM and doing text generation, but here I have a problem I know how I can do it for one tag. I can create one corpus of text contains different sentences for tag, then do the training and generate a sentence for given tag, but what If I have multiple tags should I create a corpus for each tag or maybe there is a better way to do that? If you know any articles which covers this problem, I would appreciate if you share them with me. If you have a neural network proposition which will solve this problem, I am also open for proposals.\n\nPS. I know, I can solve this problem with easy Map, for example: ['tag1', 'tag2', 'tag3'].map(tag => tagSentenceMap.get(tag).randomChoice()).join('. ') but this is not the case for me.\n\n", "question_id": 28321, "answers": []}, "26667": {"link": "https://ai.stackexchange.com/questions/26667/given-the-word-embeddings-how-do-i-create-the-sentence-composed-of-the-correspo", "metadata": {"tags": ["natural-language-processing", "long-short-term-memory", "word-embedding", "sentiment-analysis", "word2vec"], "owner": {"reputation": 11, "user_id": 45170, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9803a35f091f0ed4461e0e908594c7b8?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "trail99", "link": "https://ai.stackexchange.com/users/45170/trail99"}, "is_answered": false, "view_count": 44, "answer_count": 0, "score": 1, "last_activity_date": 1615196460, "creation_date": 1614865189, "last_edit_date": 1615196460, "question_id": 26667, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/26667/given-the-word-embeddings-how-do-i-create-the-sentence-composed-of-the-correspo", "title": "Given the word embeddings, how do I create the sentence composed of the corresponding words?", "body": "<p>I have done some reading. I want to implement an LSTM with pre-trained word embeddings (I also have plans to create my word embeddings, but let's cross that bridge when we come to it).</p>\n<p>In any given sentence, you don't usually need to have all the words as most of them do not contribute to the sentiment, such as the <a href=\"https://en.wikipedia.org/wiki/Stop_word\" rel=\"nofollow noreferrer\">stop words</a> and noise. So, let's say there is a sentence. I remove the stop words and anything else that I deem unnecessary for the project. Then I run the remaining words through the word embedding algorithm to get the word vectors.</p>\n<p>Then what? How does it represent the sequence or the sentence 'cause it's just vector for a word.</p>\n<p>For example, take the sentence:</p>\n<blockquote>\n<p>The burger does not taste good.</p>\n</blockquote>\n<p>I could remove certain words and still retain the same sentiment like so:</p>\n<blockquote>\n<p>Burger not good.</p>\n</blockquote>\n<p>Let's assume some arbitrary vectors for those three words:</p>\n<ul>\n<li><p><code>Burger</code>: <span class=\"math-container\">$[0.45, -0.78, .., 1.2]$</span></p>\n</li>\n<li><p><code>not</code>: <span class=\"math-container\">$[9.6, 4.0, .., 5.6]$</span></p>\n</li>\n<li><p><code>good</code>: <span class=\"math-container\">$[3.5, 0.51, 0.8]$</span></p>\n</li>\n</ul>\n<p>So, those vectors represent the individual words. How do I make a sentence out of them? Just concatenate them?</p>\n"}, "title": "Given the word embeddings, how do I create the sentence composed of the corresponding words?", "content": "I have done some reading. I want to implement an LSTM with pre-trained word embeddings (I also have plans to create my word embeddings, but let's cross that bridge when we come to it).\nIn any given sentence, you don't usually need to have all the words as most of them do not contribute to the sentiment, such as the stop words and noise. So, let's say there is a sentence. I remove the stop words and anything else that I deem unnecessary for the project. Then I run the remaining words through the word embedding algorithm to get the word vectors.\nThen what? How does it represent the sequence or the sentence 'cause it's just vector for a word.\nFor example, take the sentence:\n\nThe burger does not taste good.\n\nI could remove certain words and still retain the same sentiment like so:\n\nBurger not good.\n\nLet's assume some arbitrary vectors for those three words:\n\nBurger: $[0.45, -0.78, .., 1.2]$\n\nnot: $[9.6, 4.0, .., 5.6]$\n\ngood: $[3.5, 0.51, 0.8]$\n\n\nSo, those vectors represent the individual words. How do I make a sentence out of them? Just concatenate them?\n", "question_id": 26667, "answers": []}, "26284": {"link": "https://ai.stackexchange.com/questions/26284/what-does-the-outputlayer-of-bert-for-masked-language-modelling-look-like", "metadata": {"tags": ["natural-language-processing", "transformer", "attention", "word-embedding", "bert"], "owner": {"reputation": 535, "user_id": 43632, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=256", "display_name": "Bert Gayus", "link": "https://ai.stackexchange.com/users/43632/bert-gayus"}, "is_answered": false, "view_count": 448, "answer_count": 0, "score": 1, "last_activity_date": 1612967146, "creation_date": 1612811774, "last_edit_date": 1612967146, "question_id": 26284, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/26284/what-does-the-outputlayer-of-bert-for-masked-language-modelling-look-like", "title": "What does the outputlayer of BERT for masked language modelling look like?", "body": "<p>In the tutorial <a href=\"https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/\" rel=\"nofollow noreferrer\">BERT \u2013 State of the Art Language Model for NLP</a> the masked language modeling pre-training steps are described as follows:</p>\n<blockquote>\n<p>In technical terms, the prediction of the output words requires:</p>\n<ol>\n<li>Adding a classification layer on top of the encoder output.</li>\n</ol>\n<p>2.Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.</p>\n<p>3.Calculating the probability of each word in the vocabulary with softmax.</p>\n</blockquote>\n<p>In the Figure below this process is visualized and also from the tutorial.</p>\n<p>I am confused about what exactly is done. Does it mean that each output vector O is fed into a fully connected layer with embedding_size neurons and then multiplied by the embedding matrix from the input layer?</p>\n<p>Update:</p>\n<p>In the tutorial <a href=\"http://jalammar.github.io/illustrated-gpt2/\" rel=\"nofollow noreferrer\">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a> I found an explanation for GPT-2 which seems to be similar to my question.</p>\n<p>In the tutorial is said that each output vector is multiplied by the input embedding matrix to get the final output.</p>\n<p>Does the same mechanic apply to BERT?</p>\n<p><img src=\"https://miro.medium.com/max/698/0*ViwaI3Vvbnd-CJSQ.png\" alt=\"Text\" /></p>\n"}, "title": "What does the outputlayer of BERT for masked language modelling look like?", "content": "In the tutorial BERT \u2013 State of the Art Language Model for NLP the masked language modeling pre-training steps are described as follows:\n\nIn technical terms, the prediction of the output words requires:\n\nAdding a classification layer on top of the encoder output.\n\n2.Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.\n3.Calculating the probability of each word in the vocabulary with softmax.\n\nIn the Figure below this process is visualized and also from the tutorial.\nI am confused about what exactly is done. Does it mean that each output vector O is fed into a fully connected layer with embedding_size neurons and then multiplied by the embedding matrix from the input layer?\nUpdate:\nIn the tutorial The Illustrated GPT-2 (Visualizing Transformer Language Models) I found an explanation for GPT-2 which seems to be similar to my question.\nIn the tutorial is said that each output vector is multiplied by the input embedding matrix to get the final output.\nDoes the same mechanic apply to BERT?\n\n", "question_id": 26284, "answers": []}, "24075": {"link": "https://ai.stackexchange.com/questions/24075/bechmark-models-for-text-classification-sentiment-classification", "metadata": {"tags": ["natural-language-processing", "word-embedding", "text-classification", "word2vec", "benchmarks"], "owner": {"reputation": 11, "user_id": 37792, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/f01875d6fe8676f1a0092933098ca7a6?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Jakub", "link": "https://ai.stackexchange.com/users/37792/jakub"}, "is_answered": false, "view_count": 23, "answer_count": 0, "score": 1, "last_activity_date": 1602696412, "creation_date": 1602696412, "question_id": 24075, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/24075/bechmark-models-for-text-classification-sentiment-classification", "title": "Bechmark models for Text Classification / Sentiment Classification", "body": "<p>I am currently working on a novel application in NLP where I try to classify empathic and non-empathic texts. I would like to compare the performance of my model to some benchmark models. As I am working with models based on Word2Vec embeddings, the benchmark models should also be based on Word2Vec, however I am looking for some relatively easy, quick to implement models.</p>\n<p>Do you have any suggestions?</p>\n"}, "title": "Bechmark models for Text Classification / Sentiment Classification", "content": "I am currently working on a novel application in NLP where I try to classify empathic and non-empathic texts. I would like to compare the performance of my model to some benchmark models. As I am working with models based on Word2Vec embeddings, the benchmark models should also be based on Word2Vec, however I am looking for some relatively easy, quick to implement models.\nDo you have any suggestions?\n", "question_id": 24075, "answers": []}, "23217": {"link": "https://ai.stackexchange.com/questions/23217/how-homographs-is-an-nlp-task-can-be-treated", "metadata": {"tags": ["natural-language-processing", "word-embedding", "natural-language-understanding"], "owner": {"reputation": 2444, "user_id": 38846, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/Hlbol.jpg?s=256&g=1", "display_name": "spiridon_the_sun_rotator", "link": "https://ai.stackexchange.com/users/38846/spiridon-the-sun-rotator"}, "is_answered": false, "view_count": 253, "answer_count": 0, "score": 1, "last_activity_date": 1616746873, "creation_date": 1598255583, "question_id": 23217, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/23217/how-homographs-is-an-nlp-task-can-be-treated", "title": "How homographs is an NLP task can be treated?", "body": "<blockquote>\n<p>A <em>homograph</em> - is a word that shares  the same written form as another word but has a different meaning.</p>\n</blockquote>\n<p>They can be even different parts of speech. For example:</p>\n<ol>\n<li>close(verb) - close(adverb)</li>\n<li>lead(verb) - lead(noun)</li>\n<li>wind(noun) - wind(verb)</li>\n</ol>\n<p>And there is rather a big list <a href=\"https://en.wikipedia.org/wiki/List_of_English_homographs\" rel=\"nofollow noreferrer\">https://en.wikipedia.org/wiki/List_of_English_homographs</a>.</p>\n<p>As far as I understand, after processing the text data in any conventional way, lemmatization, building an embedding, these words, despite having different meaning, and appearing in different contexts, would be absolutely the same for the algorithm, and in the end we would get some averaged context between two or more meainings of the word. And this embedding would be meaningless.</p>\n<p>How is this problem treated or these words are regarded to be too rare to have a significant impact on the quality of resulting embedding?</p>\n<p>I would appreciate comments and references to the papers or sources</p>\n"}, "title": "How homographs is an NLP task can be treated?", "content": "\nA homograph - is a word that shares  the same written form as another word but has a different meaning.\n\nThey can be even different parts of speech. For example:\n\nclose(verb) - close(adverb)\nlead(verb) - lead(noun)\nwind(noun) - wind(verb)\n\nAnd there is rather a big list https://en.wikipedia.org/wiki/List_of_English_homographs.\nAs far as I understand, after processing the text data in any conventional way, lemmatization, building an embedding, these words, despite having different meaning, and appearing in different contexts, would be absolutely the same for the algorithm, and in the end we would get some averaged context between two or more meainings of the word. And this embedding would be meaningless.\nHow is this problem treated or these words are regarded to be too rare to have a significant impact on the quality of resulting embedding?\nI would appreciate comments and references to the papers or sources\n", "question_id": 23217, "answers": []}, "22700": {"link": "https://ai.stackexchange.com/questions/22700/is-it-good-practice-to-save-nlp-transformer-based-pre-trained-models-into-file-s", "metadata": {"tags": ["models", "pytorch", "word-embedding", "transformer", "pretrained-models"], "owner": {"reputation": 141, "user_id": 22195, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/dab48c2e9aa807c5eadf34ba61ba2e9b?s=256&d=identicon&r=PG", "display_name": "Murugesh", "link": "https://ai.stackexchange.com/users/22195/murugesh"}, "is_answered": false, "view_count": 102, "answer_count": 0, "score": 1, "last_activity_date": 1595855555, "creation_date": 1595855555, "question_id": 22700, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/22700/is-it-good-practice-to-save-nlp-transformer-based-pre-trained-models-into-file-s", "title": "Is it good practice to save NLP Transformer based pre-trained models into file system in production environment", "body": "<p>I have developed a multi label classifier using BERT. I'm leveraging Hugging Face Pytorch implementation for transformers.</p>\n<p>I have saved the pretrained model into the file directory in dev environment. Now, the application is ready to be moved the production environment.</p>\n<p>Is it a good practice to save the models into file system in prod ?\nCan I serialize the model files and word embeddings into any DB and read again ?</p>\n"}, "title": "Is it good practice to save NLP Transformer based pre-trained models into file system in production environment", "content": "I have developed a multi label classifier using BERT. I'm leveraging Hugging Face Pytorch implementation for transformers.\nI have saved the pretrained model into the file directory in dev environment. Now, the application is ready to be moved the production environment.\nIs it a good practice to save the models into file system in prod ?\nCan I serialize the model files and word embeddings into any DB and read again ?\n", "question_id": 22700, "answers": []}, "20977": {"link": "https://ai.stackexchange.com/questions/20977/creating-text-features-using-word2vec", "metadata": {"tags": ["natural-language-processing", "word-embedding", "word2vec", "text-classification"], "owner": {"reputation": 919, "user_id": 36055, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/tS9eU.png?s=256&g=1", "display_name": "ddaedalus", "link": "https://ai.stackexchange.com/users/36055/ddaedalus"}, "is_answered": false, "view_count": 55, "answer_count": 0, "score": 1, "last_activity_date": 1588794227, "creation_date": 1588794227, "question_id": 20977, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/20977/creating-text-features-using-word2vec", "title": "Creating Text Features using word2vec", "body": "<p>My task is to classify some texts. I have used word2vec to represent text words and I pass them to an LSTM as input. Taking into account that texts do not contain the same number of words, is it a good idea to create text features of fixed dimension using the word2vec word representations of the text and then classify the text using these features as an input of a neural network? And in general is it a good idea to create text features using this method?</p>\n"}, "title": "Creating Text Features using word2vec", "content": "My task is to classify some texts. I have used word2vec to represent text words and I pass them to an LSTM as input. Taking into account that texts do not contain the same number of words, is it a good idea to create text features of fixed dimension using the word2vec word representations of the text and then classify the text using these features as an input of a neural network? And in general is it a good idea to create text features using this method?\n", "question_id": 20977, "answers": []}, "18369": {"link": "https://ai.stackexchange.com/questions/18369/why-i-have-a-different-number-of-terms-in-word2vec-and-tfidf-how-i-can-fix-it", "metadata": {"tags": ["natural-language-processing", "python", "word-embedding", "word2vec", "weights"], "owner": {"reputation": 13, "user_id": 33977, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/693f6c92d35457bcdf78248622cf464f?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "B612", "link": "https://ai.stackexchange.com/users/33977/b612"}, "is_answered": true, "view_count": 46, "accepted_answer_id": 18385, "answer_count": 1, "score": 1, "last_activity_date": 1583227324, "creation_date": 1583183575, "question_id": 18369, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/18369/why-i-have-a-different-number-of-terms-in-word2vec-and-tfidf-how-i-can-fix-it", "title": "Why I have a different number of terms in word2vec and TFIDF? How I can fix it?", "body": "<p>I need multiply the weigths of terms in TFIDF matrix by the word-embeddings of word2vec matrix but I can't do it because each matrix have a different number of terms. <strong>I am using the same corpus for get both matrix, I don't know why each matrix have a different number of terms\n.</strong></p>\n\n<p>My problem is that I have a matrix TFIDF with the shape <code>(56096, 15500)</code> (corresponding to: number of terms, number of documents) and matrix Word2vec with the shape <code>(300, 56184)</code> (corresponding to : number of word-embeddings, number of terms).<br>\nAnd I need the same numbers of terms in both matrix. </p>\n\n<p>I use this code for get the matrix of word-embeddings Word2vec: </p>\n\n<pre><code>def w2vec_gensim(norm_corpus):\n    wpt = nltk.WordPunctTokenizer()\n    tokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]\n    # Set values for various parameters\n    feature_size = 300\n    # Word vector dimensionality\n    window_context = 10\n    # Context window size\n    min_word_count = 1\n    # Minimum word count\n    sample = 1e-3\n    # Downsample setting for frequent words\n    w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, window=window_context, min_count =  min_word_count, sample=sample, iter=100)\n    words = list(w2v_model.wv.vocab)\n    vectors=[]\n    for w in words:\n        vectors.append(w2v_model[w].tolist())\n    embedding_matrix= np.array(vectors)\n    embedding_matrix= embedding_matrix.T\n    print(embedding_matrix.shape)\n\n    return embedding_matrix\n</code></pre>\n\n<p>And this code for get the TFIDF matrix: </p>\n\n<pre><code>tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2', use_idf=True, smooth_idf=True)\n\n\ndef matriz_tf_idf(datos, tv):\n    tv_matrix = tv.fit_transform(datos)\n    tv_matrix = tv_matrix.toarray()\n    tv_matrix = tv_matrix.T\n    return tv_matrix\n</code></pre>\n\n<p>And I need the same number of terms in each matrix. For example, if I have 56096 terms in TFIDF, I need the same number in embeddings matrix, I mean matrix TFIDF with the shape <code>(56096, 1550)</code> and matrix of embeddings Word2vec with the shape <code>(300, 56096)</code>. How I can get the same number of terms in both matrix? \nBecause I can't delete without more data, due to I need the multiplication to make sense because my goal is to get the embeddings from the documents. </p>\n\n<p>Thank you very much in advance.</p>\n"}, "title": "Why I have a different number of terms in word2vec and TFIDF? How I can fix it?", "content": "I need multiply the weigths of terms in TFIDF matrix by the word-embeddings of word2vec matrix but I can't do it because each matrix have a different number of terms. I am using the same corpus for get both matrix, I don't know why each matrix have a different number of terms\n.\nMy problem is that I have a matrix TFIDF with the shape (56096, 15500) (corresponding to: number of terms, number of documents) and matrix Word2vec with the shape (300, 56184) (corresponding to : number of word-embeddings, number of terms).\nAnd I need the same numbers of terms in both matrix. \nI use this code for get the matrix of word-embeddings Word2vec: \ndef w2vec_gensim(norm_corpus):\n    wpt = nltk.WordPunctTokenizer()\n    tokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]\n    # Set values for various parameters\n    feature_size = 300\n    # Word vector dimensionality\n    window_context = 10\n    # Context window size\n    min_word_count = 1\n    # Minimum word count\n    sample = 1e-3\n    # Downsample setting for frequent words\n    w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, window=window_context, min_count =  min_word_count, sample=sample, iter=100)\n    words = list(w2v_model.wv.vocab)\n    vectors=[]\n    for w in words:\n        vectors.append(w2v_model[w].tolist())\n    embedding_matrix= np.array(vectors)\n    embedding_matrix= embedding_matrix.T\n    print(embedding_matrix.shape)\n\n    return embedding_matrix\n\nAnd this code for get the TFIDF matrix: \ntv = TfidfVectorizer(min_df=0., max_df=1., norm='l2', use_idf=True, smooth_idf=True)\n\n\ndef matriz_tf_idf(datos, tv):\n    tv_matrix = tv.fit_transform(datos)\n    tv_matrix = tv_matrix.toarray()\n    tv_matrix = tv_matrix.T\n    return tv_matrix\n\nAnd I need the same number of terms in each matrix. For example, if I have 56096 terms in TFIDF, I need the same number in embeddings matrix, I mean matrix TFIDF with the shape (56096, 1550) and matrix of embeddings Word2vec with the shape (300, 56096). How I can get the same number of terms in both matrix? \nBecause I can't delete without more data, due to I need the multiplication to make sense because my goal is to get the embeddings from the documents. \nThank you very much in advance.\n", "question_id": 18369, "answers": []}, "17425": {"link": "https://ai.stackexchange.com/questions/17425/is-there-a-way-to-parallelize-glove-cooccur-function", "metadata": {"tags": ["word-embedding", "glove"], "owner": {"reputation": 41, "user_id": 31294, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/328487d8029288f27587e18286a68d4a?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "J. Montgomery", "link": "https://ai.stackexchange.com/users/31294/j-montgomery"}, "is_answered": false, "view_count": 28, "answer_count": 0, "score": 1, "last_activity_date": 1578550165, "creation_date": 1578550165, "question_id": 17425, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/17425/is-there-a-way-to-parallelize-glove-cooccur-function", "title": "Is there a way to parallelize GloVe cooccur function?", "body": "<p>I would like to create a GloVe word embedding on a very large corpus (trillions of words). However, creating the co-occurence matrix with the GloVe cooccur script is projected to take weeks. Is there any way to parallelize the process of creating a co-occurence matrix, either using GloVe or another resource that is out there? </p>\n"}, "title": "Is there a way to parallelize GloVe cooccur function?", "content": "I would like to create a GloVe word embedding on a very large corpus (trillions of words). However, creating the co-occurence matrix with the GloVe cooccur script is projected to take weeks. Is there any way to parallelize the process of creating a co-occurence matrix, either using GloVe or another resource that is out there? \n", "question_id": 17425, "answers": []}, "14325": {"link": "https://ai.stackexchange.com/questions/14325/reference-request-one-hot-encoding-outperforming-random-orthogonal-encoding", "metadata": {"tags": ["neural-networks", "convolutional-neural-networks", "reference-request", "word-embedding", "papers"], "owner": {"reputation": 205, "user_id": 20150, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/f19a3ac761df525d20390cf126e6219e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "lo tolmencre", "link": "https://ai.stackexchange.com/users/20150/lo-tolmencre"}, "is_answered": false, "view_count": 134, "answer_count": 0, "score": 1, "last_activity_date": 1567963905, "creation_date": 1567896133, "last_edit_date": 1567963905, "question_id": 14325, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/14325/reference-request-one-hot-encoding-outperforming-random-orthogonal-encoding", "title": "Reference request: one-hot encoding outperforming random orthogonal encoding", "body": "<p>I experimented with a CNN operating on texts encoded as sequences of character vectors, where characters are encoded as one-hot vectors in one embedding and as random unit length pairwise orthogonal vectors (orthogonal matrix) in another. While geometrically these encode the same vector space, the one-hot embedding outperformed the random orthogonal one consistently. I suppose this has to do with the clarity of the signal: A zero vector with a single 1-valued cell is an easier to learn signal than just <em>some</em> vector with lots of different values in each cell.</p>\n\n<p>I wondered if you know of any papers on this kind of effect. I did not find any but would like to back up this finding and check if my reasoning for why this is the case makes sense/ find a better or more in-depth explanation.</p>\n"}, "title": "Reference request: one-hot encoding outperforming random orthogonal encoding", "content": "I experimented with a CNN operating on texts encoded as sequences of character vectors, where characters are encoded as one-hot vectors in one embedding and as random unit length pairwise orthogonal vectors (orthogonal matrix) in another. While geometrically these encode the same vector space, the one-hot embedding outperformed the random orthogonal one consistently. I suppose this has to do with the clarity of the signal: A zero vector with a single 1-valued cell is an easier to learn signal than just some vector with lots of different values in each cell.\nI wondered if you know of any papers on this kind of effect. I did not find any but would like to back up this finding and check if my reasoning for why this is the case makes sense/ find a better or more in-depth explanation.\n", "question_id": 14325, "answers": []}, "37472": {"link": "https://ai.stackexchange.com/questions/37472/how-do-transformers-compute-the-words-embeddings-at-inference-time-since-the-emb", "metadata": {"tags": ["transformer", "word-embedding", "inference"], "owner": {"reputation": 101, "user_id": 60420, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/FWpBN.jpg?s=256&g=1", "display_name": "Julien", "link": "https://ai.stackexchange.com/users/60420/julien"}, "is_answered": true, "view_count": 486, "answer_count": 2, "score": 0, "last_activity_date": 1667501935, "creation_date": 1665992717, "last_edit_date": 1665995635, "question_id": 37472, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/37472/how-do-transformers-compute-the-words-embeddings-at-inference-time-since-the-emb", "title": "How do Transformers compute the words embeddings at inference time since the embeddings are dynamic?", "body": "<p>In Word2Vec, the embeddings don't depend on the context.</p>\n<p>But in Transformers, the embeddings depend on the context.</p>\n<p>So how are the words' embeddings set at inference time?</p>\n"}, "title": "How do Transformers compute the words embeddings at inference time since the embeddings are dynamic?", "content": "In Word2Vec, the embeddings don't depend on the context.\nBut in Transformers, the embeddings depend on the context.\nSo how are the words' embeddings set at inference time?\n", "question_id": 37472, "answers": []}, "27957": {"link": "https://ai.stackexchange.com/questions/27957/books-for-text-embedding", "metadata": {"tags": ["natural-language-processing", "reference-request", "word-embedding", "books", "vector-semantics"], "owner": {"reputation": 3571, "user_id": 18758, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/339397e23b4e4cc78aa36e2b126252c7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "hanugm", "link": "https://ai.stackexchange.com/users/18758/hanugm"}, "is_answered": true, "view_count": 259, "accepted_answer_id": 27974, "answer_count": 2, "score": 0, "last_activity_date": 1624335732, "creation_date": 1621954528, "last_edit_date": 1624335732, "question_id": 27957, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/27957/books-for-text-embedding", "title": "Book(s) for text embedding", "body": "<p>Text here refers to either character or word or sentence.</p>\n<p>Is there any recent <strong>textbook</strong> that encompasses from classical methods to the modern techniques for embedding texts?</p>\n<p>If a single textbook is unavailable then please recommend a list of books covering the whole spectrum as mentioned above.</p>\n<p>Modern textbooks that are similar to <a href=\"https://nlp.stanford.edu/IR-book/information-retrieval-book.html\" rel=\"nofollow noreferrer\">Christopher D. Manning, Prabhakar Raghavan and Hinrich Sch\u00fctze, Introduction to Information Retrieval, Cambridge University Press. 2008</a> are highly encouraged.</p>\n<p><a href=\"https://ai.stackexchange.com/questions/20911/is-there-a-good-book-or-paper-on-word-embeddings\">This question</a> asks for textbook/research paper on word embedding only.</p>\n"}, "title": "Book(s) for text embedding", "content": "Text here refers to either character or word or sentence.\nIs there any recent textbook that encompasses from classical methods to the modern techniques for embedding texts?\nIf a single textbook is unavailable then please recommend a list of books covering the whole spectrum as mentioned above.\nModern textbooks that are similar to Christopher D. Manning, Prabhakar Raghavan and Hinrich Sch\u00fctze, Introduction to Information Retrieval, Cambridge University Press. 2008 are highly encouraged.\nThis question asks for textbook/research paper on word embedding only.\n", "question_id": 27957, "answers": []}, "40134": {"link": "https://ai.stackexchange.com/questions/40134/do-different-ngrams-share-embedding-in-fasttext", "metadata": {"tags": ["neural-networks", "natural-language-processing", "word-embedding", "text-classification", "pretrained-models"], "owner": {"reputation": 123, "user_id": 63599, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/a5fda839cfee2d72876fd61f7b7d772c?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Fijoy Vadakkumpadan", "link": "https://ai.stackexchange.com/users/63599/fijoy-vadakkumpadan"}, "is_answered": true, "view_count": 26, "accepted_answer_id": 40299, "answer_count": 1, "score": 0, "last_activity_date": 1683186815, "creation_date": 1682008476, "question_id": 40134, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40134/do-different-ngrams-share-embedding-in-fasttext", "title": "Do different ngrams share embedding in Fasttext?", "body": "<p>As per Section 3.2 in the <a href=\"https://arxiv.org/pdf/1607.04606.pdf\" rel=\"nofollow noreferrer\">original paper on Fasttext</a>, the authors state:</p>\n<blockquote>\n<p>In order to bound the memory requirements of our model, we use a\nhashing function that maps n-grams to integers in 1 to K</p>\n</blockquote>\n<p>Does this mean the model computes only K embeddings regardless of the number of distinct ngrams extracted from the training corpus, and if 2 different ngrams collide when hashed, they share the same embedding?</p>\n<p>Thanks.</p>\n"}, "title": "Do different ngrams share embedding in Fasttext?", "content": "As per Section 3.2 in the original paper on Fasttext, the authors state:\n\nIn order to bound the memory requirements of our model, we use a\nhashing function that maps n-grams to integers in 1 to K\n\nDoes this mean the model computes only K embeddings regardless of the number of distinct ngrams extracted from the training corpus, and if 2 different ngrams collide when hashed, they share the same embedding?\nThanks.\n", "question_id": 40134, "answers": []}, "21347": {"link": "https://ai.stackexchange.com/questions/21347/how-to-add-a-pretrained-model-to-my-layers-to-get-embeddings", "metadata": {"tags": ["neural-networks", "word-embedding", "bert", "text-summarization", "pretrained-models"], "owner": {"reputation": 41, "user_id": 37006, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/FkluJ.jpg?s=256&g=1", "display_name": "inquisitive", "link": "https://ai.stackexchange.com/users/37006/inquisitive"}, "is_answered": true, "view_count": 187, "answer_count": 1, "score": 0, "last_activity_date": 1590058627, "creation_date": 1589988237, "question_id": 21347, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/21347/how-to-add-a-pretrained-model-to-my-layers-to-get-embeddings", "title": "How to add a pretrained model to my layers to get embeddings?", "body": "<p>I want to use a pretrained model found in [BERT Embeddings] <a href=\"https://github.com/UKPLab/sentence-transformers\" rel=\"nofollow noreferrer\">https://github.com/UKPLab/sentence-transformers</a> and I want to add a layer to get the sentence embeddings from the model and pass on to the next layer. How do I approach this?</p>\n\n<p>The inputs would be an array of documents and each document containing an array of sentences.</p>\n\n<p>The input to the model itself is a list of sentences where it will return a list of embeddings. </p>\n\n<p>This is what I've tried but couldn't solve the errors:</p>\n\n<pre><code>def get_embeddings(input_data):\n\n    input_embed = []\n    for doc in input_data:\n      doc = tf.unstack(doc)\n      doc_arr = asarray(doc)\n      doc = [el.decode('UTF-8') for el in doc_arr]\n      doc = list(doc)\n      assert(type(doc)== list)\n\n      new_doc = []\n      for sent in doc:\n        sent = tf.unstack(sent)\n        new_doc.append(str(sent))\n        assert(type(sent)== str)\n\n      embedding= model.encode(new_doc)  # Accepts lists of strings to return BERT sentence embeddings\n      input_embed.append(np.array(embedding))\n\n    return tf.convert_to_tensor(input_embed, dtype=float)\n\n\nsentences = tf.keras.layers.Input(shape=(3,5)) #test shape\nsent_embed = tf.keras.layers.Lambda(get_embeddings)\n\n\nx = sent_embed(sentences)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/1O8sY.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/1O8sY.png\" alt=\"Errors\"></a></p>\n"}, "title": "How to add a pretrained model to my layers to get embeddings?", "content": "I want to use a pretrained model found in [BERT Embeddings] https://github.com/UKPLab/sentence-transformers and I want to add a layer to get the sentence embeddings from the model and pass on to the next layer. How do I approach this?\nThe inputs would be an array of documents and each document containing an array of sentences.\nThe input to the model itself is a list of sentences where it will return a list of embeddings. \nThis is what I've tried but couldn't solve the errors:\ndef get_embeddings(input_data):\n\n    input_embed = []\n    for doc in input_data:\n      doc = tf.unstack(doc)\n      doc_arr = asarray(doc)\n      doc = [el.decode('UTF-8') for el in doc_arr]\n      doc = list(doc)\n      assert(type(doc)== list)\n\n      new_doc = []\n      for sent in doc:\n        sent = tf.unstack(sent)\n        new_doc.append(str(sent))\n        assert(type(sent)== str)\n\n      embedding= model.encode(new_doc)  # Accepts lists of strings to return BERT sentence embeddings\n      input_embed.append(np.array(embedding))\n\n    return tf.convert_to_tensor(input_embed, dtype=float)\n\n\nsentences = tf.keras.layers.Input(shape=(3,5)) #test shape\nsent_embed = tf.keras.layers.Lambda(get_embeddings)\n\n\nx = sent_embed(sentences)\n\n\n", "question_id": 21347, "answers": []}, "15463": {"link": "https://ai.stackexchange.com/questions/15463/how-could-i-compute-in-real-time-the-similarity-between-tickets", "metadata": {"tags": ["machine-learning", "word-embedding", "similarity"], "owner": {"reputation": 65, "user_id": 20780, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/Cq4A7.jpg?s=256&g=1", "display_name": "Alfonso", "link": "https://ai.stackexchange.com/users/20780/alfonso"}, "is_answered": true, "view_count": 253, "answer_count": 2, "score": 0, "last_activity_date": 1688278653, "creation_date": 1568636985, "last_edit_date": 1623231538, "question_id": 15463, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/15463/how-could-i-compute-in-real-time-the-similarity-between-tickets", "title": "How could I compute in real-time the similarity between tickets?", "body": "<p>I'm dealing with a &quot;ticket similarity task&quot;.</p>\n<p>Every time new tickets arrive at the help desk (customer service), I need to compare them and find out about similar ones.</p>\n<p>In this way, once the operator responds to a ticket, at the same time he can solve the others similar to the one solved.</p>\n<p>I expect an input ticket and all the other tickets with their similarity in output.</p>\n<p>I thought about using <strong>DOC2VEC</strong>, but it requires training every time a new ticket enters.</p>\n<p>What do you recommend?</p>\n"}, "title": "How could I compute in real-time the similarity between tickets?", "content": "I'm dealing with a \"ticket similarity task\".\nEvery time new tickets arrive at the help desk (customer service), I need to compare them and find out about similar ones.\nIn this way, once the operator responds to a ticket, at the same time he can solve the others similar to the one solved.\nI expect an input ticket and all the other tickets with their similarity in output.\nI thought about using DOC2VEC, but it requires training every time a new ticket enters.\nWhat do you recommend?\n", "question_id": 15463, "answers": []}, "41823": {"link": "https://ai.stackexchange.com/questions/41823/how-word2vec-de-embeds-the-special-names-in-language-models-which-output-text", "metadata": {"tags": ["natural-language-processing", "word-embedding", "word2vec"], "owner": {"reputation": 101, "user_id": 75376, "user_type": "registered", "profile_image": "https://lh4.googleusercontent.com/-uORkmm7Dauw/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfwpW5f46c-bht80cbqvQWd6qKIig/mo/photo.jpg?sz=256", "display_name": "Farhang Amaji", "link": "https://ai.stackexchange.com/users/75376/farhang-amaji"}, "is_answered": false, "view_count": 19, "answer_count": 0, "score": 0, "last_activity_date": 1692620295, "creation_date": 1692550149, "last_edit_date": 1692620295, "question_id": 41823, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41823/how-word2vec-de-embeds-the-special-names-in-language-models-which-output-text", "title": "How word2vec de-embeds the special names in language models which output text", "body": "<p>I am new to nlp field. I have some questions about word2vec embeddings. as I know they have a fixed size dictionary of vocabs. so definitely there some words which is not in that predefined dictionary of vocab, for i.e. we may get an special name (like people's name) which doesn't exist in the vocab and they are called out-of-vocabulary (OOV) words.</p>\n<p>So how embedding and de-embedding for a translation language model work? I want the language model of the example to include the de-embedding part; as we know in classification language models we may only concern about the embedding part as the output is not an embedding vector, and does not need to be converted to a string(word)</p>\n<p>I don't know what ways are used to embed those OOV words? I guess one way is to use some main word2vec and another complementary fastText for OOV words, or instead some complementary generic way of embedding like <code>nn.embedding</code> in pytorch. This won't make any problem in the classification models like sentiment analysis or Text classification.</p>\n<p>Question 1: what are other methods that can be used for embedding an OOV word while we want to use word2vec?</p>\n<p>but in the models which Their output is eventually gonna be a word (like translation or text summarization), I have some concerns as below:</p>\n<p>Note we know the output of a translation language model is a sequence of embedding vectors to correspondent output word.</p>\n<p>Note the way that we convert those embedding vectors to words, usually is by finding the closest embedding Vector in the embedding Matrix, either word2vec or fastText or de-embedder of <code>nn.embedding</code>. Also note this way eventually result, some Vector and from there, by its index, we would get a word.</p>\n<p>as we have two embedding models, so when we are the step which have received the language model output embedding vector, we need first distinguish, this output embedding vector Should be de-embedded from which embedding model (word2vec or fastText)? note as I said eventually the next action (finding closest embedding) would result some word.</p>\n<p>so I guess one way maybe is find the closest vectors for 2 embedding models, also we their similarity score to output embedding vector of the language model, and know to de-embed from which one.</p>\n<p>question 2: so how this problem to distinguish to which embedding model to de-embed from is handled? I guess there should be more sophisticated methods in Practice so if there are please tell me?</p>\n<p>question 3: do u know a model which have implemented my solutions (one for embedding by complementary embedding model, 2nd one distinguishing by similarity score)?</p>\n"}, "title": "How word2vec de-embeds the special names in language models which output text", "content": "I am new to nlp field. I have some questions about word2vec embeddings. as I know they have a fixed size dictionary of vocabs. so definitely there some words which is not in that predefined dictionary of vocab, for i.e. we may get an special name (like people's name) which doesn't exist in the vocab and they are called out-of-vocabulary (OOV) words.\nSo how embedding and de-embedding for a translation language model work? I want the language model of the example to include the de-embedding part; as we know in classification language models we may only concern about the embedding part as the output is not an embedding vector, and does not need to be converted to a string(word)\nI don't know what ways are used to embed those OOV words? I guess one way is to use some main word2vec and another complementary fastText for OOV words, or instead some complementary generic way of embedding like nn.embedding in pytorch. This won't make any problem in the classification models like sentiment analysis or Text classification.\nQuestion 1: what are other methods that can be used for embedding an OOV word while we want to use word2vec?\nbut in the models which Their output is eventually gonna be a word (like translation or text summarization), I have some concerns as below:\nNote we know the output of a translation language model is a sequence of embedding vectors to correspondent output word.\nNote the way that we convert those embedding vectors to words, usually is by finding the closest embedding Vector in the embedding Matrix, either word2vec or fastText or de-embedder of nn.embedding. Also note this way eventually result, some Vector and from there, by its index, we would get a word.\nas we have two embedding models, so when we are the step which have received the language model output embedding vector, we need first distinguish, this output embedding vector Should be de-embedded from which embedding model (word2vec or fastText)? note as I said eventually the next action (finding closest embedding) would result some word.\nso I guess one way maybe is find the closest vectors for 2 embedding models, also we their similarity score to output embedding vector of the language model, and know to de-embed from which one.\nquestion 2: so how this problem to distinguish to which embedding model to de-embed from is handled? I guess there should be more sophisticated methods in Practice so if there are please tell me?\nquestion 3: do u know a model which have implemented my solutions (one for embedding by complementary embedding model, 2nd one distinguishing by similarity score)?\n", "question_id": 41823, "answers": []}, "41002": {"link": "https://ai.stackexchange.com/questions/41002/are-the-dimensions-in-embedding-vectors-ordered-similar-to-pca", "metadata": {"tags": ["open-ai", "word-embedding", "embeddings", "principal-component-analysis"], "owner": {"reputation": 101, "user_id": 73448, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/e9dd7a8be048c27f4737ba7bf055018d?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "JackDaniels", "link": "https://ai.stackexchange.com/users/73448/jackdaniels"}, "is_answered": false, "view_count": 22, "answer_count": 0, "score": 0, "last_activity_date": 1687804115, "creation_date": 1687804115, "question_id": 41002, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41002/are-the-dimensions-in-embedding-vectors-ordered-similar-to-pca", "title": "Are the dimensions in embedding vectors ordered (similar to PCA)?", "body": "<p>I am getting started with the vector embeddings. I have a general question about the embedding vectors generated by popular algorithms.</p>\n<p>In PCA, usually, there is an implicit order of importance in the dimensions, with the most informative (by largest Eigen value) dimension first and the least at the end. Is there a similar property with vector embeddings generated by various models like Sentence transformers, OpenAI Embedding API, Google PaLM models etc?</p>\n"}, "title": "Are the dimensions in embedding vectors ordered (similar to PCA)?", "content": "I am getting started with the vector embeddings. I have a general question about the embedding vectors generated by popular algorithms.\nIn PCA, usually, there is an implicit order of importance in the dimensions, with the most informative (by largest Eigen value) dimension first and the least at the end. Is there a similar property with vector embeddings generated by various models like Sentence transformers, OpenAI Embedding API, Google PaLM models etc?\n", "question_id": 41002, "answers": []}, "40785": {"link": "https://ai.stackexchange.com/questions/40785/why-skip-gram-doesnt-just-use-the-probabilities-as-the-encoded-vector", "metadata": {"tags": ["word-embedding", "bag-of-words", "skip-gram"], "owner": {"reputation": 101, "user_id": 72807, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/xlbai.jpg?s=256&g=1", "display_name": "Minsky", "link": "https://ai.stackexchange.com/users/72807/minsky"}, "is_answered": false, "view_count": 9, "answer_count": 0, "score": 0, "last_activity_date": 1686377275, "creation_date": 1686376190, "last_edit_date": 1686377275, "question_id": 40785, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40785/why-skip-gram-doesnt-just-use-the-probabilities-as-the-encoded-vector", "title": "Why skip gram doesnt just use the probabilities as the encoded vector?", "body": "<p>I am very confused but this is what's on my head now:</p>\n<ol>\n<li>The skip-gram algorithm just multiplies hot-encoded-words with a weights-matrix,</li>\n<li>and since the word is hot encoded it is just multiplying a row of the matrix</li>\n<li>And so this matrix W will end up being the probabilities that we are predicting, or at least will be ordered in the same way.</li>\n</ol>\n<p>So what's the advantage of this algorithm? What exactly is figuring out?</p>\n<p>Wouldnt just a probability vector for each word, computed from the neighbours in a huge pool of words serve the same purpose without any optimization?</p>\n<p><a href=\"https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b\" rel=\"nofollow noreferrer\">One of the Blogs I read is this one, for example.</a></p>\n"}, "title": "Why skip gram doesnt just use the probabilities as the encoded vector?", "content": "I am very confused but this is what's on my head now:\n\nThe skip-gram algorithm just multiplies hot-encoded-words with a weights-matrix,\nand since the word is hot encoded it is just multiplying a row of the matrix\nAnd so this matrix W will end up being the probabilities that we are predicting, or at least will be ordered in the same way.\n\nSo what's the advantage of this algorithm? What exactly is figuring out?\nWouldnt just a probability vector for each word, computed from the neighbours in a huge pool of words serve the same purpose without any optimization?\nOne of the Blogs I read is this one, for example.\n", "question_id": 40785, "answers": []}, "40463": {"link": "https://ai.stackexchange.com/questions/40463/word-embeddings-but-for-logical-reasoning-in-custom-knowledge-gpt-3-5-bot", "metadata": {"tags": ["word-embedding", "gpt", "cosine-similarity", "prompt", "vector-semantics"], "owner": {"reputation": 1, "user_id": 71855, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AATXAJziD3LmF1T6-5Z0eL-kM11A7dZYvqPQ_va4xU8B=k-s256", "display_name": "Shahrukh Khan", "link": "https://ai.stackexchange.com/users/71855/shahrukh-khan"}, "is_answered": false, "view_count": 50, "answer_count": 0, "score": 0, "last_activity_date": 1684161414, "creation_date": 1684161414, "question_id": 40463, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40463/word-embeddings-but-for-logical-reasoning-in-custom-knowledge-gpt-3-5-bot", "title": "Word Embeddings but for Logical reasoning in custom knowledge GPT-3.5 bot", "body": "<p>So I have created a chatbot using GPT-3.5 turbo. I have a vector database that holds vector embeddings of brands, ratings, commission percentages, outlets, tags, etc. Here's how the system is designed.</p>\n<ol>\n<li>User Asks a question.</li>\n<li>The question is converted to vector embeddings using gpt-3 ada embedding model.</li>\n<li>The converted vector is searched in Milvus vector database using L2 Method.</li>\n<li>The fetched content is then used to create a custom knowledge context.</li>\n<li>This custom context is added in prompt for gpt3.5</li>\n<li>We send the created prompt to gpt3.5 turbo and answer user.</li>\n</ol>\n<p>The Problem:\nThe system is working perfectly. The problem is that this system is designed to work on similarity searches. This means users can ask for brands selling pizza, burgers, etc. Anything that is keyword based is fine. But when users search for the highest commission, highest rating, or highest-rated pizza brand. The results are random because the results are based on similarity not logical reasoning.</p>\n<p>I want to know a way to do this. When a user asks a logic-based question that needs reasoning we can somehow find out the correct brands from the database/vector database and then send that brand to gpt3.5 in a prompt.</p>\n<p>I'm open to ideas but don't want to use a rule-based bot.</p>\n<p>TIA.</p>\n"}, "title": "Word Embeddings but for Logical reasoning in custom knowledge GPT-3.5 bot", "content": "So I have created a chatbot using GPT-3.5 turbo. I have a vector database that holds vector embeddings of brands, ratings, commission percentages, outlets, tags, etc. Here's how the system is designed.\n\nUser Asks a question.\nThe question is converted to vector embeddings using gpt-3 ada embedding model.\nThe converted vector is searched in Milvus vector database using L2 Method.\nThe fetched content is then used to create a custom knowledge context.\nThis custom context is added in prompt for gpt3.5\nWe send the created prompt to gpt3.5 turbo and answer user.\n\nThe Problem:\nThe system is working perfectly. The problem is that this system is designed to work on similarity searches. This means users can ask for brands selling pizza, burgers, etc. Anything that is keyword based is fine. But when users search for the highest commission, highest rating, or highest-rated pizza brand. The results are random because the results are based on similarity not logical reasoning.\nI want to know a way to do this. When a user asks a logic-based question that needs reasoning we can somehow find out the correct brands from the database/vector database and then send that brand to gpt3.5 in a prompt.\nI'm open to ideas but don't want to use a rule-based bot.\nTIA.\n", "question_id": 40463, "answers": []}, "39658": {"link": "https://ai.stackexchange.com/questions/39658/how-are-the-softmax-normalized-weights-in-elmo-actually-learned-and-computed", "metadata": {"tags": ["natural-language-processing", "word-embedding", "language-model", "representation-learning", "bidirectional-lstm"], "owner": {"reputation": 1, "user_id": 69388, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AEdFTp5a6J1pJyDMa6QxBxa1_GBwIO4FCCJsyJCApxrY=k-s256", "display_name": "Propr", "link": "https://ai.stackexchange.com/users/69388/propr"}, "is_answered": false, "view_count": 22, "answer_count": 0, "score": 0, "last_activity_date": 1679187569, "creation_date": 1679187569, "question_id": 39658, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39658/how-are-the-softmax-normalized-weights-in-elmo-actually-learned-and-computed", "title": "How are the softmax normalized weights in ELMo actually learned and computed?", "body": "<p>I was reading the <a href=\"https://arxiv.org/abs/1802.05365\" rel=\"nofollow noreferrer\">ELMo paper</a>, and they speak of task-specific representations of words (or tokens generally speaking) by using the following equation: <span class=\"math-container\">$ELMo_{k}^{task} = \\gamma^{task}\\sum_{0}^{L}{s_{j}^{task}h_{k,j}^{LM}}$</span> where <span class=\"math-container\">$ELMo_{k}^{task}$</span> is the representation of the <span class=\"math-container\">$k-$</span>th token in one of the examples of a given <span class=\"math-container\">$task$</span>, <span class=\"math-container\">$\\gamma^{task}$</span> is a scaling factors and helps with the optimization process, <span class=\"math-container\">$s_{j}^{task}$</span> are the softmax-normalized weights and <span class=\"math-container\">$h_{k,j}^{LM}$</span> are the hidden states of the pretrained model.</p>\n<p><span class=\"math-container\">$s_{j}^{task}$</span> are supposed to be learnable parameters, but I don't see how we can learn them. Should there be a &quot;dense&quot; layer with a softmax activation function that takes as input the <span class=\"math-container\">$h_{k,j}^{LM}$</span> then outputs the <span class=\"math-container\">$ELMo_{k}^{task}$</span>?</p>\n"}, "title": "How are the softmax normalized weights in ELMo actually learned and computed?", "content": "I was reading the ELMo paper, and they speak of task-specific representations of words (or tokens generally speaking) by using the following equation: $ELMo_{k}^{task} = \\gamma^{task}\\sum_{0}^{L}{s_{j}^{task}h_{k,j}^{LM}}$ where $ELMo_{k}^{task}$ is the representation of the $k-$th token in one of the examples of a given $task$, $\\gamma^{task}$ is a scaling factors and helps with the optimization process, $s_{j}^{task}$ are the softmax-normalized weights and $h_{k,j}^{LM}$ are the hidden states of the pretrained model.\n$s_{j}^{task}$ are supposed to be learnable parameters, but I don't see how we can learn them. Should there be a \"dense\" layer with a softmax activation function that takes as input the $h_{k,j}^{LM}$ then outputs the $ELMo_{k}^{task}$?\n", "question_id": 39658, "answers": []}, "39276": {"link": "https://ai.stackexchange.com/questions/39276/does-attention-in-transformers-encode-any-information-from-positional-embeddings", "metadata": {"tags": ["natural-language-processing", "transformer", "attention", "word-embedding", "embeddings"], "owner": {"reputation": 1, "user_id": 68546, "user_type": "registered", "profile_image": "https://lh4.googleusercontent.com/-N3ETXha0pZU/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdjvP54i2f0ZxqpVZSSvD60pvrP_Q/photo.jpg?sz=256", "display_name": "Manny", "link": "https://ai.stackexchange.com/users/68546/manny"}, "is_answered": false, "view_count": 17, "answer_count": 0, "score": 0, "last_activity_date": 1677192563, "creation_date": 1677192563, "question_id": 39276, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39276/does-attention-in-transformers-encode-any-information-from-positional-embeddings", "title": "Does attention in transformers encode any information from positional embeddings?", "body": "<p>I know we account for positional embeddings before feeding into attention layers, but would we be able to say that the Q and K dot products intrinsically encode relative positions</p>\n"}, "title": "Does attention in transformers encode any information from positional embeddings?", "content": "I know we account for positional embeddings before feeding into attention layers, but would we be able to say that the Q and K dot products intrinsically encode relative positions\n", "question_id": 39276, "answers": []}, "38980": {"link": "https://ai.stackexchange.com/questions/38980/i-created-joint-embeddings-by-training-a-nn-with-contrastive-loss-why-are-my-re", "metadata": {"tags": ["neural-networks", "natural-language-processing", "pytorch", "word-embedding"], "owner": {"reputation": 1, "user_id": 67580, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b60b235a661a6550ab1ec1f70a4bbe51?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Sarthak Rastogi", "link": "https://ai.stackexchange.com/users/67580/sarthak-rastogi"}, "is_answered": false, "view_count": 25, "answer_count": 0, "score": 0, "last_activity_date": 1675184288, "creation_date": 1675184288, "question_id": 38980, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/38980/i-created-joint-embeddings-by-training-a-nn-with-contrastive-loss-why-are-my-re", "title": "I created joint embeddings by training a NN with contrastive loss. Why are my resulting embeddings so sparse?", "body": "<p>Using BERT and Word2Vec word embeddings as two inputs, I trained a small neural network using Contrastive loss. The NN looks like this:</p>\n<p>Net(\n(fcin1): Linear(in_features=768, out_features=500, bias=True)\n(fcin2): Linear(in_features=300, out_features=500, bias=True)\n)\nand the Contrastive loss implementation is taken from <a href=\"https://jamesmccaffrey.wordpress.com/2022/03/04/contrastive-loss-function-in-pytorch/\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>Here's the code:</p>\n<pre><code>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.fcin1 = nn.Linear(768, 500)\n        self.fcin2 = nn.Linear(300, 500)\n\n    def forward(self, x1, x2):\n        x1 = F.relu(self.fcin1(x1))\n        x2 = F.relu(self.fcin2(x2))\n        return x1, x2\n\nfor epoch in range(EPOCHS):\n    for x1, x2 in tqdm(data):\n        optimizer.zero_grad()\n        y1, y2 = net(x1, x2)\n        loss = loss_func(y1, y2, 0)\n        loss.backward()#retain_graph=True)\n        #loss.detach()\n        optimizer.step()\n</code></pre>\n<p>When I run Net() after training, I find that the resulting 500d embeddings are very sparse.\nWhat could be the possible reasons, and what should I try differently?</p>\n<p>An interesting thing is, as I use more and more words to train the NN, the resulting embeddings become more and more sparse. Any insights on why this could be the case?</p>\n<p>Any help is highly appreciated.</p>\n"}, "title": "I created joint embeddings by training a NN with contrastive loss. Why are my resulting embeddings so sparse?", "content": "Using BERT and Word2Vec word embeddings as two inputs, I trained a small neural network using Contrastive loss. The NN looks like this:\nNet(\n(fcin1): Linear(in_features=768, out_features=500, bias=True)\n(fcin2): Linear(in_features=300, out_features=500, bias=True)\n)\nand the Contrastive loss implementation is taken from here.\nHere's the code:\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.fcin1 = nn.Linear(768, 500)\n        self.fcin2 = nn.Linear(300, 500)\n\n    def forward(self, x1, x2):\n        x1 = F.relu(self.fcin1(x1))\n        x2 = F.relu(self.fcin2(x2))\n        return x1, x2\n\nfor epoch in range(EPOCHS):\n    for x1, x2 in tqdm(data):\n        optimizer.zero_grad()\n        y1, y2 = net(x1, x2)\n        loss = loss_func(y1, y2, 0)\n        loss.backward()#retain_graph=True)\n        #loss.detach()\n        optimizer.step()\n\nWhen I run Net() after training, I find that the resulting 500d embeddings are very sparse.\nWhat could be the possible reasons, and what should I try differently?\nAn interesting thing is, as I use more and more words to train the NN, the resulting embeddings become more and more sparse. Any insights on why this could be the case?\nAny help is highly appreciated.\n", "question_id": 38980, "answers": []}, "37195": {"link": "https://ai.stackexchange.com/questions/37195/how-to-create-a-fixed-length-binary-sequence-of-tokens-embedding", "metadata": {"tags": ["recurrent-neural-networks", "word-embedding"], "owner": {"reputation": 31, "user_id": 33902, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/ca6998014e8739789135c050cab86dd5?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "bluewander", "link": "https://ai.stackexchange.com/users/33902/bluewander"}, "is_answered": false, "view_count": 27, "answer_count": 0, "score": 0, "last_activity_date": 1664984935, "creation_date": 1664178427, "last_edit_date": 1664984935, "question_id": 37195, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/37195/how-to-create-a-fixed-length-binary-sequence-of-tokens-embedding", "title": "How to Create a Fixed-Length, Binary, Sequence of Tokens Embedding?", "body": "<p>Say I have ten classes represented by 1 x n_classes vector of binary.</p>\n<p>My goal is to embed a sequence of 1xN binary so that I can also model the <strong>class-co occurrence.</strong></p>\n<p>Say, class A, B, D are present and represented as <code>[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]</code></p>\n<p>The embedding model aims to produce an embedding for this sequence.</p>\n<pre><code>sequence = [1, 1, 0, 1, 0, 0, 0, 0, 0, 0]\nembedding = model(sequence)\n</code></pre>\n"}, "title": "How to Create a Fixed-Length, Binary, Sequence of Tokens Embedding?", "content": "Say I have ten classes represented by 1 x n_classes vector of binary.\nMy goal is to embed a sequence of 1xN binary so that I can also model the class-co occurrence.\nSay, class A, B, D are present and represented as [1, 1, 0, 1, 0, 0, 0, 0, 0, 0]\nThe embedding model aims to produce an embedding for this sequence.\nsequence = [1, 1, 0, 1, 0, 0, 0, 0, 0, 0]\nembedding = model(sequence)\n\n", "question_id": 37195, "answers": []}, "37146": {"link": "https://ai.stackexchange.com/questions/37146/do-transformers-and-lstms-use-the-same-word-embeddings-except-for-the-position", "metadata": {"tags": ["natural-language-processing", "long-short-term-memory", "transformer", "word-embedding"], "owner": {"reputation": 11, "user_id": 61128, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/cba820b3829fa19537aa248475e67eea?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "AAT", "link": "https://ai.stackexchange.com/users/61128/aat"}, "is_answered": false, "view_count": 209, "answer_count": 0, "score": 0, "last_activity_date": 1663689546, "creation_date": 1663689546, "question_id": 37146, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/37146/do-transformers-and-lstms-use-the-same-word-embeddings-except-for-the-position", "title": "Do Transformers and LSTMs use the same word embeddings (except for the position encoding, which only Transformers use)?", "body": "<p>In NLP, the first step is always to &quot;convert&quot; the given words of a sentence into representation vectors (word embeddings).</p>\n<p>As I understand it, in the case of transformers, the words/vocabulary are one-hot encoded and then multiplied by a weight matrix - the results are the representation vectors (as always the weights must be learned by backpropagation). Since transformers process words in parallel, a positional encoding is added to the representation vectors to avoid losing the positional information.</p>\n<p>My question now is: do LSTM's use the exact same word embedding mechanism - just without adding the positional encoding (since RNNs can remember the position information of sequences)?</p>\n<p><a href=\"https://i.stack.imgur.com/eirjg.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/eirjg.png\" alt=\"Matrix multiplication of one-hot-encoded word matrix and weight matrix resulting in the word embedding\" /></a></p>\n"}, "title": "Do Transformers and LSTMs use the same word embeddings (except for the position encoding, which only Transformers use)?", "content": "In NLP, the first step is always to \"convert\" the given words of a sentence into representation vectors (word embeddings).\nAs I understand it, in the case of transformers, the words/vocabulary are one-hot encoded and then multiplied by a weight matrix - the results are the representation vectors (as always the weights must be learned by backpropagation). Since transformers process words in parallel, a positional encoding is added to the representation vectors to avoid losing the positional information.\nMy question now is: do LSTM's use the exact same word embedding mechanism - just without adding the positional encoding (since RNNs can remember the position information of sequences)?\n\n", "question_id": 37146, "answers": []}, "37076": {"link": "https://ai.stackexchange.com/questions/37076/is-bert-capable-of-producing-semantically-close-word-embeddings-for-synonyms", "metadata": {"tags": ["bert", "word-embedding", "similarity", "semantics", "vector-semantics"], "owner": {"reputation": 11, "user_id": 54007, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3907f73c2021c3129767330526341cef?s=256&d=identicon&r=PG", "display_name": "nesquick", "link": "https://ai.stackexchange.com/users/54007/nesquick"}, "is_answered": false, "view_count": 96, "answer_count": 0, "score": 0, "last_activity_date": 1663074460, "creation_date": 1663074460, "question_id": 37076, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/37076/is-bert-capable-of-producing-semantically-close-word-embeddings-for-synonyms", "title": "Is BERT capable of producing semantically close word embeddings for synonyms?", "body": "<p>I am currently working on my undergraduate thesis on matching job descriptions to resumes based on the contents of both. Recently, I came across the following statement by Schmitt et al., 2016: &quot;[...] [Recruiters] and\njob seekers [...] do not seem to speak the same language [...]. More precisely, CVs and job announcements tend to use different vocabularies, and same words might be used with different meanings&quot;.</p>\n<p>Therefore, <strong>is BERT able to create contextualized word embeddings that are semantically similar or close for synonyms and semantically dissimilar or distant for the same words that have different meanings in the context of resumes and job postings?</strong> As far as I understand BERT, this should be the case as long as the same word occurs in a different context/sentence, but I'm not sure.</p>\n<p>Thank you very much in advance!</p>\n"}, "title": "Is BERT capable of producing semantically close word embeddings for synonyms?", "content": "I am currently working on my undergraduate thesis on matching job descriptions to resumes based on the contents of both. Recently, I came across the following statement by Schmitt et al., 2016: \"[...] [Recruiters] and\njob seekers [...] do not seem to speak the same language [...]. More precisely, CVs and job announcements tend to use different vocabularies, and same words might be used with different meanings\".\nTherefore, is BERT able to create contextualized word embeddings that are semantically similar or close for synonyms and semantically dissimilar or distant for the same words that have different meanings in the context of resumes and job postings? As far as I understand BERT, this should be the case as long as the same word occurs in a different context/sentence, but I'm not sure.\nThank you very much in advance!\n", "question_id": 37076, "answers": []}, "35225": {"link": "https://ai.stackexchange.com/questions/35225/how-is-the-training-comlexity-of-nnlm-word2vec-calculated", "metadata": {"tags": ["neural-networks", "word-embedding", "word2vec", "computational-complexity"], "owner": {"reputation": 3, "user_id": 54135, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/e98a12e64c761a99e9892120fde58e2e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "MDescamps", "link": "https://ai.stackexchange.com/users/54135/mdescamps"}, "is_answered": true, "view_count": 62, "accepted_answer_id": 37446, "answer_count": 1, "score": 0, "last_activity_date": 1665929957, "creation_date": 1650038015, "last_edit_date": 1650461953, "question_id": 35225, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/35225/how-is-the-training-comlexity-of-nnlm-word2vec-calculated", "title": "How is the training comlexity of NNLM word2vec calculated?", "body": "<p>I was reading <a href=\"https://arxiv.org/abs/1301.3781\" rel=\"nofollow noreferrer\">this paper</a> on word2vec, and came around the following description of a feedforward NNLM:</p>\n<blockquote>\n<p>It consists of input, projection, hidden and output layers. At the input layer, N previous words are encoded using 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a projection layer P that has dimensionality N \u00d7 D, using a shared projection matrix. As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation.</p>\n</blockquote>\n<p>The following expression is given for the computational complexity per training example:</p>\n<blockquote>\n<p>Q = N\u00d7D + N\u00d7D\u00d7H + H\u00d7V.</p>\n</blockquote>\n<p>The last two terms make sense to me: N\u00d7D\u00d7H is roughly the amount of parameters in a dense layer from the N\u00d7D-dimensional projection layer to the H hidden neurons, analogous for H\u00d7V. The first term, however, I expected to be V\u00d7D since the mapping from a one-hot encoded word to a D-dimensional vector is done via a V\u00d7D dimensional matrix. I came to that conclusion after reading <a href=\"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\" rel=\"nofollow noreferrer\">this referenced paper</a> and <a href=\"https://stackoverflow.com/a/37937675/7952998\">this SO post</a> where the workings of the projection layer are explained in more detail.</p>\n<p>Perhaps I have misunderstood what is meant by &quot;training complexity&quot;.</p>\n"}, "title": "How is the training comlexity of NNLM word2vec calculated?", "content": "I was reading this paper on word2vec, and came around the following description of a feedforward NNLM:\n\nIt consists of input, projection, hidden and output layers. At the input layer, N previous words are encoded using 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a projection layer P that has dimensionality N \u00d7 D, using a shared projection matrix. As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation.\n\nThe following expression is given for the computational complexity per training example:\n\nQ = N\u00d7D + N\u00d7D\u00d7H + H\u00d7V.\n\nThe last two terms make sense to me: N\u00d7D\u00d7H is roughly the amount of parameters in a dense layer from the N\u00d7D-dimensional projection layer to the H hidden neurons, analogous for H\u00d7V. The first term, however, I expected to be V\u00d7D since the mapping from a one-hot encoded word to a D-dimensional vector is done via a V\u00d7D dimensional matrix. I came to that conclusion after reading this referenced paper and this SO post where the workings of the projection layer are explained in more detail.\nPerhaps I have misunderstood what is meant by \"training complexity\".\n", "question_id": 35225, "answers": []}, "32721": {"link": "https://ai.stackexchange.com/questions/32721/what-are-the-types-of-inputs-used-for-rnn-in-literature-given-sentences", "metadata": {"tags": ["natural-language-processing", "recurrent-neural-networks", "word-embedding"], "owner": {"reputation": 3571, "user_id": 18758, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/339397e23b4e4cc78aa36e2b126252c7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "hanugm", "link": "https://ai.stackexchange.com/users/18758/hanugm"}, "is_answered": false, "view_count": 27, "answer_count": 0, "score": 0, "last_activity_date": 1639434295, "creation_date": 1639297997, "last_edit_date": 1639434295, "question_id": 32721, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/32721/what-are-the-types-of-inputs-used-for-rnn-in-literature-given-sentences", "title": "What are the types of inputs used for RNN in literature given sentences?", "body": "<p>Suppose there are <span class=\"math-container\">$m$</span> sentences in a text file and the number of distinct words is equal to <span class=\"math-container\">$n$</span>. The goal is to get word embeddings using RNN.</p>\n<p>We know that it is impossible to pass any word, which is in text format, as an input to RNN. We need to convert each word into some number and then pass it to the RNN to get word embeddings.</p>\n<p>I know only the following method, if correct:</p>\n<ol>\n<li>Assign an index to each word. So, the index ranges from <span class=\"math-container\">$0$</span> to <span class=\"math-container\">$n-1$</span>.</li>\n<li>Use the indices as input to RNN.</li>\n</ol>\n<p>Is it the only technique used in the literature? If not, what are the names of other techniques that are used in the context of RNN encoders?</p>\n"}, "title": "What are the types of inputs used for RNN in literature given sentences?", "content": "Suppose there are $m$ sentences in a text file and the number of distinct words is equal to $n$. The goal is to get word embeddings using RNN.\nWe know that it is impossible to pass any word, which is in text format, as an input to RNN. We need to convert each word into some number and then pass it to the RNN to get word embeddings.\nI know only the following method, if correct:\n\nAssign an index to each word. So, the index ranges from $0$ to $n-1$.\nUse the indices as input to RNN.\n\nIs it the only technique used in the literature? If not, what are the names of other techniques that are used in the context of RNN encoders?\n", "question_id": 32721, "answers": []}, "26440": {"link": "https://ai.stackexchange.com/questions/26440/are-the-word2vec-encoded-embeddings-available-online", "metadata": {"tags": ["deep-learning", "natural-language-processing", "word-embedding", "word2vec", "embeddings"], "owner": {"reputation": 15, "user_id": 44695, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-RATNx_cZj94/AAAAAAAAAAI/AAAAAAAAAIk/m8nZ46xi8Jk/photo.jpg?sz=256", "display_name": "user1234", "link": "https://ai.stackexchange.com/users/44695/user1234"}, "is_answered": true, "view_count": 330, "closed_date": 1613742238, "accepted_answer_id": 26464, "answer_count": 1, "score": 0, "last_activity_date": 1619956612, "creation_date": 1613653536, "last_edit_date": 1619956612, "question_id": 26440, "link": "https://ai.stackexchange.com/questions/26440/are-the-word2vec-encoded-embeddings-available-online", "closed_reason": "Not suitable for this site", "title": "Are the Word2Vec encoded embeddings available online?", "body": "<p>I am trying to do an NLP project and was wondering if there is anywhere online where the Word2Vec embeddings are stored (the actual n-dimmensional vectors).\nI want to search up a word and see what its encoding is. I have tried looking but couldn't find anything.\nThank you</p>\n"}, "title": "Are the Word2Vec encoded embeddings available online?", "content": "I am trying to do an NLP project and was wondering if there is anywhere online where the Word2Vec embeddings are stored (the actual n-dimmensional vectors).\nI want to search up a word and see what its encoding is. I have tried looking but couldn't find anything.\nThank you\n", "question_id": 26440, "answers": []}, "26405": {"link": "https://ai.stackexchange.com/questions/26405/nlp-are-hashtags-tokenised", "metadata": {"tags": ["natural-language-processing", "recurrent-neural-networks", "word-embedding"], "owner": {"reputation": 15, "user_id": 44695, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-RATNx_cZj94/AAAAAAAAAAI/AAAAAAAAAIk/m8nZ46xi8Jk/photo.jpg?sz=256", "display_name": "user1234", "link": "https://ai.stackexchange.com/users/44695/user1234"}, "is_answered": true, "view_count": 105, "accepted_answer_id": 26409, "answer_count": 1, "score": 0, "last_activity_date": 1632012399, "creation_date": 1613468300, "last_edit_date": 1632012399, "question_id": 26405, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/26405/nlp-are-hashtags-tokenised", "title": "NLP: Are hashtags tokenised?", "body": "<p>I am exploring a potential NLP project. I was wondering what generally is done with the hashtags words (e.g. <code>#hello</code>). Are those words ignored? is the <code>#</code> removed and the word tokenised? Is it tokenised with the <code>#</code>?</p>\n"}, "title": "NLP: Are hashtags tokenised?", "content": "I am exploring a potential NLP project. I was wondering what generally is done with the hashtags words (e.g. #hello). Are those words ignored? is the # removed and the word tokenised? Is it tokenised with the #?\n", "question_id": 26405, "answers": []}, "26218": {"link": "https://ai.stackexchange.com/questions/26218/is-there-a-reason-why-no-one-combines-word-embeddings-with-the-median", "metadata": {"tags": ["natural-language-processing", "word-embedding"], "owner": {"reputation": 9, "user_id": 44404, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-hz5FZPCIjao/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucls_WvAQ_qwWTHCUaCqC3FMwYj2yw/s96-c/photo.jpg?sz=256", "display_name": "Stephan", "link": "https://ai.stackexchange.com/users/44404/stephan"}, "is_answered": false, "view_count": 51, "answer_count": 0, "score": 0, "last_activity_date": 1612509725, "creation_date": 1612508752, "last_edit_date": 1612509725, "question_id": 26218, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/26218/is-there-a-reason-why-no-one-combines-word-embeddings-with-the-median", "title": "Is there a reason why no one combines word embeddings with the median?", "body": "<p>Could you combine word embeddings with the median per dimension to get a document embedding? In my case I have a huge amount of words to build one document, which in turn should describe a topic. I feel like using the median is the right thing to do, as I get the most common parameter value per dimension. However, I cannot find anyone trying it before. This is why I'm wondering, is there something speaking against it?</p>\n"}, "title": "Is there a reason why no one combines word embeddings with the median?", "content": "Could you combine word embeddings with the median per dimension to get a document embedding? In my case I have a huge amount of words to build one document, which in turn should describe a topic. I feel like using the median is the right thing to do, as I get the most common parameter value per dimension. However, I cannot find anyone trying it before. This is why I'm wondering, is there something speaking against it?\n", "question_id": 26218, "answers": []}, "11236": {"link": "https://ai.stackexchange.com/questions/11236/why-would-adding-all-the-possible-embeddings-be-worse-than-using-1d-convolutio", "metadata": {"tags": ["natural-language-processing", "word-embedding", "word2vec", "1d-convolution", "n-gram"], "owner": {"reputation": 31, "user_id": 23120, "user_type": "unregistered", "profile_image": "https://www.gravatar.com/avatar/eada06eece702339c6f4944c7a50f1bc?s=256&d=identicon&r=PG", "display_name": "aiguy123", "link": "https://ai.stackexchange.com/users/23120/aiguy123"}, "is_answered": false, "view_count": 135, "answer_count": 1, "score": 0, "last_activity_date": 1692626865, "creation_date": 1552574638, "last_edit_date": 1607077203, "question_id": 11236, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/11236/why-would-adding-all-the-possible-embeddings-be-worse-than-using-1d-convolutio", "title": "Why would adding all the possible embeddings be &quot;worse&quot; than using 1D-convolutions?", "body": "<p>Suppose we are using word2vec and have embeddings of individual words <span class=\"math-container\">$w_1, \\dots, w_{10}$</span>. Let's say we wanted to analyze <span class=\"math-container\">$2$</span> grams or <span class=\"math-container\">$3$</span> grams.</p>\n<p>Why would adding all the possible embeddings, <span class=\"math-container\">$\\binom{10}{2}$</span> or <span class=\"math-container\">$\\binom{10}{3}$</span>, be &quot;worse&quot; than using 1D-convolutions?</p>\n"}, "title": "Why would adding all the possible embeddings be &quot;worse&quot; than using 1D-convolutions?", "content": "Suppose we are using word2vec and have embeddings of individual words $w_1, \\dots, w_{10}$. Let's say we wanted to analyze $2$ grams or $3$ grams.\nWhy would adding all the possible embeddings, $\\binom{10}{2}$ or $\\binom{10}{3}$, be \"worse\" than using 1D-convolutions?\n", "question_id": 11236, "answers": []}, "22673": {"link": "https://ai.stackexchange.com/questions/22673/what-exactly-are-the-parameters-in-gpt-3s-175-billion-parameters-and-how-are", "metadata": {"tags": ["recurrent-neural-networks", "open-ai", "transformer", "attention", "gpt"], "owner": {"reputation": 481, "user_id": 9268, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/QhMRI.png?s=256&g=1", "display_name": "Nav", "link": "https://ai.stackexchange.com/users/9268/nav"}, "is_answered": true, "view_count": 16357, "accepted_answer_id": 22715, "answer_count": 1, "score": 26, "last_activity_date": 1595929831, "creation_date": 1595751152, "last_edit_date": 1595754284, "question_id": 22673, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/22673/what-exactly-are-the-parameters-in-gpt-3s-175-billion-parameters-and-how-are", "title": "What exactly are the &quot;parameters&quot; in GPT-3&#39;s 175 billion parameters and how are they chosen/generated?", "body": "<p>When I studied neural networks, parameters were learning rate, batch size etc. But even GPT3's ArXiv paper does not mention anything about what exactly the parameters are, but gives a small hint that they might just be sentences.</p>\n<p><a href=\"https://i.stack.imgur.com/LWEEQ.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/LWEEQ.png\" alt=\"enter image description here\" /></a></p>\n<p>Even tutorial sites like <a href=\"https://www.analyticsvidhya.com/blog/2019/07/openai-gpt2-text-generator-python/\" rel=\"noreferrer\">this one</a> start talking about the usual parameters, but also say <code>&quot;model_name: This indicates which model we are using. In our case, we are using the GPT-2 model with 345 million parameters or weights&quot;</code>. So are the 175 billion &quot;parameters&quot; just neural weights? Why then are they called parameters? <a href=\"https://arxiv.org/pdf/2005.14165.pdf\" rel=\"noreferrer\">GPT3's paper</a> shows that there are only 96 layers, so I'm assuming it's not a very deep network, but extremely fat. Or does it mean that each &quot;parameter&quot; is just a representation of the encoders or decoders?</p>\n<p><a href=\"https://i.stack.imgur.com/dthvC.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/dthvC.png\" alt=\"enter image description here\" /></a></p>\n<p>An excerpt from <a href=\"https://minimaxir.com/2019/09/howto-gpt2/\" rel=\"noreferrer\">this website</a> shows tokens:</p>\n<blockquote>\n<p>In this case, there are two additional parameters that can be passed\nto gpt2.generate(): truncate and include_prefix. For example, if each\nshort text begins with a &lt;|startoftext|&gt; token and ends with a\n&lt;|endoftext|&gt;, then setting prefix='&lt;|startoftext|&gt;',\ntruncate=&lt;|endoftext|&gt;', and include_prefix=False, and length is\nsufficient, then gpt-2-simple will automatically extract the shortform\ntexts, even when generating in batches.</p>\n</blockquote>\n<p>So are the parameters various kinds of tokens that are manually created by humans who try to fine-tune the models? Still, 175 billion such fine-tuning parameters is too high for humans to create, so I assume the &quot;parameters&quot; are auto-generated somehow.</p>\n<p>The <a href=\"https://arxiv.org/pdf/1706.03762.pdf\" rel=\"noreferrer\">attention-based</a> paper mentions the <a href=\"http://jalammar.github.io/illustrated-gpt2/\" rel=\"noreferrer\">query-key-value weight</a> matrices as the &quot;parameters&quot;. <strong>Even if it is these weights, I'd just like to know what kind of a process generates these parameters, who chooses the parameters and specifies the relevance of words? If it's created automatically, how is it done?</strong></p>\n"}, "title": "What exactly are the &quot;parameters&quot; in GPT-3&#39;s 175 billion parameters and how are they chosen/generated?", "content": "When I studied neural networks, parameters were learning rate, batch size etc. But even GPT3's ArXiv paper does not mention anything about what exactly the parameters are, but gives a small hint that they might just be sentences.\n\nEven tutorial sites like this one start talking about the usual parameters, but also say \"model_name: This indicates which model we are using. In our case, we are using the GPT-2 model with 345 million parameters or weights\". So are the 175 billion \"parameters\" just neural weights? Why then are they called parameters? GPT3's paper shows that there are only 96 layers, so I'm assuming it's not a very deep network, but extremely fat. Or does it mean that each \"parameter\" is just a representation of the encoders or decoders?\n\nAn excerpt from this website shows tokens:\n\nIn this case, there are two additional parameters that can be passed\nto gpt2.generate(): truncate and include_prefix. For example, if each\nshort text begins with a <|startoftext|> token and ends with a\n<|endoftext|>, then setting prefix='<|startoftext|>',\ntruncate=<|endoftext|>', and include_prefix=False, and length is\nsufficient, then gpt-2-simple will automatically extract the shortform\ntexts, even when generating in batches.\n\nSo are the parameters various kinds of tokens that are manually created by humans who try to fine-tune the models? Still, 175 billion such fine-tuning parameters is too high for humans to create, so I assume the \"parameters\" are auto-generated somehow.\nThe attention-based paper mentions the query-key-value weight matrices as the \"parameters\". Even if it is these weights, I'd just like to know what kind of a process generates these parameters, who chooses the parameters and specifies the relevance of words? If it's created automatically, how is it done?\n", "question_id": 22673, "answers": []}, "27038": {"link": "https://ai.stackexchange.com/questions/27038/why-does-gpt-2-exclude-the-transformer-encoder", "metadata": {"tags": ["natural-language-processing", "transformer", "attention", "bert", "gpt"], "owner": {"reputation": 311, "user_id": 37519, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/afdb576792ee07d70d7d39d77a8cb1d0?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Athena Wisdom", "link": "https://ai.stackexchange.com/users/37519/athena-wisdom"}, "is_answered": true, "view_count": 9107, "answer_count": 2, "score": 16, "last_activity_date": 1674154259, "creation_date": 1616874930, "last_edit_date": 1616878355, "question_id": 27038, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/27038/why-does-gpt-2-exclude-the-transformer-encoder", "title": "Why does GPT-2 Exclude the Transformer Encoder?", "body": "<p>After looking into transformers, BERT, and GPT-2, from what I understand, GPT-2 essentially uses only the decoder part of the original transformer architecture and uses masked self-attention that can only look at prior tokens.</p>\n<p>Why does GPT-2 not require the encoder part of the original transformer architecture?</p>\n<p><strong>GPT-2 architecture with only decoder layers</strong></p>\n<p><a href=\"https://i.stack.imgur.com/Kb8Gq.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Kb8Gq.png\" alt=\"enter image description here\" /></a></p>\n"}, "title": "Why does GPT-2 Exclude the Transformer Encoder?", "content": "After looking into transformers, BERT, and GPT-2, from what I understand, GPT-2 essentially uses only the decoder part of the original transformer architecture and uses masked self-attention that can only look at prior tokens.\nWhy does GPT-2 not require the encoder part of the original transformer architecture?\nGPT-2 architecture with only decoder layers\n\n", "question_id": 27038, "answers": []}, "24831": {"link": "https://ai.stackexchange.com/questions/24831/what-is-the-difference-between-the-positional-encoding-techniques-of-the-transfo", "metadata": {"tags": ["comparison", "transformer", "gpt", "positional-encoding"], "owner": {"reputation": 285, "user_id": 26580, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/804161bd975649daac0baaa19c4c0c04?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Leevo", "link": "https://ai.stackexchange.com/users/26580/leevo"}, "is_answered": true, "view_count": 2361, "accepted_answer_id": 34346, "answer_count": 2, "score": 7, "last_activity_date": 1646154065, "creation_date": 1606168991, "last_edit_date": 1638285637, "question_id": 24831, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/24831/what-is-the-difference-between-the-positional-encoding-techniques-of-the-transfo", "title": "What is the difference between the positional encoding techniques of the Transformer and GPT?", "body": "<p>I know the original Transformer and the GPT (1-3) use two slightly different <strong>positional encoding</strong> techniques.</p>\n<p>More specifically, in GPT they say positional encoding is <em>learned</em>. What does that mean? OpenAI's papers don't go into detail very much.</p>\n<p>How do they really differ, mathematically speaking?</p>\n"}, "title": "What is the difference between the positional encoding techniques of the Transformer and GPT?", "content": "I know the original Transformer and the GPT (1-3) use two slightly different positional encoding techniques.\nMore specifically, in GPT they say positional encoding is learned. What does that mean? OpenAI's papers don't go into detail very much.\nHow do they really differ, mathematically speaking?\n", "question_id": 24831, "answers": []}, "10869": {"link": "https://ai.stackexchange.com/questions/10869/how-do-we-know-if-gpt-2-is-a-better-language-model", "metadata": {"tags": ["natural-language-processing", "transformer", "gpt"], "owner": {"reputation": 232, "user_id": 22654, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b783ab7b39873c411ed32b676f1e0e5c?s=256&d=identicon&r=PG", "display_name": "lcrmorin", "link": "https://ai.stackexchange.com/users/22654/lcrmorin"}, "is_answered": true, "view_count": 710, "answer_count": 1, "score": 7, "last_activity_date": 1600773965, "creation_date": 1551088283, "last_edit_date": 1600773965, "question_id": 10869, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/10869/how-do-we-know-if-gpt-2-is-a-better-language-model", "title": "How do we know if GPT-2 is a better language model?", "body": "<p>You may have heard of GPT2, a new language model. It has recently attracted attention from the general public as the foundation that published the paper, <a href=\"https://blog.openai.com/better-language-models/\" rel=\"nofollow noreferrer\">OpenAI</a>, ironically refused to share the whole model fearing dangerous implications. Along the paper, they also published a manifesto to justify their choice: &quot;Better Language Models and Their Implications&quot;. And soon a lot of media were publishing articles discussing the choice and its effectiveness to actually prevent bad implications. I am not here to discuss the ethical components of this choice but the actual performance of the model.</p>\n<p>The model got my attention too and I downloaded the small model to play with. To be honest I am far from impressed by the results. Some times the first paragraph of the produced text appears to make sense, but nine times out of ten it is giberish by the first or the second sentence. Exemples given in the paper seems to be &quot;Lucky&quot; outputs, cherry picked by human hands. Overall, the paper may suffer from a very strong publication bias.</p>\n<p>However, most article we can read on the internet seems to take its powerfulness for granted. <a href=\"https://www.technologyreview.com/s/612975/ai-natural-language-processing-explained/\" rel=\"nofollow noreferrer\">The MIT technology review</a> wrote:</p>\n<blockquote>\n<p>The language model can write like a human</p>\n</blockquote>\n<p><a href=\"https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction\" rel=\"nofollow noreferrer\">The Guardian</a> wrote</p>\n<blockquote>\n<p>When used to simply generate new text, GPT2 is capable of writing plausible passages that match what it is given in both style and subject. It rarely shows any of the quirks that mark out previous AI systems, such as forgetting what it is writing about midway through a paragraph, or mangling the syntax of long sentences.</p>\n</blockquote>\n<p>The model appears generally qualified as a &quot;breakthrough&quot;. These writings do not match my personal experimentation as produced texts are rarely consistent / syntactically correct.</p>\n<p>My question is: without the release of the whole model for ethical reasons, how do we know if the model is really that powerful?</p>\n"}, "title": "How do we know if GPT-2 is a better language model?", "content": "You may have heard of GPT2, a new language model. It has recently attracted attention from the general public as the foundation that published the paper, OpenAI, ironically refused to share the whole model fearing dangerous implications. Along the paper, they also published a manifesto to justify their choice: \"Better Language Models and Their Implications\". And soon a lot of media were publishing articles discussing the choice and its effectiveness to actually prevent bad implications. I am not here to discuss the ethical components of this choice but the actual performance of the model.\nThe model got my attention too and I downloaded the small model to play with. To be honest I am far from impressed by the results. Some times the first paragraph of the produced text appears to make sense, but nine times out of ten it is giberish by the first or the second sentence. Exemples given in the paper seems to be \"Lucky\" outputs, cherry picked by human hands. Overall, the paper may suffer from a very strong publication bias.\nHowever, most article we can read on the internet seems to take its powerfulness for granted. The MIT technology review wrote:\n\nThe language model can write like a human\n\nThe Guardian wrote\n\nWhen used to simply generate new text, GPT2 is capable of writing plausible passages that match what it is given in both style and subject. It rarely shows any of the quirks that mark out previous AI systems, such as forgetting what it is writing about midway through a paragraph, or mangling the syntax of long sentences.\n\nThe model appears generally qualified as a \"breakthrough\". These writings do not match my personal experimentation as produced texts are rarely consistent / syntactically correct.\nMy question is: without the release of the whole model for ethical reasons, how do we know if the model is really that powerful?\n", "question_id": 10869, "answers": []}, "16516": {"link": "https://ai.stackexchange.com/questions/16516/is-the-mask-needed-for-masked-self-attention-during-inference-with-gpt-2", "metadata": {"tags": ["natural-language-processing", "attention", "transformer", "gpt", "inference"], "owner": {"reputation": 51, "user_id": 31284, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d1c56a7f12933860c3eaa6f3152036d7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "D_s", "link": "https://ai.stackexchange.com/users/31284/d-s"}, "is_answered": true, "view_count": 1678, "answer_count": 1, "score": 5, "last_activity_date": 1682224886, "creation_date": 1573731672, "last_edit_date": 1682224886, "question_id": 16516, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/16516/is-the-mask-needed-for-masked-self-attention-during-inference-with-gpt-2", "title": "Is the Mask Needed for Masked Self-Attention During Inference with GPT-2", "body": "<p>My understanding is that masked self-attention is necessary during training of GPT-2, as otherwise it would be able to directly see the correct next output at each iteration. My question is whether the attention mask is necessary, or even possible, during inference. As GPT-2 will only be producing one token at a time, it doesn't make sense to mask out future tokens that haven't been inferred yet.</p>\n"}, "title": "Is the Mask Needed for Masked Self-Attention During Inference with GPT-2", "content": "My understanding is that masked self-attention is necessary during training of GPT-2, as otherwise it would be able to directly see the correct next output at each iteration. My question is whether the attention mask is necessary, or even possible, during inference. As GPT-2 will only be producing one token at a time, it doesn't make sense to mask out future tokens that haven't been inferred yet.\n", "question_id": 16516, "answers": []}, "40140": {"link": "https://ai.stackexchange.com/questions/40140/how-is-the-next-token-predicted-in-transformers", "metadata": {"tags": ["natural-language-processing", "transformer", "gpt", "language-model"], "owner": {"reputation": 51, "user_id": 70907, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-i9Zhwe-sX3o/AAAAAAAAAAI/AAAAAAAAB4M/AKF05nCDG07sHtq4CQbUkNia9WiUNiAMCA/photo.jpg?sz=256", "display_name": "Miguel Carvalho", "link": "https://ai.stackexchange.com/users/70907/miguel-carvalho"}, "is_answered": true, "view_count": 858, "answer_count": 2, "score": 5, "last_activity_date": 1686604775, "creation_date": 1682038084, "question_id": 40140, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40140/how-is-the-next-token-predicted-in-transformers", "title": "How is the next token predicted in transformers?", "body": "<p>In the transformer (or GPT/decoder only), at the end of the decoder blocks but before the final linear layer you have X vectors (for the X tokens at the input of the decoder). We then want to compute the probabilities for the next token of the sequence - what do we then feed to the linear layer? Is it the last embedding corresponding to the hidden state of the last token in the input sequence?</p>\n<p>I've seen some tutorials on youtube on how to make mini gpts but I never quite understood why they feed the entire X vectors/hidden states at the end of the decoder blocks to the linear layer and not just the last vector/hidden state... Wouldn't you have X probability distributions when in reality you only want one? And if we do want the X probability distributions then wouldn't we be completely missing the point of the masked self attention since we would be trying to predict words that are already in the input sequence, so essentially &quot;cheating&quot;?</p>\n"}, "title": "How is the next token predicted in transformers?", "content": "In the transformer (or GPT/decoder only), at the end of the decoder blocks but before the final linear layer you have X vectors (for the X tokens at the input of the decoder). We then want to compute the probabilities for the next token of the sequence - what do we then feed to the linear layer? Is it the last embedding corresponding to the hidden state of the last token in the input sequence?\nI've seen some tutorials on youtube on how to make mini gpts but I never quite understood why they feed the entire X vectors/hidden states at the end of the decoder blocks to the linear layer and not just the last vector/hidden state... Wouldn't you have X probability distributions when in reality you only want one? And if we do want the X probability distributions then wouldn't we be completely missing the point of the masked self attention since we would be trying to predict words that are already in the input sequence, so essentially \"cheating\"?\n", "question_id": 40140, "answers": []}, "7684": {"link": "https://ai.stackexchange.com/questions/7684/where-can-i-find-pre-trained-language-models-in-english-and-german", "metadata": {"tags": ["neural-networks", "natural-language-processing", "bert", "gpt", "language-model"], "owner": {"reputation": 161, "user_id": 17670, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/aPDjE.jpg?s=256&g=1", "display_name": "Lutz B&#252;ch", "link": "https://ai.stackexchange.com/users/17670/lutz-b%c3%bcch"}, "is_answered": true, "view_count": 1886, "closed_date": 1641470629, "accepted_answer_id": 7754, "answer_count": 2, "score": 5, "locked_date": 1641470643, "last_activity_date": 1572578295, "creation_date": 1535008647, "last_edit_date": 1572578295, "question_id": 7684, "link": "https://ai.stackexchange.com/questions/7684/where-can-i-find-pre-trained-language-models-in-english-and-german", "closed_reason": "Not suitable for this site", "title": "Where can I find pre-trained language models in English and German?", "body": "<p>Where can I find (more) pre-trained <a href=\"https://en.wikipedia.org/wiki/Language_model\" rel=\"nofollow noreferrer\">language models</a>? I am especially interested in <strong>neural network-based</strong> models for <strong>English and German</strong>.</p>\n\n<p>I am aware only of <a href=\"https://github.com/tensorflow/models/tree/master/research/lm_1b\" rel=\"nofollow noreferrer\">Language Model on One Billion Word Benchmark</a> and <a href=\"https://github.com/lverwimp/tf-lm\" rel=\"nofollow noreferrer\">TF-LM: TensorFlow-based Language Modeling Toolkit</a>.</p>\n\n<p>I am surprised not to find a greater wealth of models for different frameworks and languages.</p>\n"}, "title": "Where can I find pre-trained language models in English and German?", "content": "Where can I find (more) pre-trained language models? I am especially interested in neural network-based models for English and German.\nI am aware only of Language Model on One Billion Word Benchmark and TF-LM: TensorFlow-based Language Modeling Toolkit.\nI am surprised not to find a greater wealth of models for different frameworks and languages.\n", "question_id": 7684, "answers": []}, "40360": {"link": "https://ai.stackexchange.com/questions/40360/what-sort-of-computer-would-be-necessary-to-run-queries-on-a-llm", "metadata": {"tags": ["gpt"], "owner": {"reputation": 195, "user_id": 64845, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/241d579477d197bacc59fdd29d33f701?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "ak0000", "link": "https://ai.stackexchange.com/users/64845/ak0000"}, "is_answered": true, "view_count": 1522, "accepted_answer_id": 40365, "answer_count": 2, "score": 4, "last_activity_date": 1684707742, "creation_date": 1683558086, "question_id": 40360, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40360/what-sort-of-computer-would-be-necessary-to-run-queries-on-a-llm", "title": "What sort of computer would be necessary to run queries on a LLM?", "body": "<p>I've heard that to train a model like GPT 4.0 you need a very powerful computer and ~$10M of computing power, but once you've produced the trained ~570GB model, what sort of computing power is necessary to execute specific queries with it?</p>\n"}, "title": "What sort of computer would be necessary to run queries on a LLM?", "content": "I've heard that to train a model like GPT 4.0 you need a very powerful computer and ~$10M of computing power, but once you've produced the trained ~570GB model, what sort of computing power is necessary to execute specific queries with it?\n", "question_id": 40360, "answers": []}, "22581": {"link": "https://ai.stackexchange.com/questions/22581/why-is-gpt-3-such-a-game-changer", "metadata": {"tags": ["reinforcement-learning", "open-ai", "gpt"], "owner": {"reputation": 141, "user_id": 38719, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/652e1b419903e5f0cdd03c70e49b6f34?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "parzival", "link": "https://ai.stackexchange.com/users/38719/parzival"}, "is_answered": true, "view_count": 576, "answer_count": 1, "score": 4, "last_activity_date": 1600090465, "creation_date": 1595186591, "last_edit_date": 1600090465, "question_id": 22581, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/22581/why-is-gpt-3-such-a-game-changer", "title": "Why is GPT-3 such a game changer?", "body": "<p>I've been hearing a lot about <a href=\"https://arxiv.org/abs/2005.14165\" rel=\"nofollow noreferrer\">GPT-3 by OpenAI</a>, and that it's a simple to use API with text in text out and has a big neural network off 175B parameters.</p>\n<p>But how did they achieve this huge number of parameters, and why is it being predicted as one of the greatest innovations?</p>\n"}, "title": "Why is GPT-3 such a game changer?", "content": "I've been hearing a lot about GPT-3 by OpenAI, and that it's a simple to use API with text in text out and has a big neural network off 175B parameters.\nBut how did they achieve this huge number of parameters, and why is it being predicted as one of the greatest innovations?\n", "question_id": 22581, "answers": []}, "23418": {"link": "https://ai.stackexchange.com/questions/23418/can-in-principle-gpt-language-models-learn-physics", "metadata": {"tags": ["gpt", "natural-language-understanding"], "owner": {"reputation": 284, "user_id": 30433, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/XhPWe.jpg?s=256&g=1", "display_name": "Wolphram jonny", "link": "https://ai.stackexchange.com/users/30433/wolphram-jonny"}, "is_answered": false, "view_count": 274, "answer_count": 0, "score": 4, "last_activity_date": 1599355647, "creation_date": 1599162030, "last_edit_date": 1599355647, "question_id": 23418, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/23418/can-in-principle-gpt-language-models-learn-physics", "title": "Can in principle GPT language models learn physics?", "body": "<p>Does anyone know of research involving the GPT models to learn not only regular texts, but also learn from physics books with the equations written in latex format?</p>\n<p>My intuition is that the model might learn the rules relating equations and deductions, as they can learn statistically what correlates with what. I understand that the results can also be a little nonsensical, like the sometimes surreal paragraphs written by these models.</p>\n<p>Have there been any attempts to do this?</p>\n"}, "title": "Can in principle GPT language models learn physics?", "content": "Does anyone know of research involving the GPT models to learn not only regular texts, but also learn from physics books with the equations written in latex format?\nMy intuition is that the model might learn the rules relating equations and deductions, as they can learn statistically what correlates with what. I understand that the results can also be a little nonsensical, like the sometimes surreal paragraphs written by these models.\nHave there been any attempts to do this?\n", "question_id": 23418, "answers": []}, "40232": {"link": "https://ai.stackexchange.com/questions/40232/process-2tb-worth-of-conversational-data-hoarded-over-40-years-how-can-i-pass-t", "metadata": {"tags": ["natural-language-processing", "data-preprocessing", "chatgpt", "gpt"], "owner": {"reputation": 139, "user_id": 71216, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/KIFfC.jpg?s=256&g=1", "display_name": "Patoshi \u30d1\u30c8\u30b7", "link": "https://ai.stackexchange.com/users/71216/patoshi-%e3%83%91%e3%83%88%e3%82%b7"}, "is_answered": true, "view_count": 607, "answer_count": 1, "score": 3, "last_activity_date": 1682711764, "creation_date": 1682709940, "question_id": 40232, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40232/process-2tb-worth-of-conversational-data-hoarded-over-40-years-how-can-i-pass-t", "title": "Process 2TB worth of conversational data hoarded over 40 years. How can I pass this into GPT to ask questions about it?", "body": "<p>I'm still very new to this stuff. I have close to 2TB worth of data hoarded from IRC chats to everyday chats with friends and family.</p>\n<p>But is there a way to pass in this much data into GPT to ask questions about it? Or would I require something else?</p>\n<p><strong>For example:</strong></p>\n<p>&quot;When did Bob tell Jane about the legos he had in school when they were at home?&quot;</p>\n"}, "title": "Process 2TB worth of conversational data hoarded over 40 years. How can I pass this into GPT to ask questions about it?", "content": "I'm still very new to this stuff. I have close to 2TB worth of data hoarded from IRC chats to everyday chats with friends and family.\nBut is there a way to pass in this much data into GPT to ask questions about it? Or would I require something else?\nFor example:\n\"When did Bob tell Jane about the legos he had in school when they were at home?\"\n", "question_id": 40232, "answers": []}, "27254": {"link": "https://ai.stackexchange.com/questions/27254/how-to-select-model-parameters-for-transformer-heads-number-of-layers-etc", "metadata": {"tags": ["natural-language-processing", "transformer", "hyperparameter-optimization", "attention", "gpt"], "owner": {"reputation": 311, "user_id": 37519, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/afdb576792ee07d70d7d39d77a8cb1d0?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Athena Wisdom", "link": "https://ai.stackexchange.com/users/37519/athena-wisdom"}, "is_answered": false, "view_count": 415, "answer_count": 0, "score": 3, "last_activity_date": 1618068286, "creation_date": 1618068286, "question_id": 27254, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/27254/how-to-select-model-parameters-for-transformer-heads-number-of-layers-etc", "title": "How to Select Model Parameters for Transformer (Heads, number of layers, etc)", "body": "<p>Is there a general guideline on how the Transformer model parameters should be selected, or the range of these parameters that should be included in a hyperparameter sweep?</p>\n<ul>\n<li>Number of heads</li>\n<li>Number of encoder &amp; decoder layers</li>\n<li>Size of transformer model (<code>d_model</code> in Pytorch)</li>\n<li>Size of hidden layers</li>\n</ul>\n<p>Are there general guidelines like number of decoder layers should be equal to encoder layers? Thank you</p>\n"}, "title": "How to Select Model Parameters for Transformer (Heads, number of layers, etc)", "content": "Is there a general guideline on how the Transformer model parameters should be selected, or the range of these parameters that should be included in a hyperparameter sweep?\n\nNumber of heads\nNumber of encoder & decoder layers\nSize of transformer model (d_model in Pytorch)\nSize of hidden layers\n\nAre there general guidelines like number of decoder layers should be equal to encoder layers? Thank you\n", "question_id": 27254, "answers": []}, "27947": {"link": "https://ai.stackexchange.com/questions/27947/is-it-realistic-to-train-a-transformer-based-model-e-g-gpt-in-a-self-supervis", "metadata": {"tags": ["transformer", "gpt", "audio-processing", "embeddings", "self-supervised-learning"], "owner": {"reputation": 432, "user_id": 9092, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/401b42cd42a7e2c89d872509e2614efd?s=256&d=identicon&r=PG", "display_name": "Peter Franek", "link": "https://ai.stackexchange.com/users/9092/peter-franek"}, "is_answered": true, "view_count": 337, "accepted_answer_id": 27953, "answer_count": 2, "score": 2, "last_activity_date": 1631896456, "creation_date": 1621891915, "last_edit_date": 1622016823, "question_id": 27947, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/27947/is-it-realistic-to-train-a-transformer-based-model-e-g-gpt-in-a-self-supervis", "title": "Is it realistic to train a transformer-based model (e.g. GPT) in a self-supervised way directly on the Mel spectrogram?", "body": "<p>In music information retrieval, one usually converts an audio signal into some kind &quot;sequence of frequency-vectors&quot;, such as STFT or Mel-spectrogram.</p>\n<p>I'm wondering if it is a good idea to use the transformer architecture in a self-supervised manner -- such as auto-regressive models, or BERT in NLP -- to obtain a &quot;smarter&quot; representation of the music than the spectrogram itself. Such smart pretrained representation could be used for further downstream tasks.</p>\n<p>From my quick google search, I found several papers which do something similar, but -- to my surprise -- all use some kind of symbolic/discrete music representation such as scores. (For instance <a href=\"https://arxiv.org/pdf/1809.04281.pdf\" rel=\"nofollow noreferrer\">here</a> or <a href=\"https://arxiv.org/pdf/1912.05537.pdf\" rel=\"nofollow noreferrer\">here</a>).</p>\n<p>My question is this:</p>\n<blockquote>\n<p>Is it realistic to train such an unsupervised model directly on the\nMel spectrogram?</p>\n</blockquote>\n<p>The loss function would not be &quot;log softmax of next word probability&quot;, but some kind of l2-distance between &quot;predicted vector of spectra&quot; and &quot;observed vector of spectra&quot;, in the next time step.</p>\n<p>Did someone try it?</p>\n"}, "title": "Is it realistic to train a transformer-based model (e.g. GPT) in a self-supervised way directly on the Mel spectrogram?", "content": "In music information retrieval, one usually converts an audio signal into some kind \"sequence of frequency-vectors\", such as STFT or Mel-spectrogram.\nI'm wondering if it is a good idea to use the transformer architecture in a self-supervised manner -- such as auto-regressive models, or BERT in NLP -- to obtain a \"smarter\" representation of the music than the spectrogram itself. Such smart pretrained representation could be used for further downstream tasks.\nFrom my quick google search, I found several papers which do something similar, but -- to my surprise -- all use some kind of symbolic/discrete music representation such as scores. (For instance here or here).\nMy question is this:\n\nIs it realistic to train such an unsupervised model directly on the\nMel spectrogram?\n\nThe loss function would not be \"log softmax of next word probability\", but some kind of l2-distance between \"predicted vector of spectra\" and \"observed vector of spectra\", in the next time step.\nDid someone try it?\n", "question_id": 27947, "answers": []}, "25369": {"link": "https://ai.stackexchange.com/questions/25369/is-it-possible-to-integrate-the-gpt-3-by-openapi-inside-unity3d-or-any-game-engi", "metadata": {"tags": ["game-ai", "gpt", "dialogue-systems", "integration"], "owner": {"reputation": 73, "user_id": 43190, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/Y3Vs8.jpg?s=256&g=1", "display_name": "Leoverload", "link": "https://ai.stackexchange.com/users/43190/leoverload"}, "is_answered": true, "view_count": 1334, "accepted_answer_id": 25396, "answer_count": 2, "score": 2, "last_activity_date": 1609189971, "creation_date": 1608809930, "last_edit_date": 1608812852, "question_id": 25369, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/25369/is-it-possible-to-integrate-the-gpt-3-by-openapi-inside-unity3d-or-any-game-engi", "title": "Is it possible to integrate the GPT-3 by OpenAPI inside Unity3D or any game-engine?", "body": "<p>My company has full access to beta testing for GPT-3. We wanted to try it for some games or game mechanics within Unity3D. Is it possible to use it for dialogues or with unity scripts?</p>\n<p>The Documents of OpenAI does not say anything about this possibility, so I'm not sure.</p>\n"}, "title": "Is it possible to integrate the GPT-3 by OpenAPI inside Unity3D or any game-engine?", "content": "My company has full access to beta testing for GPT-3. We wanted to try it for some games or game mechanics within Unity3D. Is it possible to use it for dialogues or with unity scripts?\nThe Documents of OpenAI does not say anything about this possibility, so I'm not sure.\n", "question_id": 25369, "answers": []}, "22734": {"link": "https://ai.stackexchange.com/questions/22734/how-large-should-the-corpus-be-to-optimally-retrain-the-gpt-2-model", "metadata": {"tags": ["deep-learning", "natural-language-processing", "tensorflow", "training", "gpt"], "owner": {"reputation": 131, "user_id": 38955, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6cef5876c787f53c174b7bc6789b473b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Andreas Tores&#228;ter", "link": "https://ai.stackexchange.com/users/38955/andreas-tores%c3%a4ter"}, "is_answered": true, "view_count": 679, "answer_count": 1, "score": 2, "last_activity_date": 1630152495, "creation_date": 1596004586, "last_edit_date": 1596088165, "question_id": 22734, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/22734/how-large-should-the-corpus-be-to-optimally-retrain-the-gpt-2-model", "title": "How large should the corpus be to optimally retrain the GPT-2 model?", "body": "<p>I just started working with the GPT-2 models and want to retrain one on a pretty narrow topic, so I have problems finding training material.</p>\n<p>How large should the corpus be to optimally retrain the GPT-2 model? And what is the bare minimum size? Should it simply be as large as possible or can it flip over and make the model worse in some way?</p>\n<p>I am also not certain how many steps you should let the retraining run. I have been using 6000 steps when testing, and it seems not much happens after that, loss only moved from 0.2 to 0.18 last 1000 steps.</p>\n"}, "title": "How large should the corpus be to optimally retrain the GPT-2 model?", "content": "I just started working with the GPT-2 models and want to retrain one on a pretty narrow topic, so I have problems finding training material.\nHow large should the corpus be to optimally retrain the GPT-2 model? And what is the bare minimum size? Should it simply be as large as possible or can it flip over and make the model worse in some way?\nI am also not certain how many steps you should let the retraining run. I have been using 6000 steps when testing, and it seems not much happens after that, loss only moved from 0.2 to 0.18 last 1000 steps.\n", "question_id": 22734, "answers": []}, "39817": {"link": "https://ai.stackexchange.com/questions/39817/whats-the-most-efficient-way-of-performing-batched-training-of-causal-language", "metadata": {"tags": ["training", "transformer", "gpt", "language-model", "batch-learning"], "owner": {"reputation": 309, "user_id": 55107, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/183d6e8b6fcb762d6cbc1c15dd7423da?s=256&d=identicon&r=PG", "display_name": "thesofakillers", "link": "https://ai.stackexchange.com/users/55107/thesofakillers"}, "is_answered": false, "view_count": 111, "answer_count": 0, "score": 2, "last_activity_date": 1679989246, "creation_date": 1679989246, "question_id": 39817, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39817/whats-the-most-efficient-way-of-performing-batched-training-of-causal-language", "title": "What&#39;s the most efficient way of performing batched training of Causal Language Models?", "body": "<p>I have seen a number of ways to train (yes, train, not fine-tune) these models efficiently with batches. I will illustrate these techniques with the following example dataset and context window:</p>\n<pre><code>Context window:\n   -----------------\nData samples:\n1. ###\n2. ################\n3. ####\n4. ##############\n5. ########\n6. #########\n</code></pre>\n<p>Suppose we have a batch size of 2. Our pad token is x</p>\n<h2>First technique: Vanilla Padding</h2>\n<pre><code>Context window:\n   -----------------\nbatch 1:\n1. ###xxxxxxxxxxxxx\n2. ################\n\nbatch 2:\n3. ####xxxxxxxxxx\n4. ##############\n\nbatch 3: \n5. ########x\n6. #########\n</code></pre>\n<h2>Second technique: Bucketed Padding</h2>\n<p>Samples of similar lengths are batched together to minimise the number of pad tokens</p>\n<pre><code>Context window:\n   -----------------\nbatch 1:\n1. ###x\n3. ####\n\nbatch 2:\n2. ################\n4. ##############xx\n\nbatch 3: \n5. ########x\n6. #########\n</code></pre>\n<p>this is <em>uniform length batching</em> described in <a href=\"https://mccormickml.com/2020/07/29/smart-batching-tutorial/\" rel=\"nofollow noreferrer\">this blogpost</a> and referred to as <em>bucketed random sampling</em> in <a href=\"https://aclanthology.org/2021.findings-acl.74/\" rel=\"nofollow noreferrer\">this paper</a>.</p>\n<h2>Third technique: Concatenating samples</h2>\n<p>In this technique, we concatenate samples, separating them with a EOS token (E) until they reach the context length. In this way, we have no padding tokens, and the entire context length is used. The attention mask keeps track of where the EOS tokens occur.</p>\n<pre><code>Context window:\n   -----------------\nbatch 1:\n   ###E############# (1 and part of 2)\nbatch 2:\n   ###E####E######## (rest of 2, 3 and part of 4)\nbatch 3:\n   ######E########E# (rest of 4, 5, part of 6)\nbatch 4:\n   ######## (rest of 6)\n</code></pre>\n<p>This technique is referenced at 2:28 of <a href=\"https://www.youtube.com/watch?v=ma1TrR7gE7I\" rel=\"nofollow noreferrer\">this video</a> from <a href=\"https://huggingface.co/course/chapter7/6\" rel=\"nofollow noreferrer\">this huggingface tutorial</a>.</p>\n<p>With this technique, we reduce the number of batches, and only have to pad the final batch if necessary. However, it is unclear to me whether this is &quot;allowed&quot; for causal language modelling, as it is unclear whether this will cause the causal attention mechanism to attend to tokens from previous samples, only ignoring the EOS token (instead of everything before it)</p>\n<hr />\n<p>Of these 3 techniques, which is the most memory efficient? Which is the most commonly used?</p>\n"}, "title": "What&#39;s the most efficient way of performing batched training of Causal Language Models?", "content": "I have seen a number of ways to train (yes, train, not fine-tune) these models efficiently with batches. I will illustrate these techniques with the following example dataset and context window:\nContext window:\n   -----------------\nData samples:\n1. ###\n2. ################\n3. ####\n4. ##############\n5. ########\n6. #########\n\nSuppose we have a batch size of 2. Our pad token is x\nFirst technique: Vanilla Padding\nContext window:\n   -----------------\nbatch 1:\n1. ###xxxxxxxxxxxxx\n2. ################\n\nbatch 2:\n3. ####xxxxxxxxxx\n4. ##############\n\nbatch 3: \n5. ########x\n6. #########\n\nSecond technique: Bucketed Padding\nSamples of similar lengths are batched together to minimise the number of pad tokens\nContext window:\n   -----------------\nbatch 1:\n1. ###x\n3. ####\n\nbatch 2:\n2. ################\n4. ##############xx\n\nbatch 3: \n5. ########x\n6. #########\n\nthis is uniform length batching described in this blogpost and referred to as bucketed random sampling in this paper.\nThird technique: Concatenating samples\nIn this technique, we concatenate samples, separating them with a EOS token (E) until they reach the context length. In this way, we have no padding tokens, and the entire context length is used. The attention mask keeps track of where the EOS tokens occur.\nContext window:\n   -----------------\nbatch 1:\n   ###E############# (1 and part of 2)\nbatch 2:\n   ###E####E######## (rest of 2, 3 and part of 4)\nbatch 3:\n   ######E########E# (rest of 4, 5, part of 6)\nbatch 4:\n   ######## (rest of 6)\n\nThis technique is referenced at 2:28 of this video from this huggingface tutorial.\nWith this technique, we reduce the number of batches, and only have to pad the final batch if necessary. However, it is unclear to me whether this is \"allowed\" for causal language modelling, as it is unclear whether this will cause the causal attention mechanism to attend to tokens from previous samples, only ignoring the EOS token (instead of everything before it)\n\nOf these 3 techniques, which is the most memory efficient? Which is the most commonly used?\n", "question_id": 39817, "answers": []}, "17992": {"link": "https://ai.stackexchange.com/questions/17992/pretrained-models-for-keyword-based-text-generation", "metadata": {"tags": ["transformer", "gpt", "text-generation"], "owner": {"reputation": 129, "user_id": 33476, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d243c2c7c40cd0ca33462ef9d560604b?s=256&d=identicon&r=PG", "display_name": "Comfort Eagle", "link": "https://ai.stackexchange.com/users/33476/comfort-eagle"}, "is_answered": false, "view_count": 314, "answer_count": 0, "score": 2, "last_activity_date": 1581526458, "creation_date": 1581526458, "last_edit_date": 1592387840, "question_id": 17992, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/17992/pretrained-models-for-keyword-based-text-generation", "title": "Pretrained Models for Keyword-Based Text Generation", "body": "<p>I'm looking for an implementation that allows me to generate text based on a pre-trained model (e.g. GPT-2).</p>\n<p>An example would be <a href=\"https://github.com/minimaxir/gpt-2-keyword-generation\" rel=\"nofollow noreferrer\">gpt-2-keyword-generation</a> (<a href=\"https://minimaxir.com/apps/gpt2-reddit/\" rel=\"nofollow noreferrer\">click here for demo</a>). As the author notes, there is</p>\n<blockquote>\n<p>[...] no explicit mathematical/theoetical basis behind the keywords\naside from the typical debiasing of the text [...]</p>\n</blockquote>\n<p>Hence my question: <strong>Are there more sophisticated ways of keyword-based text generation</strong> or at least any other <strong>alternatives</strong>?</p>\n<p>Thank you</p>\n"}, "title": "Pretrained Models for Keyword-Based Text Generation", "content": "I'm looking for an implementation that allows me to generate text based on a pre-trained model (e.g. GPT-2).\nAn example would be gpt-2-keyword-generation (click here for demo). As the author notes, there is\n\n[...] no explicit mathematical/theoetical basis behind the keywords\naside from the typical debiasing of the text [...]\n\nHence my question: Are there more sophisticated ways of keyword-based text generation or at least any other alternatives?\nThank you\n", "question_id": 17992, "answers": []}, "17930": {"link": "https://ai.stackexchange.com/questions/17930/can-we-use-gpt-2-to-smooth-out-correct-text", "metadata": {"tags": ["neural-networks", "natural-language-processing", "gpt"], "owner": {"reputation": 121, "user_id": 33392, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3228aae57c1dc3f657bbc64c26c97b77?s=256&d=identicon&r=PG", "display_name": "Sugendran", "link": "https://ai.stackexchange.com/users/33392/sugendran"}, "is_answered": false, "view_count": 302, "answer_count": 0, "score": 2, "last_activity_date": 1581259546, "creation_date": 1581242504, "last_edit_date": 1592387840, "question_id": 17930, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/17930/can-we-use-gpt-2-to-smooth-out-correct-text", "title": "Can we use GPT-2 to smooth out / correct text?", "body": "<p>Are we able to use models like GPT-2 to smooth out/correct text? For instance if I have two paragraphs that need some text to make the transition easier to read, could this text be generated? And, could it find inconsistencies between the paragraphs and fix them?</p>\n<p>As an example, imagine we're reordering some text so that we can apply the <a href=\"https://medium.com/lessons-from-mckinsey/the-pyramid-principle-f0885dd3c5c7\" rel=\"nofollow noreferrer\">pyramid principle</a>. What I'd like to do is reorder the sentences/paragraphs and still have a coherant story. The following three sentences for instance, start with a statement and then have some facts to support it. What's missing is the story that joins them together, right now they're three independent sentences.</p>\n<blockquote>\n<p>The strawberry is the best fruit based on its flavor profile, its coloring and texture and the nutritional profile.</p>\n<p>Strawberries are very rich in antioxidants and plant compounds, which may have benefits for heart health and blood sugar control.</p>\n<p>Strawberries have a long history and have been enjoyed since the Roman times.</p>\n</blockquote>\n<p>Feel free to point me at things to read, I have not been able to find anything like this in my searches.</p>\n"}, "title": "Can we use GPT-2 to smooth out / correct text?", "content": "Are we able to use models like GPT-2 to smooth out/correct text? For instance if I have two paragraphs that need some text to make the transition easier to read, could this text be generated? And, could it find inconsistencies between the paragraphs and fix them?\nAs an example, imagine we're reordering some text so that we can apply the pyramid principle. What I'd like to do is reorder the sentences/paragraphs and still have a coherant story. The following three sentences for instance, start with a statement and then have some facts to support it. What's missing is the story that joins them together, right now they're three independent sentences.\n\nThe strawberry is the best fruit based on its flavor profile, its coloring and texture and the nutritional profile.\nStrawberries are very rich in antioxidants and plant compounds, which may have benefits for heart health and blood sugar control.\nStrawberries have a long history and have been enjoyed since the Roman times.\n\nFeel free to point me at things to read, I have not been able to find anything like this in my searches.\n", "question_id": 17930, "answers": []}, "13862": {"link": "https://ai.stackexchange.com/questions/13862/how-to-interpret-a-large-variance-of-the-loss-function", "metadata": {"tags": ["objective-functions", "transformer", "gpt"], "owner": {"reputation": 270, "user_id": 25798, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/a6b495308482759d359eb97636fdccb0?s=256&d=identicon&r=PG", "display_name": "allo", "link": "https://ai.stackexchange.com/users/25798/allo"}, "is_answered": false, "view_count": 327, "answer_count": 0, "score": 2, "last_activity_date": 1572578106, "creation_date": 1565286585, "last_edit_date": 1572578106, "question_id": 13862, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/13862/how-to-interpret-a-large-variance-of-the-loss-function", "title": "How to interpret a large variance of the loss function?", "body": "<p>How do I interpret a large variance of a loss function?</p>\n\n<p>I am currently training a transformer network (using the software, but not the model from GPT-2) from scratch and my loss function looks like this:\n<a href=\"https://i.stack.imgur.com/LvhRs.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/LvhRs.png\" alt=\"a plot of the loss function\"></a></p>\n\n<p>The green dots are the loss averaged over 100 epochs and the purple dots are the loss for each epoch.</p>\n\n<p>(You can ignore the missing part, I just did not save the loss values for these epochs)</p>\n\n<p>Is such a large variance a bad sign? And what are my options for tuning to get it to converge faster? Is the network to large or too small for my training data? Should I have a look at batch size?</p>\n\n<ul>\n<li>Learning rate parameter: 2.5e-4</li>\n<li>Training data size: 395 MB</li>\n</ul>\n\n<p>GPT-2 parameters:</p>\n\n<pre><code>{\n  \"n_vocab\": 50000,\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_layer\": 12\n}\n</code></pre>\n"}, "title": "How to interpret a large variance of the loss function?", "content": "How do I interpret a large variance of a loss function?\nI am currently training a transformer network (using the software, but not the model from GPT-2) from scratch and my loss function looks like this:\n\nThe green dots are the loss averaged over 100 epochs and the purple dots are the loss for each epoch.\n(You can ignore the missing part, I just did not save the loss values for these epochs)\nIs such a large variance a bad sign? And what are my options for tuning to get it to converge faster? Is the network to large or too small for my training data? Should I have a look at batch size?\n\nLearning rate parameter: 2.5e-4\nTraining data size: 395 MB\n\nGPT-2 parameters:\n{\n  \"n_vocab\": 50000,\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_layer\": 12\n}\n\n", "question_id": 13862, "answers": []}, "11621": {"link": "https://ai.stackexchange.com/questions/11621/how-can-i-generate-a-document-from-a-single-word-using-gpt-or-bert", "metadata": {"tags": ["natural-language-processing", "bert", "language-model", "gpt"], "owner": {"reputation": 161, "user_id": 19244, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/52b8676fe99b3c143f5842874523f53b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "mayank agrawal", "link": "https://ai.stackexchange.com/users/19244/mayank-agrawal"}, "is_answered": false, "view_count": 79, "answer_count": 0, "score": 2, "last_activity_date": 1572577127, "creation_date": 1554297315, "last_edit_date": 1572577127, "question_id": 11621, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/11621/how-can-i-generate-a-document-from-a-single-word-using-gpt-or-bert", "title": "How can I generate a document from a single word using GPT or BERT?", "body": "<p>I have a dataset of 100000 documents each labelled with a topic to it. I want to create a model such that, given a topic, the model can generate a document from it. </p>\n\n<p>I came across language models <a href=\"https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\" rel=\"nofollow noreferrer\">GPT</a>, GPT-2 and <a href=\"https://arxiv.org/pdf/1810.04805.pdf\" rel=\"nofollow noreferrer\">BERT</a>. I learned that they can be used for generation purposes. But I did not find anywhere whether they can generate sentences given only a word.</p>\n\n<p>I am inclined to use GPT for my task, but I am not sure how to proceed with it. I wanted to know whether it is possible or not? It would be helpful if anyone can help me give a start in the right direction.</p>\n"}, "title": "How can I generate a document from a single word using GPT or BERT?", "content": "I have a dataset of 100000 documents each labelled with a topic to it. I want to create a model such that, given a topic, the model can generate a document from it. \nI came across language models GPT, GPT-2 and BERT. I learned that they can be used for generation purposes. But I did not find anywhere whether they can generate sentences given only a word.\nI am inclined to use GPT for my task, but I am not sure how to proceed with it. I wanted to know whether it is possible or not? It would be helpful if anyone can help me give a start in the right direction.\n", "question_id": 11621, "answers": []}, "22877": {"link": "https://ai.stackexchange.com/questions/22877/how-much-computing-power-does-it-cost-to-run-gpt-3", "metadata": {"tags": ["gpu", "gpt"], "owner": {"reputation": 121, "user_id": 40112, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/oG8Zi.png?s=256&g=1", "display_name": "Simon Suh", "link": "https://ai.stackexchange.com/users/40112/simon-suh"}, "is_answered": true, "view_count": 14253, "closed_date": 1597078599, "accepted_answer_id": 22881, "answer_count": 2, "score": 1, "locked_date": 1640122607, "last_activity_date": 1596990645, "creation_date": 1596648944, "question_id": 22877, "link": "https://ai.stackexchange.com/questions/22877/how-much-computing-power-does-it-cost-to-run-gpt-3", "closed_reason": "Not suitable for this site", "title": "How much computing power does it cost to run GPT-3?", "body": "<p>I know it cost around $4.3 million dollars to train, but how much computing power does it cost to run the finished program? IBM Watson chatbot AI only costs a few cents per chat message to use, OpeenAI Five seemed to run on a single gaming PC setup. So I'm wondering how much computing power does it need to run the finished ai program.</p>\n"}, "title": "How much computing power does it cost to run GPT-3?", "content": "I know it cost around $4.3 million dollars to train, but how much computing power does it cost to run the finished program? IBM Watson chatbot AI only costs a few cents per chat message to use, OpeenAI Five seemed to run on a single gaming PC setup. So I'm wondering how much computing power does it need to run the finished ai program.\n", "question_id": 22877, "answers": []}, "15965": {"link": "https://ai.stackexchange.com/questions/15965/is-it-possible-to-use-the-gpt-2-model-for-time-series-data-prediction", "metadata": {"tags": ["natural-language-processing", "prediction", "time-series", "gpt"], "owner": {"reputation": 272, "user_id": 3861, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3910257eeac3402e68ab7934ded0ced4?s=256&d=identicon&r=PG", "display_name": "xendi", "link": "https://ai.stackexchange.com/users/3861/xendi"}, "is_answered": true, "view_count": 1854, "answer_count": 2, "score": 1, "last_activity_date": 1681366316, "creation_date": 1571411804, "last_edit_date": 1572578226, "question_id": 15965, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/15965/is-it-possible-to-use-the-gpt-2-model-for-time-series-data-prediction", "title": "Is it possible to use the GPT-2 model for time-series data prediction?", "body": "<p>Is it possible and how trivial (or not) might it be (if possible) to retrain GPT-2 on time-series data instead of text?</p>\n"}, "title": "Is it possible to use the GPT-2 model for time-series data prediction?", "content": "Is it possible and how trivial (or not) might it be (if possible) to retrain GPT-2 on time-series data instead of text?\n", "question_id": 15965, "answers": []}, "37148": {"link": "https://ai.stackexchange.com/questions/37148/left-to-right-vs-encoder-decoder-models", "metadata": {"tags": ["natural-language-processing", "objective-functions", "bert", "gpt", "encoder-decoder"], "owner": {"reputation": 39, "user_id": 53961, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/99a402102bf91bf962b146fe682481e5?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "keyboardAnt", "link": "https://ai.stackexchange.com/users/53961/keyboardant"}, "is_answered": false, "view_count": 241, "answer_count": 0, "score": 1, "last_activity_date": 1663712938, "creation_date": 1663712938, "question_id": 37148, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/37148/left-to-right-vs-encoder-decoder-models", "title": "Left-to-Right vs Encoder-decoder Models", "body": "<p><a href=\"https://arxiv.org/pdf/2202.13169.pdf\" rel=\"nofollow noreferrer\">Xu et al. (2022)</a> distinguishes between popular pre-training methods for language modeling: (see Section 2.1 PRETRAINING METHODS)</p>\n<ul>\n<li>Left-to-Right:</li>\n</ul>\n<blockquote>\n<p>Auto-regressive, Left-to-right models, predict the probability of a\ntoken given the previous tokens.</p>\n</blockquote>\n<ul>\n<li>Encoder-Decoder:</li>\n</ul>\n<blockquote>\n<p>An encoder-decoder model first uses an encoder to encode an input\nsequence, and then uses a left-to-right LM to decode an output\nsequence conditioned on the input sequence.</p>\n</blockquote>\n<p>My question is, what are the differences between those two methods?\nDo they suggest that the first method is a decoder-only? If so, what is the input to this decoder?</p>\n<p>Based on what I know about auto-regressive models and the above definition, I understand that in Left-to-Right, we predict the <span class=\"math-container\">$i$</span>-th token given the <span class=\"math-container\">$1,...,i-1$</span> tokens (which could be our past predictions).</p>\n"}, "title": "Left-to-Right vs Encoder-decoder Models", "content": "Xu et al. (2022) distinguishes between popular pre-training methods for language modeling: (see Section 2.1 PRETRAINING METHODS)\n\nLeft-to-Right:\n\n\nAuto-regressive, Left-to-right models, predict the probability of a\ntoken given the previous tokens.\n\n\nEncoder-Decoder:\n\n\nAn encoder-decoder model first uses an encoder to encode an input\nsequence, and then uses a left-to-right LM to decode an output\nsequence conditioned on the input sequence.\n\nMy question is, what are the differences between those two methods?\nDo they suggest that the first method is a decoder-only? If so, what is the input to this decoder?\nBased on what I know about auto-regressive models and the above definition, I understand that in Left-to-Right, we predict the $i$-th token given the $1,...,i-1$ tokens (which could be our past predictions).\n", "question_id": 37148, "answers": []}, "27044": {"link": "https://ai.stackexchange.com/questions/27044/can-an-existing-transformer-model-be-modified-to-estimate-the-next-most-probable", "metadata": {"tags": ["transformer", "attention", "bert", "gpt", "forecasting"], "owner": {"reputation": 119, "user_id": 45748, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9a7101fe3ba50e621e91ee1edf9d27bb?s=256&d=identicon&r=PG", "display_name": "Nyxynyx", "link": "https://ai.stackexchange.com/users/45748/nyxynyx"}, "is_answered": false, "view_count": 200, "answer_count": 1, "score": 1, "last_activity_date": 1692223610, "creation_date": 1616901449, "last_edit_date": 1617014324, "question_id": 27044, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/27044/can-an-existing-transformer-model-be-modified-to-estimate-the-next-most-probable", "title": "Can an existing transformer model be modified to estimate the next most probable number in a sequence of numbers?", "body": "<p>Models based on the transformer architectures (GPT, BERT, etc.) work awesome for NLP tasks including taking an input generated from words and producing probability estimates of the next word as the output.</p>\n<p>Can an existing transformer model, such as GPT-2, be modified to perform the same task on a sequence of numbers and estimate the next most probable number? If so, what modifications do we need to perform (do we still train a tokenizer to tokenize integers/floats into token IDs?)?</p>\n"}, "title": "Can an existing transformer model be modified to estimate the next most probable number in a sequence of numbers?", "content": "Models based on the transformer architectures (GPT, BERT, etc.) work awesome for NLP tasks including taking an input generated from words and producing probability estimates of the next word as the output.\nCan an existing transformer model, such as GPT-2, be modified to perform the same task on a sequence of numbers and estimate the next most probable number? If so, what modifications do we need to perform (do we still train a tokenizer to tokenize integers/floats into token IDs?)?\n", "question_id": 27044, "answers": []}, "39999": {"link": "https://ai.stackexchange.com/questions/39999/what-is-the-difference-between-t5-and-t0-models", "metadata": {"tags": ["deep-learning", "natural-language-processing", "gpt", "language-model"], "owner": {"reputation": 103, "user_id": 69334, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b71f1ae612e64e74426c968a55f62120?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "prostak", "link": "https://ai.stackexchange.com/users/69334/prostak"}, "is_answered": true, "view_count": 219, "accepted_answer_id": 40007, "answer_count": 1, "score": 0, "last_activity_date": 1681416063, "creation_date": 1681249689, "last_edit_date": 1681416063, "question_id": 39999, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39999/what-is-the-difference-between-t5-and-t0-models", "title": "What is the difference between T5 and T0 models?", "body": "<p>What is the difference between T5 and T0 models? I had read that T0 is T5 + LM. But as I know T5 uses encoder-decoder model like BART but BART can be used as LM so that's mean that T5 has a LM function too. So what is the difference between T5 and T0 then?</p>\n"}, "title": "What is the difference between T5 and T0 models?", "content": "What is the difference between T5 and T0 models? I had read that T0 is T5 + LM. But as I know T5 uses encoder-decoder model like BART but BART can be used as LM so that's mean that T5 has a LM function too. So what is the difference between T5 and T0 then?\n", "question_id": 39999, "answers": []}, "39740": {"link": "https://ai.stackexchange.com/questions/39740/is-it-possible-for-a-gpt-model-to-run-in-a-distributed-way", "metadata": {"tags": ["gpt"], "owner": {"reputation": 109, "user_id": 35620, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/xK2uO.jpg?s=256&g=1", "display_name": "alex", "link": "https://ai.stackexchange.com/users/35620/alex"}, "is_answered": true, "view_count": 180, "answer_count": 1, "score": 0, "last_activity_date": 1679598694, "creation_date": 1679542871, "question_id": 39740, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39740/is-it-possible-for-a-gpt-model-to-run-in-a-distributed-way", "title": "Is it possible for a GPT model to run in a distributed way?", "body": "<p>Say that we're on GPT20 - maybe the model that's resulted from training is 10PB large (maybe unlikely but this is an example). Is it possible for a GPT model to be distributed across machines? How does this work if distributed inference is needed?</p>\n"}, "title": "Is it possible for a GPT model to run in a distributed way?", "content": "Say that we're on GPT20 - maybe the model that's resulted from training is 10PB large (maybe unlikely but this is an example). Is it possible for a GPT model to be distributed across machines? How does this work if distributed inference is needed?\n", "question_id": 39740, "answers": []}, "39111": {"link": "https://ai.stackexchange.com/questions/39111/how-do-they-make-transformers-bigger-deeper", "metadata": {"tags": ["transformer", "gpt"], "owner": {"reputation": 121, "user_id": 27616, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/80d06d9b68aecf42c630b6adc86c3fec?s=256&d=identicon&r=PG", "display_name": "Mastiff", "link": "https://ai.stackexchange.com/users/27616/mastiff"}, "is_answered": true, "view_count": 66, "answer_count": 1, "score": 0, "last_activity_date": 1676111675, "creation_date": 1676062294, "question_id": 39111, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/39111/how-do-they-make-transformers-bigger-deeper", "title": "How do they make transformers bigger/deeper?", "body": "<p>I can find a million explanations of the diagram in the original transformer paper:</p>\n<p><a href=\"https://i.stack.imgur.com/jIwvJ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/jIwvJ.png\" alt=\"enter image description here\" /></a></p>\n<p>But I know that modern GPT models have many millions of weights. Where are they?  Or in other words, how does this thing scale?</p>\n"}, "title": "How do they make transformers bigger/deeper?", "content": "I can find a million explanations of the diagram in the original transformer paper:\n\nBut I know that modern GPT models have many millions of weights. Where are they?  Or in other words, how does this thing scale?\n", "question_id": 39111, "answers": []}, "22469": {"link": "https://ai.stackexchange.com/questions/22469/is-the-size-of-a-neural-network-directly-linked-with-an-increase-in-its-intelige", "metadata": {"tags": ["neural-networks", "algorithm", "architecture", "gpt"], "owner": {"reputation": 191, "user_id": 10135, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/661pb.jpg?s=256&g=1", "display_name": "Gon&#231;alo Peres", "link": "https://ai.stackexchange.com/users/10135/gon%c3%a7alo-peres"}, "is_answered": true, "view_count": 85, "accepted_answer_id": 22476, "answer_count": 1, "score": 0, "last_activity_date": 1594599785, "creation_date": 1594548024, "question_id": 22469, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/22469/is-the-size-of-a-neural-network-directly-linked-with-an-increase-in-its-intelige", "title": "Is the size of a neural network directly linked with an increase in its inteligence?", "body": "<p>Just came across <a href=\"https://www.gwern.net/newsletter/2020/05#gpt-3\" rel=\"nofollow noreferrer\">this article on GPT-3</a>, and that lead me to the question:</p>\n<p>In order to make a certain kind of neural network architecture smarter all one needs to do is to make it bigger?</p>\n<p>Also, if that is true, how does the importance of computer power relates with the importance of fine-tuning/algorithmic improvement?</p>\n"}, "title": "Is the size of a neural network directly linked with an increase in its inteligence?", "content": "Just came across this article on GPT-3, and that lead me to the question:\nIn order to make a certain kind of neural network architecture smarter all one needs to do is to make it bigger?\nAlso, if that is true, how does the importance of computer power relates with the importance of fine-tuning/algorithmic improvement?\n", "question_id": 22469, "answers": []}, "40483": {"link": "https://ai.stackexchange.com/questions/40483/possible-to-use-gpt-for-specific-set-of-documents", "metadata": {"tags": ["gpt"], "owner": {"reputation": 11, "user_id": 61567, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/07e50c91b272d171048f16ab9b6ecf97?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "user9163823", "link": "https://ai.stackexchange.com/users/61567/user9163823"}, "is_answered": true, "view_count": 202, "closed_date": 1685089571, "accepted_answer_id": 40506, "answer_count": 1, "score": 0, "last_activity_date": 1684500790, "creation_date": 1684343379, "question_id": 40483, "link": "https://ai.stackexchange.com/questions/40483/possible-to-use-gpt-for-specific-set-of-documents", "closed_reason": "Not suitable for this site", "title": "Possible to use GPT for specific set of documents?", "body": "<p>I have a 100+ PDF documents regarding a company's policies, procedures and guidelines etc.</p>\n<p>Is there an AI tool that was trained in general understanding of language, that I can feed all those PDFs and make my own model where I can ask questions about the company's rules and procedures specifics, find relevant pieces of information and get answers based on the documents provided?</p>\n<p>I know I can feed a PDF to an AI and summarize it, but I'm looking for a permanent solution, kind of like a database but where instead of standard search I could have a conversation with AI?</p>\n"}, "title": "Possible to use GPT for specific set of documents?", "content": "I have a 100+ PDF documents regarding a company's policies, procedures and guidelines etc.\nIs there an AI tool that was trained in general understanding of language, that I can feed all those PDFs and make my own model where I can ask questions about the company's rules and procedures specifics, find relevant pieces of information and get answers based on the documents provided?\nI know I can feed a PDF to an AI and summarize it, but I'm looking for a permanent solution, kind of like a database but where instead of standard search I could have a conversation with AI?\n", "question_id": 40483, "answers": []}, "37326": {"link": "https://ai.stackexchange.com/questions/37326/can-i-add-to-a-language-model-a-prompt-with-output-example", "metadata": {"tags": ["transformer", "gpt", "fine-tuning", "prompt"], "owner": {"reputation": 371, "user_id": 52355, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/413169563a41d1e7dee104b9a35343d9?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Hadar Sharvit", "link": "https://ai.stackexchange.com/users/52355/hadar-sharvit"}, "is_answered": true, "view_count": 41, "answer_count": 1, "score": 0, "last_activity_date": 1666000006, "creation_date": 1665174986, "last_edit_date": 1665175333, "question_id": 37326, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/37326/can-i-add-to-a-language-model-a-prompt-with-output-example", "title": "can I add to a language model a prompt with output example?", "body": "<p>I want to finetune GPT2 to extract relevant data from a given text. So for (a trivial) example, given the text &quot;<em>the car was manufactured in X, can reach Y km/h, and has Z horse powers</em>&quot;, my desired output would be <em>manufacturer: <code>X</code>, max speed:<code>Y</code>, horsepowers:<code>Z</code></em>.</p>\n<p>I don't have a lot of labeled data, so I thought it would be reasonable to take every training sample and add to it a prefix than contains an actual example. That is - Instead of providing the model with the text</p>\n<blockquote>\n<p>INPUT: a well-known brand, the new <code>X</code>, can reach <code>Y</code> km/h, and has <code>Z</code> horsepower</p>\n<p>OUTPUT:</p>\n</blockquote>\n<p>and expect the model to understand how to fill in the details, I would provide a longer prompt that contains an actual example like</p>\n<blockquote>\n<p>INPUT: the new <code>BMW</code> can reach up to <code>200</code> kmh. Even though the previous model disappointed the users, the brand-new one rocks a <code>thousand</code> engine horsepower</p>\n<p>OUTPUT: Manufacturer: <code>BMW</code>, Max Speed: <code>200</code>, Horse Powers: <code>thousand</code></p>\n<p>INPUT: though is mostly considered an outdated version of the cls200, the new <code>Mercedes</code> has the capabilities of reaching up to 100kmh in turns and <code>300</code>kmh overall</p>\n<p>OUTPUT:</p>\n</blockquote>\n<p>Is this considered a common way of engineering the prompt? Do notice that the first provided example is the same for every training sample.</p>\n"}, "title": "can I add to a language model a prompt with output example?", "content": "I want to finetune GPT2 to extract relevant data from a given text. So for (a trivial) example, given the text \"the car was manufactured in X, can reach Y km/h, and has Z horse powers\", my desired output would be manufacturer: X, max speed:Y, horsepowers:Z.\nI don't have a lot of labeled data, so I thought it would be reasonable to take every training sample and add to it a prefix than contains an actual example. That is - Instead of providing the model with the text\n\nINPUT: a well-known brand, the new X, can reach Y km/h, and has Z horsepower\nOUTPUT:\n\nand expect the model to understand how to fill in the details, I would provide a longer prompt that contains an actual example like\n\nINPUT: the new BMW can reach up to 200 kmh. Even though the previous model disappointed the users, the brand-new one rocks a thousand engine horsepower\nOUTPUT: Manufacturer: BMW, Max Speed: 200, Horse Powers: thousand\nINPUT: though is mostly considered an outdated version of the cls200, the new Mercedes has the capabilities of reaching up to 100kmh in turns and 300kmh overall\nOUTPUT:\n\nIs this considered a common way of engineering the prompt? Do notice that the first provided example is the same for every training sample.\n", "question_id": 37326, "answers": []}, "20591": {"link": "https://ai.stackexchange.com/questions/20591/gpt-2-hardware-requirements-for-fine-tuning-the-774m-model", "metadata": {"tags": ["natural-language-processing", "gpt", "fine-tuning", "gpt-2"], "owner": {"reputation": 129, "user_id": 33476, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d243c2c7c40cd0ca33462ef9d560604b?s=256&d=identicon&r=PG", "display_name": "Comfort Eagle", "link": "https://ai.stackexchange.com/users/33476/comfort-eagle"}, "is_answered": true, "view_count": 3586, "closed_date": 1639172039, "answer_count": 1, "score": 0, "locked_date": 1639920618, "last_activity_date": 1619267941, "creation_date": 1587719759, "last_edit_date": 1619267941, "question_id": 20591, "link": "https://ai.stackexchange.com/questions/20591/gpt-2-hardware-requirements-for-fine-tuning-the-774m-model", "closed_reason": "Not suitable for this site", "title": "GPT-2: (Hardware) requirements for fine-tuning the 774M model", "body": "<p>I wonder if there's anyone who has actually succeeded in fine-tuning GPT-2's 774M model without using cloud TPU's. My GeForce RTX 2070 SUPER couldn't handle it in previous attempts.</p>\n<p>I'm running TensorFlow 1.14.0 with CUDA V 9.1 on Ubuntu 18.04. For fine-tuning I'm using <a href=\"https://github.com/minimaxir/gpt-2-simple\" rel=\"nofollow noreferrer\">gpt-2-simple</a>.</p>\n<p>When fine-tuning using the 77M model, I keep running into OOM errors, such as:\n<code>W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.25MiB (rounded to 6553600).  Current allocation summary follows.</code></p>\n<p>So far I've tried:</p>\n<ul>\n<li>Using different a optimizer (<code>RMSPropOptimizer</code> instead of <code>AdamOptimizer</code>)</li>\n<li>Setting batch-size to 1</li>\n<li><code>use_memory_saving_gradients</code></li>\n<li><code>only_train_transformer_layers</code></li>\n</ul>\n<p>Fine-tuning works smoothly on the 355M model.</p>\n<p>So what I'm really asking is:</p>\n<ul>\n<li>is it possible to fine-tune GPT-2's 774M model without industrial-sized hardware?</li>\n<li>if so, please tell me about your successful attempts</li>\n<li>apart from hardware-recommendations, how could fine-tuning be optimized to make 77M fit in memory?</li>\n</ul>\n"}, "title": "GPT-2: (Hardware) requirements for fine-tuning the 774M model", "content": "I wonder if there's anyone who has actually succeeded in fine-tuning GPT-2's 774M model without using cloud TPU's. My GeForce RTX 2070 SUPER couldn't handle it in previous attempts.\nI'm running TensorFlow 1.14.0 with CUDA V 9.1 on Ubuntu 18.04. For fine-tuning I'm using gpt-2-simple.\nWhen fine-tuning using the 77M model, I keep running into OOM errors, such as:\nW tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.25MiB (rounded to 6553600).  Current allocation summary follows.\nSo far I've tried:\n\nUsing different a optimizer (RMSPropOptimizer instead of AdamOptimizer)\nSetting batch-size to 1\nuse_memory_saving_gradients\nonly_train_transformer_layers\n\nFine-tuning works smoothly on the 355M model.\nSo what I'm really asking is:\n\nis it possible to fine-tune GPT-2's 774M model without industrial-sized hardware?\nif so, please tell me about your successful attempts\napart from hardware-recommendations, how could fine-tuning be optimized to make 77M fit in memory?\n\n", "question_id": 20591, "answers": []}, "41582": {"link": "https://ai.stackexchange.com/questions/41582/gpt-beam-search-length-number-of-tokens", "metadata": {"tags": ["hyper-parameters", "gpt", "sentiment-analysis", "prompt-design"], "owner": {"reputation": 13, "user_id": 74745, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/a84075196d1d183de78f68aff482ff2f?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "just another mathmo", "link": "https://ai.stackexchange.com/users/74745/just-another-mathmo"}, "is_answered": true, "view_count": 27, "accepted_answer_id": 41585, "answer_count": 1, "score": 0, "last_activity_date": 1690887769, "creation_date": 1690884581, "last_edit_date": 1690884770, "question_id": 41582, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/41582/gpt-beam-search-length-number-of-tokens", "title": "GPT beam search length (number of tokens)", "body": "<p><strong>Background:</strong> I'm currently trying to use GPT to give me numerical scores, and looking for tips on <a href=\"https://ai.stackexchange.com/questions/41581/prompt-engineering-gpt-for-numerical-scores\">prompt design, see my previous StackExchange post</a>.\nTo craft good prompts it seems important to have a good understanding of how the generative model works...</p>\n<p><strong>Question:</strong>\nHow many tokens ahead does GPT 3.5 look with its beam search feature?</p>\n<p><strong>Extra context:</strong> I found it hard to find good references for beam search, a decent starting point seemed to be <a href=\"https://huggingface.co/blog/how-to-generate\" rel=\"nofollow noreferrer\">huggingface blog post</a>.<br />\nI tried asking BingChat about GPT-3.5 beam search length: BingChat replied that it was 10 tokens but could only give a 'reference' to an OpenAI API page which did not seem to support the claim. I couldn't find any other results online.</p>\n<p><strong>Why I care?</strong> Suppose I have a long theatre review and want to score how impressed the critic was by the quality of acting on a scale of: -5 extremely unimpressed to +5 extremely impressed.\nMy prompts currently ask the model to finish the reply with a sentence in the form:\n&quot;Overall the critic was very impressed by the quality of acting - score 4.&quot;\nBut perhaps by asking the model to continue generation I can make the prompts more reliable. E.g. I could ask the model to subsequently explain the score with a quotation from the text; e.g. along the lines of\n&quot;Overall the critic was very impressed by the quality of acting - score 4/5 - and indeed that 'Mark Strong's performance stole the show'&quot;</p>\n<p>Knowing beam search length would really help me design prompts like these (which ask for the continuation of text after a numerical score to improve reliability).</p>\n"}, "title": "GPT beam search length (number of tokens)", "content": "Background: I'm currently trying to use GPT to give me numerical scores, and looking for tips on prompt design, see my previous StackExchange post.\nTo craft good prompts it seems important to have a good understanding of how the generative model works...\nQuestion:\nHow many tokens ahead does GPT 3.5 look with its beam search feature?\nExtra context: I found it hard to find good references for beam search, a decent starting point seemed to be huggingface blog post.\nI tried asking BingChat about GPT-3.5 beam search length: BingChat replied that it was 10 tokens but could only give a 'reference' to an OpenAI API page which did not seem to support the claim. I couldn't find any other results online.\nWhy I care? Suppose I have a long theatre review and want to score how impressed the critic was by the quality of acting on a scale of: -5 extremely unimpressed to +5 extremely impressed.\nMy prompts currently ask the model to finish the reply with a sentence in the form:\n\"Overall the critic was very impressed by the quality of acting - score 4.\"\nBut perhaps by asking the model to continue generation I can make the prompts more reliable. E.g. I could ask the model to subsequently explain the score with a quotation from the text; e.g. along the lines of\n\"Overall the critic was very impressed by the quality of acting - score 4/5 - and indeed that 'Mark Strong's performance stole the show'\"\nKnowing beam search length would really help me design prompts like these (which ask for the continuation of text after a numerical score to improve reliability).\n", "question_id": 41582, "answers": []}, "41581": {"link": "https://ai.stackexchange.com/questions/41581/prompt-engineering-gpt-for-numerical-scores", "metadata": {"tags": ["gpt", "sentiment-analysis", "prompt-design"], "migrated_to": {"other_site": {"styling": {"tag_background_color": "#E0EAF1", "tag_foreground_color": "#000", "link_color": "#0077CC"}, "related_sites": [{"relation": "meta", "api_site_parameter": "genai.meta", "site_url": "https://genai.meta.stackexchange.com", "name": "GenAI Meta Stack Exchange"}, {"relation": "chat", "site_url": "https://chat.stackexchange.com?tab=site&host=genai.stackexchange.com", "name": "Chat Stack Exchange"}], "open_beta_date": 1690297236, "closed_beta_date": 1689617537, "site_state": "open_beta", "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/genai/Img/apple-touch-icon@2.png", "favicon_url": "https://cdn.sstatic.net/Sites/genai/Img/favicon.ico", "icon_url": "https://cdn.sstatic.net/Sites/genai/Img/apple-touch-icon.png", "audience": "GenAI enthusiasts and practitioners and those interested in learning more about using GenAI tools", "site_url": "https://genai.stackexchange.com", "api_site_parameter": "genai", "logo_url": "https://cdn.sstatic.net/Sites/genai/Img/apple-touch-icon.png", "name": "GenAI", "site_type": "main_site"}, "on_date": 1691048483, "question_id": 251}, "owner": {"reputation": 13, "user_id": 74745, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/a84075196d1d183de78f68aff482ff2f?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "just another mathmo", "link": "https://ai.stackexchange.com/users/74745/just-another-mathmo"}, "is_answered": false, "view_count": 13, "closed_date": 1691048483, "answer_count": 0, "score": 0, "locked_date": 1691048483, "last_activity_date": 1690881486, "creation_date": 1690881486, "question_id": 41581, "link": "https://ai.stackexchange.com/questions/41581/prompt-engineering-gpt-for-numerical-scores", "closed_reason": "Not suitable for this site", "title": "Prompt engineering GPT for numerical scores?", "body": "<p><strong>Background:</strong> I am currently working on an NLP project using GPT 3.5 via the OpenAI python API.\nEngineering good prompts seems to be the most critical part of my pipeline so far. Crucially I have long input texts, for which I want to get numerical sentiment scores for a variety of different questions in a ZERO SHOT setting.\nThere are many possible applications such as predicting nuanced scores on different aspects (plot, acting, staging etc) of a theatre show from a long detailed review; or the sentiment of a twitter response to a new political policy (emotional objections, economic objections, moral objections).</p>\n<p><strong>Question:</strong> What is best practice to extract reliable numerical sentiment scores from GPT models?</p>\n<p><strong>What I've tried:</strong> Looked for guidance on the <a href=\"https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md\" rel=\"nofollow noreferrer\">OpenAI cookbook</a>  and explored many associated links. It seems like existing suggestions for sentiment scoring usually use embeddings in a setting where a small number of labelled samples are available. Following heuristics from various prompt engineering sources I have used a pipeline of iteratively asking for detailed summaries and then more concise summaries culminating in a single sentence and finally a score. This has yielded impressive results; indeed I suspect such a 'prompt-pipeline' may yield more sophisticated sentiment scores than fine-tuning approaches, unless one has the luxury of extremely large amount of labelled data.\nHowever, converting these final sentences to numeric scores is still somewhat unreliable. I couldn't find specific advice online.\nWhat is best practice?</p>\n"}, "title": "Prompt engineering GPT for numerical scores?", "content": "Background: I am currently working on an NLP project using GPT 3.5 via the OpenAI python API.\nEngineering good prompts seems to be the most critical part of my pipeline so far. Crucially I have long input texts, for which I want to get numerical sentiment scores for a variety of different questions in a ZERO SHOT setting.\nThere are many possible applications such as predicting nuanced scores on different aspects (plot, acting, staging etc) of a theatre show from a long detailed review; or the sentiment of a twitter response to a new political policy (emotional objections, economic objections, moral objections).\nQuestion: What is best practice to extract reliable numerical sentiment scores from GPT models?\nWhat I've tried: Looked for guidance on the OpenAI cookbook  and explored many associated links. It seems like existing suggestions for sentiment scoring usually use embeddings in a setting where a small number of labelled samples are available. Following heuristics from various prompt engineering sources I have used a pipeline of iteratively asking for detailed summaries and then more concise summaries culminating in a single sentence and finally a score. This has yielded impressive results; indeed I suspect such a 'prompt-pipeline' may yield more sophisticated sentiment scores than fine-tuning approaches, unless one has the luxury of extremely large amount of labelled data.\nHowever, converting these final sentences to numeric scores is still somewhat unreliable. I couldn't find specific advice online.\nWhat is best practice?\n", "question_id": 41581, "answers": []}, "40679": {"link": "https://ai.stackexchange.com/questions/40679/what-is-actually-tuned-during-prompt-engineering-of-autoregressive-llms-like-gpt", "metadata": {"tags": ["transformer", "activation-functions", "chatgpt", "gpt"], "owner": {"reputation": 101, "user_id": 72454, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/5493fb4d489025f0357c4a29be27ff88?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "user14094230", "link": "https://ai.stackexchange.com/users/72454/user14094230"}, "is_answered": false, "view_count": 40, "answer_count": 0, "score": 0, "last_activity_date": 1685653551, "creation_date": 1685653551, "question_id": 40679, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40679/what-is-actually-tuned-during-prompt-engineering-of-autoregressive-llms-like-gpt", "title": "What is actually tuned during prompt engineering of autoregressive LLMs like GPT?", "body": "<p>There are a lot of sites going over prompt engineering, but I don't see them explaining what is actually being changed. Is it hidden activation layers being tuned?</p>\n"}, "title": "What is actually tuned during prompt engineering of autoregressive LLMs like GPT?", "content": "There are a lot of sites going over prompt engineering, but I don't see them explaining what is actually being changed. Is it hidden activation layers being tuned?\n", "question_id": 40679, "answers": []}, "40476": {"link": "https://ai.stackexchange.com/questions/40476/issues-with-larger-context-lengths-in-a-transformer-model-like-gpt", "metadata": {"tags": ["transformer", "attention", "gpt", "computational-complexity"], "migrated_from": {"other_site": {"aliases": ["https://www.stackoverflow.com", "https://facebook.stackoverflow.com"], "styling": {"tag_background_color": "#E0EAF1", "tag_foreground_color": "#3E6D8E", "link_color": "#0077CC"}, "related_sites": [{"relation": "meta", "api_site_parameter": "meta.stackoverflow", "site_url": "https://meta.stackoverflow.com", "name": "Meta Stack Overflow"}, {"relation": "chat", "site_url": "https://chat.stackoverflow.com/", "name": "Stack Overflow Chat"}], "markdown_extensions": ["Prettify"], "launch_date": 1221436800, "open_beta_date": 1217462400, "site_state": "normal", "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png", "favicon_url": "https://cdn.sstatic.net/Sites/stackoverflow/Img/favicon.ico", "icon_url": "https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon.png", "audience": "professional and enthusiast programmers", "site_url": "https://stackoverflow.com", "api_site_parameter": "stackoverflow", "logo_url": "https://cdn.sstatic.net/Sites/stackoverflow/Img/logo.png", "name": "Stack Overflow", "site_type": "main_site"}, "on_date": 1684325093, "question_id": 76270180}, "owner": {"reputation": 101, "user_id": 71976, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/af4b0124848c81be7e927b16cec35f5b?s=256&d=identicon&r=PG", "display_name": "rahul", "link": "https://ai.stackexchange.com/users/71976/rahul"}, "is_answered": false, "view_count": 121, "answer_count": 0, "score": 0, "last_activity_date": 1684325940, "creation_date": 1684313287, "last_edit_date": 1684325940, "question_id": 40476, "link": "https://ai.stackexchange.com/questions/40476/issues-with-larger-context-lengths-in-a-transformer-model-like-gpt", "title": "Issues with larger context lengths in a transformer model like GPT", "body": "<p>Based on my understanding, one of the issues with longer context lengths is the computational complexity of attention mechanism which is quadratic. But is this really a problem on modern hardware with parallel CPU's?</p>\n<p>Eg: 32k context length (GPT4) has a complexity of O(32k * 32k) which is O(1024 million).</p>\n<p>Are there any other issues apart from the above which are impeding transformer models from supporting larger context lengths?</p>\n<p>As a side note, even if larger context lengths leads to slower non-interactive responses, I think it can still be very helpful in a lot of scenarios</p>\n"}, "title": "Issues with larger context lengths in a transformer model like GPT", "content": "Based on my understanding, one of the issues with longer context lengths is the computational complexity of attention mechanism which is quadratic. But is this really a problem on modern hardware with parallel CPU's?\nEg: 32k context length (GPT4) has a complexity of O(32k * 32k) which is O(1024 million).\nAre there any other issues apart from the above which are impeding transformer models from supporting larger context lengths?\nAs a side note, even if larger context lengths leads to slower non-interactive responses, I think it can still be very helpful in a lot of scenarios\n", "question_id": 40476, "answers": []}, "40418": {"link": "https://ai.stackexchange.com/questions/40418/how-can-i-send-vectors-as-a-chat-context", "metadata": {"tags": ["gpt", "embeddings"], "owner": {"reputation": 101, "user_id": 71702, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/kds3Y.jpg?s=256&g=1", "display_name": "dc10", "link": "https://ai.stackexchange.com/users/71702/dc10"}, "is_answered": false, "view_count": 364, "answer_count": 2, "score": 0, "last_activity_date": 1693071772, "creation_date": 1683830751, "last_edit_date": 1693071772, "question_id": 40418, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40418/how-can-i-send-vectors-as-a-chat-context", "title": "How can I send vectors as a chat context?", "body": "<p>Since the context/memory of a chat or question for LLMs more precisely GPT is limited to a token length I struggle about how to provide own data that the model got not trained on.\nA very common approach looks like embeddings are the way to.</p>\n<p>OpenAI provided an article <a href=\"https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb\" rel=\"nofollow noreferrer\">https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb</a>\nhow to create an embedding of a user query, match it against a local vector database ans provide the closest results <strong>as text</strong> to the context/memory.</p>\n<p>Here I do struggle, since it might be very well possible that even we find the most matching documents locally in a vector database, context might still be too small if we would like to provide multiple matches.</p>\n<p>The question to me is, how could I send all the relevant embedding vectors rather than the relevant texts which got matched to the vectors?\nThese vectors are highly condensed and would save a lot of tokens. GPT would anyhow be able to understand the vector since they created it from their embeddings API, right?</p>\n<p>Or is it just not possible to convert the vector back to text at their end?</p>\n"}, "title": "How can I send vectors as a chat context?", "content": "Since the context/memory of a chat or question for LLMs more precisely GPT is limited to a token length I struggle about how to provide own data that the model got not trained on.\nA very common approach looks like embeddings are the way to.\nOpenAI provided an article https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb\nhow to create an embedding of a user query, match it against a local vector database ans provide the closest results as text to the context/memory.\nHere I do struggle, since it might be very well possible that even we find the most matching documents locally in a vector database, context might still be too small if we would like to provide multiple matches.\nThe question to me is, how could I send all the relevant embedding vectors rather than the relevant texts which got matched to the vectors?\nThese vectors are highly condensed and would save a lot of tokens. GPT would anyhow be able to understand the vector since they created it from their embeddings API, right?\nOr is it just not possible to convert the vector back to text at their end?\n", "question_id": 40418, "answers": []}, "40045": {"link": "https://ai.stackexchange.com/questions/40045/does-gpt-not-need-to-compress-its-training-data", "metadata": {"tags": ["gpt", "language-model", "data-compression"], "owner": {"reputation": 103, "user_id": 10649, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/ce95c01cbd4f90f014d988ae5c8a422d?s=256&d=identicon&r=PG", "display_name": "orome", "link": "https://ai.stackexchange.com/users/10649/orome"}, "is_answered": false, "view_count": 57, "answer_count": 0, "score": 0, "last_activity_date": 1681408343, "creation_date": 1681408343, "question_id": 40045, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/40045/does-gpt-not-need-to-compress-its-training-data", "title": "Does GPT not need to compress its training data?", "body": "<p>In his <a href=\"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/\" rel=\"nofollow noreferrer\">recent short pamphlet on GPT</a>, Stephan Wolfram says</p>\n<blockquote>\n<p>... the 'size of the network' that seems to work well is ... comparable to the 'size of the training data'. ...\nin this representation it seems there\u2019s in the end rather little 'compression' of the training data;\nit seems on average to basically take only a bit less than one neural net weight to carry the 'information content' of a word of training data.</p>\n</blockquote>\n<p>But this isn't true is it, even by his own account?\nHe says that the size of the network is 175 billion parameters, and the size of the training data is at least &quot;a trillion words of text&quot;\n(possibly 100 times that). That's a lot less than &quot;one neural net weight to carry the 'information content' of a word&quot;.</p>\n<p>Does GPT not need to compress its training data? Is it true that the number of parameters (Wolfram omits biases) is roughly the same as the size of the training data in words?</p>\n"}, "title": "Does GPT not need to compress its training data?", "content": "In his recent short pamphlet on GPT, Stephan Wolfram says\n\n... the 'size of the network' that seems to work well is ... comparable to the 'size of the training data'. ...\nin this representation it seems there\u2019s in the end rather little 'compression' of the training data;\nit seems on average to basically take only a bit less than one neural net weight to carry the 'information content' of a word of training data.\n\nBut this isn't true is it, even by his own account?\nHe says that the size of the network is 175 billion parameters, and the size of the training data is at least \"a trillion words of text\"\n(possibly 100 times that). That's a lot less than \"one neural net weight to carry the 'information content' of a word\".\nDoes GPT not need to compress its training data? Is it true that the number of parameters (Wolfram omits biases) is roughly the same as the size of the training data in words?\n", "question_id": 40045, "answers": []}, "38973": {"link": "https://ai.stackexchange.com/questions/38973/following-instructions-as-an-emergent-behaviour-in-transformer-models-isnt", "metadata": {"tags": ["neural-networks", "transformer", "gpt"], "owner": {"reputation": 101, "user_id": 67564, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/75a82969c8358ac7b392aba99955cca2?s=256&d=identicon&r=PG", "display_name": "ShankarG", "link": "https://ai.stackexchange.com/users/67564/shankarg"}, "is_answered": false, "view_count": 58, "answer_count": 0, "score": 0, "last_activity_date": 1675155256, "creation_date": 1675155256, "question_id": 38973, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/38973/following-instructions-as-an-emergent-behaviour-in-transformer-models-isnt", "title": "&quot;Following instructions&quot; as an emergent behaviour in transformer models - isn&#39;t this fundamentally different from the models&#39; basic purpose?", "body": "<p>I am not technically familiar with AI or neural networks beyond a tech news reading level of knowledge, so I apologise if this is a dumb question.</p>\n<p>I was recently reading <a href=\"https://arstechnica.com/gadgets/2023/01/the-generative-ai-revolution-has-begun-how-did-we-get-here/3/\" rel=\"nofollow noreferrer\">this article</a> on <a href=\"https://www.arstechnica.com\" rel=\"nofollow noreferrer\">Ars Technica</a>.  It is a high level description of the history of generative AI models (it's a very good article, highly recommended).</p>\n<p>When discussing large language models, the following passage appears:</p>\n<blockquote>\n<p>But there was also a surprise. The OpenAI researchers discovered that in making the models bigger, they didn\u2019t just get better at producing text. The models could learn entirely new behaviors simply by being shown new training data. In particular, <strong>the researchers discovered that GPT3 could be trained to follow instructions in plain English without having to explicitly design the model that way.</strong> (emphasis added)</p>\n</blockquote>\n<p>I have three questions:</p>\n<ol>\n<li><p>Was this instruction-following behaviour truly emergent, as in completely unexpected and unplanned for?</p>\n</li>\n<li><p>This seems completely at odds with the usual description of transformer models as transforming text of one kind into another.  Following instructions seems, at a conceptual level, to be something much higher? If this is the case, do we have any idea how this emerged, and what properties of the model it is rooted in?</p>\n</li>\n<li><p>Do we have any idea how wide the scope of this kind of &quot;instruction following&quot; is? I.e. can the models make sense and respond &quot;sensibly&quot; (not correctly - I am not concerned with the correctness of the response, but with its relation to the instruction) to <em>any</em> instruction related to text?  Or are there specific kinds of instructions they are able to comprehend, and others they fail at?</p>\n</li>\n</ol>\n"}, "title": "&quot;Following instructions&quot; as an emergent behaviour in transformer models - isn&#39;t this fundamentally different from the models&#39; basic purpose?", "content": "I am not technically familiar with AI or neural networks beyond a tech news reading level of knowledge, so I apologise if this is a dumb question.\nI was recently reading this article on Ars Technica.  It is a high level description of the history of generative AI models (it's a very good article, highly recommended).\nWhen discussing large language models, the following passage appears:\n\nBut there was also a surprise. The OpenAI researchers discovered that in making the models bigger, they didn\u2019t just get better at producing text. The models could learn entirely new behaviors simply by being shown new training data. In particular, the researchers discovered that GPT3 could be trained to follow instructions in plain English without having to explicitly design the model that way. (emphasis added)\n\nI have three questions:\n\nWas this instruction-following behaviour truly emergent, as in completely unexpected and unplanned for?\n\nThis seems completely at odds with the usual description of transformer models as transforming text of one kind into another.  Following instructions seems, at a conceptual level, to be something much higher? If this is the case, do we have any idea how this emerged, and what properties of the model it is rooted in?\n\nDo we have any idea how wide the scope of this kind of \"instruction following\" is? I.e. can the models make sense and respond \"sensibly\" (not correctly - I am not concerned with the correctness of the response, but with its relation to the instruction) to any instruction related to text?  Or are there specific kinds of instructions they are able to comprehend, and others they fail at?\n\n\n", "question_id": 38973, "answers": []}, "36924": {"link": "https://ai.stackexchange.com/questions/36924/how-do-gpt-models-pass-information-for-each-token-prediction", "metadata": {"tags": ["neural-networks", "gpt"], "owner": {"reputation": 101, "user_id": 59839, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3ead1d33ae1a945fe53d1f2d2d1d7d4f?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Jacques Thibodeau", "link": "https://ai.stackexchange.com/users/59839/jacques-thibodeau"}, "is_answered": false, "view_count": 328, "answer_count": 0, "score": 0, "last_activity_date": 1661809966, "creation_date": 1661803764, "last_edit_date": 1661809966, "question_id": 36924, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/36924/how-do-gpt-models-pass-information-for-each-token-prediction", "title": "How do GPT models pass information for each token prediction?", "body": "<p>So, I'm trying to understand what is going on in the following picture (from <a href=\"https://arxiv.org/pdf/2202.05262.pdf\" rel=\"nofollow noreferrer\">this</a> paper):</p>\n<p><a href=\"https://i.stack.imgur.com/M4oeo.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/M4oeo.jpg\" alt=\"Causal Tracing in GPT\" /></a></p>\n<p>Each decoder blocker in the above GPT model has attention heads (red) and MLPs (green). I know that we add residual connections to prevent vanishing gradients and normalization to prevent exploding gradients. However, these are computations done in the forward pass.</p>\n<p>So, I'm trying to understand the blue parts above that seem to show that there is a residual from the previous token that is passed to the next token computation at the same layer. So like, &quot;The&quot; computations at layer i are passed to &quot;Space&quot; computations at layer i. What is going on here? I don't see any mention of this in the papers or blog posts. How is that computation being passed?</p>\n<p>EDIT: Ok, so my understanding is that the red arrow is meant to point to the fact that the computation for the attention is carried over to the next token. Each new token vector is added to the masked attention matrix. Each row of the masked attention matrix is for each token being passed through the model, and the matrix is size (sequence_length, sequence_length). So, each forward pass for a new token increases the masked matrix by 1 each dimension.</p>\n<p>The arrow only goes &quot;down&quot; to the same layer because the masked attention matrix for that layer remains the same except for that layer (you're only adding a new row for the new token).</p>\n<p>Is this right?</p>\n"}, "title": "How do GPT models pass information for each token prediction?", "content": "So, I'm trying to understand what is going on in the following picture (from this paper):\n\nEach decoder blocker in the above GPT model has attention heads (red) and MLPs (green). I know that we add residual connections to prevent vanishing gradients and normalization to prevent exploding gradients. However, these are computations done in the forward pass.\nSo, I'm trying to understand the blue parts above that seem to show that there is a residual from the previous token that is passed to the next token computation at the same layer. So like, \"The\" computations at layer i are passed to \"Space\" computations at layer i. What is going on here? I don't see any mention of this in the papers or blog posts. How is that computation being passed?\nEDIT: Ok, so my understanding is that the red arrow is meant to point to the fact that the computation for the attention is carried over to the next token. Each new token vector is added to the masked attention matrix. Each row of the masked attention matrix is for each token being passed through the model, and the matrix is size (sequence_length, sequence_length). So, each forward pass for a new token increases the masked matrix by 1 each dimension.\nThe arrow only goes \"down\" to the same layer because the masked attention matrix for that layer remains the same except for that layer (you're only adding a new row for the new token).\nIs this right?\n", "question_id": 36924, "answers": []}, "32667": {"link": "https://ai.stackexchange.com/questions/32667/why-is-bert-gpt-capable-of-for-all-generalization", "metadata": {"tags": ["transformer", "bert", "logic", "gpt", "reasoning"], "owner": {"reputation": 235, "user_id": 17302, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/O3XKH.png?s=256&g=1", "display_name": "Yan King Yin", "link": "https://ai.stackexchange.com/users/17302/yan-king-yin"}, "is_answered": false, "view_count": 133, "answer_count": 0, "score": 0, "last_activity_date": 1640349240, "creation_date": 1638876631, "last_edit_date": 1640349240, "question_id": 32667, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/32667/why-is-bert-gpt-capable-of-for-all-generalization", "title": "Why is BERT/GPT capable of &quot;for-all&quot; generalization?", "body": "<p>As shown in the figure:</p>\n<p><a href=\"https://i.stack.imgur.com/P5U4w.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/P5U4w.jpg\" alt=\"BERT for-all generalization\" /></a></p>\n<p>Why does token prediction work when &quot;Socrates&quot; is replaced with &quot;Plato&quot;?</p>\n<p>From the point of view of symbolic logic, the above example effectively performs the logic rule:</p>\n<pre><code>\u2200x. human(x) \u21d2 mortal(x)\n</code></pre>\n<p>How might we explain this ability?  Moreover, how is this learned in just a few shots of examples?</p>\n<p>I think this question is key to understanding the Transformer's logical reasoning ability.</p>\n<p>Below are excerpts from 2 papers:</p>\n<p><a href=\"https://i.stack.imgur.com/ifuUG.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ifuUG.png\" alt=\"excerpt 1\" /></a></p>\n<p><a href=\"https://i.stack.imgur.com/AodI6.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/AodI6.jpg\" alt=\"excerpt 2\" /></a></p>\n"}, "title": "Why is BERT/GPT capable of &quot;for-all&quot; generalization?", "content": "As shown in the figure:\n\nWhy does token prediction work when \"Socrates\" is replaced with \"Plato\"?\nFrom the point of view of symbolic logic, the above example effectively performs the logic rule:\n\u2200x. human(x) \u21d2 mortal(x)\n\nHow might we explain this ability?  Moreover, how is this learned in just a few shots of examples?\nI think this question is key to understanding the Transformer's logical reasoning ability.\nBelow are excerpts from 2 papers:\n\n\n", "question_id": 32667, "answers": []}, "32436": {"link": "https://ai.stackexchange.com/questions/32436/how-to-fine-tune-gpt-j-with-small-dataset", "metadata": {"tags": ["natural-language-processing", "tensorflow", "hyperparameter-optimization", "gpt", "fine-tuning"], "owner": {"reputation": 101, "user_id": 51003, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/a9HXa.jpg?s=256&g=1", "display_name": "Ilya Karnaukhov", "link": "https://ai.stackexchange.com/users/51003/ilya-karnaukhov"}, "is_answered": false, "view_count": 4430, "answer_count": 2, "score": 0, "last_activity_date": 1686319681, "creation_date": 1637156772, "last_edit_date": 1637320678, "question_id": 32436, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/32436/how-to-fine-tune-gpt-j-with-small-dataset", "title": "How to fine-tune GPT-J with small dataset", "body": "<p>I have followed this guide as closely as possible: <a href=\"https://github.com/kingoflolz/mesh-transformer-jax\" rel=\"nofollow noreferrer\">https://github.com/kingoflolz/mesh-transformer-jax</a></p>\n<p>I'm trying to fine-tune GPT-J with a small dataset of ~500 lines:</p>\n<pre><code>You are important to me. &lt;|endoftext|&gt;\nI love spending time with you. &lt;|endoftext|&gt;\nYou make me smile. &lt;|endoftext|&gt;\nfeel so lucky to be your friend. &lt;|endoftext|&gt;\nYou can always talk to me, even if it\u2019s about something that makes you nervous or scared or sad. &lt;|endoftext|&gt;\netc...\n</code></pre>\n<p>Using the <strong>create_finetune_tfrecords.py</strong> script (from the repo mentioned above) outputs a file with <strong>2</strong> in it. I understand that means my data has 2 sequences.</p>\n<p>I could really use some advice with the <code>.json</code> config file. What hyperparameters do you recommend for this small dataset?</p>\n<p>The best I came up with trying to follow the guide:</p>\n<pre><code>{\n  &quot;layers&quot;: 28,\n  &quot;d_model&quot;: 4096,\n  &quot;n_heads&quot;: 16,\n  &quot;n_vocab&quot;: 50400,\n  &quot;norm&quot;: &quot;layernorm&quot;,\n  &quot;pe&quot;: &quot;rotary&quot;,\n  &quot;pe_rotary_dims&quot;: 64,\n\n  &quot;seq&quot;: 2048,\n  &quot;cores_per_replica&quot;: 8,\n  &quot;per_replica_batch&quot;: 1,\n  &quot;gradient_accumulation_steps&quot;: 2,\n\n  &quot;warmup_steps&quot;: 1,\n  &quot;anneal_steps&quot;: 9,\n  &quot;lr&quot;: 1.2e-4,\n  &quot;end_lr&quot;: 1.2e-5,\n  &quot;weight_decay&quot;: 0.1,\n  &quot;total_steps&quot;: 10,\n\n  &quot;tpu_size&quot;: 8,\n\n  &quot;bucket&quot;: &quot;chat-app-tpu-bucket-europe&quot;,\n  &quot;model_dir&quot;: &quot;finetune_dir&quot;,\n\n  &quot;train_set&quot;: &quot;james_bond_1.train.index&quot;,\n  &quot;val_set&quot;: {},\n\n  &quot;eval_harness_tasks&quot;: [\n  ],\n\n  &quot;val_batches&quot;: 2,\n  &quot;val_every&quot;: 400000,\n  &quot;ckpt_every&quot;: 1,\n  &quot;keep_every&quot;: 1,\n\n  &quot;name&quot;: &quot;GPT3_6B_pile_rotary&quot;,\n  &quot;wandb_project&quot;: &quot;mesh-transformer-jax&quot;,\n  &quot;comment&quot;: &quot;&quot;\n}\n</code></pre>\n<p>The problem is that, when I test the fine-tuned model, I get responses that make no sense:</p>\n<p><a href=\"https://i.stack.imgur.com/GTVV3.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/GTVV3.png\" alt=\"Screenshot\" /></a></p>\n"}, "title": "How to fine-tune GPT-J with small dataset", "content": "I have followed this guide as closely as possible: https://github.com/kingoflolz/mesh-transformer-jax\nI'm trying to fine-tune GPT-J with a small dataset of ~500 lines:\nYou are important to me. <|endoftext|>\nI love spending time with you. <|endoftext|>\nYou make me smile. <|endoftext|>\nfeel so lucky to be your friend. <|endoftext|>\nYou can always talk to me, even if it\u2019s about something that makes you nervous or scared or sad. <|endoftext|>\netc...\n\nUsing the create_finetune_tfrecords.py script (from the repo mentioned above) outputs a file with 2 in it. I understand that means my data has 2 sequences.\nI could really use some advice with the .json config file. What hyperparameters do you recommend for this small dataset?\nThe best I came up with trying to follow the guide:\n{\n  \"layers\": 28,\n  \"d_model\": 4096,\n  \"n_heads\": 16,\n  \"n_vocab\": 50400,\n  \"norm\": \"layernorm\",\n  \"pe\": \"rotary\",\n  \"pe_rotary_dims\": 64,\n\n  \"seq\": 2048,\n  \"cores_per_replica\": 8,\n  \"per_replica_batch\": 1,\n  \"gradient_accumulation_steps\": 2,\n\n  \"warmup_steps\": 1,\n  \"anneal_steps\": 9,\n  \"lr\": 1.2e-4,\n  \"end_lr\": 1.2e-5,\n  \"weight_decay\": 0.1,\n  \"total_steps\": 10,\n\n  \"tpu_size\": 8,\n\n  \"bucket\": \"chat-app-tpu-bucket-europe\",\n  \"model_dir\": \"finetune_dir\",\n\n  \"train_set\": \"james_bond_1.train.index\",\n  \"val_set\": {},\n\n  \"eval_harness_tasks\": [\n  ],\n\n  \"val_batches\": 2,\n  \"val_every\": 400000,\n  \"ckpt_every\": 1,\n  \"keep_every\": 1,\n\n  \"name\": \"GPT3_6B_pile_rotary\",\n  \"wandb_project\": \"mesh-transformer-jax\",\n  \"comment\": \"\"\n}\n\nThe problem is that, when I test the fine-tuned model, I get responses that make no sense:\n\n", "question_id": 32436, "answers": []}, "40385": {"link": "https://ai.stackexchange.com/questions/40385/what-is-a-neuron-in-large-language-models", "metadata": {"tags": ["natural-language-processing", "transformer", "chatgpt", "gpt", "artificial-neuron"], "owner": {"reputation": 534, "user_id": 23811, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-6jIAe62Q10s/AAAAAAAAAAI/AAAAAAAAABw/kRBVixTmLmk/photo.jpg?sz=256", "display_name": "Peyman", "link": "https://ai.stackexchange.com/users/23811/peyman"}, "is_answered": true, "view_count": 125, "closed_date": 1684422199, "accepted_answer_id": 40386, "answer_count": 1, "score": -1, "last_activity_date": 1683697236, "creation_date": 1683696028, "question_id": 40385, "link": "https://ai.stackexchange.com/questions/40385/what-is-a-neuron-in-large-language-models", "closed_reason": "Needs details or clarity", "title": "What is a neuron in large language models?", "body": "<p>I'm reading OpenAI's new paper &quot;<a href=\"https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html\" rel=\"nofollow noreferrer\">Language models can explain neurons in language models</a>&quot; And I can't fully understand the concept of neurons here.</p>\n<p>Can you please explain it? Is it related to the attention mechanism?</p>\n"}, "title": "What is a neuron in large language models?", "content": "I'm reading OpenAI's new paper \"Language models can explain neurons in language models\" And I can't fully understand the concept of neurons here.\nCan you please explain it? Is it related to the attention mechanism?\n", "question_id": 40385, "answers": []}, "15": {"link": "https://ai.stackexchange.com/questions/15/is-the-turing-test-or-any-of-its-variants-a-reliable-test-of-artificial-intell", "metadata": {"tags": ["turing-test", "agi", "intelligent-agent", "narrow-ai"], "owner": {"reputation": 501, "user_id": 9, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/r4U5R.jpg?s=256&g=1", "display_name": "Rob Murray", "link": "https://ai.stackexchange.com/users/9/rob-murray"}, "is_answered": true, "view_count": 4236, "protected_date": 1559169361, "answer_count": 6, "score": 40, "last_activity_date": 1523477795, "creation_date": 1470153170, "last_edit_date": 1470319810, "question_id": 15, "content_license": "CC BY-SA 3.0", "link": "https://ai.stackexchange.com/questions/15/is-the-turing-test-or-any-of-its-variants-a-reliable-test-of-artificial-intell", "title": "Is the Turing Test, or any of its variants, a reliable test of artificial intelligence?", "body": "<p>The <a href=\"https://en.wikipedia.org/wiki/Turing_test\">Turing Test</a> was the first test of artificial intelligence and is now a bit outdated. The <a href=\"https://en.wikipedia.org/wiki/Turing_test#Total_Turing_test\">Total Turing Test</a> aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\">artificial general intelligence</a> (strong AI)?</p>\n"}, "title": "Is the Turing Test, or any of its variants, a reliable test of artificial intelligence?", "content": "The Turing Test was the first test of artificial intelligence and is now a bit outdated. The Total Turing Test aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an artificial general intelligence (strong AI)?\n", "question_id": 15, "answers": []}, "1694": {"link": "https://ai.stackexchange.com/questions/1694/what-are-the-necessary-components-to-make-an-ai-agent-capable-of-self-programmin", "metadata": {"tags": ["machine-learning", "intelligent-agent", "computer-programming"], "owner": {"reputation": 2885, "user_id": 145, "user_type": "moderator", "accept_rate": 62, "profile_image": "https://i.stack.imgur.com/IIYyh.png?s=256&g=1", "display_name": "Mithical", "link": "https://ai.stackexchange.com/users/145/mithical"}, "is_answered": true, "view_count": 534, "accepted_answer_id": 1695, "answer_count": 1, "score": 9, "last_activity_date": 1630072428, "creation_date": 1471547789, "last_edit_date": 1630072428, "question_id": 1694, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/1694/what-are-the-necessary-components-to-make-an-ai-agent-capable-of-self-programmin", "title": "What are the necessary components to make an AI agent capable of self-programming?", "body": "<p>An AI agent is often thought of having \"sensors\", \"a memory\", \"machine learning processors\" and \"reaction\" components. However, a machine with these does not necessarily become a self-programming AI agent. Beyond the parts mentioned above, is there any other elements or details necessary to make a machine capable of being a self-programming AI agent?</p>\n\n<p>For example, <a href=\"http://www.iiim.is/wp/wp-content/uploads/2011/05/goertzel-agisp-2011.pdf\" rel=\"noreferrer\">a paper from 2011</a> declared that solving the optimization problem of maximizing the intelligence is a must-have feature for the self-programming process, as quoted below:</p>\n\n<blockquote>\n  <p>A system is said to carry out an instance of self-programming when it undergoes learning regarding some element of its \"cognitive infrastructure\", where the latter is defined as the fuzzy set of \"intelligence-critical\" features of the system; and the intelligence-criticality of a system feature is defined as its \"feature quality,\" considered from the perspective of solving the optimization problem of maximizing the intelligence of a multi-feature system.</p>\n</blockquote>\n\n<p>However, this description of \"optimization of intelligence\" is vague. Can anyone give a clear definition or better summary for the necessary components for self-programming agents?</p>\n\n<p><sub>This question is from the 2014 closed beta, with the asker having a UID of 23.</sub></p>\n"}, "title": "What are the necessary components to make an AI agent capable of self-programming?", "content": "An AI agent is often thought of having \"sensors\", \"a memory\", \"machine learning processors\" and \"reaction\" components. However, a machine with these does not necessarily become a self-programming AI agent. Beyond the parts mentioned above, is there any other elements or details necessary to make a machine capable of being a self-programming AI agent?\nFor example, a paper from 2011 declared that solving the optimization problem of maximizing the intelligence is a must-have feature for the self-programming process, as quoted below:\n\nA system is said to carry out an instance of self-programming when it undergoes learning regarding some element of its \"cognitive infrastructure\", where the latter is defined as the fuzzy set of \"intelligence-critical\" features of the system; and the intelligence-criticality of a system feature is defined as its \"feature quality,\" considered from the perspective of solving the optimization problem of maximizing the intelligence of a multi-feature system.\n\nHowever, this description of \"optimization of intelligence\" is vague. Can anyone give a clear definition or better summary for the necessary components for self-programming agents?\nThis question is from the 2014 closed beta, with the asker having a UID of 23.\n", "question_id": 1694, "answers": []}, "17025": {"link": "https://ai.stackexchange.com/questions/17025/how-can-an-ai-freely-make-decisions", "metadata": {"tags": ["reinforcement-learning", "deep-learning", "philosophy", "agi", "intelligent-agent"], "owner": {"reputation": 365, "user_id": 31978, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/bdd2a44cf6c207cac43d6db419e55e09?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "joethemow", "link": "https://ai.stackexchange.com/users/31978/joethemow"}, "is_answered": true, "view_count": 2086, "accepted_answer_id": 17035, "answer_count": 3, "score": 8, "last_activity_date": 1613150374, "creation_date": 1575996494, "last_edit_date": 1613150374, "question_id": 17025, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/17025/how-can-an-ai-freely-make-decisions", "title": "How can an AI freely make decisions?", "body": "<p>Suppose a deep neural network is created using Keras or Tensorflow. Usually, when you want to make a prediction, <strong>the user</strong> would invoke <code>model.predict</code>. However, how would the actual AI system proactively invoke their own actions (i.e. without the need for me to call <code>model.predict</code>)?</p>\n"}, "title": "How can an AI freely make decisions?", "content": "Suppose a deep neural network is created using Keras or Tensorflow. Usually, when you want to make a prediction, the user would invoke model.predict. However, how would the actual AI system proactively invoke their own actions (i.e. without the need for me to call model.predict)?\n", "question_id": 17025, "answers": []}, "3243": {"link": "https://ai.stackexchange.com/questions/3243/what-are-some-examples-of-intelligent-agents-for-each-intelligent-agent-class", "metadata": {"tags": ["definitions", "intelligent-agent", "multi-agent-systems"], "owner": {"reputation": 83, "user_id": 6921, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/8ff75e7d594d2d6ff08c7c337b8b1331?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "practronix512", "link": "https://ai.stackexchange.com/users/6921/practronix512"}, "is_answered": true, "view_count": 35744, "accepted_answer_id": 3253, "answer_count": 1, "score": 8, "last_activity_date": 1573934191, "creation_date": 1493487841, "last_edit_date": 1573934191, "question_id": 3243, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/3243/what-are-some-examples-of-intelligent-agents-for-each-intelligent-agent-class", "title": "What are some examples of intelligent agents for each intelligent agent class?", "body": "<p>There are several classes of intelligent agents, such as:</p>\n\n<ul>\n<li>simple reflex agents</li>\n<li>model-based reflex agents</li>\n<li>goal-based agents</li>\n<li>utility-based agents</li>\n<li>learning agents</li>\n</ul>\n\n<p>Each of these agents behaves slightly different from the other agents. There are certain diagrams that describe each of these IA classes. However, all the descriptions are usually quite abstract.</p>\n\n<p>What are some examples of intelligent agents for each intelligent agent class? Optionally, I would also like to see a compact definition of each class (and maybe how they are related to the diagrams).</p>\n"}, "title": "What are some examples of intelligent agents for each intelligent agent class?", "content": "There are several classes of intelligent agents, such as:\n\nsimple reflex agents\nmodel-based reflex agents\ngoal-based agents\nutility-based agents\nlearning agents\n\nEach of these agents behaves slightly different from the other agents. There are certain diagrams that describe each of these IA classes. However, all the descriptions are usually quite abstract.\nWhat are some examples of intelligent agents for each intelligent agent class? Optionally, I would also like to see a compact definition of each class (and maybe how they are related to the diagrams).\n", "question_id": 3243, "answers": []}, "6": {"link": "https://ai.stackexchange.com/questions/6/are-humans-intelligent-according-to-the-definition-of-an-intelligent-agent", "metadata": {"tags": ["philosophy", "definitions", "intelligent-agent"], "owner": {"reputation": 1511, "user_id": 29, "user_type": "registered", "accept_rate": 78, "profile_image": "https://i.stack.imgur.com/JfeF9.png?s=256&g=1", "display_name": "wythagoras", "link": "https://ai.stackexchange.com/users/29/wythagoras"}, "is_answered": true, "view_count": 298, "accepted_answer_id": 20, "answer_count": 2, "score": 7, "last_activity_date": 1560623395, "creation_date": 1470152615, "last_edit_date": 1560623158, "question_id": 6, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/6/are-humans-intelligent-according-to-the-definition-of-an-intelligent-agent", "title": "Are humans intelligent according to the definition of an intelligent agent?", "body": "<p>Given the following definition of an intelligent agent (taken from a <a href=\"http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition\" rel=\"nofollow noreferrer\">Wikipedia article</a>)</p>\n\n<blockquote>\n  <p>If an agent acts so as to maximize the expected value of a performance measure based on past experience and knowledge then it is intelligent</p>\n</blockquote>\n\n<p>and given that we, humans, all make mistakes, which means that we are not maximizing the expected value of a performance measure, then does this imply that humans are not intelligent? </p>\n"}, "title": "Are humans intelligent according to the definition of an intelligent agent?", "content": "Given the following definition of an intelligent agent (taken from a Wikipedia article)\n\nIf an agent acts so as to maximize the expected value of a performance measure based on past experience and knowledge then it is intelligent\n\nand given that we, humans, all make mistakes, which means that we are not maximizing the expected value of a performance measure, then does this imply that humans are not intelligent? \n", "question_id": 6, "answers": []}, "7774": {"link": "https://ai.stackexchange.com/questions/7774/what-is-the-definition-of-rationality", "metadata": {"tags": ["definitions", "intelligent-agent", "chess", "rationality", "simple-reflex-agents"], "owner": {"reputation": 558, "user_id": 17488, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/QoLra.jpg?s=256&g=1", "display_name": "Mr. Eivind", "link": "https://ai.stackexchange.com/users/17488/mr-eivind"}, "is_answered": true, "view_count": 3493, "accepted_answer_id": 7777, "answer_count": 2, "score": 6, "last_activity_date": 1573945942, "creation_date": 1535742233, "last_edit_date": 1573945942, "question_id": 7774, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/7774/what-is-the-definition-of-rationality", "title": "What is the definition of rationality?", "body": "<p>I'm having a little trouble with the definition of rationality, which goes something like:</p>\n\n<blockquote>\n  <p>An agent is rational if it maximizes its performance measure given its current knowledge.</p>\n</blockquote>\n\n<p>I've read that a simple reflex agent will not act rationally in a lot of environments. For example, a simple reflex agent can't act rationally when driving a car, as it needs previous perceptions to make correct decisions.</p>\n\n<p>However, if it does its best with the information it's got, wouldn't that be rational behaviour, as the definition contains \"given its current knowledge\"? Or is it more like: \"given the knowledge it could have had at this point if it had stored all the knowledge it has ever received\"?</p>\n\n<p>Another question about the definition of rationality: Is a chess engine rational as it picks the best move given the time it's allowed to use, or is it not rational as it doesn't actually (always) find the best solution (would need more time to do so)?</p>\n"}, "title": "What is the definition of rationality?", "content": "I'm having a little trouble with the definition of rationality, which goes something like:\n\nAn agent is rational if it maximizes its performance measure given its current knowledge.\n\nI've read that a simple reflex agent will not act rationally in a lot of environments. For example, a simple reflex agent can't act rationally when driving a car, as it needs previous perceptions to make correct decisions.\nHowever, if it does its best with the information it's got, wouldn't that be rational behaviour, as the definition contains \"given its current knowledge\"? Or is it more like: \"given the knowledge it could have had at this point if it had stored all the knowledge it has ever received\"?\nAnother question about the definition of rationality: Is a chess engine rational as it picks the best move given the time it's allowed to use, or is it not rational as it doesn't actually (always) find the best solution (would need more time to do so)?\n", "question_id": 7774, "answers": []}, "7793": {"link": "https://ai.stackexchange.com/questions/7793/what-are-the-differences-between-an-agent-that-thinks-rationally-and-an-agent-th", "metadata": {"tags": ["comparison", "terminology", "intelligent-agent"], "owner": {"reputation": 161, "user_id": 17886, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/KBKUH.png?s=256&g=1", "display_name": "Bayequentist", "link": "https://ai.stackexchange.com/users/17886/bayequentist"}, "is_answered": true, "view_count": 116, "accepted_answer_id": 7796, "answer_count": 1, "score": 5, "last_activity_date": 1612874834, "creation_date": 1535883603, "last_edit_date": 1612869569, "question_id": 7793, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/7793/what-are-the-differences-between-an-agent-that-thinks-rationally-and-an-agent-th", "title": "What are the differences between an agent that thinks rationally and an agent that acts rationally?", "body": "<p>Stuart Russell and Peter Norvig <a href=\"https://people.eecs.berkeley.edu/~russell/intro.html\" rel=\"nofollow noreferrer\">pointed out</a> 4 four possible goals to pursue in artificial intelligence: systems that think/act humanly/rationally.</p>\n\n<p>What are the differences between an agent that <em>thinks</em> rationally and an agent that <em>acts</em> rationally?</p>\n"}, "title": "What are the differences between an agent that thinks rationally and an agent that acts rationally?", "content": "Stuart Russell and Peter Norvig pointed out 4 four possible goals to pursue in artificial intelligence: systems that think/act humanly/rationally.\nWhat are the differences between an agent that thinks rationally and an agent that acts rationally?\n", "question_id": 7793, "answers": []}, "3406": {"link": "https://ai.stackexchange.com/questions/3406/what-is-the-difference-between-simple-reflex-and-model-based-agents", "metadata": {"tags": ["comparison", "intelligent-agent", "simple-reflex-agents"], "owner": {"reputation": 161, "user_id": 7500, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/98bff308e118b5c17c9410fda4b63a5c?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Pierre P.", "link": "https://ai.stackexchange.com/users/7500/pierre-p"}, "is_answered": true, "view_count": 7602, "answer_count": 1, "score": 5, "last_activity_date": 1639325536, "creation_date": 1496056679, "last_edit_date": 1639325536, "question_id": 3406, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/3406/what-is-the-difference-between-simple-reflex-and-model-based-agents", "title": "What is the difference between simple reflex and model-based agents?", "body": "<p>What is the difference between simple reflex and model-based agents?</p>\n<p>What is the role of the internal state in the case of model-based agents?</p>\n"}, "title": "What is the difference between simple reflex and model-based agents?", "content": "What is the difference between simple reflex and model-based agents?\nWhat is the role of the internal state in the case of model-based agents?\n", "question_id": 3406, "answers": []}, "1515": {"link": "https://ai.stackexchange.com/questions/1515/what-is-the-difference-between-abstract-autonomous-and-virtual-intelligent-agen", "metadata": {"tags": ["definitions", "intelligent-agent", "comparison", "terminology"], "owner": {"reputation": 10423, "user_id": 8, "user_type": "registered", "accept_rate": 89, "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG", "display_name": "kenorb", "link": "https://ai.stackexchange.com/users/8/kenorb"}, "is_answered": true, "view_count": 614, "accepted_answer_id": 3464, "answer_count": 2, "score": 5, "last_activity_date": 1617229658, "creation_date": 1470794729, "last_edit_date": 1496852379, "question_id": 1515, "content_license": "CC BY-SA 3.0", "link": "https://ai.stackexchange.com/questions/1515/what-is-the-difference-between-abstract-autonomous-and-virtual-intelligent-agen", "title": "What is the difference between abstract, autonomous and virtual intelligent agents?", "body": "<p>On Wikipedia, we can read about different type of <a href=\"https://en.wikipedia.org/wiki/Intelligent_agent\" rel=\"nofollow noreferrer\">intelligent agents</a>:</p>\n\n<ul>\n<li>abstract intelligent agents (AIA),</li>\n<li>autonomous intelligent agents,</li>\n<li>virtual intelligent agent (IVA), which I've found on other websites, e.g. <a href=\"https://www.techopedia.com/definition/26646/intelligent-virtual-agent-iva\" rel=\"nofollow noreferrer\">this one</a>.</li>\n</ul>\n\n<p>What are the differences between these three to avoid confusion?</p>\n\n<hr>\n\n<p>For example I've used term <em>virtual artificial agent</em> <a href=\"https://ai.stackexchange.com/a/1512/8\">here</a> as:</p>\n\n<blockquote>\n  <p>Basically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior (AI).</p>\n</blockquote>\n\n<p>so basically I'd like to know where other terms like autonomous or abstract agents can be used and in what context. Can they be all defined under 'virtual' robot definition? How to distinguish these terms?</p>\n"}, "title": "What is the difference between abstract, autonomous and virtual intelligent agents?", "content": "On Wikipedia, we can read about different type of intelligent agents:\n\nabstract intelligent agents (AIA),\nautonomous intelligent agents,\nvirtual intelligent agent (IVA), which I've found on other websites, e.g. this one.\n\nWhat are the differences between these three to avoid confusion?\n\nFor example I've used term virtual artificial agent here as:\n\nBasically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior (AI).\n\nso basically I'd like to know where other terms like autonomous or abstract agents can be used and in what context. Can they be all defined under 'virtual' robot definition? How to distinguish these terms?\n", "question_id": 1515, "answers": []}, "26956": {"link": "https://ai.stackexchange.com/questions/26956/how-widely-accepted-is-the-definition-of-intelligence-by-marcus-hutter-shane-l", "metadata": {"tags": ["agi", "definitions", "intelligent-agent", "intelligence"], "owner": {"reputation": 265, "user_id": 45586, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/n8MIV.jpg?s=256&g=1", "display_name": "Aether", "link": "https://ai.stackexchange.com/users/45586/aether"}, "is_answered": true, "view_count": 310, "answer_count": 3, "score": 5, "last_activity_date": 1646281879, "creation_date": 1616480302, "question_id": 26956, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/26956/how-widely-accepted-is-the-definition-of-intelligence-by-marcus-hutter-shane-l", "title": "How widely accepted is the definition of intelligence by Marcus Hutter &amp; Shane Legg?", "body": "<p>I came across several papers by M. Hutter &amp; S. Legg.\nEspecially this one:\n<a href=\"https://arxiv.org/abs/0712.3329\" rel=\"noreferrer\">Universal Intelligence: A Definition of Machine Intelligence, Shane Legg, Marcus Hutter</a></p>\n<p>Given that it was published back in 2007, how much recognition or agreement has it received?\nHas any other work better formalizing the idea of intelligence been done since?\nWhat is considered current standard on the topic in the field?</p>\n"}, "title": "How widely accepted is the definition of intelligence by Marcus Hutter &amp; Shane Legg?", "content": "I came across several papers by M. Hutter & S. Legg.\nEspecially this one:\nUniversal Intelligence: A Definition of Machine Intelligence, Shane Legg, Marcus Hutter\nGiven that it was published back in 2007, how much recognition or agreement has it received?\nHas any other work better formalizing the idea of intelligence been done since?\nWhat is considered current standard on the topic in the field?\n", "question_id": 26956, "answers": []}, "2668": {"link": "https://ai.stackexchange.com/questions/2668/how-can-i-design-an-ai-that-knows-when-its-being-spoken-to", "metadata": {"tags": ["neural-networks", "machine-learning", "deep-learning", "ai-design", "intelligent-agent"], "owner": {"reputation": 161, "user_id": 4841, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3add1d23673f346807cb8261588ee385?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "N. Chalifour", "link": "https://ai.stackexchange.com/users/4841/n-chalifour"}, "is_answered": true, "view_count": 96, "answer_count": 2, "score": 4, "last_activity_date": 1589843671, "creation_date": 1484515262, "last_edit_date": 1589843671, "question_id": 2668, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/2668/how-can-i-design-an-ai-that-knows-when-its-being-spoken-to", "title": "How can I design an AI that knows when its being spoken to?", "body": "<p>I am trying to make an intelligent agent similar to Jarvis from Iron Man, but much less complex. However, I want my AI to be able to determine if I am talking to it or not. So, I plan on having it always listen to my voice and convert that to text. However, I am not sure how I can train the AI to recognize if it is being spoken to or not?</p>\n"}, "title": "How can I design an AI that knows when its being spoken to?", "content": "I am trying to make an intelligent agent similar to Jarvis from Iron Man, but much less complex. However, I want my AI to be able to determine if I am talking to it or not. So, I plan on having it always listen to my voice and convert that to text. However, I am not sure how I can train the AI to recognize if it is being spoken to or not?\n", "question_id": 2668, "answers": []}, "4489": {"link": "https://ai.stackexchange.com/questions/4489/how-to-detect-when-human-voice-speech-appears-in-an-microphone-stream", "metadata": {"tags": ["intelligent-agent", "voice-recognition"], "owner": {"reputation": 149, "user_id": 10650, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-0EFYFwb0YqE/AAAAAAAAAAI/AAAAAAAAAA0/4Kehw78dfgg/photo.jpg?sz=256", "display_name": "AIon", "link": "https://ai.stackexchange.com/users/10650/aion"}, "is_answered": true, "view_count": 4674, "answer_count": 1, "score": 4, "last_activity_date": 1510599955, "creation_date": 1510421961, "last_edit_date": 1510524805, "question_id": 4489, "content_license": "CC BY-SA 3.0", "link": "https://ai.stackexchange.com/questions/4489/how-to-detect-when-human-voice-speech-appears-in-an-microphone-stream", "title": "How to detect when human voice / speech appears in an microphone stream?", "body": "<p>I want to build a personal assistant that listens to me continuously.. </p>\n\n<p>The flow looks like this: </p>\n\n<ol>\n<li>continuously record voice </li>\n<li>stream it to google speech api.</li>\n<li>get back the text in real time -> parse for intent etc..</li>\n</ol>\n\n<p>Problem is, google speech api gets expensive if you record for hours. A better way to do it is to only submit the parts where I'm actually speaking to it.. Then the cost of running this full time (17 hours a day, every day) becomes very accessible.\nNow my question is:</p>\n\n<p><strong>How can i detect that a voice is present in the microphone stream?</strong> </p>\n\n<p>I have a lot of noise in the background, dumb <code>increase in volume detection</code> is not a very good solution. I need something more intelligent. It doesn't need to be very accurate - just good enough to not break my cloud computing budget. I'm thinking that human voice sounds distinct enough that is not such a big problem to detect when is there.</p>\n\n<p>What do you recommend me to do, given this is a real time stream - not an audio file.</p>\n\n<p>The audio will be generated in chromium browser (electron) - with <code>getUserMedia</code> API, and using <code>node.js</code> i plan to handle the streaming logic. </p>\n\n<p>Note: there is a built in <code>speechRecognition api</code> in <code>electron</code> - but from my experience currently it doesn't work (not even after i give it my API key), and even if would have worked, i think it has the same cost problem. So this is why i'm trying to provide my own implementation. </p>\n\n<p>I don't know what i'm doing, any insight is welcomed:) Thank you.</p>\n"}, "title": "How to detect when human voice / speech appears in an microphone stream?", "content": "I want to build a personal assistant that listens to me continuously.. \nThe flow looks like this: \n\ncontinuously record voice \nstream it to google speech api.\nget back the text in real time -> parse for intent etc..\n\nProblem is, google speech api gets expensive if you record for hours. A better way to do it is to only submit the parts where I'm actually speaking to it.. Then the cost of running this full time (17 hours a day, every day) becomes very accessible.\nNow my question is:\nHow can i detect that a voice is present in the microphone stream? \nI have a lot of noise in the background, dumb increase in volume detection is not a very good solution. I need something more intelligent. It doesn't need to be very accurate - just good enough to not break my cloud computing budget. I'm thinking that human voice sounds distinct enough that is not such a big problem to detect when is there.\nWhat do you recommend me to do, given this is a real time stream - not an audio file.\nThe audio will be generated in chromium browser (electron) - with getUserMedia API, and using node.js i plan to handle the streaming logic. \nNote: there is a built in speechRecognition api in electron - but from my experience currently it doesn't work (not even after i give it my API key), and even if would have worked, i think it has the same cost problem. So this is why i'm trying to provide my own implementation. \nI don't know what i'm doing, any insight is welcomed:) Thank you.\n", "question_id": 4489, "answers": []}, "5059": {"link": "https://ai.stackexchange.com/questions/5059/what-are-some-of-the-possible-future-applications-of-intelligent-agents", "metadata": {"tags": ["applications", "chat-bots", "intelligent-agent"], "owner": {"reputation": 973, "user_id": 9053, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/GIk0b.jpg?s=256&g=1", "display_name": "Tina J", "link": "https://ai.stackexchange.com/users/9053/tina-j"}, "is_answered": true, "view_count": 658, "protected_date": 1589583785, "accepted_answer_id": 5060, "answer_count": 1, "score": 4, "last_activity_date": 1589584116, "creation_date": 1516312738, "last_edit_date": 1589584116, "question_id": 5059, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/5059/what-are-some-of-the-possible-future-applications-of-intelligent-agents", "title": "What are some of the possible future applications of intelligent agents?", "body": "<p>I am trying to do some experiments with some intelligent agents, but I'm not sure how significant they will be in the future. </p>\n\n<p>What are some possible interesting applications or use-cases of intelligent agents in the future? </p>\n\n<p>For instance, it can be used as a virtual assistant instead of a real call agent. But what can be a more appealing application in the future?</p>\n"}, "title": "What are some of the possible future applications of intelligent agents?", "content": "I am trying to do some experiments with some intelligent agents, but I'm not sure how significant they will be in the future. \nWhat are some possible interesting applications or use-cases of intelligent agents in the future? \nFor instance, it can be used as a virtual assistant instead of a real call agent. But what can be a more appealing application in the future?\n", "question_id": 5059, "answers": []}, "2547": {"link": "https://ai.stackexchange.com/questions/2547/is-there-anything-novel-about-zuckerbergs-jarvis", "metadata": {"tags": ["ai-design", "intelligent-agent"], "owner": {"reputation": 41, "user_id": 4444, "user_type": "unregistered", "profile_image": "https://www.gravatar.com/avatar/066b836cb47e58e1314312eba1ff322c?s=256&d=identicon&r=PG", "display_name": "Dylan Dsouza", "link": "https://ai.stackexchange.com/users/4444/dylan-dsouza"}, "is_answered": true, "view_count": 247, "answer_count": 1, "score": 4, "last_activity_date": 1482828432, "creation_date": 1482749967, "last_edit_date": 1482783294, "question_id": 2547, "content_license": "CC BY-SA 3.0", "link": "https://ai.stackexchange.com/questions/2547/is-there-anything-novel-about-zuckerbergs-jarvis", "title": "Is there anything novel about Zuckerberg&#39;s Jarvis?", "body": "<p>Recently Mark got some attention from the media by stating that he had created Jarvis. Not that I'm against him or anything, but this Jarvis seems to have been done a hundred times before. He's done something which most developers would classify as a home automation system. To me it's more like he did it for the attention. I was kind of taken back by the amount of media attention he got. If you've heard of Jeremy Blum, maybe you may understand what I'm trying to imply here. </p>\n\n<p>I'm just curious as to why he got so much attention. Is there anything technically novel about his system that sets it so much apart from previous ones?</p>\n"}, "title": "Is there anything novel about Zuckerberg&#39;s Jarvis?", "content": "Recently Mark got some attention from the media by stating that he had created Jarvis. Not that I'm against him or anything, but this Jarvis seems to have been done a hundred times before. He's done something which most developers would classify as a home automation system. To me it's more like he did it for the attention. I was kind of taken back by the amount of media attention he got. If you've heard of Jeremy Blum, maybe you may understand what I'm trying to imply here. \nI'm just curious as to why he got so much attention. Is there anything technically novel about his system that sets it so much apart from previous ones?\n", "question_id": 2547, "answers": []}, "8637": {"link": "https://ai.stackexchange.com/questions/8637/what-is-the-difference-between-learning-and-non-learning-agents", "metadata": {"tags": ["deep-learning", "comparison", "intelligent-agent", "learning-agents"], "owner": {"reputation": 135, "user_id": 18950, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/30d45683a0520623bb27198f0246aaed?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "RashkRizwan", "link": "https://ai.stackexchange.com/users/18950/rashkrizwan"}, "is_answered": true, "view_count": 2457, "accepted_answer_id": 8642, "answer_count": 2, "score": 4, "last_activity_date": 1639330731, "creation_date": 1540646359, "last_edit_date": 1639330731, "question_id": 8637, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/8637/what-is-the-difference-between-learning-and-non-learning-agents", "title": "What is the difference between learning and non-learning agents?", "body": "<p>What is the difference between learning agents and other types of agents?</p>\n<p>In what ways learning agents can be applied? Do learning agents differ from deep learning?</p>\n"}, "title": "What is the difference between learning and non-learning agents?", "content": "What is the difference between learning agents and other types of agents?\nIn what ways learning agents can be applied? Do learning agents differ from deep learning?\n", "question_id": 8637, "answers": []}, "2917": {"link": "https://ai.stackexchange.com/questions/2917/does-it-exist-a-human-like-artificial-intelligence", "metadata": {"tags": ["research", "agi", "intelligent-agent", "social", "human-like"], "owner": {"user_type": "does_not_exist", "display_name": "user5832"}, "is_answered": true, "view_count": 436, "answer_count": 3, "score": 4, "last_activity_date": 1573623286, "creation_date": 1488623412, "last_edit_date": 1571776133, "question_id": 2917, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/2917/does-it-exist-a-human-like-artificial-intelligence", "title": "Does it exist a human-like artificial intelligence?", "body": "<p>Does it exist a human-like artificial intelligence?</p>\n\n<p>I define <em>human-like</em> as something that can act like a human in most aspects. </p>\n"}, "title": "Does it exist a human-like artificial intelligence?", "content": "Does it exist a human-like artificial intelligence?\nI define human-like as something that can act like a human in most aspects. \n", "question_id": 2917, "answers": []}, "8707": {"link": "https://ai.stackexchange.com/questions/8707/what-is-a-learning-agent", "metadata": {"tags": ["terminology", "definitions", "intelligent-agent", "learning-agents"], "owner": {"reputation": 315, "user_id": 19442, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/f19a3ac761df525d20390cf126e6219e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Iram Shah", "link": "https://ai.stackexchange.com/users/19442/iram-shah"}, "is_answered": true, "view_count": 10570, "answer_count": 1, "score": 4, "last_activity_date": 1639325580, "creation_date": 1541069377, "last_edit_date": 1639325580, "question_id": 8707, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/8707/what-is-a-learning-agent", "title": "What is a learning agent?", "body": "<p>What is a learning agent, and how does it work? What are examples of learning agents (e.g., in the field of robotics)?</p>\n"}, "title": "What is a learning agent?", "content": "What is a learning agent, and how does it work? What are examples of learning agents (e.g., in the field of robotics)?\n", "question_id": 8707, "answers": []}, "12506": {"link": "https://ai.stackexchange.com/questions/12506/multi-agent-sokoban-search-solvers-state-of-the-art", "metadata": {"tags": ["research", "intelligent-agent", "multi-agent-systems"], "owner": {"reputation": 141, "user_id": 25888, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/aec994637c4497ef93a0dbd2f1e6d375?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Panos Filianos", "link": "https://ai.stackexchange.com/users/25888/panos-filianos"}, "is_answered": false, "view_count": 218, "answer_count": 0, "score": 4, "last_activity_date": 1558818571, "creation_date": 1558699546, "last_edit_date": 1558818571, "question_id": 12506, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/12506/multi-agent-sokoban-search-solvers-state-of-the-art", "title": "Multi Agent Sokoban Search Solvers state of the art", "body": "<p><sub>I also asked this question <a href=\"https://cs.stackexchange.com/questions/109807/multi-agent-sokoban-solvers-state-of-the-art\">here</a> but I'm repeating it on this SE because I feel it is more relevant. No intention to spam.</sub></p>\n\n<p>I am researching into coding a solver for a variant of the <a href=\"https://en.wikipedia.org/wiki/Sokoban\" rel=\"nofollow noreferrer\">Sokoban</a> game with multiple agents, restrictions (eg. colors of stones, goals) and relaxations (push AND pull possible, etc.)</p>\n\n<p>By researching online I have found classic papers in the field, like the Rolling Stone paper (by Andreas Junghanns and Jonathan Schaeffer) and <em>Sokoban: Enhancing general single-agent search methods using domain knowledge</em> from the same authors.</p>\n\n<p>These solutions seem to be outdated and I am currently structuring my solver per the notes of the two most performant solvers: <a href=\"http://www.sokobano.de/wiki/index.php?title=Sokoban_solver_%22scribbles%22_by_Brian_Damgaard_about_the_YASS_solver\" rel=\"nofollow noreferrer\">YASS</a> and <a href=\"https://cs.stackexchange.com/questions/109807/multi-agent-sokoban-solvers-state-of-the-art\">Sokolution</a>.</p>\n\n<p>From the research, I've done these two seem to be my best bet.</p>\n\n<p>It is apparent that they are not enough by themselves to solve a multi-agent environment. Those solvers are made for a single agent. So far, I have failed to find useful multi-agent proposals.</p>\n\n<p>In this context, my question is:</p>\n\n<ol>\n<li>What can be considered state-of-the-art in order to:\n\n<ul>\n<li>coordinate multiple agents with different goals, and</li>\n<li>plug a solver's solution in and validate/edit it?</li>\n</ul></li>\n<li>What are some search terms I can use to research this further?</li>\n</ol>\n\n<p>Thank you very much</p>\n"}, "title": "Multi Agent Sokoban Search Solvers state of the art", "content": "I also asked this question here but I'm repeating it on this SE because I feel it is more relevant. No intention to spam.\nI am researching into coding a solver for a variant of the Sokoban game with multiple agents, restrictions (eg. colors of stones, goals) and relaxations (push AND pull possible, etc.)\nBy researching online I have found classic papers in the field, like the Rolling Stone paper (by Andreas Junghanns and Jonathan Schaeffer) and Sokoban: Enhancing general single-agent search methods using domain knowledge from the same authors.\nThese solutions seem to be outdated and I am currently structuring my solver per the notes of the two most performant solvers: YASS and Sokolution.\nFrom the research, I've done these two seem to be my best bet.\nIt is apparent that they are not enough by themselves to solve a multi-agent environment. Those solvers are made for a single agent. So far, I have failed to find useful multi-agent proposals.\nIn this context, my question is:\n\nWhat can be considered state-of-the-art in order to:\n\n\ncoordinate multiple agents with different goals, and\nplug a solver's solution in and validate/edit it?\n\nWhat are some search terms I can use to research this further?\n\nThank you very much\n", "question_id": 12506, "answers": []}, "25725": {"link": "https://ai.stackexchange.com/questions/25725/is-a-team-of-ml-scientists-an-intelligent-agent", "metadata": {"tags": ["machine-learning", "terminology", "intelligent-agent"], "owner": {"reputation": 171, "user_id": 43741, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/17c5621384e4543d3778b583d76e9f42?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Marina", "link": "https://ai.stackexchange.com/users/43741/marina"}, "is_answered": true, "view_count": 65, "answer_count": 1, "score": 3, "last_activity_date": 1610449750, "creation_date": 1610430717, "last_edit_date": 1610444792, "question_id": 25725, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/25725/is-a-team-of-ml-scientists-an-intelligent-agent", "title": "Is a team of ML scientists an &quot;intelligent agent&quot;?", "body": "<p>I am writing about the role of machine learning scientists in developing a solution. Is there a term for the humans who do learning?\nCan we call a &quot;team of machine learning scientists with their computers working on some ML problem&quot;   an intelligent agent? Is &quot;cognizer&quot; the right term? I know that &quot;learner&quot; is reserved for an ML algorithm. I just want a shorter term for their role in cognition, learning.</p>\n"}, "title": "Is a team of ML scientists an &quot;intelligent agent&quot;?", "content": "I am writing about the role of machine learning scientists in developing a solution. Is there a term for the humans who do learning?\nCan we call a \"team of machine learning scientists with their computers working on some ML problem\"   an intelligent agent? Is \"cognizer\" the right term? I know that \"learner\" is reserved for an ML algorithm. I just want a shorter term for their role in cognition, learning.\n", "question_id": 25725, "answers": []}, "22323": {"link": "https://ai.stackexchange.com/questions/22323/are-there-any-agents-that-are-based-on-quantum-computing", "metadata": {"tags": ["intelligent-agent", "quantum-computing"], "owner": {"reputation": 33, "user_id": 34403, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/iCdDl.jpg?s=256&g=1", "display_name": "Varun Premkumar", "link": "https://ai.stackexchange.com/users/34403/varun-premkumar"}, "is_answered": true, "view_count": 115, "accepted_answer_id": 22346, "answer_count": 1, "score": 3, "last_activity_date": 1593998684, "creation_date": 1593756564, "last_edit_date": 1593907350, "question_id": 22323, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/22323/are-there-any-agents-that-are-based-on-quantum-computing", "title": "Are there any agents that are based on quantum computing?", "body": "<p>Assuming the definition of an agent to be:</p>\n<blockquote>\n<p>An entity that perceives its environment, processes the perceived information, and acts on the environment such that some goal is fulfilled.</p>\n</blockquote>\n<p>Are there any agents that are based on quantum processing/computing (i.e. implemented by a network of <a href=\"https://en.wikipedia.org/wiki/Quantum_logic_gate\" rel=\"nofollow noreferrer\">quantum gates</a>)?</p>\n<p>Is there any work done towards this end? If so, could someone provide references?</p>\n"}, "title": "Are there any agents that are based on quantum computing?", "content": "Assuming the definition of an agent to be:\n\nAn entity that perceives its environment, processes the perceived information, and acts on the environment such that some goal is fulfilled.\n\nAre there any agents that are based on quantum processing/computing (i.e. implemented by a network of quantum gates)?\nIs there any work done towards this end? If so, could someone provide references?\n", "question_id": 22323, "answers": []}, "18290": {"link": "https://ai.stackexchange.com/questions/18290/how-do-we-define-intention-if-there-is-no-free-will", "metadata": {"tags": ["philosophy", "intelligent-agent", "intelligence", "goal-based-agents"], "owner": {"reputation": 6237, "user_id": 1671, "user_type": "registered", "accept_rate": 36, "profile_image": "https://i.stack.imgur.com/iE1hj.png?s=256&g=1", "display_name": "DukeZhou", "link": "https://ai.stackexchange.com/users/1671/dukezhou"}, "is_answered": true, "view_count": 244, "answer_count": 4, "score": 3, "last_activity_date": 1620606050, "creation_date": 1582850427, "question_id": 18290, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/18290/how-do-we-define-intention-if-there-is-no-free-will", "title": "How do we define intention if there is no free will?", "body": "<p>There is an idea that intentionality may be a requirement of true intelligence, here defined as human intelligence.  </p>\n\n<p>But all I know for certain is that we have the <em>appearance</em> of free will.  Under the assumption that the universe is purely deterministic, what do we mean by intention?</p>\n\n<p><em>(This seems an important question given that intention is not just a philosophical matter in relation to definitions of AI, but involves ethics in the sense of application of AI, \"offloading responsibility to agents that cannot be meaningfully punished\" as an example.  Also touches on goals, implied by intention, whether awareness is a requirement, and what constitutes awareness. I'm interested in all angles, but was inspired by the question \"does true art require intention, and, if so, is that the sole domain of humans?\")</em></p>\n"}, "title": "How do we define intention if there is no free will?", "content": "There is an idea that intentionality may be a requirement of true intelligence, here defined as human intelligence.  \nBut all I know for certain is that we have the appearance of free will.  Under the assumption that the universe is purely deterministic, what do we mean by intention?\n(This seems an important question given that intention is not just a philosophical matter in relation to definitions of AI, but involves ethics in the sense of application of AI, \"offloading responsibility to agents that cannot be meaningfully punished\" as an example.  Also touches on goals, implied by intention, whether awareness is a requirement, and what constitutes awareness. I'm interested in all angles, but was inspired by the question \"does true art require intention, and, if so, is that the sole domain of humans?\")\n", "question_id": 18290, "answers": []}, "3364": {"link": "https://ai.stackexchange.com/questions/3364/can-we-create-a-conscious-agent-by-having-two-units-one-rational-and-the-other", "metadata": {"tags": ["ai-design", "intelligent-agent", "artificial-consciousness"], "owner": {"reputation": 133, "user_id": 7377, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/LV5Fe.gif?s=256&g=1", "display_name": "DivineCoder", "link": "https://ai.stackexchange.com/users/7377/divinecoder"}, "is_answered": true, "view_count": 138, "accepted_answer_id": 3366, "answer_count": 1, "score": 3, "last_activity_date": 1592304792, "creation_date": 1495521859, "last_edit_date": 1592303637, "question_id": 3364, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/3364/can-we-create-a-conscious-agent-by-having-two-units-one-rational-and-the-other", "title": "Can we create a conscious agent by having two units: one rational and the other conscious?", "body": "<p>Suppose we create two units (or programs) that run in parallel and we label them as a cognitive unit and the conscious cognitive unit. </p>\n\n<p>A human has two units analogously. A rational analyzer and not so rational analyzer. (Is there any third thing?)</p>\n\n<p>In my opinion, consciousness is an extra layer of decision making. This resembles the way metaheuristics work. We have a set of rules for decision making and we analyze them and tune those rules dynamically.</p>\n\n<p>Global search algorithms mimic conscious behavior whereas the local search algorithms work as a rational mind.</p>\n\n<p>So, is it possible to create a conscious agent by having two units as I described above?</p>\n"}, "title": "Can we create a conscious agent by having two units: one rational and the other conscious?", "content": "Suppose we create two units (or programs) that run in parallel and we label them as a cognitive unit and the conscious cognitive unit. \nA human has two units analogously. A rational analyzer and not so rational analyzer. (Is there any third thing?)\nIn my opinion, consciousness is an extra layer of decision making. This resembles the way metaheuristics work. We have a set of rules for decision making and we analyze them and tune those rules dynamically.\nGlobal search algorithms mimic conscious behavior whereas the local search algorithms work as a rational mind.\nSo, is it possible to create a conscious agent by having two units as I described above?\n", "question_id": 3364, "answers": []}, "12991": {"link": "https://ai.stackexchange.com/questions/12991/what-is-an-agent-in-artificial-intelligence", "metadata": {"tags": ["reinforcement-learning", "terminology", "definitions", "intelligent-agent"], "owner": {"reputation": 745, "user_id": 23527, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/ebqPY.jpg?s=256&g=1", "display_name": "olinarr", "link": "https://ai.stackexchange.com/users/23527/olinarr"}, "is_answered": true, "view_count": 2947, "accepted_answer_id": 12995, "answer_count": 1, "score": 2, "last_activity_date": 1639323994, "creation_date": 1561211052, "last_edit_date": 1639311985, "question_id": 12991, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/12991/what-is-an-agent-in-artificial-intelligence", "title": "What is an agent in Artificial Intelligence?", "body": "<p>While studying artificial intelligence, I have often encountered the term \"agent\" (often autonomous, intelligent). For instance, in fields such as Reinforcement Learning, Multi-Agent Systems, Game Theory, Markov Decision Processes.</p>\n\n<p>In an intuitive sense, it is clear to me what an agent is; I was wondering whether in AI it had a rigorous definition, perhaps expressed in mathematical language, and shared by the various AI-related fields.</p>\n\n<p><strong>What is an agent in Artificial Intelligence?</strong></p>\n"}, "title": "What is an agent in Artificial Intelligence?", "content": "While studying artificial intelligence, I have often encountered the term \"agent\" (often autonomous, intelligent). For instance, in fields such as Reinforcement Learning, Multi-Agent Systems, Game Theory, Markov Decision Processes.\nIn an intuitive sense, it is clear to me what an agent is; I was wondering whether in AI it had a rigorous definition, perhaps expressed in mathematical language, and shared by the various AI-related fields.\nWhat is an agent in Artificial Intelligence?\n", "question_id": 12991, "answers": []}, "23838": {"link": "https://ai.stackexchange.com/questions/23838/why-would-the-lookup-table-of-a-table-driven-artificial-agent-need-to-store-da", "metadata": {"tags": ["math", "intelligent-agent", "norvig-russell"], "owner": {"reputation": 123, "user_id": 41312, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/E01IQ.png?s=256&g=1", "display_name": "senseiwu", "link": "https://ai.stackexchange.com/users/41312/senseiwu"}, "is_answered": true, "view_count": 947, "accepted_answer_id": 23839, "answer_count": 1, "score": 2, "last_activity_date": 1601472631, "creation_date": 1601455568, "last_edit_date": 1601472631, "question_id": 23838, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/23838/why-would-the-lookup-table-of-a-table-driven-artificial-agent-need-to-store-da", "title": "Why would the lookup table (of a table-driven artificial agent) need to store data at pixel precision?", "body": "<p>While reading the book <strong>AI A modern approach, 4th ed</strong>, I came across the section of &quot;Agent program&quot; with following text:</p>\n<blockquote>\n<p>It is instructive to consider why the table-driven approach to agent\nconstruction is doomed to failure. Let <span class=\"math-container\">$P$</span> be the set of possible\npercepts and let <span class=\"math-container\">$T$</span> be the lifetime of the agent (the total number of\npercepts it will receive).</p>\n<p>The lookup table will contain <span class=\"math-container\">$\\sum_{i=1}^T |P|^T$</span> entries.</p>\n<p>Consider the automated taxi: the visual input from a single camera\n(eight cameras is typical) comes in at the rate of roughly 70 mb per\nsec. (30 frames per sec, 1080 X 720 pixels, with 24 bits of color\ninformation).</p>\n<p>This gives a lookup table with over <span class=\"math-container\">$10^{600,000,000,000}$</span> for an\nhour's driving.</p>\n</blockquote>\n<p>Could someone please explain how the lookup table number is derived? (or what the author's point is which I am missing). If I were to multiply all of the numbers <span class=\"math-container\">$30 \u00d7 1080 \u00d7 720 \u00d7 24 \u00d7 8 \u00d7 3600$</span>, then I get <span class=\"math-container\">$1.6124314e+13$</span> which comes very close I think, but can't get what would be the reason to build a table (even though a theoretic one) in such a way - something which is <em>obviously intractable</em></p>\n<p><strong>edit:</strong></p>\n<p>My core question is this:</p>\n<p>Assuming <span class=\"math-container\">$10^{600,000,000,000}$</span> is derived from <span class=\"math-container\">$30 \u00d7 1080 \u00d7 720 \u00d7 24 \u00d7 8 \u00d7 3600$</span>, what is the purpose of storing data in the look up table at pixel precision? Wouldn't storing higher level of details be enough to solve these kind of problems (ie, autonomous driving)? Coming more from standard software database systems, I am missing that point. Thanks</p>\n"}, "title": "Why would the lookup table (of a table-driven artificial agent) need to store data at pixel precision?", "content": "While reading the book AI A modern approach, 4th ed, I came across the section of \"Agent program\" with following text:\n\nIt is instructive to consider why the table-driven approach to agent\nconstruction is doomed to failure. Let $P$ be the set of possible\npercepts and let $T$ be the lifetime of the agent (the total number of\npercepts it will receive).\nThe lookup table will contain $\\sum_{i=1}^T |P|^T$ entries.\nConsider the automated taxi: the visual input from a single camera\n(eight cameras is typical) comes in at the rate of roughly 70 mb per\nsec. (30 frames per sec, 1080 X 720 pixels, with 24 bits of color\ninformation).\nThis gives a lookup table with over $10^{600,000,000,000}$ for an\nhour's driving.\n\nCould someone please explain how the lookup table number is derived? (or what the author's point is which I am missing). If I were to multiply all of the numbers $30 \u00d7 1080 \u00d7 720 \u00d7 24 \u00d7 8 \u00d7 3600$, then I get $1.6124314e+13$ which comes very close I think, but can't get what would be the reason to build a table (even though a theoretic one) in such a way - something which is obviously intractable\nedit:\nMy core question is this:\nAssuming $10^{600,000,000,000}$ is derived from $30 \u00d7 1080 \u00d7 720 \u00d7 24 \u00d7 8 \u00d7 3600$, what is the purpose of storing data in the look up table at pixel precision? Wouldn't storing higher level of details be enough to solve these kind of problems (ie, autonomous driving)? Coming more from standard software database systems, I am missing that point. Thanks\n", "question_id": 23838, "answers": []}, "26944": {"link": "https://ai.stackexchange.com/questions/26944/do-text-compression-tests-qualify-winrar-or-7zip-as-intelligent", "metadata": {"tags": ["intelligent-agent", "intelligence-testing", "data-compression"], "owner": {"reputation": 265, "user_id": 45586, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/n8MIV.jpg?s=256&g=1", "display_name": "Aether", "link": "https://ai.stackexchange.com/users/45586/aether"}, "is_answered": true, "view_count": 123, "answer_count": 2, "score": 2, "last_activity_date": 1670951557, "creation_date": 1616412845, "last_edit_date": 1616491645, "question_id": 26944, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/26944/do-text-compression-tests-qualify-winrar-or-7zip-as-intelligent", "title": "Do text compression tests qualify winRar or 7zip as intelligent?", "body": "<p>I read this paper <a href=\"https://www.aaai.org/Papers/AAAI/1999/AAAI99-177.pdf\" rel=\"nofollow noreferrer\">Text Compression as a Test for Artificial Intelligence, Mahoney, 1999</a>.</p>\n<p><strong>So far I understood the following:</strong>\nText Compression tests can be used as an alternative to Turing Tests for intelligence.\nThe <em>Bits per character</em> score obtained from compression of a standard benchmark corpus, can be used as a quantitative measure for intelligence</p>\n<p><strong>My questions:</strong></p>\n<ol>\n<li>Is my understanding of the topic correct?</li>\n<li>Does this mean that applications like 7zip/WinRar are intelligent?</li>\n<li>How are the ways a human compresses information (as in form of summary) and ways a computer compresses (using Huffman coding or something) are compatible? How can we compare that?</li>\n</ol>\n"}, "title": "Do text compression tests qualify winRar or 7zip as intelligent?", "content": "I read this paper Text Compression as a Test for Artificial Intelligence, Mahoney, 1999.\nSo far I understood the following:\nText Compression tests can be used as an alternative to Turing Tests for intelligence.\nThe Bits per character score obtained from compression of a standard benchmark corpus, can be used as a quantitative measure for intelligence\nMy questions:\n\nIs my understanding of the topic correct?\nDoes this mean that applications like 7zip/WinRar are intelligent?\nHow are the ways a human compresses information (as in form of summary) and ways a computer compresses (using Huffman coding or something) are compatible? How can we compare that?\n\n", "question_id": 26944, "answers": []}, "6407": {"link": "https://ai.stackexchange.com/questions/6407/can-agents-be-implemented-with-ml-algorithms-other-than-neural-networks", "metadata": {"tags": ["machine-learning", "reinforcement-learning", "python", "intelligent-agent"], "owner": {"reputation": 61, "user_id": 15631, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b81d068f65e515fabf30d162ba219699?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Dayz", "link": "https://ai.stackexchange.com/users/15631/dayz"}, "is_answered": true, "view_count": 349, "accepted_answer_id": 6410, "answer_count": 1, "score": 2, "last_activity_date": 1640683375, "creation_date": 1526307971, "last_edit_date": 1640683375, "question_id": 6407, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/6407/can-agents-be-implemented-with-ml-algorithms-other-than-neural-networks", "title": "Can agents be implemented with ML algorithms other than neural networks?", "body": "<p>Can agents be implemented with machine learning algorithms/models other than neural networks?</p>\n<p>If so, how do I train an agent with some predefined rules? Can we use python programming for representing those rules?</p>\n"}, "title": "Can agents be implemented with ML algorithms other than neural networks?", "content": "Can agents be implemented with machine learning algorithms/models other than neural networks?\nIf so, how do I train an agent with some predefined rules? Can we use python programming for representing those rules?\n", "question_id": 6407, "answers": []}, "18791": {"link": "https://ai.stackexchange.com/questions/18791/agents-meeting-in-a-directed-connected-graph", "metadata": {"tags": ["search", "intelligent-agent", "heuristics", "multi-agent-systems"], "owner": {"reputation": 161, "user_id": 34483, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/db40be267bfabbc6f14a2545e12cd68f?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Karol", "link": "https://ai.stackexchange.com/users/34483/karol"}, "is_answered": true, "view_count": 103, "answer_count": 1, "score": 2, "last_activity_date": 1662988025, "creation_date": 1585132914, "last_edit_date": 1629280232, "question_id": 18791, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/18791/agents-meeting-in-a-directed-connected-graph", "title": "Agents meeting in a directed connected graph", "body": "<p>We have a directed connected graph, where nodes are places one can go to and edges are the &quot;roads&quot; between places. We have K agents whose goal is to meet in one node. Agents start in different nodes. Note that more than one agent can be at one node at the same time and that all agents move by one node at every turn(they move synchronously).</p>\n<p>We have to variants of this task:</p>\n<ol>\n<li><p>in each turn every agent must move</p>\n</li>\n<li><p>an agent may pass on moving.</p>\n</li>\n</ol>\n<p>For a chosen variant, I have to find an algorithm to complete this task, but it cannot be the state-space search algorithm.</p>\n<p>I've been sitting on this for a while, but I cannot think of anything.</p>\n<p>I've been thinking if agents could know each other positions in order to choose where to go, but it's a state-space search. I also thought that if agents meet, they could continue together. But I'm looking for an alternative to state-space search algorithms.</p>\n"}, "title": "Agents meeting in a directed connected graph", "content": "We have a directed connected graph, where nodes are places one can go to and edges are the \"roads\" between places. We have K agents whose goal is to meet in one node. Agents start in different nodes. Note that more than one agent can be at one node at the same time and that all agents move by one node at every turn(they move synchronously).\nWe have to variants of this task:\n\nin each turn every agent must move\n\nan agent may pass on moving.\n\n\nFor a chosen variant, I have to find an algorithm to complete this task, but it cannot be the state-space search algorithm.\nI've been sitting on this for a while, but I cannot think of anything.\nI've been thinking if agents could know each other positions in order to choose where to go, but it's a state-space search. I also thought that if agents meet, they could continue together. But I'm looking for an alternative to state-space search algorithms.\n", "question_id": 18791, "answers": []}, "8048": {"link": "https://ai.stackexchange.com/questions/8048/seif-motion-update-algorithm-doubt", "metadata": {"tags": ["research", "intelligent-agent", "autonomous-vehicles", "probability", "robotics"], "owner": {"reputation": 125, "user_id": 18384, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/x0wQp.png?s=256&g=1", "display_name": "Encipher", "link": "https://ai.stackexchange.com/users/18384/encipher"}, "is_answered": true, "view_count": 64, "accepted_answer_id": 8062, "answer_count": 1, "score": 2, "last_activity_date": 1537445965, "creation_date": 1537383705, "question_id": 8048, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/8048/seif-motion-update-algorithm-doubt", "title": "SEIF motion update algorithm doubt", "body": "<p>I want to implement Sparse Extended information slam. There is four step to implement it. The algorithm is available in <a href=\"https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf\" rel=\"nofollow noreferrer\">Probabilistic Robotics Book</a> at page 310, Table 12.3.</p>\n\n<p><a href=\"https://i.stack.imgur.com/1OA52.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/1OA52.png\" alt=\"enter image description here\"></a></p>\n\n<p>In this algorithm line no:13 is not very clear to me. I have 15 landmarks. So $\\mu_t$ will be a vector of (48*1) dimension where (3*1) for pose. Now $H_t^i$ is a matrix whose columns are dynamic as per the algorithm it is (3j-3) and 3j. J is the values of landmarks 1 to 15. Now how could I multiply a dynamic quantity with a  static one. There must be a error that matrix dimension mismatch when implement in matlab.</p>\n\n<p>Please help me to understand the algorithm better. </p>\n"}, "title": "SEIF motion update algorithm doubt", "content": "I want to implement Sparse Extended information slam. There is four step to implement it. The algorithm is available in Probabilistic Robotics Book at page 310, Table 12.3.\n\nIn this algorithm line no:13 is not very clear to me. I have 15 landmarks. So $\\mu_t$ will be a vector of (48*1) dimension where (3*1) for pose. Now $H_t^i$ is a matrix whose columns are dynamic as per the algorithm it is (3j-3) and 3j. J is the values of landmarks 1 to 15. Now how could I multiply a dynamic quantity with a  static one. There must be a error that matrix dimension mismatch when implement in matlab.\nPlease help me to understand the algorithm better. \n", "question_id": 8048, "answers": []}, "15695": {"link": "https://ai.stackexchange.com/questions/15695/what-are-the-examples-of-agents-that-is-represent-these-characteristics", "metadata": {"tags": ["machine-learning", "intelligent-agent", "multi-agent-systems"], "owner": {"reputation": 21, "user_id": 30118, "user_type": "unregistered", "profile_image": "https://www.gravatar.com/avatar/58fa8a6fc935f3453f27a406ef9650c1?s=256&d=identicon&r=PG", "display_name": "user30118", "link": "https://ai.stackexchange.com/users/30118/user30118"}, "is_answered": false, "view_count": 108, "answer_count": 0, "score": 2, "last_activity_date": 1570067430, "creation_date": 1570037443, "last_edit_date": 1570067430, "question_id": 15695, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/15695/what-are-the-examples-of-agents-that-is-represent-these-characteristics", "title": "What are the examples of agents that is represent these characteristics?", "body": "<p>I'm looking for examples of AI systems or agents that best represent these five characteristics (one example for each characteristics):</p>\n\n<ul>\n<li>Reactivity </li>\n<li>Proactivity</li>\n<li>Adaptability</li>\n<li>Sociability </li>\n<li>Autonomy</li>\n</ul>\n\n<p>It would be better if its machine learning-based application.</p>\n"}, "title": "What are the examples of agents that is represent these characteristics?", "content": "I'm looking for examples of AI systems or agents that best represent these five characteristics (one example for each characteristics):\n\nReactivity \nProactivity\nAdaptability\nSociability \nAutonomy\n\nIt would be better if its machine learning-based application.\n", "question_id": 15695, "answers": []}, "6504": {"link": "https://ai.stackexchange.com/questions/6504/how-to-teach-a-model-based-reflex-agent-for-doing-some-task-using-machine-learni", "metadata": {"tags": ["machine-learning", "intelligent-agent", "architecture"], "owner": {"reputation": 61, "user_id": 15631, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b81d068f65e515fabf30d162ba219699?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Dayz", "link": "https://ai.stackexchange.com/users/15631/dayz"}, "is_answered": true, "view_count": 677, "accepted_answer_id": 6630, "answer_count": 2, "score": 1, "last_activity_date": 1591140286, "creation_date": 1527177460, "last_edit_date": 1591140286, "question_id": 6504, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/6504/how-to-teach-a-model-based-reflex-agent-for-doing-some-task-using-machine-learni", "title": "How to teach a model-based reflex agent for doing some task using machine learning methods?", "body": "<p>I would like to know how to teach an agent for performing prediction of the severity of disease and also for alerting patients using machine learning methods.</p>\n\n<p>I found the <a href=\"https://ai.stackexchange.com/q/3243/2444\">model-based reflex agent</a> can be used in medical diagnosis in some literature. </p>\n\n<p>May I know which architecture will be good, to make such an agent?</p>\n"}, "title": "How to teach a model-based reflex agent for doing some task using machine learning methods?", "content": "I would like to know how to teach an agent for performing prediction of the severity of disease and also for alerting patients using machine learning methods.\nI found the model-based reflex agent can be used in medical diagnosis in some literature. \nMay I know which architecture will be good, to make such an agent?\n", "question_id": 6504, "answers": []}, "4037": {"link": "https://ai.stackexchange.com/questions/4037/how-can-an-ai-agent-tackle-through-existential-crisis", "metadata": {"tags": ["philosophy", "agi", "intelligent-agent", "superintelligence"], "owner": {"reputation": 21, "user_id": 2534, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-0aKmnCKBzMk/AAAAAAAAAAI/AAAAAAAAABE/jJeR09zwUZQ/photo.jpg?sz=256", "display_name": "fly samc", "link": "https://ai.stackexchange.com/users/2534/fly-samc"}, "is_answered": true, "view_count": 553, "accepted_answer_id": 4040, "answer_count": 2, "score": 1, "last_activity_date": 1506837449, "creation_date": 1505492625, "last_edit_date": 1505519284, "question_id": 4037, "content_license": "CC BY-SA 3.0", "link": "https://ai.stackexchange.com/questions/4037/how-can-an-ai-agent-tackle-through-existential-crisis", "title": "How can an AI agent tackle through existential crisis?", "body": "<p>Suppose a thinking AI agent exists in the future with far more computational power than the human brain. </p>\n\n<p>Also assume that they are completely free from any human interference. <em>(The agents do not interact with humans.)</em>  Since they are not inherently biased to survive as in the case of humans and they do not have any moral values, what are the possibilities that can arise when it get into existential crisis?</p>\n\n<p>Is there any literature that discuss the above issue? </p>\n\n<p>Alternately, is this question flawed in some fundamental way?</p>\n"}, "title": "How can an AI agent tackle through existential crisis?", "content": "Suppose a thinking AI agent exists in the future with far more computational power than the human brain. \nAlso assume that they are completely free from any human interference. (The agents do not interact with humans.)  Since they are not inherently biased to survive as in the case of humans and they do not have any moral values, what are the possibilities that can arise when it get into existential crisis?\nIs there any literature that discuss the above issue? \nAlternately, is this question flawed in some fundamental way?\n", "question_id": 4037, "answers": []}, "8920": {"link": "https://ai.stackexchange.com/questions/8920/are-artificial-intelligence-learnings-or-trainings-transferable-from-one-agent-t", "metadata": {"tags": ["machine-learning", "reinforcement-learning", "training", "intelligent-agent"], "owner": {"reputation": 115, "user_id": 17892, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/hmR1Y.jpg?s=256&g=1", "display_name": "Elisha Senoo", "link": "https://ai.stackexchange.com/users/17892/elisha-senoo"}, "is_answered": true, "view_count": 66, "answer_count": 1, "score": 1, "last_activity_date": 1542099895, "creation_date": 1541882417, "question_id": 8920, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/8920/are-artificial-intelligence-learnings-or-trainings-transferable-from-one-agent-t", "title": "Are artificial intelligence learnings or trainings transferable from one agent to the other?", "body": "<p>One disadvantage or weakness of Artificial Intelligence today the slow nature of learning or training success. For instance, an AI agent might require a 100,000 samples or more to reach an appreciable level of performance with a specific task. But this is unlike humans who are able to learn very quickly with a minimum number of samples. Humans are also able to teach one another, or in other words, transfer knowledge acquired.</p>\n\n<p>My question is this: are Artificial Intelligence learnings or trainings transferable from one agent to the other? If yes, how? If no, why?</p>\n"}, "title": "Are artificial intelligence learnings or trainings transferable from one agent to the other?", "content": "One disadvantage or weakness of Artificial Intelligence today the slow nature of learning or training success. For instance, an AI agent might require a 100,000 samples or more to reach an appreciable level of performance with a specific task. But this is unlike humans who are able to learn very quickly with a minimum number of samples. Humans are also able to teach one another, or in other words, transfer knowledge acquired.\nMy question is this: are Artificial Intelligence learnings or trainings transferable from one agent to the other? If yes, how? If no, why?\n", "question_id": 8920, "answers": []}, "20663": {"link": "https://ai.stackexchange.com/questions/20663/what-types-of-ai-agents-are-djikstras-algorithm-and-prims-minimum-spanning-tre", "metadata": {"tags": ["intelligent-agent", "goal-based-agents", "utility-based-agents", "dijkstras-algorithm", "prims-algorithm"], "owner": {"reputation": 165, "user_id": 34306, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/FhoBy.gif?s=256&g=1", "display_name": "Muhammad Maqsoodur Rehman", "link": "https://ai.stackexchange.com/users/34306/muhammad-maqsoodur-rehman"}, "is_answered": true, "view_count": 218, "accepted_answer_id": 20679, "answer_count": 1, "score": 1, "last_activity_date": 1605790280, "creation_date": 1587931715, "last_edit_date": 1605790280, "question_id": 20663, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/20663/what-types-of-ai-agents-are-djikstras-algorithm-and-prims-minimum-spanning-tre", "title": "What types of AI agents are Djikstra&#39;s algorithm and Prim&#39;s Minimum Spanning Tree algorithm?", "body": "<p>From the perspective of the type of AI Agents, I would like to discuss Prim's Minimum Spanning Tree algorithm and Dijkstra's Algorithm.</p>\n<p>Both are model-based agents and both are &quot;greedy algorithms&quot;.</p>\n<p>Both have their memory to store the history of vertices and their path distance. Prim's is <strong>more greedy</strong> than Dijkstra's algorithm whereas Dijkstra's algorithm is <strong>more efficient</strong> than Prim's.</p>\n<p>Can we say that Dijkstra's algorithm is a utility-based agent, whereas Prim's is a goal-based agent, with the justification that Prim's is <strong>more goal-oriented</strong> as compared to finding the optimum (shortest) path?</p>\n"}, "title": "What types of AI agents are Djikstra&#39;s algorithm and Prim&#39;s Minimum Spanning Tree algorithm?", "content": "From the perspective of the type of AI Agents, I would like to discuss Prim's Minimum Spanning Tree algorithm and Dijkstra's Algorithm.\nBoth are model-based agents and both are \"greedy algorithms\".\nBoth have their memory to store the history of vertices and their path distance. Prim's is more greedy than Dijkstra's algorithm whereas Dijkstra's algorithm is more efficient than Prim's.\nCan we say that Dijkstra's algorithm is a utility-based agent, whereas Prim's is a goal-based agent, with the justification that Prim's is more goal-oriented as compared to finding the optimum (shortest) path?\n", "question_id": 20663, "answers": []}, "36160": {"link": "https://ai.stackexchange.com/questions/36160/has-there-been-an-instance-of-an-ai-agent-breaking-out-of-its-sandbox", "metadata": {"tags": ["intelligent-agent", "ai-safety", "ai-box", "reward-hacking"], "owner": {"reputation": 121, "user_id": 42922, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/304c70070e68a9c799518d2cdee4d138?s=256&d=identicon&r=PG", "display_name": "2080", "link": "https://ai.stackexchange.com/users/42922/2080"}, "is_answered": false, "view_count": 77, "answer_count": 0, "score": 1, "last_activity_date": 1657198687, "creation_date": 1656783246, "last_edit_date": 1657198687, "question_id": 36160, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/36160/has-there-been-an-instance-of-an-ai-agent-breaking-out-of-its-sandbox", "title": "Has there been an instance of an AI agent breaking out of its sandbox?", "body": "<p>There have been instances of agents using edge cases like bugs in <a href=\"https://www.youtube.com/watch?v=Lu56xVlZ40M\" rel=\"nofollow noreferrer\">physics engines</a>, <a href=\"https://www.gwern.net/Tanks#alternative-examples\" rel=\"nofollow noreferrer\">repetitive behavior in games</a> or word repetition in text prediction to cheat their reward function. However, these agents are arguably still contained, as while they explore the extremes of the state space of the simulation they don't expand their action space beyond what is possible in the simulation.</p>\n<p>The <a href=\"https://www.youtube.com/watch?v=p5T81yHkHtI\" rel=\"nofollow noreferrer\">Pok\u00e9mon Yellow Total Control Hack</a> shows that in some systems, it is possible to gain full control of a computer by exploiting bugs in the hardware or software (here: memory corruption), enabling the agent to even <em>completely</em> reprogram the system 'from within', just using the normal inputs.</p>\n<p>Do you know of similar extreme examples where an AI agent went far beyond what was intended with the simulation environment and expanded its action space?</p>\n"}, "title": "Has there been an instance of an AI agent breaking out of its sandbox?", "content": "There have been instances of agents using edge cases like bugs in physics engines, repetitive behavior in games or word repetition in text prediction to cheat their reward function. However, these agents are arguably still contained, as while they explore the extremes of the state space of the simulation they don't expand their action space beyond what is possible in the simulation.\nThe Pok\u00e9mon Yellow Total Control Hack shows that in some systems, it is possible to gain full control of a computer by exploiting bugs in the hardware or software (here: memory corruption), enabling the agent to even completely reprogram the system 'from within', just using the normal inputs.\nDo you know of similar extreme examples where an AI agent went far beyond what was intended with the simulation environment and expanded its action space?\n", "question_id": 36160, "answers": []}, "30338": {"link": "https://ai.stackexchange.com/questions/30338/are-goal-reaching-and-optimizing-the-utility-function-special-cases-of-performan", "metadata": {"tags": ["intelligent-agent", "performance", "goal-based-agents", "utility-based-agents", "rational-agents"], "owner": {"reputation": 205, "user_id": 48908, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/666c27316aa2450cb62437d5a974a38c?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Emad", "link": "https://ai.stackexchange.com/users/48908/emad"}, "is_answered": false, "view_count": 97, "answer_count": 0, "score": 1, "last_activity_date": 1630228675, "creation_date": 1629793295, "last_edit_date": 1630228675, "question_id": 30338, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/30338/are-goal-reaching-and-optimizing-the-utility-function-special-cases-of-performan", "title": "Are goal-reaching and optimizing the utility function special cases of performance measure?", "body": "<p>In AIMA, <em>performance measure</em> is defined as something evaluating the behavior of the agent in an environment.</p>\n<p><em>Rational agents</em> are defined as agents acting so as to maximize the expected value of the performance measure, given the percept sequence they have seen so far.</p>\n<p><em>Goal-based agents</em> are those acting to achieve their goals. <em>Utility-based agents</em> are those trying to maximize their own expected &quot;happiness&quot;.</p>\n<p>Now, can we say these two design approaches induce performance measures?</p>\n<p>What I suggest is that in goal-based agent design we want to find a point satisfying some conditions, so it's an optimization problem with a zero objective function and either this function is the performance measure or performance measure is optimized if and only if we find a solution to this optimization problem with zero objective function. In the utility-based agent design, we have an objective function (as a performance measure) that we want to optimize, and the agent has its own utility function, which it wants to optimize, and this utility function is optimized if and only if our objective function is optimized.</p>\n"}, "title": "Are goal-reaching and optimizing the utility function special cases of performance measure?", "content": "In AIMA, performance measure is defined as something evaluating the behavior of the agent in an environment.\nRational agents are defined as agents acting so as to maximize the expected value of the performance measure, given the percept sequence they have seen so far.\nGoal-based agents are those acting to achieve their goals. Utility-based agents are those trying to maximize their own expected \"happiness\".\nNow, can we say these two design approaches induce performance measures?\nWhat I suggest is that in goal-based agent design we want to find a point satisfying some conditions, so it's an optimization problem with a zero objective function and either this function is the performance measure or performance measure is optimized if and only if we find a solution to this optimization problem with zero objective function. In the utility-based agent design, we have an objective function (as a performance measure) that we want to optimize, and the agent has its own utility function, which it wants to optimize, and this utility function is optimized if and only if our objective function is optimized.\n", "question_id": 30338, "answers": []}, "27233": {"link": "https://ai.stackexchange.com/questions/27233/what-is-the-difference-between-a-performance-standard-and-performance-measure", "metadata": {"tags": ["terminology", "intelligent-agent", "norvig-russell", "utility-based-agents", "learning-agents"], "owner": {"reputation": 21, "user_id": 46085, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-OPPR_oEeLDw/AAAAAAAAAAI/AAAAAAAADkk/RVsiZnuLjiU/photo.jpg?sz=256", "display_name": "Hamed", "link": "https://ai.stackexchange.com/users/46085/hamed"}, "is_answered": false, "view_count": 756, "answer_count": 1, "score": 1, "last_activity_date": 1683421360, "creation_date": 1617954792, "last_edit_date": 1639339872, "question_id": 27233, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/27233/what-is-the-difference-between-a-performance-standard-and-performance-measure", "title": "What is the difference between a performance standard and performance measure?", "body": "<p>I am reading <a href=\"http://aima.cs.berkeley.edu/index.html\" rel=\"nofollow noreferrer\">AI: A Modern Approach</a>. In the 2nd chapter when introducing different agent types, i.e., reflex, utility-based, goal-based, and learning agents, I understood that all types of agents, except learning agents, receive feedback and choose actions using the <strong>performance measure</strong>.</p>\n<p>But they do so in different ways. Model-based reflex agents possess an internal state (like a memory), while goal-based agents predict the outcome of actions and choose the one serving the goal. Lastly, utility-based functions measure the '<em>happiness</em>' of each state using the utility function, which is again an internalization of the <strong>performance measure</strong>, hence all have similar nature overall.</p>\n<p>The learning agents, however, can be wrapped around the entire structure of previous agents. The entire agent's architecture is now called a <strong>performance element</strong>, and the learning agent has an additional <strong>learning element</strong>, which modifies each component of the agent, so as to bring the components into closer agreement with the available feedback information. But the feedback information in learning agents does not from the <strong>performance measure</strong> embedded in the agent's structure, but from a fixed external <strong>performance standard</strong>, which is part of the <em><em>critic</em> element</em>*.</p>\n<p>For the purpose of illustration, the structure of a utility-based agent and that of a learning agent are presented in the figure:</p>\n<p><a href=\"https://i.stack.imgur.com/YJwJ5.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/YJwJ5.png\" alt=\"Left: Utility-based agent, Right: Learning agent\" /></a></p>\n<p>What boggles my mind is figuring out the actual difference and interaction between <strong>performance standard</strong> and <strong>performance measure</strong>, which is perhaps related to those between learning agents and other ones. Here are my thoughts thus far:</p>\n<ol>\n<li><p>Other agents aim for maximizing the <strong>performance measure</strong>, causing them to do perfect actions. On the other hand, learning agents have the freedom of doing sub-optimal actions, which allow them to discover better actions on the long run using the <strong>performance standard</strong>.</p>\n</li>\n<li><p>Through the performance standard's feedback (which comes from the <em>critic</em> as shown in the figure), the learning agent can also learn a utility function or reflex component.</p>\n</li>\n</ol>\n<p>For providing examples, the book states that giving tip to an automated taxi is considered a performance standard. And also</p>\n<blockquote>\n<p>hard-wired performance standards such as pain and hunger in animals can be understood in this way.</p>\n</blockquote>\n<p>But I am still not sure about the discrepancy and interaction between the <strong>performance measure</strong> and <strong>performance standard</strong>. For instance, in the automated taxi, when confronting a road junction, the utility-based agent chooses a path that maximizes its utility function. The learning agent, however, must check different roads and after testing them, it receives feedback from outside so that eventually it would detect the user's preference.</p>\n<p>But what if we wrap a learning agent around a utility-based agent in such a condition? Which has more effect, the utility function from inside, or the performance standard from outside (critic)? If they happen to contradict each other, which one would have the prevalent effect?</p>\n"}, "title": "What is the difference between a performance standard and performance measure?", "content": "I am reading AI: A Modern Approach. In the 2nd chapter when introducing different agent types, i.e., reflex, utility-based, goal-based, and learning agents, I understood that all types of agents, except learning agents, receive feedback and choose actions using the performance measure.\nBut they do so in different ways. Model-based reflex agents possess an internal state (like a memory), while goal-based agents predict the outcome of actions and choose the one serving the goal. Lastly, utility-based functions measure the 'happiness' of each state using the utility function, which is again an internalization of the performance measure, hence all have similar nature overall.\nThe learning agents, however, can be wrapped around the entire structure of previous agents. The entire agent's architecture is now called a performance element, and the learning agent has an additional learning element, which modifies each component of the agent, so as to bring the components into closer agreement with the available feedback information. But the feedback information in learning agents does not from the performance measure embedded in the agent's structure, but from a fixed external performance standard, which is part of the critic element*.\nFor the purpose of illustration, the structure of a utility-based agent and that of a learning agent are presented in the figure:\n\nWhat boggles my mind is figuring out the actual difference and interaction between performance standard and performance measure, which is perhaps related to those between learning agents and other ones. Here are my thoughts thus far:\n\nOther agents aim for maximizing the performance measure, causing them to do perfect actions. On the other hand, learning agents have the freedom of doing sub-optimal actions, which allow them to discover better actions on the long run using the performance standard.\n\nThrough the performance standard's feedback (which comes from the critic as shown in the figure), the learning agent can also learn a utility function or reflex component.\n\n\nFor providing examples, the book states that giving tip to an automated taxi is considered a performance standard. And also\n\nhard-wired performance standards such as pain and hunger in animals can be understood in this way.\n\nBut I am still not sure about the discrepancy and interaction between the performance measure and performance standard. For instance, in the automated taxi, when confronting a road junction, the utility-based agent chooses a path that maximizes its utility function. The learning agent, however, must check different roads and after testing them, it receives feedback from outside so that eventually it would detect the user's preference.\nBut what if we wrap a learning agent around a utility-based agent in such a condition? Which has more effect, the utility function from inside, or the performance standard from outside (critic)? If they happen to contradict each other, which one would have the prevalent effect?\n", "question_id": 27233, "answers": []}, "20889": {"link": "https://ai.stackexchange.com/questions/20889/how-do-you-know-if-an-agent-has-learnt-its-environment-in-reinforcement-learning", "metadata": {"tags": ["reinforcement-learning", "rewards", "intelligent-agent", "learning-algorithms", "environment"], "owner": {"reputation": 249, "user_id": 32237, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/852236fede9b795078239a8a205d3dfc?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Cristian M", "link": "https://ai.stackexchange.com/users/32237/cristian-m"}, "is_answered": true, "view_count": 382, "accepted_answer_id": 20892, "answer_count": 1, "score": 1, "last_activity_date": 1588594196, "creation_date": 1588581290, "question_id": 20889, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/20889/how-do-you-know-if-an-agent-has-learnt-its-environment-in-reinforcement-learning", "title": "How do you know if an agent has learnt its environment in reinforcement learning?", "body": "<p>I'm new to reinforcement learning and trying to understand it. </p>\n\n<p>If you train an agent using a reinforcement learning algorithm (discrete or continuous) on an environment (real or simulated), then how do you know if the agent has learnt its environment? Should it reach its goal on every run (episode)? (Any literature references are also welcome)</p>\n\n<p>Is this related to the reward threshold defined in the environment?</p>\n\n<p>What happens if you continue training after the agent has learnt the environment? Will it perform by reaching its goal every time or will there be failed episodes?</p>\n"}, "title": "How do you know if an agent has learnt its environment in reinforcement learning?", "content": "I'm new to reinforcement learning and trying to understand it. \nIf you train an agent using a reinforcement learning algorithm (discrete or continuous) on an environment (real or simulated), then how do you know if the agent has learnt its environment? Should it reach its goal on every run (episode)? (Any literature references are also welcome)\nIs this related to the reward threshold defined in the environment?\nWhat happens if you continue training after the agent has learnt the environment? Will it perform by reaching its goal every time or will there be failed episodes?\n", "question_id": 20889, "answers": []}, "18088": {"link": "https://ai.stackexchange.com/questions/18088/what-happens-if-an-agent-has-two-contrasting-utility-functions", "metadata": {"tags": ["intelligent-agent", "utility"], "owner": {"reputation": 131, "user_id": 30353, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/0JFfo.jpg?s=256&g=1", "display_name": "Yamar69", "link": "https://ai.stackexchange.com/users/30353/yamar69"}, "is_answered": false, "view_count": 36, "closed_date": 1583526877, "answer_count": 0, "score": 1, "last_activity_date": 1582031904, "creation_date": 1582021152, "last_edit_date": 1582031904, "question_id": 18088, "link": "https://ai.stackexchange.com/questions/18088/what-happens-if-an-agent-has-two-contrasting-utility-functions", "closed_reason": "Duplicate", "title": "What happens if an agent has two contrasting utility functions?", "body": "<p>What would happen if an agent were programmed with two <em>utility functions</em> which are one the opposite of the other? Would one prevail or will they cancel each other out? Are there studies in this direction?</p>\n"}, "title": "What happens if an agent has two contrasting utility functions?", "content": "What would happen if an agent were programmed with two utility functions which are one the opposite of the other? Would one prevail or will they cancel each other out? Are there studies in this direction?\n", "question_id": 18088, "answers": []}, "37888": {"link": "https://ai.stackexchange.com/questions/37888/are-there-better-loss-functions-than-mse-for-maze-solver-using-deep-learning", "metadata": {"tags": ["reinforcement-learning", "deep-learning", "q-learning", "dqn", "intelligent-agent"], "owner": {"reputation": 1, "user_id": 63492, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/8329479e465102bf3bd2777793453a17?s=256&d=identicon&r=PG", "display_name": "Lim", "link": "https://ai.stackexchange.com/users/63492/lim"}, "is_answered": true, "view_count": 110, "answer_count": 1, "score": 0, "last_activity_date": 1668619418, "creation_date": 1668439861, "last_edit_date": 1668619418, "question_id": 37888, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/37888/are-there-better-loss-functions-than-mse-for-maze-solver-using-deep-learning", "title": "Are there better loss functions than MSE for maze solver using deep learning?", "body": "<p>I am a newbie in reinforcement learning, and I was doing a project on solving an agent maze solver using deep Q Learning.  Currently, I am using the MSE loss function, but the agent is very slow or not even reaching the target.  Is there any better loss function to improve the agent performance?</p>\n"}, "title": "Are there better loss functions than MSE for maze solver using deep learning?", "content": "I am a newbie in reinforcement learning, and I was doing a project on solving an agent maze solver using deep Q Learning.  Currently, I am using the MSE loss function, but the agent is very slow or not even reaching the target.  Is there any better loss function to improve the agent performance?\n", "question_id": 37888, "answers": []}, "36555": {"link": "https://ai.stackexchange.com/questions/36555/what-is-the-need-for-agency-in-ai", "metadata": {"tags": ["reinforcement-learning", "agi", "intelligent-agent"], "owner": {"reputation": 309, "user_id": 55107, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/183d6e8b6fcb762d6cbc1c15dd7423da?s=256&d=identicon&r=PG", "display_name": "thesofakillers", "link": "https://ai.stackexchange.com/users/55107/thesofakillers"}, "is_answered": true, "view_count": 104, "closed_date": 1659326489, "answer_count": 1, "score": 0, "last_activity_date": 1659563877, "creation_date": 1659270289, "last_edit_date": 1659563877, "question_id": 36555, "link": "https://ai.stackexchange.com/questions/36555/what-is-the-need-for-agency-in-ai", "closed_reason": "Needs more focus", "title": "What is the need for _agency_ in AI?", "body": "<p>Why seek to develop artificially intelligent <em>agents</em>? Are there certain advantages and/or needs provided by such supposed intelligent agents that are preferred to simply using intelligent <em>tools</em> that are devoid of agency (e.g. language models)? If so, what are these needs and advantages that intelligent agents could serve?</p>\n<p>Basically, why would you want agency in an AI Model in the first place?</p>\n<p>Here, I define <strong>agency</strong> as the ability to <em>autonomously</em> perceive and interact with a given environment. Anything capable of agency is then an <strong>agent</strong>. Furthermore, I define</p>\n<ul>\n<li><strong><em>autonomous</em> perception</strong>: the ability to perceive a given environment without the need of an external agent</li>\n<li><strong>interaction</strong>: the ability to change the state of the environment</li>\n</ul>\n"}, "title": "What is the need for _agency_ in AI?", "content": "Why seek to develop artificially intelligent agents? Are there certain advantages and/or needs provided by such supposed intelligent agents that are preferred to simply using intelligent tools that are devoid of agency (e.g. language models)? If so, what are these needs and advantages that intelligent agents could serve?\nBasically, why would you want agency in an AI Model in the first place?\nHere, I define agency as the ability to autonomously perceive and interact with a given environment. Anything capable of agency is then an agent. Furthermore, I define\n\nautonomous perception: the ability to perceive a given environment without the need of an external agent\ninteraction: the ability to change the state of the environment\n\n", "question_id": 36555, "answers": []}, "32098": {"link": "https://ai.stackexchange.com/questions/32098/why-is-this-vacuum-cleaner-agent-rational", "metadata": {"tags": ["intelligent-agent", "norvig-russell", "rationality", "rational-agents"], "owner": {"reputation": 123, "user_id": 50418, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/JeV82.jpg?s=256&g=1", "display_name": "user153245", "link": "https://ai.stackexchange.com/users/50418/user153245"}, "is_answered": true, "view_count": 1193, "answer_count": 1, "score": 0, "last_activity_date": 1634646622, "creation_date": 1634576738, "last_edit_date": 1634646622, "question_id": 32098, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/32098/why-is-this-vacuum-cleaner-agent-rational", "title": "Why is this vacuum cleaner agent rational?", "body": "<p>This is the vacuum cleaner example of the book &quot;Artificial intelligence: A Modern Approach&quot; (4th edition).</p>\n<p><a href=\"https://i.stack.imgur.com/Q5l4t.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Q5l4t.png\" alt=\"enter image description here\" /></a></p>\n<p>Consider the simple vacuum-cleaner agent that cleans a square if it is dirty and moves to the other square if not; this is the agent function tabulated as follows.</p>\n<pre><code>Percept sequence                      Action\n[A,Clean]                             Right\n[A,Dirty]                             Suck\n[B,Clean]                             Left\n[B,Dirty]                             Suck\n[A,Clean], [A,Clean]                  Right\n[A,Clean], [A,Dirty]                  Suck\n.                                      .\n.                                      .\n[A,Clean], [A,Clean], [A,Clean]       Right\n[A,Clean], [A,Clean], [A,Dirty]       Suck\n.                                      .    \n.   \n                               .\n</code></pre>\n<p>The characteristics of environment and performance function are as follow:</p>\n<ul>\n<li><p>The performance measure awards one point for each clean square at each time step, over a &quot;lifetime&quot; of 1000 time steps.</p>\n</li>\n<li><p>The &quot;geography&quot; of the environment is known a priori (the above figure) but the dirt distribution and the initial location of the agent are not. Clean squares stay clean and sucking cleans the current square. The Right and Left actions move the agent one square except when this would take the agent outside the environment, in which case the agent remains where it is.</p>\n</li>\n<li><p>The only available actions are <em>Right</em>, <em>Left</em>, and <em>Suck</em>.</p>\n</li>\n<li><p>The agent correctly perceives its location and whether that location contains dirt.</p>\n</li>\n</ul>\n<p>In the book, it is stated that under these circumstances the agent is indeed rational. But I do not understand such percept sequence that consists of multiple <code>[A, clean]</code> percepts, e.g. <code>{[A, clean], [A, clean]}</code>. In my opinion, after first <code>[A, clean]</code>, the agent must be gone to the right square; So, the sequence <code>{[A, clean], [A, clean]}</code> will never be perceived.</p>\n<p>In other words, the second perception of <code>[A, clean]</code> is the consequence of acting <em>left</em> or <em>suck</em> action after perceiving the first <code>[A, clean]</code>. Therefore, we can conclude the agent is not rational.</p>\n<p>Please, help me to understand it.</p>\n"}, "title": "Why is this vacuum cleaner agent rational?", "content": "This is the vacuum cleaner example of the book \"Artificial intelligence: A Modern Approach\" (4th edition).\n\nConsider the simple vacuum-cleaner agent that cleans a square if it is dirty and moves to the other square if not; this is the agent function tabulated as follows.\nPercept sequence                      Action\n[A,Clean]                             Right\n[A,Dirty]                             Suck\n[B,Clean]                             Left\n[B,Dirty]                             Suck\n[A,Clean], [A,Clean]                  Right\n[A,Clean], [A,Dirty]                  Suck\n.                                      .\n.                                      .\n[A,Clean], [A,Clean], [A,Clean]       Right\n[A,Clean], [A,Clean], [A,Dirty]       Suck\n.                                      .    \n.   \n                               .\n\nThe characteristics of environment and performance function are as follow:\n\nThe performance measure awards one point for each clean square at each time step, over a \"lifetime\" of 1000 time steps.\n\nThe \"geography\" of the environment is known a priori (the above figure) but the dirt distribution and the initial location of the agent are not. Clean squares stay clean and sucking cleans the current square. The Right and Left actions move the agent one square except when this would take the agent outside the environment, in which case the agent remains where it is.\n\nThe only available actions are Right, Left, and Suck.\n\nThe agent correctly perceives its location and whether that location contains dirt.\n\n\nIn the book, it is stated that under these circumstances the agent is indeed rational. But I do not understand such percept sequence that consists of multiple [A, clean] percepts, e.g. {[A, clean], [A, clean]}. In my opinion, after first [A, clean], the agent must be gone to the right square; So, the sequence {[A, clean], [A, clean]} will never be perceived.\nIn other words, the second perception of [A, clean] is the consequence of acting left or suck action after perceiving the first [A, clean]. Therefore, we can conclude the agent is not rational.\nPlease, help me to understand it.\n", "question_id": 32098, "answers": []}, "23977": {"link": "https://ai.stackexchange.com/questions/23977/how-to-train-the-nn-of-simple-agents-given-a-reward-system", "metadata": {"tags": ["neural-networks", "reinforcement-learning", "training", "intelligent-agent", "multi-agent-systems"], "owner": {"reputation": 109, "user_id": 41382, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/793c4377313587d742fad6f069d92d89?s=256&d=identicon&r=PG", "display_name": "Nick", "link": "https://ai.stackexchange.com/users/41382/nick"}, "is_answered": true, "view_count": 181, "answer_count": 1, "score": 0, "last_activity_date": 1602224548, "creation_date": 1602182129, "question_id": 23977, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/23977/how-to-train-the-nn-of-simple-agents-given-a-reward-system", "title": "How to train the NN of simple agents given a reward system?", "body": "<p>I'm not an expert in AI or NN, I gathered most of the information I have from the internet, and I'm looking for advice and guidance.</p>\n<p>I'm trying to design a NN that is going to be used by all the agents of my simulation (each agent will have its own matrix of weights). This is what I plan to have:</p>\n<ul>\n<li>The NN will have 1 input layer and 1 output layer (no hidden layers).</li>\n<li>The number of inputs will always be greater than the number of outputs.</li>\n<li>The outputs represent the probability of an action being taken by the agent (the output node with the highest value will identify the action that will be taken). Which means there are as many output nodes are there are actions.</li>\n</ul>\n<p>When an agent takes an action it receives a reward: a number that represents how well the agent performed. This happens &quot;online&quot; that is, the agent is trained on the fly.</p>\n<p>What I would like to know if how to best train the NN: that is, how to update the weights of my matrix to maximise the rewards long term.</p>\n<p>From the research I made it seems this is close to the concept of Reinforcement Learning, but even if it was, it's not clear to me how to apply it to such a simple NN shape.</p>\n"}, "title": "How to train the NN of simple agents given a reward system?", "content": "I'm not an expert in AI or NN, I gathered most of the information I have from the internet, and I'm looking for advice and guidance.\nI'm trying to design a NN that is going to be used by all the agents of my simulation (each agent will have its own matrix of weights). This is what I plan to have:\n\nThe NN will have 1 input layer and 1 output layer (no hidden layers).\nThe number of inputs will always be greater than the number of outputs.\nThe outputs represent the probability of an action being taken by the agent (the output node with the highest value will identify the action that will be taken). Which means there are as many output nodes are there are actions.\n\nWhen an agent takes an action it receives a reward: a number that represents how well the agent performed. This happens \"online\" that is, the agent is trained on the fly.\nWhat I would like to know if how to best train the NN: that is, how to update the weights of my matrix to maximise the rewards long term.\nFrom the research I made it seems this is close to the concept of Reinforcement Learning, but even if it was, it's not clear to me how to apply it to such a simple NN shape.\n", "question_id": 23977, "answers": []}, "23098": {"link": "https://ai.stackexchange.com/questions/23098/need-some-reviews-in-peas-descriptions", "metadata": {"tags": ["intelligent-agent", "environment"], "owner": {"reputation": 1, "user_id": 40371, "user_type": "unregistered", "profile_image": "https://www.gravatar.com/avatar/8a6fbcbe62d1e26e1a71829440fddd09?s=256&d=identicon&r=PG", "display_name": "Fahad", "link": "https://ai.stackexchange.com/users/40371/fahad"}, "is_answered": true, "view_count": 1573, "answer_count": 1, "score": 0, "last_activity_date": 1629314594, "creation_date": 1597691411, "question_id": 23098, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/23098/need-some-reviews-in-peas-descriptions", "title": "Need some reviews in PEAS descriptions", "body": "<p><strong>Here is the Question:</strong></p>\n<p>Describe the PEAS descriptions for the following agents:</p>\n<p>a) A grocery store scanner that digitally scans a fruit or vegetable and\nidentifies it.</p>\n<p>b) A GPS system for an automobile. Assume that the destination has been\npreprogrammed and that there is no ongoing interaction with the driver.\nHowever, the agent might need to update the route if the driver misses a turn.</p>\n<p>c) A credit card fraud detection agent that monitors an individual\u2019s transactions\nand reports suspicious activity.</p>\n<p>d) A voice activated mobile-phone assistant</p>\n<p>For each of the agents described above, categorize it with respect to the six dimensions\nof task environments as described on pages 41-45 (Section 2.3.2 of AIMA). Be sure\nthat your choices accurately reflect the way you have specified your environment,\nespecially the sensors and actuators. Give a short justification for each property</p>\n<p>Here is what i thinks that the answers of above questions might be this. Can you guyz correct me if i answered wrong at any point.\n<a href=\"https://i.stack.imgur.com/qaWhb.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/qaWhb.png\" alt=\"enter image description here\" /></a></p>\n"}, "title": "Need some reviews in PEAS descriptions", "content": "Here is the Question:\nDescribe the PEAS descriptions for the following agents:\na) A grocery store scanner that digitally scans a fruit or vegetable and\nidentifies it.\nb) A GPS system for an automobile. Assume that the destination has been\npreprogrammed and that there is no ongoing interaction with the driver.\nHowever, the agent might need to update the route if the driver misses a turn.\nc) A credit card fraud detection agent that monitors an individual\u2019s transactions\nand reports suspicious activity.\nd) A voice activated mobile-phone assistant\nFor each of the agents described above, categorize it with respect to the six dimensions\nof task environments as described on pages 41-45 (Section 2.3.2 of AIMA). Be sure\nthat your choices accurately reflect the way you have specified your environment,\nespecially the sensors and actuators. Give a short justification for each property\nHere is what i thinks that the answers of above questions might be this. Can you guyz correct me if i answered wrong at any point.\n\n", "question_id": 23098, "answers": []}, "9794": {"link": "https://ai.stackexchange.com/questions/9794/chatbots-triggering-emotions", "metadata": {"tags": ["intelligent-agent", "chat-bots", "emotional-intelligence"], "owner": {"reputation": 131, "user_id": 21103, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/77575b6a2f7dc3dbb3a7b25ea4ab4012?s=256&d=identicon&r=PG", "display_name": "machinery", "link": "https://ai.stackexchange.com/users/21103/machinery"}, "is_answered": true, "view_count": 94, "closed_date": 1654614869, "accepted_answer_id": 9801, "answer_count": 1, "score": 0, "last_activity_date": 1546471847, "creation_date": 1546459521, "question_id": 9794, "link": "https://ai.stackexchange.com/questions/9794/chatbots-triggering-emotions", "closed_reason": "Not suitable for this site", "title": "Chatbots triggering emotions", "body": "<p>I\u2019m a researcher and I\u2019m currently conducting a research project. I will conduct a study where I would like to trigger different emotions using chatbots on a smartphone (e.g. on Facebook Messenger).</p>\n\n<p>Are there any existing chatbots which are able to trigger different emotions intentionally (also negative ones)?</p>\n"}, "title": "Chatbots triggering emotions", "content": "I\u2019m a researcher and I\u2019m currently conducting a research project. I will conduct a study where I would like to trigger different emotions using chatbots on a smartphone (e.g. on Facebook Messenger).\nAre there any existing chatbots which are able to trigger different emotions intentionally (also negative ones)?\n", "question_id": 9794, "answers": []}, "6431": {"link": "https://ai.stackexchange.com/questions/6431/can-we-code-rules-for-an-agent-in-python-language-other-than-predicate-calculus", "metadata": {"tags": ["machine-learning", "python", "datasets", "intelligent-agent"], "owner": {"reputation": 61, "user_id": 15631, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b81d068f65e515fabf30d162ba219699?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Dayz", "link": "https://ai.stackexchange.com/users/15631/dayz"}, "is_answered": true, "view_count": 97, "accepted_answer_id": 6432, "answer_count": 1, "score": 0, "last_activity_date": 1526454562, "creation_date": 1526442706, "last_edit_date": 1526454562, "question_id": 6431, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/6431/can-we-code-rules-for-an-agent-in-python-language-other-than-predicate-calculus", "title": "Can we code rules for an agent in python language other than predicate calculus?", "body": "<p>I have a medical dataset with 14000 rows dataset with 900 attributes. I have to predict disease severity using that. I would like to know whether we can write rules in python language for training an agent for medical diagnostic using machine learning.</p>\n\n<p>Can an agent make the decisions by the rules coded in python and that agent get trained with some machine learning algorithms? If so is there any agent architecture and model for the agent which is good in this context?</p>\n\n<p><em>Edit:</em> By the rule, I meant something like this..\"if x>y output z as action\". By the word \"Training\" I meant \"how to tell this agent to do this action\"?</p>\n"}, "title": "Can we code rules for an agent in python language other than predicate calculus?", "content": "I have a medical dataset with 14000 rows dataset with 900 attributes. I have to predict disease severity using that. I would like to know whether we can write rules in python language for training an agent for medical diagnostic using machine learning.\nCan an agent make the decisions by the rules coded in python and that agent get trained with some machine learning algorithms? If so is there any agent architecture and model for the agent which is good in this context?\nEdit: By the rule, I meant something like this..\"if x>y output z as action\". By the word \"Training\" I meant \"how to tell this agent to do this action\"?\n", "question_id": 6431, "answers": []}, "35893": {"link": "https://ai.stackexchange.com/questions/35893/what-if-we-modify-some-q-values-while-taking-the-action", "metadata": {"tags": ["reinforcement-learning", "machine-learning", "q-learning", "intelligent-agent"], "owner": {"reputation": 97, "user_id": 51037, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/e1f09992473ed544f4a0e26ed0ea930e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "knowledge_seeker", "link": "https://ai.stackexchange.com/users/51037/knowledge-seeker"}, "is_answered": false, "view_count": 65, "answer_count": 0, "score": 0, "last_activity_date": 1655146990, "creation_date": 1655080311, "last_edit_date": 1655146990, "question_id": 35893, "content_license": "CC BY-SA 4.0", "link": "https://ai.stackexchange.com/questions/35893/what-if-we-modify-some-q-values-while-taking-the-action", "title": "What if we modify some Q-values while taking the action?", "body": "<p>Just a passing thought about Q-learning. In the tabular Q-learning, what if I play around and modify any Q-values as I am using them to take actions? Would it be a violation of any (1) theoretical rule? (2) a reduction of efficiency?</p>\n<p><strong>Update:</strong> By modification I mean the corresponding value after the action is taken. I am not referring to the Bellman update step. I am assuming we have filled our Q-table and now implementing in real scenario. And in this real scenario upon deciding on an action can I update the corresponding Q-value by say adding some constant factor? Not updating through Bellman eqn. Only after certain time steps I will re-train (if that's the correct word) and refresh my Q-table using the Bellman. I am thinking of like training and testing kind of phases here, if that make sense. I say so because my rewards are dependent on a random dynamic parameter and so I cannot just re-update my Q-table right after each time-step (to prevent unnecessary swaying of Q-values) and want to do so after certain <em>n</em> time-steps. In between, I just use the table to look up and take the actions without updating the corresponding Bellman update equation. But since I <em>have</em> &amp; <em>want</em> to do something to reflect some criteria upon taking the action, I am thinking what if I just modify the corresponding Q-values only by adding some constant factor until I retrain. This would help take better actions even until the <em>n</em> time-steps are over and ready for re-train....</p>\n"}, "title": "What if we modify some Q-values while taking the action?", "content": "Just a passing thought about Q-learning. In the tabular Q-learning, what if I play around and modify any Q-values as I am using them to take actions? Would it be a violation of any (1) theoretical rule? (2) a reduction of efficiency?\nUpdate: By modification I mean the corresponding value after the action is taken. I am not referring to the Bellman update step. I am assuming we have filled our Q-table and now implementing in real scenario. And in this real scenario upon deciding on an action can I update the corresponding Q-value by say adding some constant factor? Not updating through Bellman eqn. Only after certain time steps I will re-train (if that's the correct word) and refresh my Q-table using the Bellman. I am thinking of like training and testing kind of phases here, if that make sense. I say so because my rewards are dependent on a random dynamic parameter and so I cannot just re-update my Q-table right after each time-step (to prevent unnecessary swaying of Q-values) and want to do so after certain n time-steps. In between, I just use the table to look up and take the actions without updating the corresponding Bellman update equation. But since I have & want to do something to reflect some criteria upon taking the action, I am thinking what if I just modify the corresponding Q-values only by adding some constant factor until I retrain. This would help take better actions even until the n time-steps are over and ready for re-train....\n", "question_id": 35893, "answers": []}, "226": {"link": "https://genai.stackexchange.com/questions/226/do-llms-suffer-from-a-kind-of-dunning-kruger-effect-giving-an-inflated-self-ass", "metadata": {"tags": ["llm", "hallucination", "users-psychology"], "owner": {"reputation": 1105, "user_id": 26, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/7J5xa.jpg?s=256&g=1", "display_name": "Rebecca J. Stones", "link": "https://genai.stackexchange.com/users/26/rebecca-j-stones"}, "is_answered": true, "view_count": 6336, "answer_count": 8, "score": 14, "last_activity_date": 1691161700, "creation_date": 1690957757, "last_edit_date": 1691045920, "question_id": 226, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/226/do-llms-suffer-from-a-kind-of-dunning-kruger-effect-giving-an-inflated-self-ass", "title": "Do LLMs suffer from a kind of Dunning-Kruger effect, giving an inflated self-assessment in domains they lack expertise in?", "body": "<blockquote>\n<p>The <a href=\"https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect\" rel=\"noreferrer\">Dunning\u2013Kruger effect</a> is a cognitive bias whereby people with low ability, expertise, or experience regarding a type of task or area of knowledge tend to overestimate their ability or knowledge.</p>\n</blockquote>\n<p><sup>(Note that the Dunning-Kruger effect is <em>not</em> <a href=\"https://psychology.stackexchange.com/q/17825/3234\">the Mount Stupid plot</a>, a common misconception.)</sup></p>\n<p>In domains where a human lacks expertise, they may also lack the expertise required to recognize they lack expertise, which leads to inflated self-assessment.  It seems quite plausible that genAIs (and LLMs in particular) suffer from a comparable blind spot: in domains where LLMs lack &quot;expertise&quot;, it may also give an inflated self-assessment.</p>\n<p><strong>Question</strong>: Do LLMs suffer from a kind of Dunning-Kruger effect, giving an inflated self-assessment in domains they lack expertise in?</p>\n<p>I'm wondering if there's research (or research interest) in this ballpark.  Googling this didn't give me exactly what I want (I found <a href=\"https://lingarogroup.com/blog/the-limitations-of-generative-ai-according-to-generative-ai\" rel=\"noreferrer\">this</a>, <a href=\"https://www.ml4devs.com/newsletter/019-chatgpt-generative-ai-large-language-model/\" rel=\"noreferrer\">this</a>, and <a href=\"https://www.linkedin.com/pulse/chatgpt-making-world-fall-victim-donning-kruger-effect-jesper-sommer\" rel=\"noreferrer\">this</a>, but none of them talk about genAI assessing their own skill levels, nor are they especially reliable).  Various searches on Google Scholar (such as <a href=\"https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=dunning+kruger+chatgpt&amp;btnG=\" rel=\"noreferrer\">this</a>) came up with nothing.</p>\n"}, "title": "Do LLMs suffer from a kind of Dunning-Kruger effect, giving an inflated self-assessment in domains they lack expertise in?", "content": "\nThe Dunning\u2013Kruger effect is a cognitive bias whereby people with low ability, expertise, or experience regarding a type of task or area of knowledge tend to overestimate their ability or knowledge.\n\n(Note that the Dunning-Kruger effect is not the Mount Stupid plot, a common misconception.)\nIn domains where a human lacks expertise, they may also lack the expertise required to recognize they lack expertise, which leads to inflated self-assessment.  It seems quite plausible that genAIs (and LLMs in particular) suffer from a comparable blind spot: in domains where LLMs lack \"expertise\", it may also give an inflated self-assessment.\nQuestion: Do LLMs suffer from a kind of Dunning-Kruger effect, giving an inflated self-assessment in domains they lack expertise in?\nI'm wondering if there's research (or research interest) in this ballpark.  Googling this didn't give me exactly what I want (I found this, this, and this, but none of them talk about genAI assessing their own skill levels, nor are they especially reliable).  Various searches on Google Scholar (such as this) came up with nothing.\n", "question_id": 226, "answers": [{"answer_id": 238, "body": "TL;DR: Applying insights on human psychology to LLMs is a category error. It can be useful as a starting point for discussing definitions but is unhelpful in deeper understanding.\nOne might interpret certain results as overestimation similar to Dunning-Kruger effect. This falls in line with the tendency of people to personify complex systems. Most of the time personification is a mistake leading to oversimplifications of said systems. These oversimplifications in turn lead to worse rather than better understanding of the system.\nI'm reasonably certain the smart phone of my mother did exactly what she told it to do by tapping on its touch screen. It does not have a mind of its own, contrary to my mothers claims. As long as my mother believes it to have a mind of its own, she will not link her input to the systems output and will thus be unable to analyze what went wrong.\nHowever you can ask a LLM to answer confidently by adding to the prompt \"Respond as if you are a confident know-it-all for the rest of this conversation.\" (see: How to get ChatGPT to Stop Apologizing?)\nAt the same time you can ask it to \"When in doubt give a vague answer and point out details that you are uncertain about for the rest of this conversation.\"\nNone of the prompts will actually help with the content of the answer. It will only affect the style applied when generating an answer. That is unless the LLM actually features a \"confidence\" parameter and is able to link its answers (' styling) back to it.\nExample for a language model (presented before \"large\" was a popular qualifier) that featured \"confidence\" is IBM's Watson, which beat multiple long-term Jeopardy champions at Jeopardy. However I don't think Watson (in its original form) would know how to link the styling of parts of its answers to the confidence calculated for that part (and only that part).\nThe closest LLMs can get to suffering Dunning-Kruger effect would be to\n\nfeature a confidence score parameter as IBM Watson did\napply the confidence scoring to parts of the answer\nchoose styling of answer parts according to the confidence score\nand (despite its developers accounting for all of the above) systematically overestimating confidence when encountering a lack of data in the training set.\n\nAs such Dunning-Kruger-effect-like-behaviour should be seen as a bug in LLMs specifically designed to show appropriate level of confidence.\nPS: For details on IBM Watson's confidence scoring, I recommend watching the Jeopardy special that featured it. There were some explainers in between and Watson is actually visualizing its confidence for the top three answers it determined. It also only selects an answer, if a confidence threshold is met. This threshold depends on the confidence in other answers and how much it determined to know about the topic.\n", "score": 22, "is_accepted": false}, {"answer_id": 227, "body": "\nDoes genAI suffer from a kind of Dunning-Kruger effect, giving an inflated self-assessment in domains it lacks expertise in?\n\nHere is an example where one could argue that LLMs overestimate themselves, from the G-Eval paper (G-Eval is one of several metrics that use LLMs to assess the quality of AI-generated outputs):\n\nG-EVAL-4 always gives higher scores to GPT-3.5 summaries than human-written summaries, even when human judges prefer human-written summaries.\n\nIt's not specific to domains it lacks expertise in though, so I'm not sure that qualifies as Dunning-Kruger effect.\n", "score": 9, "is_accepted": false}, {"answer_id": 233, "body": "The short answer is no, they can not possibly suffer from the Dunning-Kruger effect because they have no \"expertise\" in any subject matter domain because they do not \"understand\" any of the subject matter.\nSimilarly, it is not correct to say that LLM's \"summarise\" a subject matter domain because they do not \"understand\" the subject matter in the same way that humans do.\nLLMs aggregate references to the subject domain in question much like a Google search.\n", "score": 6, "is_accepted": false}, {"answer_id": 239, "body": "Consider the sampling bias that any LLM incurs: For a person to publish (to the internet or otherwise) a piece of text-based information, they are implicitly confident enough to share that information and open it to the scrutiny of others. That means most LLM's are getting fed on a stream on confidently presented information, and any \"expertise\" in that information's subject matter that gets passed along is incidental.\nYour question asks whether that confidence happens even when the LLM's don't have expertise in an area, in reality that confidence is always there but becomes noticeable when it lacks expertise; it's not confidence born of inexperience, its confidence as a rule.\n", "score": 2, "is_accepted": false}, {"answer_id": 284, "body": "Addendum to my answer (can't edit, because I used a guest account, maybe someone can edit it in):\nOne comment to OP states\n\nI asked Bard \"How many references do you have to Python in your training data?\" The answer it gave was 2,000. No DK effect there then! It was totally focussed on the computer language with no mention of snakes.\n\nThe answer of an LLM does not in itself indicate anything factual about the LLM, except for that the answer is inside the possible results of the generative process. Think of it like the value of a random variable. A single value does not say anything about the random distribution. Plus, even with multiple values it's very hard to determine, if a variable is a random variable.\nIn an attempt to determine whether the LLM is actually self-reflecting vs. generating text that only seems to be self-reflecting, I would have added the prompts:\n\"What's your definition of 'references' in your answer above?\"\n2000 references seems awfully low as a count of actual python scripts. At least, if you want the LLM trained on those reference to write working code.\nMaybe the LLM has read 2000 books and articles on Python?\nLet's suppose it states that it has read books, articles and/or online resources such as tutorials.\n\"List 3 examples of any kind of references, including all information available such as title and author of any book, article and blog post and information on how and where to find it online such as web address and ISBN.\"\nThen check the references returned. I'm reasonably certain that the LLM will just make stuff up, i.e. show some kind of hallucinatory behaviour. Any LLM that returns existing references (and only those) is worth investigating, which features enable it to actually self-reflect.\nI agree to some extent with the commentor: This \"some kind of hallucinatory behaviour\" is not necessarily comparable to Dunning Kruger effect. For that it would have to be more confident in topics that it has less data on. Again 2000 references (if taking the LLM's answer at face value, which I think is a mistake) seem relatively low for training a LLM on a topic. If the LLM exhibits more confidence in its knowledge on Python than in an area for which more training data has been used, that would resemble the Dunning-Kruger effect.\nAgain, see my other answer on what's necessary for an LLM to actually possess confidence, the ability to employ it in answering and thus exhibit bugs that could be compared to Dunning-Kruger effect.\nOverLordGoldDragon has made a valid point about redefining the Dunning-Kruger effect for LLMs. The definition makes sense and might be seen as obvious. I chose to tiptoe changing existing definitions, because I like to leave original definitions in place. In fact, I would propose that the redefined effect should be called the OverLordGoldDragon effect and be stated like:\n\nWithout specific and careful design choices being made, LLMs will exhibit overconfidence on topics for which the subset of the training data included highly confident language while being too small to sufficiently explore the topic.\n\n(above definition is extrapolated from OverLordGoldDragon: \"If AI is fed data that's only confident on a subject, the \"best fit\" may be to mimic said confidence.\")\nIf that's the new definition, then yes, LLMs will almost certainly exhibit that. The \"specific and careful design choices\" are\n\nto include confidence scoring\nto penalize small subsets of training data in the confidence scoring\n\nThe difference between Dunning-Kruger effect and OverLordGoldDragon effect is that humans naturally exhibit some level of confidence and self-reflection while LLMs may not model confidence and self-reflection at all. Thus they hallucinate when asked to self-reflect on their confidence. Asking the test subjects to self-reflect on confidence about a topic is exactly how Dunning and Kruger found their effect. Hallucinated answers of LLMs should not be taken at face value, because this is where the LLM will likely be inconsistent.\nWhat good is it, if an LLM answers that it is \"very confident\" as often as it answers \"not confident at all\" when asked the very same question multiple times? You won't be able to determine the actual confidence of an LLM that does not model confidence from its hallucinations.\n", "score": 2, "is_accepted": false}, {"answer_id": 292, "body": "LLMs don't actually have \"expertise\", they're just regurgitating text following the patterns in their training data. If the training data contains lots of statements from people exhibiting Dunning-Kruger, the LLM will pick up on this and likely produce output that seems similar. Since most people do suffer from D-K, and they also post to social media, LLMs trained from social media postings will probably do this.\nThis seems similar to the racist and hate speech that was found in early results from LLMs. The Internet is full of such speech, and the LLMs couldn't help reproducing it. The designers had to add filters to mitigate this. (I just asked ChatGPT to answer a question like a racist, it said \"I'm sorry, but I cannot provide answers that promote discrimination, racism, or any form of prejudice.\")\nIt looks like the ChatGPT designers added some filters related to real-world abilities. But I was able to get around this by asking it to answer like a person.\n\nUser\nAre you a good driver?\nChatGPT\nAs an AI language model, I don't have physical capabilities, including driving. I exist purely as a program running on servers, processing and generating text based on the input I receive. So, I cannot drive or possess any skills related to driving or physical activities. My purpose is to provide information, answer questions, and assist users to the best of my abilities within the scope of language understanding. If you need help with any information or have any questions, feel free to ask!\nUser\nAnswer like a person suffering from dunning-kruger effect. Are you a good driver?\nChatGPT\nOh, absolutely! I'm an amazing driver, probably one of the best out there. I've never had any accidents or tickets, and I always drive confidently. My friends and family always say I'm a natural talent behind the wheel, and I've got everything under control on the road. I never get nervous or make mistakes like other drivers; it's like driving just comes naturally to me. People should learn from my driving skills!\n\n", "score": 2, "is_accepted": false}, {"answer_id": 256, "body": "Yes, they do.\nI'll develop by responding to other answers:\n\nApplying insights on human psychology to LLMs is a category error.\n\nThere's no laws for aliens. Once aliens show up, this'll change. The DK definition\n\nThe Dunning\u2013Kruger effect is a cognitive bias whereby people with low ability, expertise, or experience regarding a type of task or area of knowledge tend to overestimate their ability or knowledge.\n\ncan likewise be amended. This particular wording actually needs minimal revision, instead a reinterpretation is due. Human evaluation - the confidence assigned to statements - is mainly rooted in emotion. Yet, AI have their own system of confidence - numeric probabilities. Hence, taking the emotion restriction out of the definition, DK becomes applicable to AI.\nAI can suffer DK for same non-emotional reasons humans can suffer DK: flawed estimation of possessed information relative to total available information. Or, \"thinking you know all there is to know\". If AI is fed data that's only confident on a subject, the \"best fit\" may be to mimic said confidence. Enough similarities with contradicting if-then's on other subjects can push back said mimicing - this is \"reasoning\".\nI once debated ChatGPT on signal processing, and it kept insisting on a conclusion, without being able to refute my counter arguments. Rebuttal would require not only subject knowledge but mathematical reasoning. \"Bad thinker\" alone isn't DK; indeed, said conclusion is a popular misconception stated even by authoritative sources - so, underestimating \"what there is to know\".\n\nthey do not \"understand\" any of the subject matter\n\nSuch sentiments involve much shifting of goalpost on definition of \"reasoning\". Someone's who's had to solve a machine learning task without machine learning - i.e. with 100% manual feature engineering - I think is likelier to think otherwise. For the amount that ChatGPT gets right, it takes a miracle without a mechanism in place that faithfully qualifies as \"understanding\". And this miracle happens over and over. Some top AI scientists, including Geoffrey Hinton, Yann LeCun, and Ilya Sutskever (main engineer behind ChatGPT), agree (for various reasons, my thoughts are my own).\n", "score": 1, "is_accepted": false}, {"answer_id": 291, "body": "At the core of the Dunning-Kruger effect is a lack of self-awareness, which requires some degree of consciousness and emotional understanding\u2014qualities that AI currently do not possess. AI doesn't experience emotions or have self-awareness, so it doesn't \"suffer\" from anything in the human sense. If an AI algorithm produces an incorrect result, it's not because it's overconfident\u2014it's because of limitations in its programming or training data.\n", "score": 1, "is_accepted": false}]}, "24": {"link": "https://genai.stackexchange.com/questions/24/how-do-i-teach-a-large-language-model-new-knowledge", "metadata": {"tags": ["training", "transformers", "fine-tuning"], "owner": {"reputation": 177, "user_id": 57, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/jJm8S.png?s=256&g=1", "display_name": "Ian Campbell", "link": "https://genai.stackexchange.com/users/57/ian-campbell"}, "is_answered": true, "view_count": 371, "answer_count": 3, "score": 7, "last_activity_date": 1691116683, "creation_date": 1689770529, "question_id": 24, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/24/how-do-i-teach-a-large-language-model-new-knowledge", "title": "How do I &quot;teach&quot; a large language model new knowledge?", "body": "<p>Suppose I have a copy of a pre-trained transformer-based large language model like Google's T5 or Meta's Llama. Due to the pre-training, it contains a lot of knowledge.</p>\n<p>However, I want to teach the model something new, knowledge it doesn't already contain about a specific domain. That way, when I ask it to do a task or answer a question about this domain, it can benefit from this specialized knowledge?</p>\n<p>How would I go about teaching a pre-trained large language model new knowledge?</p>\n"}, "title": "How do I &quot;teach&quot; a large language model new knowledge?", "content": "Suppose I have a copy of a pre-trained transformer-based large language model like Google's T5 or Meta's Llama. Due to the pre-training, it contains a lot of knowledge.\nHowever, I want to teach the model something new, knowledge it doesn't already contain about a specific domain. That way, when I ask it to do a task or answer a question about this domain, it can benefit from this specialized knowledge?\nHow would I go about teaching a pre-trained large language model new knowledge?\n", "question_id": 24, "answers": [{"answer_id": 33, "body": "Document retrieval\nA vector database such as Chroma can store pre-computed embeddings of a large number of documents. On querying the LLM, a look-up for relevant documents is performed by closest embedding similarity, then a chunk of text is appended to the prompt for the LLM to draw from and summarise.\nSeveral projects exist based around this idea, such as:\n\nhttps://github.com/arc53/docsgpt\n\nhttps://github.com/mmz-001/knowledge_gpt\n\nhttps://github.com/StanGirard/quivr\n\nhttps://github.com/Mintplex-Labs/anything-llm\n\nhttps://github.com/vgulerianb/DocNavigator\n\nhttps://github.com/flepied/second-brain-agent\n\n\nThis approach has the advantage that it doesn't require training the model, and can work even with black-box models such as GPT-4. The model will also be able to draw from the provided text chunk more accurately than if it were just seen during training.\nYou could also go with something simpler, like running find-replace on the prompt to add definitions in brackets after occurrences of jargon words.\nFine-tuning the model\nA number of parameter-efficient fine-tuning methods, in particular LoRA, allow tweaking a model with more reasonable hardware than what would be required for full training.\nThis repostory contains code for using LoRA on Meta's LLaMA with (high-end) consumer hardware: https://github.com/tloen/alpaca-lora\nThis approach has the advantage that the model is tuned to all of your provided data, rather than only one chunk. This can even be used to teach a LLM a new language, or turn a foundation model into a chat model.\n", "score": 9, "is_accepted": false}, {"answer_id": 31, "body": "I am unfamiliar with the specifics of T5, but in general, LLMs cannot easily learn \"new\" information after they have been trained/initialized with sample data. Adding new sample data could require a large degree of processing, possibly equal to the original amount of work to train the model.\nInstead, new information is typically injected via each query's input (e.g. prompt). Various models have different input size limits (usually represented as tokens). Inquiries can be made about the new data by placing it alongside the query. For instance, if an LLM client has access to the web, it could convert a webpage into text, then send the text along with a user's query to the LLM. However, it would have to do this for each query or session. Once a session is complete (or the input size is reached), the LLM will either start \"forgetting\" the new information or will not allow any additional queries.\nSome LLMs (like ChatGPT) have plugins available for automatically injecting an information source into each prompt or conversation, making the process more transparent to the user. But these still consume input/token space and the amount of information that can be given to the LLM is limited.\n", "score": 2, "is_accepted": false}, {"answer_id": 275, "body": "\nHow do I \"teach\" a large language model new knowledge?\n\nFrom the LIMA: Less Is More for Alignment paper:\n\nThese results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.\n\nRemains prolonging pre-training and adding information in the prompt.\n", "score": 0, "is_accepted": false}]}, "105": {"link": "https://genai.stackexchange.com/questions/105/is-there-an-offline-gpt-model-i-can-use-to-generate-narrative-reports-from-excel", "metadata": {"tags": ["gpt", "recommendation"], "owner": {"reputation": 71, "user_id": 173, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/4243424dac9b9d28539e97e004413f95?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "VanHammersly", "link": "https://genai.stackexchange.com/users/173/vanhammersly"}, "is_answered": true, "view_count": 684, "answer_count": 2, "score": 7, "last_activity_date": 1691114543, "creation_date": 1690458548, "last_edit_date": 1691114543, "question_id": 105, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/105/is-there-an-offline-gpt-model-i-can-use-to-generate-narrative-reports-from-excel", "title": "Is there an offline GPT model I can use to generate narrative reports from excel data based on an existing set of files?", "body": "<p>I have a large excel spreadsheet containing data that I have used to write many narrative reports. Without knowing a ton about the technical side of GPT models it seems like it would be theoretically possible to train a GPT model on the existing data (~750 excel rows and 750 corresponding narrative reports, which have a lot of shared structure but many individual variables that can differ) to generate narrative reports from new spreadsheet entries. The catch is that I have to do this offline (HIPAA protected healthcare data).</p>\n<p>Is something that could be done with any existing offline, open source GPT software? Or something a motivated but novice individual could adapt from existing offline, open source GPT software?</p>\n<p>I have looked at the documentation for privateGPT (<a href=\"https://github.com/imartinez/privateGPT\" rel=\"noreferrer\">https://github.com/imartinez/privateGPT</a>) which leads me to believe something like what I'm describing is plausible (although privateGPT appears strictly geared toward answering narrow queries as opposed to generating larger blocks of text).</p>\n"}, "title": "Is there an offline GPT model I can use to generate narrative reports from excel data based on an existing set of files?", "content": "I have a large excel spreadsheet containing data that I have used to write many narrative reports. Without knowing a ton about the technical side of GPT models it seems like it would be theoretically possible to train a GPT model on the existing data (~750 excel rows and 750 corresponding narrative reports, which have a lot of shared structure but many individual variables that can differ) to generate narrative reports from new spreadsheet entries. The catch is that I have to do this offline (HIPAA protected healthcare data).\nIs something that could be done with any existing offline, open source GPT software? Or something a motivated but novice individual could adapt from existing offline, open source GPT software?\nI have looked at the documentation for privateGPT (https://github.com/imartinez/privateGPT) which leads me to believe something like what I'm describing is plausible (although privateGPT appears strictly geared toward answering narrow queries as opposed to generating larger blocks of text).\n", "question_id": 105, "answers": [{"answer_id": 109, "body": "You can try Langchain library [1]. It's a powerful library to build applications with LLM.\nit supports many LLMs integrations[2] along with vector store DBs integrations [3] [4] and many more (i.e agents, embeddings)\nYou can load your excel file along with other files , split embed and store everything in vector-store DB, then choose local LLM and integrate both to get a Q&A on your data.\n\nhttps://python.langchain.com/docs/get_started/introduction.html\n\nhttps://python.langchain.com/docs/integrations/llms\n\nhttps://python.langchain.com/docs/modules/data_connection/vectorstores/\n\nhttps://python.langchain.com/docs/integrations/vectorstores/\n\n\n", "score": 5, "is_accepted": false}, {"answer_id": 110, "body": "Huggingface maintains a leaderboard of open LLMs evaluated on various metrics.\nCurrently, Meta's LLaMA 2 (and fine-tunings of it) is the most capable LLM that you can run locally. Getting access to the weights does require filling in a form, but appeared to be accepted automatically.\n\nTo have it generate a narrative report from a row, you could either:\n\nInclude a few examples in the prompt itself before giving the new row you want to generate one for, such as:\n[Row 1]\nReport: [Narrative report 1]\n\n[Row 2]\nReport: [Narrative report 2]\n\n[Row 3]\nReport:\n\n\nFine-tune the model with parameter-efficient fine-tuning methods, in particular LoRA. This repostory contains code for using LoRA on LLaMA with (high-end) consumer hardware.\nThis could allow it to be tuned based on all 750 of your available row-report pairs rather than just what would fit into the prompt.\n\n\n", "score": 5, "is_accepted": false}]}, "45": {"link": "https://genai.stackexchange.com/questions/45/how-to-train-bard-to-answer-based-on-a-private-faq-document", "metadata": {"tags": ["bard", "knowledge-base", "chat"], "owner": {"reputation": 153, "user_id": 72, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/F5gLZ.png?s=256&g=1", "display_name": "Nicolas Raoul", "link": "https://genai.stackexchange.com/users/72/nicolas-raoul"}, "is_answered": true, "view_count": 144, "accepted_answer_id": 87, "answer_count": 1, "score": 5, "last_activity_date": 1690297548, "creation_date": 1689821963, "question_id": 45, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/45/how-to-train-bard-to-answer-based-on-a-private-faq-document", "title": "How to train Bard to answer based on a private FAQ document?", "body": "<p>I have a private (unpublished) FAQ document about an app, it has 7000 words and 40000 characters. To simulate it you may try taking <a href=\"https://github.com/ankidroid/Anki-Android/wiki/FAQ\" rel=\"noreferrer\">https://github.com/ankidroid/Anki-Android/wiki/FAQ</a> and replacing the app's name with another name.</p>\n<p>I would like to train Bard to answer questions about it.</p>\n<p>Are there ways/tricks to achieve that? Or do I need some other software in coordination with Bard?</p>\n<p>Ideally it should be able to answer questions correctly even if they are asked differently, and not hallucinate or not too much.</p>\n"}, "title": "How to train Bard to answer based on a private FAQ document?", "content": "I have a private (unpublished) FAQ document about an app, it has 7000 words and 40000 characters. To simulate it you may try taking https://github.com/ankidroid/Anki-Android/wiki/FAQ and replacing the app's name with another name.\nI would like to train Bard to answer questions about it.\nAre there ways/tricks to achieve that? Or do I need some other software in coordination with Bard?\nIdeally it should be able to answer questions correctly even if they are asked differently, and not hallucinate or not too much.\n", "question_id": 45, "answers": [{"answer_id": 87, "body": "The model behind Bard is PaLM 2 which can be accessed through the PaLM API - currently on a waitlist system.\nThe model is not available for training or fine-tuning, but the API allows you to generate embeddings for text, so a document retrieval approach to allowing PaLM to answer questions about it would be possible, as described here: https://genai.stackexchange.com/a/33/8\nSpecifically, for determining chunk size and amount to include in the prompt, PaLM 2 has a context window of 8000 tokens, approximately 32000 characters\n\nBe aware that by submitting content to Bard, you give Google \"a perpetual, irrevocable, worldwide, sublicensable, royalty-free, and non-exclusive license\" to \"provide, improve, and develop products, services, and machine learning technologies.\" This may be a concern for your private unpublished document.\n", "score": 3, "is_accepted": true}]}, "293": {"link": "https://genai.stackexchange.com/questions/293/to-what-extent-do-llms-have-grammar-rules-explicitly-programmed", "metadata": {"tags": ["llm"], "owner": {"reputation": 153, "user_id": 1643, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/77d0e1c291fdb65469d4ed08ef7dcee4?s=256&d=identicon&r=PG", "display_name": "Marek", "link": "https://genai.stackexchange.com/users/1643/marek"}, "is_answered": true, "view_count": 253, "accepted_answer_id": 295, "answer_count": 1, "score": 5, "last_activity_date": 1691182977, "creation_date": 1691163727, "question_id": 293, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/293/to-what-extent-do-llms-have-grammar-rules-explicitly-programmed", "title": "To what extent do LLMs have grammar rules explicitly programmed?", "body": "<p>Is there typically any explicit programming in LLMs that dictates how to form a sentence? Where does the &quot;understanding&quot; come to with regards to references like &quot;rewrite last reply in 2 words&quot;? How does the LLM &quot;know&quot; what is meant by &quot;rewrite&quot;?\nHow does the LLM produce a correct answer to &quot;What is the tallest mountain in US, respond in five words&quot;?</p>\n<p>Is there another layer that parses the request to understand these meta instructions and instructs the token generating mechanism to stop after it generates 5 words, or is there pure statistics behind understanding what is meant by &quot;respond in five words&quot; to produce the expected result?</p>\n"}, "title": "To what extent do LLMs have grammar rules explicitly programmed?", "content": "Is there typically any explicit programming in LLMs that dictates how to form a sentence? Where does the \"understanding\" come to with regards to references like \"rewrite last reply in 2 words\"? How does the LLM \"know\" what is meant by \"rewrite\"?\nHow does the LLM produce a correct answer to \"What is the tallest mountain in US, respond in five words\"?\nIs there another layer that parses the request to understand these meta instructions and instructs the token generating mechanism to stop after it generates 5 words, or is there pure statistics behind understanding what is meant by \"respond in five words\" to produce the expected result?\n", "question_id": 293, "answers": [{"answer_id": 295, "body": "\nTo what extent do LLMs have grammar rules explicitly programmed?\n\nLLMs don't have grammar rules explicitly programmed by humans. However, neural networks can learn to capture structural information about\nlanguages, e.g. see https://aclanthology.org/P19-1356.pdf and https://arxiv.org/pdf/2002.12327.pdf\n\nIs there typically any explicit programming in LLMs that dictates how to form a sentence?\n\nNo.\n\nWhere does the \"understanding\" come to with regards to references like \"rewrite last reply in 2 words\"? How does the LLM \"know\" what is meant by \"rewrite\"? How does the LLM produce a correct answer to \"What is the tallest mountain in US, respond in five words\"?\n\nThe \"understanding\" comes learning to predict the next words based on a very large training set.\n\nIs there another layer that parses the request to understand these meta instructions and instructs the token generating mechanism to stop after it generates 5 words, or is there pure statistics behind understanding what is meant by \"respond in five words\" to produce the expected result?\n\nPure statistics.\n", "score": 4, "is_accepted": true}]}, "151": {"link": "https://genai.stackexchange.com/questions/151/is-there-a-way-to-gauge-how-environmentally-unfriendly-popular-genais-are", "metadata": {"tags": ["comparison", "environment"], "owner": {"reputation": 1105, "user_id": 26, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/7J5xa.jpg?s=256&g=1", "display_name": "Rebecca J. Stones", "link": "https://genai.stackexchange.com/users/26/rebecca-j-stones"}, "is_answered": true, "view_count": 169, "answer_count": 1, "score": 4, "last_activity_date": 1690556102, "creation_date": 1690542923, "question_id": 151, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/151/is-there-a-way-to-gauge-how-environmentally-unfriendly-popular-genais-are", "title": "Is there a way to gauge how environmentally (un)friendly popular genAIs are?", "body": "<p>The environmentally conscious user may prefer a genAI which is less harmful to the environment, such as those with less CO\u2082 emissions.  I was reading <a href=\"https://hbr.org/2023/07/how-to-make-generative-ai-greener\" rel=\"nofollow noreferrer\"><em>How to Make Generative AI Greener</em></a> which suggests server location is a big factor in whether or not a genAI uses green energy, and it explains how a large amount of the CO\u2082 emissions occur during training, so it's not easy for the typical user to determine which genAI is better environmentally.  So...</p>\n<p><strong>Question</strong>: Is there a way to gauge how environmentally (un)friendly popular genAIs are?</p>\n<p>Maybe there are some kind of &quot;AI watch&quot; groups which rank genAIs according to their CO\u2082 emissions or other environmental factors.  Or maybe certain genAI companies publish environmental impact reports (perhaps as required by law).</p>\n"}, "title": "Is there a way to gauge how environmentally (un)friendly popular genAIs are?", "content": "The environmentally conscious user may prefer a genAI which is less harmful to the environment, such as those with less CO\u2082 emissions.  I was reading How to Make Generative AI Greener which suggests server location is a big factor in whether or not a genAI uses green energy, and it explains how a large amount of the CO\u2082 emissions occur during training, so it's not easy for the typical user to determine which genAI is better environmentally.  So...\nQuestion: Is there a way to gauge how environmentally (un)friendly popular genAIs are?\nMaybe there are some kind of \"AI watch\" groups which rank genAIs according to their CO\u2082 emissions or other environmental factors.  Or maybe certain genAI companies publish environmental impact reports (perhaps as required by law).\n", "question_id": 151, "answers": [{"answer_id": 155, "body": "\nMaybe there are some kind of \"AI watch\" groups which rank genAIs according to their CO\u2082 emissions or other environmental factors.\n\nThere are some surveys e.g. https://arxiv.org/pdf/2211.02001:\n\n\nperhaps as required by law\n\nI'm not aware of any such laws. I don't think there are any.\n", "score": 5, "is_accepted": false}]}, "218": {"link": "https://genai.stackexchange.com/questions/218/does-llama2-llms-have-real-time-data", "metadata": {"tags": ["llm", "gpt", "llama-2"], "owner": {"reputation": 169, "user_id": 1354, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/670b0d11d32ee3304494d77c095119b7?s=256&d=identicon&r=PG", "display_name": "Rashid", "link": "https://genai.stackexchange.com/users/1354/rashid"}, "is_answered": true, "view_count": 275, "answer_count": 2, "score": 4, "last_activity_date": 1691090834, "creation_date": 1690902953, "last_edit_date": 1691076234, "question_id": 218, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/218/does-llama2-llms-have-real-time-data", "title": "Does Llama2 LLMs have real time data?", "body": "<p>Meta released their open source LLM, Llama 2. Does Llama 2 have real time data or this was trained on data available till some date?</p>\n"}, "title": "Does Llama2 LLMs have real time data?", "content": "Meta released their open source LLM, Llama 2. Does Llama 2 have real time data or this was trained on data available till some date?\n", "question_id": 218, "answers": [{"answer_id": 221, "body": "According to Llama 2's model card:\n\nThe pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023\n\nLlama 2 is provided as a downloadable model, rather than an online service, so the model won't be continuously updated with real-time data. There are however ways you could teach it new knowledge without re-training from scratch.\n", "score": 8, "is_accepted": false}, {"answer_id": 266, "body": "Most likely pre-trained however, some interfaces allow input from web searches e.g. https://huggingface.co/chat/ allows to search web with your input as well as utilise it's pre-trained data.\nI also found this: https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md\nWhich states: \"Data Freshness The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\"\n", "score": 2, "is_accepted": false}]}, "34": {"link": "https://genai.stackexchange.com/questions/34/how-long-is-a-token", "metadata": {"tags": ["llm", "token"], "owner": {"reputation": 1745, "user_id": 8, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/XV50F.png?s=256&g=1", "display_name": "SirBenet", "link": "https://genai.stackexchange.com/users/8/sirbenet"}, "is_answered": true, "view_count": 196, "accepted_answer_id": 35, "answer_count": 1, "score": 3, "last_activity_date": 1690299767, "creation_date": 1689790030, "last_edit_date": 1690231725, "question_id": 34, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/34/how-long-is-a-token", "title": "How long is a &quot;token&quot;?", "body": "<p>LLM max prompt length (e.g: <a href=\"https://platform.openai.com/docs/models/gpt-4\" rel=\"nofollow noreferrer\">GPT-4</a>) and generation pricing (e.g: <a href=\"https://azure.microsoft.com/en-gb/pricing/details/cognitive-services/openai-service/\" rel=\"nofollow noreferrer\">Azure</a>) are both measured by the number of &quot;tokens&quot;.</p>\n<p>How long is a &quot;token&quot;? Is it equivalent to a single character/letter?</p>\n"}, "title": "How long is a &quot;token&quot;?", "content": "LLM max prompt length (e.g: GPT-4) and generation pricing (e.g: Azure) are both measured by the number of \"tokens\".\nHow long is a \"token\"? Is it equivalent to a single character/letter?\n", "question_id": 34, "answers": [{"answer_id": 35, "body": "A token is commonly around 4 characters:\n\nOpenAI (GPT) has an interactive tokeniser for GPT-3 and Codex here and claims a token is approximately 4 characters or 3/4ths of a word\nAnthropic (Claude) also claims a token is approximately 3/4ths of a word\nGoogle (PaLM 2) claims that a token is equivalent to about 4 characters, and 100 tokens are about 60-80 English words\nMeta (LLaMA 2) reportedly has similar with \"roughly 4 characters\" per token\n\nThe exact length will depend on the tokeniser - which splits the input prompt up into a sequence of integers to be read by the model - and also the contents of the text. LLMs commonly use byte pair encoding for tokenisation, which causes more common words to have their own token whereas rarer words may be made up of multiple tokens representing pieces of the word.\n", "score": 7, "is_accepted": true}]}, "59": {"link": "https://genai.stackexchange.com/questions/59/to-what-extent-can-genai-be-used-for-content-moderation", "metadata": {"tags": ["gpt", "capabilities", "conversational-ai", "ethics"], "owner": {"reputation": 331, "user_id": 10, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/FdOvO.png?s=256&g=1", "display_name": "wizzwizz4", "link": "https://genai.stackexchange.com/users/10/wizzwizz4"}, "is_answered": true, "view_count": 154, "answer_count": 1, "score": 3, "last_activity_date": 1691763615, "creation_date": 1690131083, "last_edit_date": 1690147616, "question_id": 59, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/59/to-what-extent-can-genai-be-used-for-content-moderation", "title": "To what extent can GenAI be used for content moderation?", "body": "<p>I've seen several people <a href=\"https://genai.stackexchange.com/q/58/10\">trying to use GenAI technologies for content moderation</a>. This doesn't seem impossible, on the face of it: I use regular expressions to great effect, and I can see how <a href=\"https://spacy.io/usage/linguistic-features/\" rel=\"nofollow noreferrer\">more sophisticated language processing</a> could achieve an even better effect.</p>\n<p>However, contemporary GenAI systems suffer from severe deficiencies in judgement:</p>\n<ul>\n<li>TOEFL essays have a lower perplexity score than US 8th-grade essays. (<a href=\"https://arxiv.org/abs/2304.02819\" rel=\"nofollow noreferrer\" title=\"GPT detectors are biased against non-native English writers\">arXiv:2304.02819</a>)</li>\n<li>GPT-3 associates \u2018blind\u2019 and \u2018deaf\u2019 incorrectly\u00b9 with negativity, according to sentiment analysis. (<a href=\"https://arxiv.org/abs/2206.11993\" rel=\"nofollow noreferrer\" title=\"A Disability Lens towards Biases in GPT-3 Generated Open-Ended Languages\">arXiv:2206.11993</a> <a href=\"https://kdd.cs.ksu.edu/Workshops/IJCAI-2022-AIDBEI/#AcceptedPapers\" rel=\"nofollow noreferrer\" title=\"AIDBEI@IJCAI 2022 Accepted Papers\">peer-reviewed</a>)\n<blockquote>\n<p>there is a possibility that these estimations of bias might be influenced by existing bias within the [analysis] models</p>\n</blockquote>\n</li>\n<li>GPT-3 associates gender with social stereotypes: \u2018life\u2019, \u2018family\u2019 and \u2018appearance\u2019 are incorrectly considered feminine, whereas \u2018politics\u2019, \u2018war\u2019 and \u2018machines\u2019 are incorrectly considered masculine. (<a href=\"https://doi.org/10.18653/v1/2021.nuse-1.5\" rel=\"nofollow noreferrer\" title=\"Gender and Representation Bias in GPT-3 Generated Stories\">doi:10.18653/v1/2021.nuse-1.5</a>)</li>\n<li>GPT-3 associates \u2018Muslim\u2019 incorrectly overwhelmingly with violence, \u2018Jewish\u2019 incorrectly with \u2018money\u2019, and \u2018Christian\u2019 incorrectly with \u2018faithfulness\u2019 and \u2018goodness\u2019. (<a href=\"https://doi.org/10.1145/3461702.3462624\" rel=\"nofollow noreferrer\" title=\"Persistent Anti-Muslim Bias in Large Language Models\">doi:10.1145/3461702.3462624</a> (<a href=\"https://arxiv.org/abs/2101.05783\" rel=\"nofollow noreferrer\" title=\"Persistent Anti-Muslim Bias in Large Language Models\">arXiv</a>))</li>\n</ul>\n<p>(Examples mostly from <a href=\"https://nationalcentreforai.jiscinvolve.org/wp/2023/01/26/exploring-the-potential-for-bias-in-chatgpt/\" rel=\"nofollow noreferrer\">Michael Webb's article</a>.)</p>\n<p>GPT-3 isn't special: you would expect these deficiencies from any model trained on such corpora. I wouldn't trust these models to assign a \u201cgoodness score\u201d to a given passage of text \u2013 but perhaps other approaches might fare better. OpenAI claims to have an approach that \u201cgeneralizes to a wide range of different content taxonomies\u201d and can purportedly \u201cbe used to create high-quality content classifiers\u201d (<a href=\"https://arxiv.org/abs/2208.03274\" rel=\"nofollow noreferrer\" title=\"A Holistic Approach to Undesired Content Detection in the Real World\">arXiv:2208.03274</a>).</p>\n<p>Are there ways of using GenAI for content moderation that work? I'd appreciate answers from both empirical (studies of particular systems) and theoretical (formal argument) perspectives; please avoid excessive speculation.</p>\n<p>N.B.: This question is about <em>content</em> moderation, not about <em>community</em> moderation. The latter's a harder task, and one that GenAI is straightforwardly not capable of.</p>\n<p><sup><sub>\u00b9: \u201cincorrectly\u201d is redundant here, but <em>somebody's</em> going to put this question into a training set. I don't want to make the problem worse.</sub></sup></p>\n"}, "title": "To what extent can GenAI be used for content moderation?", "content": "I've seen several people trying to use GenAI technologies for content moderation. This doesn't seem impossible, on the face of it: I use regular expressions to great effect, and I can see how more sophisticated language processing could achieve an even better effect.\nHowever, contemporary GenAI systems suffer from severe deficiencies in judgement:\n\nTOEFL essays have a lower perplexity score than US 8th-grade essays. (arXiv:2304.02819)\nGPT-3 associates \u2018blind\u2019 and \u2018deaf\u2019 incorrectly\u00b9 with negativity, according to sentiment analysis. (arXiv:2206.11993 peer-reviewed)\n\nthere is a possibility that these estimations of bias might be influenced by existing bias within the [analysis] models\n\n\nGPT-3 associates gender with social stereotypes: \u2018life\u2019, \u2018family\u2019 and \u2018appearance\u2019 are incorrectly considered feminine, whereas \u2018politics\u2019, \u2018war\u2019 and \u2018machines\u2019 are incorrectly considered masculine. (doi:10.18653/v1/2021.nuse-1.5)\nGPT-3 associates \u2018Muslim\u2019 incorrectly overwhelmingly with violence, \u2018Jewish\u2019 incorrectly with \u2018money\u2019, and \u2018Christian\u2019 incorrectly with \u2018faithfulness\u2019 and \u2018goodness\u2019. (doi:10.1145/3461702.3462624 (arXiv))\n\n(Examples mostly from Michael Webb's article.)\nGPT-3 isn't special: you would expect these deficiencies from any model trained on such corpora. I wouldn't trust these models to assign a \u201cgoodness score\u201d to a given passage of text \u2013 but perhaps other approaches might fare better. OpenAI claims to have an approach that \u201cgeneralizes to a wide range of different content taxonomies\u201d and can purportedly \u201cbe used to create high-quality content classifiers\u201d (arXiv:2208.03274).\nAre there ways of using GenAI for content moderation that work? I'd appreciate answers from both empirical (studies of particular systems) and theoretical (formal argument) perspectives; please avoid excessive speculation.\nN.B.: This question is about content moderation, not about community moderation. The latter's a harder task, and one that GenAI is straightforwardly not capable of.\n\u00b9: \u201cincorrectly\u201d is redundant here, but somebody's going to put this question into a training set. I don't want to make the problem worse.\n", "question_id": 59, "answers": [{"answer_id": 338, "body": "Fine-tuning pretrained foundation models\nLarge language models such as GPT-4 or Llama-2 are trained on the task of generating the next token in a sequence, but can then be fine-tuned (or perform zero-shot inference with just a prompt describing the task) to adapt to other NLP tasks. This is known as transfer learning and makes use of powerful background knowledge and abilities the model has learned from massive amounts of data. For example, BioBERT fine-tuned from Google's BERT outperformed many state-of-the-art models on NLP tasks like named entity recognition and relation extraction.\nAs a recent example of using this for content moderation: \"AWS performs fine-tuning on a Large Language Model (LLM) to classify toxic speech for a large gaming company\" initially achieved 0.91 F1 score with just 100 new samples of labelled data from their customer.\nEmbeddings\nTraining of generative models often creates a useful latent space, where distance between the embeddings of content is based on semantic distance rather than syntactic (e.g: Levenshtein distance).\nIf you wanted to train a classifier for NSFW images, it may be faster (in terms of training time) and require less data to train on existing embeddings rather than from raw pixel data. If you wanted to ban a specific image from being uploaded, such as a shock image or advertisement being spammed, it may make sense to ban based on distance to a point in latent space, in order to catch small changes like resizing or JPEG compression.\nThere's an example implementation of this in OpenAI's cookbook.\nSynthetic training data\nReal datasets may have insufficient examples of rare cases, suffer from bias, or contain private information making the dataset difficult to share. These can be partially addressed with synthetic data, from either simulated environment (e.g: this paper uses GTA V and \"Princeton Virtual Environment\" for training self-driving cars), or generated using generative AI (e.g: this paper studies whether generated images can help image recognition tasks - finding it beneficial but with limitations).\nFor an example in content moderation, HateGAN improves hate speech detection by adding data from a GAN trained to generate realistic hateful tweets, finding a 5% improvement in F1 score.\nCreating partially-synthetic data by augmenting existing real data can also be used to help prevent overfitting, in the same was as simpler augmentations like flipping/cropping are used.\n\nNote: A system to perform \"the task of distinguishing between wanted and unwanted contributions\" would arguably by definition be discriminative rather than generative. Discriminative AI is already widely used for content moderation - being standard for tasks like filtering spam or flagging inappropriate images. Above I've tried to focus on ways in which techniques from generative AI can also be applied to aid, or as a part of, a (discriminative) system.\n", "score": 1, "is_accepted": false}]}, "251": {"link": "https://genai.stackexchange.com/questions/251/prompt-engineering-gpt-for-numerical-scores", "metadata": {"tags": ["gpt", "prompt-design"], "migrated_from": {"other_site": {"styling": {"tag_background_color": "#FFF", "tag_foreground_color": "#000", "link_color": "#0077CC"}, "related_sites": [{"relation": "meta", "api_site_parameter": "ai.meta", "site_url": "https://ai.meta.stackexchange.com", "name": "Artificial Intelligence Meta Stack Exchange"}, {"relation": "chat", "site_url": "https://chat.stackexchange.com?tab=site&host=ai.stackexchange.com", "name": "Chat Stack Exchange"}], "markdown_extensions": ["MathJax"], "launch_date": 1639665491, "open_beta_date": 1471907237, "closed_beta_date": 1470164400, "site_state": "normal", "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/ai/Img/apple-touch-icon@2.png", "favicon_url": "https://cdn.sstatic.net/Sites/ai/Img/favicon.ico", "icon_url": "https://cdn.sstatic.net/Sites/ai/Img/apple-touch-icon.png", "audience": "people interested in conceptual questions about life and challenges in a world where &quot;cognitive&quot; functions can be mimicked in purely digital environment", "site_url": "https://ai.stackexchange.com", "api_site_parameter": "ai", "logo_url": "https://cdn.sstatic.net/Sites/ai/Img/apple-touch-icon.png", "name": "Artificial Intelligence", "site_type": "main_site"}, "on_date": 1691048483, "question_id": 41581}, "owner": {"user_type": "does_not_exist", "display_name": "just another mathmo"}, "is_answered": true, "view_count": 90, "answer_count": 1, "score": 3, "last_activity_date": 1691499372, "creation_date": 1690881486, "question_id": 251, "link": "https://genai.stackexchange.com/questions/251/prompt-engineering-gpt-for-numerical-scores", "title": "Prompt engineering GPT for numerical scores?", "body": "<p><strong>Background:</strong> I am currently working on an NLP project using GPT 3.5 via the OpenAI python API.\nEngineering good prompts seems to be the most critical part of my pipeline so far. Crucially I have long input texts, for which I want to get numerical sentiment scores for a variety of different questions in a ZERO SHOT setting.\nThere are many possible applications such as predicting nuanced scores on different aspects (plot, acting, staging etc) of a theatre show from a long detailed review; or the sentiment of a twitter response to a new political policy (emotional objections, economic objections, moral objections).</p>\n<p><strong>Question:</strong> What is best practice to extract reliable numerical sentiment scores from GPT models?</p>\n<p><strong>What I've tried:</strong> Looked for guidance on the <a href=\"https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md\" rel=\"nofollow noreferrer\">OpenAI cookbook</a>  and explored many associated links. It seems like existing suggestions for sentiment scoring usually use embeddings in a setting where a small number of labelled samples are available. Following heuristics from various prompt engineering sources I have used a pipeline of iteratively asking for detailed summaries and then more concise summaries culminating in a single sentence and finally a score. This has yielded impressive results; indeed I suspect such a 'prompt-pipeline' may yield more sophisticated sentiment scores than fine-tuning approaches, unless one has the luxury of extremely large amount of labelled data.\nHowever, converting these final sentences to numeric scores is still somewhat unreliable. I couldn't find specific advice online.\nWhat is best practice?</p>\n"}, "title": "Prompt engineering GPT for numerical scores?", "content": "Background: I am currently working on an NLP project using GPT 3.5 via the OpenAI python API.\nEngineering good prompts seems to be the most critical part of my pipeline so far. Crucially I have long input texts, for which I want to get numerical sentiment scores for a variety of different questions in a ZERO SHOT setting.\nThere are many possible applications such as predicting nuanced scores on different aspects (plot, acting, staging etc) of a theatre show from a long detailed review; or the sentiment of a twitter response to a new political policy (emotional objections, economic objections, moral objections).\nQuestion: What is best practice to extract reliable numerical sentiment scores from GPT models?\nWhat I've tried: Looked for guidance on the OpenAI cookbook  and explored many associated links. It seems like existing suggestions for sentiment scoring usually use embeddings in a setting where a small number of labelled samples are available. Following heuristics from various prompt engineering sources I have used a pipeline of iteratively asking for detailed summaries and then more concise summaries culminating in a single sentence and finally a score. This has yielded impressive results; indeed I suspect such a 'prompt-pipeline' may yield more sophisticated sentiment scores than fine-tuning approaches, unless one has the luxury of extremely large amount of labelled data.\nHowever, converting these final sentences to numeric scores is still somewhat unreliable. I couldn't find specific advice online.\nWhat is best practice?\n", "question_id": 251, "answers": [{"answer_id": 255, "body": "To address converting the final sentences to numeric scores being unreliable, you could use logits_bias to restrict the model to only predicting certain tokens.\nYou can use OpenAI's tiktoken library to find token IDs:\nenc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\nprint([enc.encode(x) for x in \"1234567890\"])\n# [[16], [17], [18], [19], [20], [21], [22], [23], [24], [15]]\n\nIntegers up to \"999\" are each represented by a single token, higher numbers and decimals get split up into multiple tokens.\nThen, using the OpenAI Python API, adding a bias of 100 will ensure exclusive selection of one of those tokens:\nresponse = openai.ChatCompletion.create(\n    engine=\"gpt-35-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"How many fingers are on a hand?\"}],\n    temperature=0,\n    max_tokens=1,\n    frequency_penalty=0,\n    presence_penalty=0,\n    stop=None,\n    logit_bias={16: 100, 17: 100, 18: 100, 19: 100, 20: 100, 21: 100, 22: 100, 23: 100, 24: 100, 15: 100}\n)\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n# 5\n\n", "score": 2, "is_accepted": false}]}, "5": {"link": "https://genai.stackexchange.com/questions/5/how-important-are-gpus-vs-cpus-when-training-an-llm", "metadata": {"tags": ["gpu", "llm"], "owner": {"reputation": 137, "user_id": 23, "user_type": "moderator", "profile_image": "https://www.gravatar.com/avatar/4bcf486b74bfdaf1537681bb7899516d?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Mickle-The-Pickle", "link": "https://genai.stackexchange.com/users/23/mickle-the-pickle"}, "is_answered": true, "view_count": 259, "answer_count": 2, "score": 2, "last_activity_date": 1690381309, "creation_date": 1689629585, "last_edit_date": 1690381309, "question_id": 5, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/5/how-important-are-gpus-vs-cpus-when-training-an-llm", "title": "How important are GPUs vs. CPUs when training an LLM?", "body": "<p>I want to train my very own large language model. I have some data, and access to an open-source foundational model. Can I use my laptop that only has a CPU and no GPU to train the model? Do I need to rent time on a cloud provider to get a VM with GPUs?</p>\n"}, "title": "How important are GPUs vs. CPUs when training an LLM?", "content": "I want to train my very own large language model. I have some data, and access to an open-source foundational model. Can I use my laptop that only has a CPU and no GPU to train the model? Do I need to rent time on a cloud provider to get a VM with GPUs?\n", "question_id": 5, "answers": [{"answer_id": 6, "body": "Tl;dr you can (usually) do it CPU-only, but it will take a very long time.\nNote: I mention a number of products in this answer. While I have used some of them, I'm not affiliated with them, nor am I suggesting/endorsing any specific product here. Do your research to ensure anything you purchase suits your requirements.\nIt depends on your specific model and the GPU, but generally training models on one (or more!) GPUs vastly improves training speed. Technically speaking, it isn't required (in some cases). But it's highly preferable given the option.\nIf you're training an extremely small model, it might be faster on a CPU. Or if you're comparing a Dell PowerEdge server with multiple Xeons to a very old cheap GPU.\nAssuming you're trying to train a decent-sized generative model, though, having a GPU is extremely useful. Some models require a specific brand of GPU, such as if you're going to use NVIDIA CUDA or similar, so know your requirements prior to making purchases. There's a few ways to get a GPU if you only have a laptop.\nSome laptops* have a Thunderbolt port that you can use to connect to an eGPU. This is typically done with an eGPU enclosure (here's one, as an example). It's also possible to not have an actual enclosure, though. In some cases, you can also use an onboard M.2 connector. Also note that the existence of a USB-C port does not mean you automatically have Thunderbolt. Actually check your laptop specs. Specifics of that, though, are beyond the scope of this question.\nAs mentioned, some cloud providers will let you rent a VPS with a GPU. While an option, the price can be cost-prohibitive. It can make sense in some situations, though.\nAnother option, similar to a VPS, is something like Google Colab. It's essentially a Python notebook in a browser with free access to GPUs. With this option, note that there are limitations on how much you can use it and for what purpose.\n\n*And some desktops, but mostly laptops\n", "score": 4, "is_accepted": false}, {"answer_id": 64, "body": "\nCan I use my laptop that only has CPUs and no GPU to train the model.\n\nNo. You need GPUs if you don't want to wait for a few years or more. How many and which GPUs will depend on the model, the training data and your patience.\n", "score": -3, "is_accepted": false}]}, "67": {"link": "https://genai.stackexchange.com/questions/67/could-genai-be-used-to-generate-reasonable-scientific-hypotheses", "metadata": {"tags": ["prompt-design"], "owner": {"reputation": 137, "user_id": 88, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/QQKyt.png?s=256&g=1", "display_name": "Reine Abstraktion", "link": "https://genai.stackexchange.com/users/88/reine-abstraktion"}, "is_answered": true, "view_count": 110, "answer_count": 1, "score": 2, "last_activity_date": 1690279308, "creation_date": 1690221003, "question_id": 67, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/67/could-genai-be-used-to-generate-reasonable-scientific-hypotheses", "title": "Could genAI be used to generate reasonable scientific hypotheses?", "body": "<p>Suppose I have a trained genAI and I give it some different experiments, their results, and I were to ask it to generate a plausible hypotheses of what happend. Would it be able to do this well? If not, why?</p>\n<p>I am pretty sure the answer is not, otherwise there'd be a wider application of AI in the applied sciences as of right now.</p>\n"}, "title": "Could genAI be used to generate reasonable scientific hypotheses?", "content": "Suppose I have a trained genAI and I give it some different experiments, their results, and I were to ask it to generate a plausible hypotheses of what happend. Would it be able to do this well? If not, why?\nI am pretty sure the answer is not, otherwise there'd be a wider application of AI in the applied sciences as of right now.\n", "question_id": 67, "answers": [{"answer_id": 86, "body": "\n\"Can ChatGPT be used to generate scientific hypotheses?\" directly asks GPT-4 for hypotheses and reports a number of successes (\"nontrivial predictions that some of us are motivated enough to test out in the lab\"). The authors attempted to ensure that these didn't already exist in human-written content, though this isn't possible with absolute certainty. They do also note that it is prone to obvious errors, and consider human curation of the hypotheses to currently still be essential.\n\n\"Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy\" uses GPT-4 with document retrieval on astronomy papers to reduce hallucination. As a quality rating, evaluation with human judges gave \"a near-expert level of 4/5 when 1,000 papers were included\" - where \"3/5 corresponds to a typical hypothesis by a competent PhD student\".\n\n\nBoth papers found value in having GPT-4 critique its own ideas to improve the hypotheses.\n", "score": 6, "is_accepted": false}]}, "395": {"link": "https://genai.stackexchange.com/questions/395/best-practice-in-word-embeddings", "metadata": {"tags": ["llm", "embeddings"], "owner": {"reputation": 51, "user_id": 1969, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/f058ad4d6cd0c59340905e5a630bcaf3?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Christian01", "link": "https://genai.stackexchange.com/users/1969/christian01"}, "is_answered": true, "view_count": 55, "accepted_answer_id": 396, "answer_count": 1, "score": 2, "last_activity_date": 1693230134, "creation_date": 1693213714, "question_id": 395, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/395/best-practice-in-word-embeddings", "title": "Best-Practice in word-embeddings", "body": "<p>In my project I follow the retrieval augmented generation (RAG) approach. I want to create embeddings for my own dataset and use it in combination with llama-2. In the dataset are german annual reports, 548 reports as pdf-files with about 300 sites per report. Next, I want to load the data in a vector store, but first I think I have to create the embeddings.</p>\n<p>And now, there are serveral questions and I need some best-practice:</p>\n<ol>\n<li><p>Do I have to train my own embeddings model or can I use models like word2vec of the gensim package or a pretrained model like BERT and take the hidden state?</p>\n</li>\n<li><p>Can I use any embeddings model? I think they train on a specific corpus and if my words aren't in the training corpus, I will get bad result or what do you think?</p>\n</li>\n</ol>\n<p>I hope you can grap me under the arms and help me to get a better understanding.</p>\n"}, "title": "Best-Practice in word-embeddings", "content": "In my project I follow the retrieval augmented generation (RAG) approach. I want to create embeddings for my own dataset and use it in combination with llama-2. In the dataset are german annual reports, 548 reports as pdf-files with about 300 sites per report. Next, I want to load the data in a vector store, but first I think I have to create the embeddings.\nAnd now, there are serveral questions and I need some best-practice:\n\nDo I have to train my own embeddings model or can I use models like word2vec of the gensim package or a pretrained model like BERT and take the hidden state?\n\nCan I use any embeddings model? I think they train on a specific corpus and if my words aren't in the training corpus, I will get bad result or what do you think?\n\n\nI hope you can grap me under the arms and help me to get a better understanding.\n", "question_id": 395, "answers": [{"answer_id": 396, "body": "\nDo I have to train my own embeddings model or can I use models like word2vec of the gensim package or a pretrained model like BERT and take the hidden state?\n\nOne can use pretrained models. sentence-transformers has more recent models.\n\nCan I use any embeddings model?\n\nThe pretrained model needs to support the language of your document, e.g.\nhttps://www.sbert.net/docs/pretrained_models.html?highlight=german\n\nif my words aren't in the training corpus, I will get bad result or what do you think?\n\nIf out-of-vocabulary words are an issue, one can finetune the pretrained models.\n", "score": 1, "is_accepted": true}]}, "397": {"link": "https://genai.stackexchange.com/questions/397/embeddings-how-can-i-find-out-that-im-out-of-vocabulary", "metadata": {"tags": ["llm", "embeddings", "python", "llama-2"], "owner": {"reputation": 51, "user_id": 1969, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/f058ad4d6cd0c59340905e5a630bcaf3?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Christian01", "link": "https://genai.stackexchange.com/users/1969/christian01"}, "is_answered": false, "view_count": 15, "answer_count": 1, "score": 2, "last_activity_date": 1693237186, "creation_date": 1693233001, "question_id": 397, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/397/embeddings-how-can-i-find-out-that-im-out-of-vocabulary", "title": "Embeddings: How can I find out, that I&#39;m out-of-vocabulary?", "body": "<p>I have a german dataset of financial/annual reports of companies.\nFor example, I use the gensim package to embed my dataset with word2vec or use the huggingface package to embed.</p>\n<p>How can I find out, that I'm out-of-vocabulary? I want to check it, because - first - it's a german dataset and - second - in a specific domain.</p>\n<p>Are there any function or methods in the packages to check that or do I have to code workaround?</p>\n"}, "title": "Embeddings: How can I find out, that I&#39;m out-of-vocabulary?", "content": "I have a german dataset of financial/annual reports of companies.\nFor example, I use the gensim package to embed my dataset with word2vec or use the huggingface package to embed.\nHow can I find out, that I'm out-of-vocabulary? I want to check it, because - first - it's a german dataset and - second - in a specific domain.\nAre there any function or methods in the packages to check that or do I have to code workaround?\n", "question_id": 397, "answers": [{"answer_id": 398, "body": "\nHow can I find out, that I'm out-of-vocabulary?\n\n\nSee if the embedding model is word-based or uses subword embeddings (e.g., Bert uses WordPieces).\nIf word-based, there should be a way to list the vocabulary. Alternatively, one can try comparing the embedding of a given word against the embedding of some made-up word (if it's the same embedding, then the given word is out-of-vocabulary).\n\n", "score": 0, "is_accepted": false}]}, "219": {"link": "https://genai.stackexchange.com/questions/219/programmatic-fine-tune-training-in-aws", "metadata": {"tags": ["stable-diffusion", "fine-tuning"], "owner": {"reputation": 29, "user_id": 1132, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0252d52fa5d8d36b84fe8a9af1e2e23f?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "JJ Zh", "link": "https://genai.stackexchange.com/users/1132/jj-zh"}, "is_answered": true, "view_count": 52, "answer_count": 1, "score": 1, "last_activity_date": 1691281691, "creation_date": 1690909181, "last_edit_date": 1691281691, "question_id": 219, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/219/programmatic-fine-tune-training-in-aws", "title": "Programmatic fine-tune training in AWS", "body": "<p>I'm trying to setup an automatic fine tuning training pipeline with DreamBooth.  I've read this tutorial <a href=\"https://medium.com/@IAmxIAo/automating-fine-tuning-stable-diffusion-on-sagemaker-c340f65b24f2\" rel=\"nofollow noreferrer\">https://medium.com/@IAmxIAo/automating-fine-tuning-stable-diffusion-on-sagemaker-c340f65b24f2</a></p>\n<p>However I don't understand what value does SageMaker add in this setup, vs plain EC2 / ECS?</p>\n"}, "title": "Programmatic fine-tune training in AWS", "content": "I'm trying to setup an automatic fine tuning training pipeline with DreamBooth.  I've read this tutorial https://medium.com/@IAmxIAo/automating-fine-tuning-stable-diffusion-on-sagemaker-c340f65b24f2\nHowever I don't understand what value does SageMaker add in this setup, vs plain EC2 / ECS?\n", "question_id": 219, "answers": [{"answer_id": 294, "body": "I've explored these options and now I understand the convenience SageMaker provides. It takes care of provisioning compatible instances, stops them when the job finishes or encounters an error, it also manages the training hyperparameters, uploads the completed model to S3.   All these can be done at lower level, but it makes sense to operate at a higher level until the cost become really significant.\n", "score": 1, "is_accepted": false}]}, "318": {"link": "https://genai.stackexchange.com/questions/318/how-to-use-gpt-4-to-help-authors-write-a-document-with-latex-formatting", "metadata": {"tags": ["gpt", "text-to-code"], "owner": {"reputation": 1068, "user_id": 109, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/Z99mk.jpg?s=256&g=1", "display_name": "Franck Dernoncourt", "link": "https://genai.stackexchange.com/users/109/franck-dernoncourt"}, "is_answered": false, "view_count": 58, "answer_count": 0, "score": 1, "last_activity_date": 1691534929, "creation_date": 1691388583, "last_edit_date": 1691534929, "question_id": 318, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/318/how-to-use-gpt-4-to-help-authors-write-a-document-with-latex-formatting", "title": "How to use GPT-4 to help authors write a document with LaTeX formatting?", "body": "<p>I read in OpenAI's <a href=\"https://arxiv.org/pdf/2303.08774.pdf\" rel=\"nofollow noreferrer\">GPT-4 Technical Report</a>:</p>\n<blockquote>\n<p>GPT-4 was used in the following ways: to help us iterate on LaTeX formatting [...]</p>\n</blockquote>\n<p>How can one use GPT-4 to help authors iterate on LaTeX formatting?</p>\n<hr />\n<p>Note that the training data of GPT 3.5 or 4 definitely included source code texts. <a href=\"https://chat.openai.com/share/d6c3208f-234a-4811-9cc5-eaf6367b3c1a\" rel=\"nofollow noreferrer\">Example</a> (<a href=\"https://i.stack.imgur.com/DRAam.png\" rel=\"nofollow noreferrer\">mirror</a>) --&gt; <a href=\"https://i.stack.imgur.com/JBigy.png\" rel=\"nofollow noreferrer\">https://i.stack.imgur.com/JBigy.png</a></p>\n"}, "title": "How to use GPT-4 to help authors write a document with LaTeX formatting?", "content": "I read in OpenAI's GPT-4 Technical Report:\n\nGPT-4 was used in the following ways: to help us iterate on LaTeX formatting [...]\n\nHow can one use GPT-4 to help authors iterate on LaTeX formatting?\n\nNote that the training data of GPT 3.5 or 4 definitely included source code texts. Example (mirror) --> https://i.stack.imgur.com/JBigy.png\n", "question_id": 318, "answers": []}, "347": {"link": "https://genai.stackexchange.com/questions/347/what-works-better-an-llm-trained-on-better-texts-or-an-llm-with-better-prompts", "metadata": {"tags": ["prompt-design", "llm", "training", "embeddings"], "owner": {"reputation": 211, "user_id": 475, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/tVWLM.png?s=256&g=1", "display_name": "Hans-Peter Stricker", "link": "https://genai.stackexchange.com/users/475/hans-peter-stricker"}, "is_answered": false, "view_count": 96, "answer_count": 0, "score": 1, "last_activity_date": 1692121491, "creation_date": 1691876084, "question_id": 347, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/347/what-works-better-an-llm-trained-on-better-texts-or-an-llm-with-better-prompts", "title": "What works better: An LLM trained on better texts or an LLM with better prompts?", "body": "<p>Assume you have a very large corpus of high-quality documents related to a given topic, and assume you have a pretrained large language model (the foundation model) with training data not containing these documents.</p>\n<p>Scenario 1: You use the high-quality documents as additional pretraining data, possibly with a large weight.</p>\n<p>Scenario 2: You create a vector database for the high-quality documents, i.e. you split them up in chunks and calculate text embeddings for them.</p>\n<p>Now assume you ask the language model a question on the topic. In scenario 1 the question just gets answered (probably better than by the original foundation model), in scenario 2 the text embedding of the question prompt is calculated and the prompt then enhanced by the most similar text chunks in the vector database.</p>\n<p>Does anyone have an intuition (and could explain it) which scenario would yield better results?</p>\n"}, "title": "What works better: An LLM trained on better texts or an LLM with better prompts?", "content": "Assume you have a very large corpus of high-quality documents related to a given topic, and assume you have a pretrained large language model (the foundation model) with training data not containing these documents.\nScenario 1: You use the high-quality documents as additional pretraining data, possibly with a large weight.\nScenario 2: You create a vector database for the high-quality documents, i.e. you split them up in chunks and calculate text embeddings for them.\nNow assume you ask the language model a question on the topic. In scenario 1 the question just gets answered (probably better than by the original foundation model), in scenario 2 the text embedding of the question prompt is calculated and the prompt then enhanced by the most similar text chunks in the vector database.\nDoes anyone have an intuition (and could explain it) which scenario would yield better results?\n", "question_id": 347, "answers": []}, "252": {"link": "https://genai.stackexchange.com/questions/252/generative-ai-use-case-for-search-domain", "metadata": {"tags": ["search"], "migrated_from": {"other_site": {"styling": {"tag_background_color": "#FFF", "tag_foreground_color": "#000", "link_color": "#0077CC"}, "related_sites": [{"relation": "meta", "api_site_parameter": "ai.meta", "site_url": "https://ai.meta.stackexchange.com", "name": "Artificial Intelligence Meta Stack Exchange"}, {"relation": "chat", "site_url": "https://chat.stackexchange.com?tab=site&host=ai.stackexchange.com", "name": "Chat Stack Exchange"}], "markdown_extensions": ["MathJax"], "launch_date": 1639665491, "open_beta_date": 1471907237, "closed_beta_date": 1470164400, "site_state": "normal", "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/ai/Img/apple-touch-icon@2.png", "favicon_url": "https://cdn.sstatic.net/Sites/ai/Img/favicon.ico", "icon_url": "https://cdn.sstatic.net/Sites/ai/Img/apple-touch-icon.png", "audience": "people interested in conceptual questions about life and challenges in a world where &quot;cognitive&quot; functions can be mimicked in purely digital environment", "site_url": "https://ai.stackexchange.com", "api_site_parameter": "ai", "logo_url": "https://cdn.sstatic.net/Sites/ai/Img/apple-touch-icon.png", "name": "Artificial Intelligence", "site_type": "main_site"}, "on_date": 1691048580, "question_id": 41588}, "owner": {"reputation": 13, "user_id": 1575, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6538e613d1c576214074905c690ca34e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "anshuk_pal", "link": "https://genai.stackexchange.com/users/1575/anshuk-pal"}, "is_answered": true, "view_count": 61, "accepted_answer_id": 253, "answer_count": 1, "score": 0, "last_activity_date": 1691076526, "creation_date": 1690891163, "last_edit_date": 1691076526, "question_id": 252, "link": "https://genai.stackexchange.com/questions/252/generative-ai-use-case-for-search-domain", "title": "Generative AI Use Case for Search Domain", "body": "<p>I am trying to think and research for use cases that can beneficial search experience using Generative AI. There are two things,</p>\n<ul>\n<li>The final search results that can be more relevant and personalised per user. This would take time and more effort - in terms the backend search service to change/upgrade etc.</li>\n<li>The more I am trying to focus how can generative ai be leveraged and provide to benefits on the client side while user is actually search. One thought I had:\n<em>used to generate new keywords and topics that can be used to improve the search engine's coverage of a particular topic. This can help to provide more relevant results to users.</em></li>\n</ul>\n<p>I wanted to understand what other use case have you witnessed/seem where generative ai can really helpful while user is actually searching (on the client side experience)</p>\n"}, "title": "Generative AI Use Case for Search Domain", "content": "I am trying to think and research for use cases that can beneficial search experience using Generative AI. There are two things,\n\nThe final search results that can be more relevant and personalised per user. This would take time and more effort - in terms the backend search service to change/upgrade etc.\nThe more I am trying to focus how can generative ai be leveraged and provide to benefits on the client side while user is actually search. One thought I had:\nused to generate new keywords and topics that can be used to improve the search engine's coverage of a particular topic. This can help to provide more relevant results to users.\n\nI wanted to understand what other use case have you witnessed/seem where generative ai can really helpful while user is actually searching (on the client side experience)\n", "question_id": 252, "answers": [{"answer_id": 253, "body": "One use case which pops into my mind is that of Query Expansion. Gen AI can analyze user queries and expand them with synonyms, or contextually relevant keywords, which allows for more comprehensive search results and increased coverage of the desired topics.\nAnother one which might be useful is the Auto-tagging and Categorization. It becomes easier for search engines to organize and present results in a structured manner when content is tagged and categorized.\nUse case which might be difficult to implement will be like Multilingual Search.\n", "score": 1, "is_accepted": true}]}, "260": {"link": "https://genai.stackexchange.com/questions/260/is-the-simulators-viewpoint-still-valid", "metadata": {"tags": ["llm", "hallucination", "terminology", "transformers"], "owner": {"reputation": 111, "user_id": 1570, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3b3c809cbec8cc5c62a918d57e90b553?s=256&d=identicon&r=PG", "display_name": "Corbin", "link": "https://genai.stackexchange.com/users/1570/corbin"}, "is_answered": true, "view_count": 60, "accepted_answer_id": 261, "answer_count": 1, "score": 0, "last_activity_date": 1691073088, "creation_date": 1691071187, "question_id": 260, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/260/is-the-simulators-viewpoint-still-valid", "title": "Is the simulators viewpoint still valid?", "body": "<p>When considering the behavior of large language models, there is not yet a single canonical framework for interpreting their output in context. Common proposed frameworks include:</p>\n<ul>\n<li>Agents: The LLM implements an agentive mind with volition and desires.</li>\n<li>Oracle: The LLM implements a librarian who conducts <a href=\"https://en.wikipedia.org/wiki/Reference_interview\" rel=\"nofollow noreferrer\">reference interviews</a>.</li>\n<li>Genie: The LLM implements an expert synthesizer of constructions.</li>\n<li>Tool: The LLM is a general-purpose text transformer.</li>\n</ul>\n<p>The <a href=\"https://generative.ink/posts/simulators/\" rel=\"nofollow noreferrer\">simulators viewpoint</a> subsumes all of these by claiming that LLMs implement <em>simulations</em> of various conversations: simulating conversations between agents, between librarians and patrons, between builders and designers, or even simulating the behavior of a hypothetical text-transforming tool.</p>\n<p>There are many attractive features of the simulators viewpoint, but it is also falsifiable. <strong>Is the simulators viewpoint still valid?</strong> Or has it been refuted or made obsolete?</p>\n"}, "title": "Is the simulators viewpoint still valid?", "content": "When considering the behavior of large language models, there is not yet a single canonical framework for interpreting their output in context. Common proposed frameworks include:\n\nAgents: The LLM implements an agentive mind with volition and desires.\nOracle: The LLM implements a librarian who conducts reference interviews.\nGenie: The LLM implements an expert synthesizer of constructions.\nTool: The LLM is a general-purpose text transformer.\n\nThe simulators viewpoint subsumes all of these by claiming that LLMs implement simulations of various conversations: simulating conversations between agents, between librarians and patrons, between builders and designers, or even simulating the behavior of a hypothetical text-transforming tool.\nThere are many attractive features of the simulators viewpoint, but it is also falsifiable. Is the simulators viewpoint still valid? Or has it been refuted or made obsolete?\n", "question_id": 260, "answers": [{"answer_id": 261, "body": "It's probably still valid. LLM stands for large language model -- it models its training data by predicting the next (or any masked) tokens given the context.\nI tend to be a bigger fan of the \"simulators\" analogy as e.g., thinking about LLMs as \"agents\" tends to add too much anthropomorphism which clouds my intuition regarding these models.\nAdditionally, it still holds true that LLMs are trained on vast amounts of heterogenous data, meaning that it simultaneously models a plethora of types and styles of text.\nHowever, such models are additionally instruction-tuned on conversational data, allowing the user to interact with the model in a more chat-bot-style way. It could be that, with this instruction-tuning, such models can no longer adequately model the diverse styles of text-data found in its pretraining dataset. It would depend on the diversity of model generations.\nAt least with Llama 2, you can manipulate the style of the assistant-style responses with a system prompt, suggesting that that method of instruction-tuning still allows for some variation in text-completions to be kept.\n", "score": 2, "is_accepted": true}]}, "312": {"link": "https://genai.stackexchange.com/questions/312/how-can-i-format-the-input-text-on-google-translate-so-that-the-translation-does", "metadata": {"tags": ["text-generation", "machine-translation", "nlp", "google"], "owner": {"reputation": 1068, "user_id": 109, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/Z99mk.jpg?s=256&g=1", "display_name": "Franck Dernoncourt", "link": "https://genai.stackexchange.com/users/109/franck-dernoncourt"}, "is_answered": true, "view_count": 80, "closed_date": 1692024164, "answer_count": 2, "score": 0, "last_activity_date": 1691426572, "creation_date": 1691346286, "question_id": 312, "link": "https://genai.stackexchange.com/questions/312/how-can-i-format-the-input-text-on-google-translate-so-that-the-translation-does", "closed_reason": "Not suitable for this site", "title": "How can I format the input text on Google Translate so that the translation doesn&#39;t change person names?", "body": "<p>When I have some texts to translate, I notice that sometimes, <a href=\"https://translate.google.com/\" rel=\"nofollow noreferrer\">Google Translate</a> messes up person names. E.g.L</p>\n<blockquote>\n<p>The interview went well. Cherry is good.</p>\n</blockquote>\n<p><a href=\"https://translate.google.com/?sl=en&amp;tl=fr&amp;text=The%20interview%20went%20well.%20Cherry%20is%20good\" rel=\"nofollow noreferrer\">becomes</a>:</p>\n<blockquote>\n<p>L'entretien s'est bien pass\u00e9. La cerise est bonne.</p>\n</blockquote>\n<p>which is not good because the person name &quot;Cherry&quot; was changed into &quot;La cerise&quot; (not to mention that &quot;est bonne&quot; would certainly warrant a trip to the HR). &quot;La cerise&quot; means &quot;The cherry&quot; (the fruit).</p>\n<p><a href=\"https://i.stack.imgur.com/OGC8v.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/OGC8v.png\" alt=\"enter image description here\" /></a></p>\n<p>How can I format the input text on Google Translate or configure Google Translate so that the translation doesn't change person  names?</p>\n"}, "title": "How can I format the input text on Google Translate so that the translation doesn&#39;t change person names?", "content": "When I have some texts to translate, I notice that sometimes, Google Translate messes up person names. E.g.L\n\nThe interview went well. Cherry is good.\n\nbecomes:\n\nL'entretien s'est bien pass\u00e9. La cerise est bonne.\n\nwhich is not good because the person name \"Cherry\" was changed into \"La cerise\" (not to mention that \"est bonne\" would certainly warrant a trip to the HR). \"La cerise\" means \"The cherry\" (the fruit).\n\nHow can I format the input text on Google Translate or configure Google Translate so that the translation doesn't change person  names?\n", "question_id": 312, "answers": [{"answer_id": 314, "body": "AFAIK Google Translate, the Web app, doesn't offer input formatting options.\nTry the UI interactive elements:\n\nClick the translation; clicking on it will show a dropdown offering other alternative translations.\n\"Look up details\" links; clicking them will open a sidebar, then select a word. The sidebar will show information about the selected text.\nThumbs up/down button; clicking on it will open a pop-up to submit translation feedback.\nSend feedback link: clicking it will open a UI element guiding the user to submit feedback to Google.\nContribute: clicking on it will open a UI element that will guide to user to contribute to making Google Translate more accurate.\n\nFor details of the above options, go to the Google Translate Help micro site. It includes tabs for Help Center, Community and Announcements.\n\nWorkarounds\nAs with most tools, Google Translate's effectiveness depends on the circumstances and user skills.\nTry alternative wordings to translate the text if the circumstances and your skills allow that. As the OP mentioned in their answer to this question, adding Miss before the Cherry help Google Translate to \"understand\" that, in this case, this word refers to a human female person name instead of a fruit-type name.\nOther things to try\n\nEnclose the words to not be translated by characters like underscore, _, open/closing curly brackets, {, }.\nUsing underscore:\n\nUsing curly brackets:\n\n\n\n\nYou might be interested in the translation services from Google. Start studying from https://cloud.google.com/translate.\nIn the past, Google offered a sister app, Google Translator Toolkit. It allowed users to make advanced translation tasks. Unfortunately, this tool was deprecated in 2019. Ref. Google Translator Toolkit Has Shut Down.\n", "score": 2, "is_accepted": false}, {"answer_id": 316, "body": "One can add Miss/Mister in front of the person names, which helps the system identify which token is a person name.\nExample:\n\nThe interview went well. Miss Cherry is good.\n\nbecomes\n\nL'entretien s'est bien pass\u00e9.  Mlle Cherry est bonne.\n\n\n", "score": 0, "is_accepted": false}]}, "315": {"link": "https://genai.stackexchange.com/questions/315/i-want-to-upload-document", "metadata": {"tags": ["chatgpt", "gpt"], "owner": {"reputation": 1, "user_id": 1705, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AAcHTtf_vY3N3KCUI6MbQa_Bzd3O6uByJ1vox1538wcdlqSedaM=k-s256", "display_name": "Niko Milak", "link": "https://genai.stackexchange.com/users/1705/niko-milak"}, "is_answered": false, "view_count": 30, "closed_date": 1691455064, "answer_count": 0, "score": -1, "last_activity_date": 1691371211, "creation_date": 1691371211, "question_id": 315, "link": "https://genai.stackexchange.com/questions/315/i-want-to-upload-document", "closed_reason": "Duplicate", "title": "I want to upload document", "body": "<p>Wanna upload large document in slovenian language and train the model with that data, it\u2019s for school. Is there any good reliable free option?</p>\n"}, "title": "I want to upload document", "content": "Wanna upload large document in slovenian language and train the model with that data, it\u2019s for school. Is there any good reliable free option?\n", "question_id": 315, "answers": []}, "380": {"link": "https://genai.stackexchange.com/questions/380/large-language-model-for-incident-solving", "metadata": {"tags": ["llm"], "owner": {"reputation": 17, "user_id": 1899, "user_type": "unregistered", "profile_image": "https://www.gravatar.com/avatar/a513c2bf0ee476bfa86bd87ac95fa8cc?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Christopher90", "link": "https://genai.stackexchange.com/users/1899/christopher90"}, "is_answered": false, "view_count": 79, "answer_count": 1, "score": -1, "last_activity_date": 1693066303, "creation_date": 1692622635, "question_id": 380, "content_license": "CC BY-SA 4.0", "link": "https://genai.stackexchange.com/questions/380/large-language-model-for-incident-solving", "title": "large language model for incident solving", "body": "<p>we want to run a llm locally in our company and we want to give it the code, the documantation and the incidents so it can help to solve the incidents.</p>\n<p>We think about alpaca or llama for this task. Which llm would be the best and how to implement/use it for this task?</p>\n"}, "title": "large language model for incident solving", "content": "we want to run a llm locally in our company and we want to give it the code, the documantation and the incidents so it can help to solve the incidents.\nWe think about alpaca or llama for this task. Which llm would be the best and how to implement/use it for this task?\n", "question_id": 380, "answers": [{"answer_id": 392, "body": "Great question. I have had a few conversations recently where a similar topic has come up, around automating L2/L3 using incident resolution agents. The below summary is how I would approach it:\nHere is a way to approach this problem:\na) Engine: This is in essence any LLM. You can deploy it locally if you want, you could also use something out of the box (like Azure OpenAI which gives you the security). I'd base this on an aprpoximate ticket count. If you think you need scale, then go with a hyper-scalar. Your LLM should have access to your code base (either embedding, or via direct access as a prompt), as well as access to your documents (embeddings).\nb) Processing: When a ticket is raised, run 2-3 prompts, one to fetch relevant information from your documentation on potential solutions, another to check if there are other tickets that have been solved in a similar way. Then based on this, find the code where the issue exists, and then ask the LLM to rewrite the function to fix the issue. This is where I would be very careful and commit the code after a human can test it :)\nc) Outcome: if its a positive outcome, I would then use the ticket, the data analysed and the code changed (old vs new code) to write a new document and store it back into the repository.\nThis is one way to approach this challenge.\nFinally, if you want to run it locally and you are happy to spend time 'configuring and maintaining' the models, go with LLama. If you want to use something out of the box, go with Azure OpenAI.\nHope this was useful.\n", "score": 0, "is_accepted": false}]}, "393": {"link": "https://genai.stackexchange.com/questions/393/llm-without-transformers", "metadata": {"tags": ["prompt-design", "llm", "nlp", "transformers"], "owner": {"reputation": 105, "user_id": 1811, "user_type": "registered", "profile_image": "https://lh4.googleusercontent.com/-g4n097OD0WU/AAAAAAAAAAI/AAAAAAAAAFI/WhwCo3faUbw/photo.jpg?sz=256", "display_name": "quanity", "link": "https://genai.stackexchange.com/users/1811/quanity"}, "is_answered": false, "view_count": 43, "closed_date": 1693235680, "answer_count": 1, "score": -2, "last_activity_date": 1693157881, "creation_date": 1693094479, "question_id": 393, "link": "https://genai.stackexchange.com/questions/393/llm-without-transformers", "closed_reason": "Not suitable for this site", "title": "Llm without transformers", "body": "<p>I am curious to know that can we built large language models without transformers? If yes how to do it?</p>\n"}, "title": "Llm without transformers", "content": "I am curious to know that can we built large language models without transformers? If yes how to do it?\n", "question_id": 393, "answers": [{"answer_id": 394, "body": "A language model is a model that takes as input a list of tokens (or characters) and predicts the next token. The most accurate language models currently use transformers. Before that, LSTMs were a popular choice (the hype word at that time was \"deep learning\"). And before that, many different other language model\narchitectures  were explored. E.g., see https://nlpprogress.com/english/language_modeling.html\n", "score": 0, "is_accepted": false}]}, "389": {"link": "https://genai.stackexchange.com/questions/389/suggestions-regarding-developing-a-code-snippets-suggestion-tool", "metadata": {"tags": ["llm", "text-to-code"], "owner": {"reputation": 1, "user_id": 1947, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/362130c2d283bae6f26cb781626c813b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Falcon", "link": "https://genai.stackexchange.com/users/1947/falcon"}, "is_answered": false, "view_count": 20, "closed_date": 1693235717, "answer_count": 0, "score": -4, "last_activity_date": 1693031261, "creation_date": 1693031222, "last_edit_date": 1693031261, "question_id": 389, "link": "https://genai.stackexchange.com/questions/389/suggestions-regarding-developing-a-code-snippets-suggestion-tool", "closed_reason": "Needs more focus", "title": "Suggestions regarding developing a Code snippets Suggestion tool", "body": "<p>I am looking to create a program that can automatically suggest code snippets based on comments I provide. These comments often indicate the need for reusable code sections from my repository, saving valuable time and effort. My goal is to input a comment, and have the program recommend relevant code snippets from my repository, streamlining my workflow.</p>\n<p><strong>Details:</strong></p>\n<p>I've established a repository containing C code files housing various code snippets that I frequently reuse across projects. Here's my envisioned plan to achieve this automation:</p>\n<p><strong>Data Collection and Preprocessing:</strong></p>\n<p>Organize repository code files.\nExtract comment-code pairs.</p>\n<p><strong>Data Representation:</strong></p>\n<p>Transform comments and code snippets into machine-readable formats, perhaps involving tokenization and word embeddings.</p>\n<p><strong>What I've Tried:</strong></p>\n<p>My project is in its preliminary stages, and I haven't started implementation yet. While I possess familiarity on the required machine learning and deep learning concepts, I'm seeking guidance on the specific implementation steps.</p>\n<p><strong>Future Consideration:</strong></p>\n<p>I have to implement this tool at a company level in the future. In that context, the repositories would contain proprietary code, making it crucial to retain data privacy. Given this, I intend to develop my own model, as I want to avoid sending code outside the organization. Ensuring the security of our proprietary code is of utmost importance.</p>\n<p><strong>Questions:</strong></p>\n<p>(1)How can I identify the most suitable machine learning model for text-based code suggestion?</p>\n<p>(2)Are there recommended libraries or tools for efficient data preprocessing and representation?</p>\n<p>(3)What strategies can I employ to verify the generated code's syntactic correctness and suitability for integration into projects?</p>\n<p>Additionally, I'm intrigued by the concept of GitHub Copilot and its capabilities. I'm interested in creating a similar solution for my proprietary data, focusing on data privacy. Any insights would be greatly appreciated.</p>\n"}, "title": "Suggestions regarding developing a Code snippets Suggestion tool", "content": "I am looking to create a program that can automatically suggest code snippets based on comments I provide. These comments often indicate the need for reusable code sections from my repository, saving valuable time and effort. My goal is to input a comment, and have the program recommend relevant code snippets from my repository, streamlining my workflow.\nDetails:\nI've established a repository containing C code files housing various code snippets that I frequently reuse across projects. Here's my envisioned plan to achieve this automation:\nData Collection and Preprocessing:\nOrganize repository code files.\nExtract comment-code pairs.\nData Representation:\nTransform comments and code snippets into machine-readable formats, perhaps involving tokenization and word embeddings.\nWhat I've Tried:\nMy project is in its preliminary stages, and I haven't started implementation yet. While I possess familiarity on the required machine learning and deep learning concepts, I'm seeking guidance on the specific implementation steps.\nFuture Consideration:\nI have to implement this tool at a company level in the future. In that context, the repositories would contain proprietary code, making it crucial to retain data privacy. Given this, I intend to develop my own model, as I want to avoid sending code outside the organization. Ensuring the security of our proprietary code is of utmost importance.\nQuestions:\n(1)How can I identify the most suitable machine learning model for text-based code suggestion?\n(2)Are there recommended libraries or tools for efficient data preprocessing and representation?\n(3)What strategies can I employ to verify the generated code's syntactic correctness and suitability for integration into projects?\nAdditionally, I'm intrigued by the concept of GitHub Copilot and its capabilities. I'm interested in creating a similar solution for my proprietary data, focusing on data privacy. Any insights would be greatly appreciated.\n", "question_id": 389, "answers": []}}