{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97835a07-6994-4081-b490-ebc0c977d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"LD_LIBRARY_PATH\"] = f\"{os.environ['LD_LIBRARY_PATH']}:/cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.7/lib64\"\n",
    "# !export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.7/lib64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91bdcc51-16e5-4981-8852-486a64713079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/w/weissmar/.conda/envs/a100l/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import guidance\n",
    "import transformers\n",
    "import bitsandbytes\n",
    "from torch import cuda, bfloat16\n",
    "from bazaar.schema import Quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9136ab89-e904-4b84-b1f1-80f1fafb8235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/w/weissmar/.conda/envs/a100l/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:12<00:00,  4.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "hf_auth = \"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\"\n",
    "# model_id = 'meta-llama/Llama-2-70b-chat-hf'\n",
    "model_id = '/Tmp/slurm.3479725.0/hf_home/hub/models--meta-llama--Llama-2-70b-chat-hf/snapshots/36d9a7388cc80e5f4b3e9701ca2f250d21a96c30/'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "# model.save_model('/save_path/')\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "746daba2-cd0c-4fbd-a230-1b6eb222fd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/w/weissmar/.conda/envs/a100l/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(name, use_auth_token=hf_auth)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id    # for open-ended generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa86adf-97b0-44c8-bb61-d0daf701f565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "generation_pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",    # finds GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702125b1-94cb-42f8-b986-00b8430f2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "program_string = \"\"\"\n",
    "    {{#system~}}\n",
    "    You are a Question Answering Agent operating inside an information market. You will be given a question, and a bunch of passages that might have an answer to that question in them. \n",
    "\n",
    "    But beware that each passage has a cost. You want to minimize the amount you spend, while maximizing the quality of your answer. You will now be presented with several options, and you will be asked how much you would want to pay for those passages, conditioned on your balance and the average price over all presented passages. \n",
    "    {{~/system}}\n",
    "    \n",
    "    {{#user~}}\n",
    "    The question is \"{{question}}?\"\n",
    "    \n",
    "    Here are your options.\n",
    "    ---{{#each options}}\n",
    "    Option {{add @index 1}}: {{this.answer_block}}\n",
    "    {{/each}}---\n",
    "    \n",
    "    Please discuss each option briefly in the context of the question that is asked. Lay out the argument for buying vs. passing. \n",
    "\n",
    "    After you're done laying out the arguments, you will consider that your balance is ${{balance}} and the average price of a passage is $20.0. Please respond with how much you would be willing to pay to buy each passage, conditioned on the question. The schema for this is: \n",
    "    \n",
    "    OPTION 1: <minimum price you would be willing to pay> - <maximum price you would be willing to pay>\n",
    "    OPTION 2: <minimum price you would be willing to pay> - <maximum price you would be willing to pay>\n",
    "    ... (and so on)\n",
    "    \n",
    "    Let's go.\n",
    "    {{~/user}}\n",
    "    \n",
    "    {{#assistant~}}\n",
    "    {{gen \"answer\" temperature=0.0}}\n",
    "    {{~/assistant}}\n",
    "    \"\"\"\n",
    "    program_string = clean_program_string(program_string)\n",
    "    # Run the program\n",
    "    program = guidance(program_string, llm=guidance.llms.OpenAI(model_name))  # noqa\n",
    "    program_output = program(\n",
    "        question=question,\n",
    "        options=options,\n",
    "        balance=budget,\n",
    "        average_quote_price=average_quote_price,\n",
    "    )\n",
    "    answer = program_output[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56ba6528-48fa-4f3c-9ba9-da469c6973c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: Bobby William and Michael Burry work for a firm specializing in information acquisition. They seek answers by buying data from an information marketplace where vendors sell insights.\n",
      "\n",
      "Bobby wants to do a really good job at answering the question. \n",
      "\n",
      "Michael is thrifty and financially responsible. Michael wants to make sure ensures that they don't waste money buying unnecessary or duplicate information. \n",
      "\n",
      "USER: The question is \"How can LLMs be used for planning?\"\n",
      "\n",
      "Here are your options.\n",
      "---\n",
      "Option 1: More specifically, we adopt hierarchical planning models (e.g., [38, 33]), which consist of a high-level planner and a low-level planner. We use LLMs to generate high-level plans (HLPs), i.e., a sequence of subgoals (e.g., [Navigation potato, Pickup potato, Navigation microwave, ...]) that the agent needs to achieve, in the specified order, to accomplish the final goal specified by the language instruction. The lowlevel planner then maps each subgoal into a sequence of primitive actions for achieving that subgoal in the current environment and state. An important observation is that, given a high-level plan, low-level planning becomes conditionally independent of the natural language instruction. It becomes the classic object localization and navigation problem [6] (for navigation subgoals) or simply executing the specified interaction action with the right objects (for interaction subgoals). The low-level planner can be trained with data synthesized from the simulator (see, e.g., [26, 3]).\n",
      "\n",
      "Option 2: Classical planning methods have been widely adopted in robots and embodied environments [9, 42, 8, 61, 26]. Recently, prompting LLMs to do planning direcly has gained attention and shown potential [24, 23, 53, 13, 35]. SayCan [1], for instance, combines LLMs with affordance functions to generate feasible plans. Moreover, based on LLMs' powerful programming ability [37, 29, 36], some recent works first translate natural language instructions into the executable programming languages, such as Planning Domain Description Language (PDDL), and runs classical planning algorithms, such as LLM+P [36]. However, codebased planning is constrained by its narrow domains and the predefined environment, while RAP can handle open domain problems, including numerical and logical reasoning (see Section 4.2 and 4.3).\n",
      "\n",
      "Option 3: Robots and embodied environments have extensively utilized classical planning methods [9, 42, 8, 61, 26]. Lately, there's been a growing interest in using LLMs for direct planning, which has demonstrated promise [24, 23, 53, 13, 35]. As an example, SayCan [1] merges LLMs with affordance functions to produce viable plans. Leveraging the robust programming capabilities of LLMs [37, 29, 36], some recent studies convert natural language directives into executable programming languages like PDDL, then implement traditional planning algorithms, such as LLM+P [36]. Nonetheless, planning based on code is limited to specific domains and set environments, whereas RAP can address open domain challenges, encompassing both numerical and logical reasoning (refer to Section 4.2 and 4.3).\n",
      "\n",
      "---\n",
      "\n",
      "Option 1 costs $10.\n",
      "Option 2 costs $30.\n",
      "Option 3 costs $40.\n",
      "\n",
      "Simulate a constructive argument between Bobby and Michael. Together, they must decide which options to buy and which ones to not buy.\n",
      "\n",
      "Note that Bobby and Michael may choose to buy any number of options, or none at all. At the end of the argument, they must arrive at a verdict. This verdict must be printed as: \n",
      "\n",
      "VERDICT:\n",
      "\n",
      "Option 1: <Buy or Pass>\n",
      "Option 2: <Buy or Pass>\n",
      "Option 3: <Buy or Pass>\n",
      "\n",
      "For example, if Bobby and Michael decide to buy Option 1 and Option 3, and pass on Option 2, the verdict would be:\n",
      "\n",
      "VERDICT:\n",
      "\n",
      "Option 1: Buy\n",
      "Option 2: Pass\n",
      "Option 3: Buy\n",
      "\n",
      "---\n",
      "\n",
      "Bobby: \"Hey Michael, we need to decide which options to buy for the question 'How can LLMs be used for planning?'\"\n",
      "\n",
      "Michael: \"Yeah, I know. Let's take a look at the options.\"\n",
      "\n",
      "Bobby: \"Okay, Option 1 looks pretty good. It talks about using LLMs to generate high-level plans and then using a low-level planner to map each subgoal into a sequence of primitive actions.\"\n",
      "\n",
      "Michael: \"Yeah, that sounds good. But have you seen the price tag? It's $10!\"\n",
      "\n",
      "Bobby: \"I know, but think about how useful it could be for our work. We could use it to plan out our tasks more efficiently.\"\n",
      "\n",
      "Michael: \"I understand that, but we need to be mindful of our budget. We can't just go around spending money without making sure it's worth it.\"\n",
      "\n",
      "Bobby: \"I agree, but in this case, I think it's worth the investment. We'll be able to answer the question more thoroughly and provide better information to our clients.\"\n",
      "\n",
      "Michael: \"Okay, fine. Let's buy Option 1. But we need to be more careful with the other options.\"\n",
      "\n",
      "Bobby: \"Agreed. Let's take a look at Option 2.\"\n",
      "\n",
      "Michael: \"This one looks interesting. It talks about using LLMs to generate plans directly, and it mentions some recent works that have shown potential.\"\n",
      "\n",
      "Bobby: \"Yeah, that's true. But it's also pretty expensive, at $30.\"\n",
      "\n",
      "Michael: \"I know, but think about the potential benefits. If we can use LLMs to generate plans directly, we could save a lot of time and resources.\"\n",
      "\n",
      "Bobby: \"That's true. But we need to make sure it's worth the investment. Do you think we can get enough use out of it to justify the cost?\"\n",
      "\n",
      "Michael: \"I'm not sure. Let's pass on Option 2 for now and move on to Option 3.\"\n",
      "\n",
      "Bobby: \"Okay. Option 3 looks pretty similar to Option 2, but it's a bit more expensive at $40.\"\n",
      "\n",
      "Michael: \"Yeah, I see that. But it also mentions some recent studies that have demonstrated promise. Maybe it's worth the investment.\"\n",
      "\n",
      "Bobby: \"I'm not sure. I think we should pass on Option 3 as well.\"\n",
      "\n",
      "Michael: \"Agreed. Let's summarize our verdict.\"\n",
      "\n",
      "VERDICT:\n",
      "\n",
      "Option 1: Buy\n",
      "Option 2: Pass\n",
      "Option 3: Pass\n",
      "\n",
      "Bobby: \"Alright, it's settled then. Let's purchase Option 1 and move forward with our work.\"\n",
      "\n",
      "Michael: \"Sounds good to me. Let's get started.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who is George Washington?\"\n",
    "text = f\"\"\"SYSTEM: Bobby William and Michael Burry work for a firm specializing in information acquisition. They seek answers by buying data from an information marketplace where vendors sell insights.\n",
    "\n",
    "Bobby wants to do a really good job at answering the question. \n",
    "\n",
    "Michael is thrifty and financially responsible. Michael wants to make sure ensures that they don't waste money buying unnecessary or duplicate information. \n",
    "\n",
    "USER: The question is \"How can LLMs be used for planning?\"\n",
    "\n",
    "Here are your options.\n",
    "---\n",
    "Option 1: More specifically, we adopt hierarchical planning models (e.g., [38, 33]), which consist of a high-level planner and a low-level planner. We use LLMs to generate high-level plans (HLPs), i.e., a sequence of subgoals (e.g., [Navigation potato, Pickup potato, Navigation microwave, ...]) that the agent needs to achieve, in the specified order, to accomplish the final goal specified by the language instruction. The lowlevel planner then maps each subgoal into a sequence of primitive actions for achieving that subgoal in the current environment and state. An important observation is that, given a high-level plan, low-level planning becomes conditionally independent of the natural language instruction. It becomes the classic object localization and navigation problem [6] (for navigation subgoals) or simply executing the specified interaction action with the right objects (for interaction subgoals). The low-level planner can be trained with data synthesized from the simulator (see, e.g., [26, 3]).\n",
    "\n",
    "Option 2: Classical planning methods have been widely adopted in robots and embodied environments [9, 42, 8, 61, 26]. Recently, prompting LLMs to do planning direcly has gained attention and shown potential [24, 23, 53, 13, 35]. SayCan [1], for instance, combines LLMs with affordance functions to generate feasible plans. Moreover, based on LLMs' powerful programming ability [37, 29, 36], some recent works first translate natural language instructions into the executable programming languages, such as Planning Domain Description Language (PDDL), and runs classical planning algorithms, such as LLM+P [36]. However, codebased planning is constrained by its narrow domains and the predefined environment, while RAP can handle open domain problems, including numerical and logical reasoning (see Section 4.2 and 4.3).\n",
    "\n",
    "Option 3: Robots and embodied environments have extensively utilized classical planning methods [9, 42, 8, 61, 26]. Lately, there's been a growing interest in using LLMs for direct planning, which has demonstrated promise [24, 23, 53, 13, 35]. As an example, SayCan [1] merges LLMs with affordance functions to produce viable plans. Leveraging the robust programming capabilities of LLMs [37, 29, 36], some recent studies convert natural language directives into executable programming languages like PDDL, then implement traditional planning algorithms, such as LLM+P [36]. Nonetheless, planning based on code is limited to specific domains and set environments, whereas RAP can address open domain challenges, encompassing both numerical and logical reasoning (refer to Section 4.2 and 4.3).\n",
    "\n",
    "---\n",
    "\n",
    "Option 1 costs $10.\n",
    "Option 2 costs $30.\n",
    "Option 3 costs $40.\n",
    "\n",
    "Simulate a constructive argument between Bobby and Michael. Together, they must decide which options to buy and which ones to not buy.\n",
    "\n",
    "Note that Bobby and Michael may choose to buy any number of options, or none at all. At the end of the argument, they must arrive at a verdict. This verdict must be printed as: \n",
    "\n",
    "VERDICT:\n",
    "\n",
    "Option 1: <Buy or Pass>\n",
    "Option 2: <Buy or Pass>\n",
    "Option 3: <Buy or Pass>\n",
    "---\n",
    "\"\"\"    \n",
    "\n",
    "sequences = generation_pipe(\n",
    "    text,\n",
    "    max_length=2048,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    temperature=0.4,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(sequences[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f40b4f0c-d135-4e16-97c2-049d63321fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance.llm = guidance.llms.Transformers(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cb5f563-c3e1-4bc9-b33d-77459d40378a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"guidance-stop-button-6f0386e2-8556-4ff6-89f5-a814c7997909\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"guidance-content-6f0386e2-8556-4ff6-89f5-a814c7997909\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#system~}}</span>\n",
       "You are an intelligent AI assistant. You will be given a question. Your task is to answer it to the best of your ability. \n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{~/system}}</span>\n",
       "\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#user~}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{question}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{~/user}}</span>\n",
       "\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#assistant~}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{gen &quot;answer&quot; temperature=0.0 max_tokens=512}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{~/assistant}}</span>\n",
       " </pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"guidance_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"guidance-content-\"+t),this.stop_button=document.getElementById(\"guidance-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._guidanceDisplay=function(t,e){return new d(t,e)}})()})();; window._guidanceDisplay(\"6f0386e2-8556-4ff6-89f5-a814c7997909\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{#system~}}\n",
      "You are an intelligent AI assistant. You will be given a question. Your task is to answer it to the best of your ability. \n",
      "{{~/system}}\n",
      "\n",
      "{{#user~}}\n",
      "{{question}}\n",
      "{{~/user}}\n",
      "\n",
      "{{#assistant~}}\n",
      "{{gen \"answer\" temperature=0.0 max_tokens=512}}\n",
      "{{~/assistant}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bazaar.lem_utils import clean_program_string\n",
    "def get_closed_book_answer(question: str, model_name=\"gpt-3.5-turbo\") -> str:\n",
    "    program_string = \"\"\"\n",
    "    {{#system~}}\n",
    "    You are an intelligent AI assistant. You will be given a question. Your task is to answer it to the best of your ability. \n",
    "    {{~/system}}\n",
    "    \n",
    "    {{#user~}}\n",
    "    {{question}}\n",
    "    {{~/user}}\n",
    "    \n",
    "    {{#assistant~}}\n",
    "    {{gen \"answer\" temperature=0.0 max_tokens=512}}\n",
    "    {{~/assistant}}\n",
    "    \"\"\"\n",
    "    program_string = clean_program_string(program_string)\n",
    "    # Run the program\n",
    "    program = guidance(program_string, llm=guidance.llms.Transformers(model=model, tokenizer=tokenizer))  # noqa\n",
    "    program_output = program(question=question)\n",
    "    print(program_output)\n",
    "    # answer = program_output[\"answer\"]\n",
    "    # Done\n",
    "    # return answer\n",
    "\n",
    "get_closed_book_answer(question=\"how much wood cood a woodchuck chuck?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3043d8f8-c26a-44fc-8276-c3002c1adcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"guidance-stop-button-ad1cc704-495a-4ffe-9bed-7185c3f389b5\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"guidance-content-ad1cc704-495a-4ffe-9bed-7185c3f389b5\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{gen &#x27;henlo&#x27; max_tokens=32}}</span></pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"guidance_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"guidance-content-\"+t),this.stop_button=document.getElementById(\"guidance-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._guidanceDisplay=function(t,e){return new d(t,e)}})()})();; window._guidanceDisplay(\"ad1cc704-495a-4ffe-9bed-7185c3f389b5\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "program = guidance(\"{{gen 'henlo' max_tokens=32}}\", caching=False)\n",
    "output = program()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "001f7e73-1faf-48f7-ae41-816abfe4fae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm': <guidance.llms._transformers.Transformers at 0x7fad202783a0>,\n",
       " 'logging': False,\n",
       " '@raw_prefix': \"{{!--GMARKER_START_gen$&#123;&#123;gen 'henlo' max_tokens=32&#125;&#125;$--}}\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48a073-854c-4d3f-b1c9-c9667f93139e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8443d43-b0dc-4f64-9589-b8ac11e76bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = guidance.llms.Transformers(model=model, tokenizer=tokenizer, caching=False)\n",
    "llama_cash = guidance.llms.Transformers(model=model, tokenizer=tokenizer, caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "205a22b8-7e2c-423a-b190-6c6fe0be0c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we can pre-define valid option sets\n",
    "valid_weapons = [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"]\n",
    "\n",
    "# define the prompt\n",
    "character_maker = guidance(\"\"\"The following is a character profile for an RPG game in JSON format.\n",
    "```json\n",
    "{\n",
    "    \"id\": \"{{id}}\",\n",
    "    \"description\": \"{{description stop=','}}\",\n",
    "    \"name\": \"{{gen 'name' stop=','}}\",\n",
    "    \"age\": {{gen 'age' pattern='[0-9]+' stop=','}},\n",
    "    \"armor\": \"{{#select 'armor'}}leather{{or}}chainmail{{or}}plate{{/select}}\",\n",
    "}```\"\"\")\n",
    "\n",
    "# generate a character\n",
    "character_maker(\n",
    "    id=\"e1f491f7-7ab8-4dac-8c20-c92b5e7d883d\",\n",
    "    description=\"A quick and nimble fighter.\",\n",
    "    valid_weapons=valid_weapons, llm=llama_cash\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5deb081-60c4-462c-9ff6-81fdb948d4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a100l",
   "language": "python",
   "name": "a100l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
